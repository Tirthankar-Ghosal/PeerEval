{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of OUR_Review_wise_summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCGdYeJjs2V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "with open('/content/dic.pickle', 'rb') as handle:\n",
        "    dic = pickle.load(handle)\n",
        "df= pd.DataFrame.from_dict(dic,orient='index',columns=['real_summary','reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Wgzn3SVMDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ZH1PfRVNF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document= train['reviews']\n",
        "summary= train['real_summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbxhyl_zFlWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z55AhpKIdK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "24537268-174f-4e8e-e360-1f303016f516"
      },
      "source": [
        "document[30], summary[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015). My main concern for this paper is that the description of the Visual Concepts is completely unclear for me. The paper proposes a method for few-shot learning using a new image representation called visual concept embedding.',\n",
              " 'The paper builds on earlier work by Wang et al (2015) on Visual Concepts (VCs) and explores the use of VCs for few-shot learning setting for novel classes.\\n\\nThe work, as pointed out by two reviewers is somewhat incremental in nature, with main novelty being the demonstration of utilities of VCs for few shot learning. This would not have been a big limitation if the paper had a carefully conducted empirical evaluation providing insights on the effect of various configuration settings/hyperparameters on the performance in few shot learning, which two of the reviewers (Anon3, Anon2) state are missing. The paper falls short of the acceptance threshold in its current form.\\n\\nPS: The authors posted a github link to the code on Jan 12 which may potentially compromise the anonymity of the submission (though it was after all the reviews were already in) https://openreview.net/forum?id=BJ_QxP1AZ&noteId=BJaIDpBEM\\n')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6LE4MrJjC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0889071d-b32d-4c53-9f6a-f0ff75120a50"
      },
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "By0ANxbRW    <go> Proposed network compression method offer...\n",
              "BkCV_W-AZ    <go> The reviewers agree this is a really inte...\n",
              "H1Y8hhg0b    <go> The results in the paper are interesting,...\n",
              "BkVsWbbAW    <go> Thank you for submitting you paper to ICL...\n",
              "HyXBcYg0b    <go> The authors make an experimental study of...\n",
              "Name: real_summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqbpEyPMRqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHw2csoYImsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU9Xu7OKVab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ESm-aYR-tvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVyErXAei5_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9482542-5845-4642-e19d-b40639a2701d"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10, 7, 6, 507]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryx9qx90jwXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d360cbf-d678-47ed-a1e4-d51a5ae69dee"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['architecture with for et']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoizyBvLKv8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ba0ad68-0319-4d73-87df-997a828eea47"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2700, 3013)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr",
        "colab_type": "text"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4o2nGdK5Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXZlO99C-UXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "10460ef1-d5c4-4e95-fc3a-3e08ae6fc58a"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     250.000000\n",
              "mean      407.688000\n",
              "std       113.429058\n",
              "min       187.000000\n",
              "25%       326.000000\n",
              "50%       389.000000\n",
              "75%       479.500000\n",
              "max      1002.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMwKMx--ZF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "862907c7-965f-4fdc-97b9-80824221cf63"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     250.000000\n",
              "mean      550.724000\n",
              "std       340.260779\n",
              "min        71.000000\n",
              "25%       309.250000\n",
              "50%       482.500000\n",
              "75%       693.500000\n",
              "max      2312.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVeMilXr-bpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D",
        "colab_type": "text"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyUBeu7ACRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm",
        "colab_type": "text"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzO6l3-AB7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZ5f4P4DurS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "#BATCH_SIZE = 64\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-fV7eABWN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Purv7oyhETDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40J2pc2NEXp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN1wVQAdMVYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmjAPLWuMREE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7",
        "colab_type": "text"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_B6M9OBNBKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuFrdXnNZEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-qoKuTNwKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNuoJoFWO335",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVmvs6dPMRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrbnTwijQJ-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmeqkZrIRbSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh",
        "colab_type": "text"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHRG-o4R9Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMTZJdIoSbuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj",
        "colab_type": "text"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiynCLlTL8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY",
        "colab_type": "text"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1-943kTXXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKwyvKtTvF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4LA_45T4Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze0u6xxXT7dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO",
        "colab_type": "text"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-RcxqFUCuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk",
        "colab_type": "text"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZxHuyZxU5Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d",
        "colab_type": "text"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOc1_3c-VGaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c",
        "colab_type": "text"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVOMzkrczgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORKpv69dSW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "843415e3-bb71-4272-c752-0a47d921628f"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 18 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.4983\n",
            "Epoch 1 Loss 7.5009\n",
            "Time taken for 1 epoch: 2.9488730430603027 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.4304\n",
            "Epoch 2 Loss 7.4277\n",
            "Time taken for 1 epoch: 2.865457773208618 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 7.3972\n",
            "Epoch 3 Loss 7.3455\n",
            "Time taken for 1 epoch: 2.8492867946624756 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 7.2770\n",
            "Epoch 4 Loss 7.2568\n",
            "Time taken for 1 epoch: 2.8262152671813965 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 7.2447\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-2\n",
            "Epoch 5 Loss 7.1590\n",
            "Time taken for 1 epoch: 3.098968029022217 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 7.1524\n",
            "Epoch 6 Loss 7.0579\n",
            "Time taken for 1 epoch: 2.8199408054351807 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 6.9775\n",
            "Epoch 7 Loss 6.9551\n",
            "Time taken for 1 epoch: 2.8271260261535645 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 6.9115\n",
            "Epoch 8 Loss 6.8509\n",
            "Time taken for 1 epoch: 2.8274171352386475 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 6.8205\n",
            "Epoch 9 Loss 6.7488\n",
            "Time taken for 1 epoch: 2.8242228031158447 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 6.6404\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-3\n",
            "Epoch 10 Loss 6.6446\n",
            "Time taken for 1 epoch: 3.0942671298980713 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 6.5500\n",
            "Epoch 11 Loss 6.5459\n",
            "Time taken for 1 epoch: 2.82563853263855 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 6.4602\n",
            "Epoch 12 Loss 6.4644\n",
            "Time taken for 1 epoch: 2.8735995292663574 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 6.3520\n",
            "Epoch 13 Loss 6.3768\n",
            "Time taken for 1 epoch: 2.845050573348999 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 6.3130\n",
            "Epoch 14 Loss 6.3101\n",
            "Time taken for 1 epoch: 2.824145793914795 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 6.2510\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-4\n",
            "Epoch 15 Loss 6.2478\n",
            "Time taken for 1 epoch: 3.1099064350128174 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 6.1301\n",
            "Epoch 16 Loss 6.1921\n",
            "Time taken for 1 epoch: 2.831294298171997 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 6.1356\n",
            "Epoch 17 Loss 6.1319\n",
            "Time taken for 1 epoch: 2.8121345043182373 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 6.0974\n",
            "Epoch 18 Loss 6.0828\n",
            "Time taken for 1 epoch: 2.8171443939208984 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 6.0568\n",
            "Epoch 19 Loss 6.0338\n",
            "Time taken for 1 epoch: 2.82098126411438 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 5.9010\n",
            "Saving checkpoint for epoch 20 at checkpoints/ckpt-5\n",
            "Epoch 20 Loss 5.9859\n",
            "Time taken for 1 epoch: 3.0868427753448486 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 5.9801\n",
            "Epoch 21 Loss 5.9359\n",
            "Time taken for 1 epoch: 2.8181283473968506 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 5.9391\n",
            "Epoch 22 Loss 5.8759\n",
            "Time taken for 1 epoch: 2.8772127628326416 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 5.8948\n",
            "Epoch 23 Loss 5.8198\n",
            "Time taken for 1 epoch: 2.847602605819702 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 5.9285\n",
            "Epoch 24 Loss 5.7607\n",
            "Time taken for 1 epoch: 2.8187015056610107 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 5.6980\n",
            "Saving checkpoint for epoch 25 at checkpoints/ckpt-6\n",
            "Epoch 25 Loss 5.6944\n",
            "Time taken for 1 epoch: 3.1098079681396484 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 5.6207\n",
            "Epoch 26 Loss 5.6242\n",
            "Time taken for 1 epoch: 2.824298143386841 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 5.6291\n",
            "Epoch 27 Loss 5.5646\n",
            "Time taken for 1 epoch: 2.8373842239379883 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 5.4435\n",
            "Epoch 28 Loss 5.4861\n",
            "Time taken for 1 epoch: 2.823209524154663 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 5.4143\n",
            "Epoch 29 Loss 5.4170\n",
            "Time taken for 1 epoch: 2.8266310691833496 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 5.1732\n",
            "Saving checkpoint for epoch 30 at checkpoints/ckpt-7\n",
            "Epoch 30 Loss 5.3463\n",
            "Time taken for 1 epoch: 3.080294370651245 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 5.2438\n",
            "Epoch 31 Loss 5.2822\n",
            "Time taken for 1 epoch: 2.8336920738220215 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 5.2787\n",
            "Epoch 32 Loss 5.2055\n",
            "Time taken for 1 epoch: 2.8839824199676514 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 5.1049\n",
            "Epoch 33 Loss 5.1234\n",
            "Time taken for 1 epoch: 2.84330153465271 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 4.9537\n",
            "Epoch 34 Loss 5.0598\n",
            "Time taken for 1 epoch: 2.838711738586426 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 4.9115\n",
            "Saving checkpoint for epoch 35 at checkpoints/ckpt-8\n",
            "Epoch 35 Loss 4.9919\n",
            "Time taken for 1 epoch: 3.09067702293396 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 5.0208\n",
            "Epoch 36 Loss 4.9167\n",
            "Time taken for 1 epoch: 2.8228907585144043 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 4.9381\n",
            "Epoch 37 Loss 4.8512\n",
            "Time taken for 1 epoch: 2.8273303508758545 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 4.8794\n",
            "Epoch 38 Loss 4.7812\n",
            "Time taken for 1 epoch: 2.8376903533935547 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 4.7702\n",
            "Epoch 39 Loss 4.7149\n",
            "Time taken for 1 epoch: 2.8182177543640137 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 4.6243\n",
            "Saving checkpoint for epoch 40 at checkpoints/ckpt-9\n",
            "Epoch 40 Loss 4.6490\n",
            "Time taken for 1 epoch: 3.105656147003174 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 4.6249\n",
            "Epoch 41 Loss 4.5817\n",
            "Time taken for 1 epoch: 2.8501431941986084 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 4.5019\n",
            "Epoch 42 Loss 4.5149\n",
            "Time taken for 1 epoch: 2.8797237873077393 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 4.4947\n",
            "Epoch 43 Loss 4.4482\n",
            "Time taken for 1 epoch: 2.829052209854126 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 4.5056\n",
            "Epoch 44 Loss 4.3799\n",
            "Time taken for 1 epoch: 2.812952995300293 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 4.2083\n",
            "Saving checkpoint for epoch 45 at checkpoints/ckpt-10\n",
            "Epoch 45 Loss 4.3041\n",
            "Time taken for 1 epoch: 3.103200912475586 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 4.1812\n",
            "Epoch 46 Loss 4.2523\n",
            "Time taken for 1 epoch: 2.8208811283111572 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 4.0867\n",
            "Epoch 47 Loss 4.1806\n",
            "Time taken for 1 epoch: 2.8285624980926514 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 4.3072\n",
            "Epoch 48 Loss 4.1122\n",
            "Time taken for 1 epoch: 2.8282573223114014 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 3.9005\n",
            "Epoch 49 Loss 4.0344\n",
            "Time taken for 1 epoch: 2.8382081985473633 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 3.8583\n",
            "Saving checkpoint for epoch 50 at checkpoints/ckpt-11\n",
            "Epoch 50 Loss 3.9786\n",
            "Time taken for 1 epoch: 3.1375577449798584 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 3.8738\n",
            "Epoch 51 Loss 3.9036\n",
            "Time taken for 1 epoch: 2.868952751159668 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 3.8996\n",
            "Epoch 52 Loss 3.8291\n",
            "Time taken for 1 epoch: 2.8624942302703857 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 3.8278\n",
            "Epoch 53 Loss 3.7827\n",
            "Time taken for 1 epoch: 2.828458309173584 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 3.7607\n",
            "Epoch 54 Loss 3.7076\n",
            "Time taken for 1 epoch: 2.825143575668335 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 3.6147\n",
            "Saving checkpoint for epoch 55 at checkpoints/ckpt-12\n",
            "Epoch 55 Loss 3.6261\n",
            "Time taken for 1 epoch: 3.2297048568725586 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 3.6312\n",
            "Epoch 56 Loss 3.5534\n",
            "Time taken for 1 epoch: 2.8253962993621826 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 3.4401\n",
            "Epoch 57 Loss 3.4888\n",
            "Time taken for 1 epoch: 2.8229925632476807 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 3.4143\n",
            "Epoch 58 Loss 3.4196\n",
            "Time taken for 1 epoch: 2.8228306770324707 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 3.4166\n",
            "Epoch 59 Loss 3.3468\n",
            "Time taken for 1 epoch: 2.8224446773529053 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 3.3284\n",
            "Saving checkpoint for epoch 60 at checkpoints/ckpt-13\n",
            "Epoch 60 Loss 3.2865\n",
            "Time taken for 1 epoch: 3.0939438343048096 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 3.3412\n",
            "Epoch 61 Loss 3.2178\n",
            "Time taken for 1 epoch: 2.8702452182769775 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 3.2177\n",
            "Epoch 62 Loss 3.1437\n",
            "Time taken for 1 epoch: 2.8418045043945312 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 3.1432\n",
            "Epoch 63 Loss 3.0657\n",
            "Time taken for 1 epoch: 2.819495439529419 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 2.9346\n",
            "Epoch 64 Loss 2.9954\n",
            "Time taken for 1 epoch: 2.8234269618988037 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 2.9339\n",
            "Saving checkpoint for epoch 65 at checkpoints/ckpt-14\n",
            "Epoch 65 Loss 2.9326\n",
            "Time taken for 1 epoch: 3.087991237640381 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 2.7122\n",
            "Epoch 66 Loss 2.8524\n",
            "Time taken for 1 epoch: 2.827150583267212 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 2.8162\n",
            "Epoch 67 Loss 2.7755\n",
            "Time taken for 1 epoch: 2.828144073486328 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 2.6942\n",
            "Epoch 68 Loss 2.7000\n",
            "Time taken for 1 epoch: 2.8143184185028076 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 2.6691\n",
            "Epoch 69 Loss 2.6349\n",
            "Time taken for 1 epoch: 2.835496425628662 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 2.5546\n",
            "Saving checkpoint for epoch 70 at checkpoints/ckpt-15\n",
            "Epoch 70 Loss 2.5598\n",
            "Time taken for 1 epoch: 3.089757204055786 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 2.3963\n",
            "Epoch 71 Loss 2.4978\n",
            "Time taken for 1 epoch: 2.8782389163970947 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 2.3470\n",
            "Epoch 72 Loss 2.4112\n",
            "Time taken for 1 epoch: 2.8420064449310303 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 2.3814\n",
            "Epoch 73 Loss 2.3433\n",
            "Time taken for 1 epoch: 2.829394578933716 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 2.2561\n",
            "Epoch 74 Loss 2.2641\n",
            "Time taken for 1 epoch: 2.8223936557769775 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 2.2027\n",
            "Saving checkpoint for epoch 75 at checkpoints/ckpt-16\n",
            "Epoch 75 Loss 2.2107\n",
            "Time taken for 1 epoch: 3.1031506061553955 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 2.1250\n",
            "Epoch 76 Loss 2.1292\n",
            "Time taken for 1 epoch: 2.8190183639526367 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 2.0744\n",
            "Epoch 77 Loss 2.0574\n",
            "Time taken for 1 epoch: 2.832418918609619 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 1.9559\n",
            "Epoch 78 Loss 2.0002\n",
            "Time taken for 1 epoch: 2.823286294937134 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 1.9312\n",
            "Epoch 79 Loss 1.9196\n",
            "Time taken for 1 epoch: 2.817729949951172 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 1.8894\n",
            "Saving checkpoint for epoch 80 at checkpoints/ckpt-17\n",
            "Epoch 80 Loss 1.8403\n",
            "Time taken for 1 epoch: 3.1061158180236816 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 1.7622\n",
            "Epoch 81 Loss 1.7613\n",
            "Time taken for 1 epoch: 2.8737785816192627 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 1.7777\n",
            "Epoch 82 Loss 1.7028\n",
            "Time taken for 1 epoch: 2.8322038650512695 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 1.6602\n",
            "Epoch 83 Loss 1.6432\n",
            "Time taken for 1 epoch: 2.824986696243286 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 1.5508\n",
            "Epoch 84 Loss 1.5762\n",
            "Time taken for 1 epoch: 2.8266959190368652 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 1.4300\n",
            "Saving checkpoint for epoch 85 at checkpoints/ckpt-18\n",
            "Epoch 85 Loss 1.5042\n",
            "Time taken for 1 epoch: 3.08929705619812 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 1.5179\n",
            "Epoch 86 Loss 1.4502\n",
            "Time taken for 1 epoch: 2.8348495960235596 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 1.3746\n",
            "Epoch 87 Loss 1.3911\n",
            "Time taken for 1 epoch: 2.823859214782715 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 1.2875\n",
            "Epoch 88 Loss 1.3119\n",
            "Time taken for 1 epoch: 2.8176732063293457 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 1.2449\n",
            "Epoch 89 Loss 1.2633\n",
            "Time taken for 1 epoch: 2.826282501220703 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 1.1738\n",
            "Saving checkpoint for epoch 90 at checkpoints/ckpt-19\n",
            "Epoch 90 Loss 1.1904\n",
            "Time taken for 1 epoch: 3.1026885509490967 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 1.1741\n",
            "Epoch 91 Loss 1.1286\n",
            "Time taken for 1 epoch: 2.876389503479004 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 1.0504\n",
            "Epoch 92 Loss 1.0792\n",
            "Time taken for 1 epoch: 2.8246240615844727 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 1.0184\n",
            "Epoch 93 Loss 1.0424\n",
            "Time taken for 1 epoch: 2.8290488719940186 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.9761\n",
            "Epoch 94 Loss 0.9762\n",
            "Time taken for 1 epoch: 2.820133924484253 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.9383\n",
            "Saving checkpoint for epoch 95 at checkpoints/ckpt-20\n",
            "Epoch 95 Loss 0.9232\n",
            "Time taken for 1 epoch: 3.096304416656494 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.8063\n",
            "Epoch 96 Loss 0.8611\n",
            "Time taken for 1 epoch: 2.819455862045288 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.8296\n",
            "Epoch 97 Loss 0.8174\n",
            "Time taken for 1 epoch: 2.8250865936279297 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.7471\n",
            "Epoch 98 Loss 0.7620\n",
            "Time taken for 1 epoch: 2.820401668548584 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.7136\n",
            "Epoch 99 Loss 0.7202\n",
            "Time taken for 1 epoch: 2.841074228286743 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.6605\n",
            "Saving checkpoint for epoch 100 at checkpoints/ckpt-21\n",
            "Epoch 100 Loss 0.6779\n",
            "Time taken for 1 epoch: 3.1132843494415283 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.6776\n",
            "Epoch 101 Loss 0.6310\n",
            "Time taken for 1 epoch: 2.859647274017334 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.6039\n",
            "Epoch 102 Loss 0.6058\n",
            "Time taken for 1 epoch: 2.8437652587890625 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.5838\n",
            "Epoch 103 Loss 0.5829\n",
            "Time taken for 1 epoch: 2.8239848613739014 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.5353\n",
            "Epoch 104 Loss 0.5410\n",
            "Time taken for 1 epoch: 2.821536064147949 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.4862\n",
            "Saving checkpoint for epoch 105 at checkpoints/ckpt-22\n",
            "Epoch 105 Loss 0.5033\n",
            "Time taken for 1 epoch: 3.0852348804473877 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.4232\n",
            "Epoch 106 Loss 0.4527\n",
            "Time taken for 1 epoch: 2.821917772293091 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.3950\n",
            "Epoch 107 Loss 0.4384\n",
            "Time taken for 1 epoch: 2.8283321857452393 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.4143\n",
            "Epoch 108 Loss 0.4061\n",
            "Time taken for 1 epoch: 2.8154470920562744 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.3939\n",
            "Epoch 109 Loss 0.3933\n",
            "Time taken for 1 epoch: 2.8331048488616943 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.3599\n",
            "Saving checkpoint for epoch 110 at checkpoints/ckpt-23\n",
            "Epoch 110 Loss 0.3831\n",
            "Time taken for 1 epoch: 3.1206958293914795 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.3822\n",
            "Epoch 111 Loss 0.3552\n",
            "Time taken for 1 epoch: 2.8529775142669678 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.3287\n",
            "Epoch 112 Loss 0.3340\n",
            "Time taken for 1 epoch: 2.833831787109375 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.3116\n",
            "Epoch 113 Loss 0.3189\n",
            "Time taken for 1 epoch: 2.831782579421997 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.3047\n",
            "Epoch 114 Loss 0.3065\n",
            "Time taken for 1 epoch: 2.8336102962493896 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.2719\n",
            "Saving checkpoint for epoch 115 at checkpoints/ckpt-24\n",
            "Epoch 115 Loss 0.2729\n",
            "Time taken for 1 epoch: 3.10127592086792 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.2643\n",
            "Epoch 116 Loss 0.2722\n",
            "Time taken for 1 epoch: 2.8259642124176025 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.2560\n",
            "Epoch 117 Loss 0.2568\n",
            "Time taken for 1 epoch: 2.826385021209717 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.2034\n",
            "Epoch 118 Loss 0.2448\n",
            "Time taken for 1 epoch: 2.825791597366333 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.2068\n",
            "Epoch 119 Loss 0.2311\n",
            "Time taken for 1 epoch: 2.8196849822998047 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.2031\n",
            "Saving checkpoint for epoch 120 at checkpoints/ckpt-25\n",
            "Epoch 120 Loss 0.2251\n",
            "Time taken for 1 epoch: 3.142925500869751 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.2050\n",
            "Epoch 121 Loss 0.2160\n",
            "Time taken for 1 epoch: 2.8512134552001953 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.1961\n",
            "Epoch 122 Loss 0.2016\n",
            "Time taken for 1 epoch: 2.8218228816986084 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.1828\n",
            "Epoch 123 Loss 0.1967\n",
            "Time taken for 1 epoch: 2.828050136566162 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1761\n",
            "Epoch 124 Loss 0.1930\n",
            "Time taken for 1 epoch: 2.824495792388916 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.1453\n",
            "Saving checkpoint for epoch 125 at checkpoints/ckpt-26\n",
            "Epoch 125 Loss 0.1750\n",
            "Time taken for 1 epoch: 3.1156668663024902 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.1651\n",
            "Epoch 126 Loss 0.1759\n",
            "Time taken for 1 epoch: 2.820446491241455 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.1619\n",
            "Epoch 127 Loss 0.1628\n",
            "Time taken for 1 epoch: 2.819690465927124 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.1542\n",
            "Epoch 128 Loss 0.1600\n",
            "Time taken for 1 epoch: 2.8167481422424316 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.1668\n",
            "Epoch 129 Loss 0.1676\n",
            "Time taken for 1 epoch: 2.8308322429656982 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.1301\n",
            "Saving checkpoint for epoch 130 at checkpoints/ckpt-27\n",
            "Epoch 130 Loss 0.1583\n",
            "Time taken for 1 epoch: 3.151643753051758 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.1508\n",
            "Epoch 131 Loss 0.1601\n",
            "Time taken for 1 epoch: 2.8297979831695557 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.1617\n",
            "Epoch 132 Loss 0.1567\n",
            "Time taken for 1 epoch: 2.8231401443481445 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.1264\n",
            "Epoch 133 Loss 0.1462\n",
            "Time taken for 1 epoch: 2.829437255859375 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.1401\n",
            "Epoch 134 Loss 0.1293\n",
            "Time taken for 1 epoch: 2.8306305408477783 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.1047\n",
            "Saving checkpoint for epoch 135 at checkpoints/ckpt-28\n",
            "Epoch 135 Loss 0.1328\n",
            "Time taken for 1 epoch: 3.0971474647521973 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.1118\n",
            "Epoch 136 Loss 0.1300\n",
            "Time taken for 1 epoch: 2.8350958824157715 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.1221\n",
            "Epoch 137 Loss 0.1278\n",
            "Time taken for 1 epoch: 2.823310136795044 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0952\n",
            "Epoch 138 Loss 0.1302\n",
            "Time taken for 1 epoch: 2.830176591873169 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.1068\n",
            "Epoch 139 Loss 0.1273\n",
            "Time taken for 1 epoch: 2.822901487350464 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.1197\n",
            "Saving checkpoint for epoch 140 at checkpoints/ckpt-29\n",
            "Epoch 140 Loss 0.1222\n",
            "Time taken for 1 epoch: 3.1425821781158447 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0921\n",
            "Epoch 141 Loss 0.1190\n",
            "Time taken for 1 epoch: 2.826514959335327 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0931\n",
            "Epoch 142 Loss 0.1153\n",
            "Time taken for 1 epoch: 2.823974370956421 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.1093\n",
            "Epoch 143 Loss 0.1212\n",
            "Time taken for 1 epoch: 2.8375751972198486 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.1022\n",
            "Epoch 144 Loss 0.1254\n",
            "Time taken for 1 epoch: 2.8402740955352783 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.1101\n",
            "Saving checkpoint for epoch 145 at checkpoints/ckpt-30\n",
            "Epoch 145 Loss 0.1233\n",
            "Time taken for 1 epoch: 3.0858099460601807 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.1107\n",
            "Epoch 146 Loss 0.1184\n",
            "Time taken for 1 epoch: 2.8423943519592285 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0937\n",
            "Epoch 147 Loss 0.1120\n",
            "Time taken for 1 epoch: 2.838582754135132 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0865\n",
            "Epoch 148 Loss 0.1074\n",
            "Time taken for 1 epoch: 2.8223109245300293 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0998\n",
            "Epoch 149 Loss 0.1027\n",
            "Time taken for 1 epoch: 2.8438730239868164 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0938\n",
            "Saving checkpoint for epoch 150 at checkpoints/ckpt-31\n",
            "Epoch 150 Loss 0.0982\n",
            "Time taken for 1 epoch: 3.1581671237945557 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0811\n",
            "Epoch 151 Loss 0.0908\n",
            "Time taken for 1 epoch: 2.8374972343444824 secs\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.0925\n",
            "Epoch 152 Loss 0.0910\n",
            "Time taken for 1 epoch: 2.825131416320801 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0843\n",
            "Epoch 153 Loss 0.0939\n",
            "Time taken for 1 epoch: 2.820578098297119 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.1123\n",
            "Epoch 154 Loss 0.1091\n",
            "Time taken for 1 epoch: 2.834472179412842 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.1161\n",
            "Saving checkpoint for epoch 155 at checkpoints/ckpt-32\n",
            "Epoch 155 Loss 0.1128\n",
            "Time taken for 1 epoch: 3.1064276695251465 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.1346\n",
            "Epoch 156 Loss 0.1070\n",
            "Time taken for 1 epoch: 2.8272221088409424 secs\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.1173\n",
            "Epoch 157 Loss 0.1013\n",
            "Time taken for 1 epoch: 2.8284943103790283 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.0821\n",
            "Epoch 158 Loss 0.0927\n",
            "Time taken for 1 epoch: 2.8496129512786865 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.0666\n",
            "Epoch 159 Loss 0.0956\n",
            "Time taken for 1 epoch: 2.865013837814331 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.0838\n",
            "Saving checkpoint for epoch 160 at checkpoints/ckpt-33\n",
            "Epoch 160 Loss 0.0970\n",
            "Time taken for 1 epoch: 3.2025837898254395 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0725\n",
            "Epoch 161 Loss 0.0915\n",
            "Time taken for 1 epoch: 2.8429811000823975 secs\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.0763\n",
            "Epoch 162 Loss 0.0892\n",
            "Time taken for 1 epoch: 2.8242390155792236 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0712\n",
            "Epoch 163 Loss 0.0820\n",
            "Time taken for 1 epoch: 2.8227224349975586 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.1158\n",
            "Epoch 164 Loss 0.0866\n",
            "Time taken for 1 epoch: 2.823914051055908 secs\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0800\n",
            "Saving checkpoint for epoch 165 at checkpoints/ckpt-34\n",
            "Epoch 165 Loss 0.0878\n",
            "Time taken for 1 epoch: 3.0815141201019287 secs\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.0942\n",
            "Epoch 166 Loss 0.0890\n",
            "Time taken for 1 epoch: 2.822760581970215 secs\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0866\n",
            "Epoch 167 Loss 0.0832\n",
            "Time taken for 1 epoch: 2.8282456398010254 secs\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.0626\n",
            "Epoch 168 Loss 0.0875\n",
            "Time taken for 1 epoch: 2.81807279586792 secs\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0962\n",
            "Epoch 169 Loss 0.0927\n",
            "Time taken for 1 epoch: 2.8552660942077637 secs\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.0660\n",
            "Saving checkpoint for epoch 170 at checkpoints/ckpt-35\n",
            "Epoch 170 Loss 0.0922\n",
            "Time taken for 1 epoch: 3.1313259601593018 secs\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.0954\n",
            "Epoch 171 Loss 0.0907\n",
            "Time taken for 1 epoch: 2.827526569366455 secs\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.0609\n",
            "Epoch 172 Loss 0.0842\n",
            "Time taken for 1 epoch: 2.8332033157348633 secs\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0796\n",
            "Epoch 173 Loss 0.0829\n",
            "Time taken for 1 epoch: 2.813556671142578 secs\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.0867\n",
            "Epoch 174 Loss 0.0779\n",
            "Time taken for 1 epoch: 2.8233327865600586 secs\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.1164\n",
            "Saving checkpoint for epoch 175 at checkpoints/ckpt-36\n",
            "Epoch 175 Loss 0.0957\n",
            "Time taken for 1 epoch: 3.0903475284576416 secs\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.0785\n",
            "Epoch 176 Loss 0.0922\n",
            "Time taken for 1 epoch: 2.828852653503418 secs\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0640\n",
            "Epoch 177 Loss 0.0965\n",
            "Time taken for 1 epoch: 2.827709674835205 secs\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0734\n",
            "Epoch 178 Loss 0.0856\n",
            "Time taken for 1 epoch: 2.834139823913574 secs\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0606\n",
            "Epoch 179 Loss 0.0866\n",
            "Time taken for 1 epoch: 2.8762474060058594 secs\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.0776\n",
            "Saving checkpoint for epoch 180 at checkpoints/ckpt-37\n",
            "Epoch 180 Loss 0.0852\n",
            "Time taken for 1 epoch: 3.1275126934051514 secs\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0980\n",
            "Epoch 181 Loss 0.0825\n",
            "Time taken for 1 epoch: 2.830638885498047 secs\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0657\n",
            "Epoch 182 Loss 0.0744\n",
            "Time taken for 1 epoch: 2.8360729217529297 secs\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0692\n",
            "Epoch 183 Loss 0.0714\n",
            "Time taken for 1 epoch: 2.8171188831329346 secs\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0747\n",
            "Epoch 184 Loss 0.0738\n",
            "Time taken for 1 epoch: 2.831847906112671 secs\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0790\n",
            "Saving checkpoint for epoch 185 at checkpoints/ckpt-38\n",
            "Epoch 185 Loss 0.0732\n",
            "Time taken for 1 epoch: 3.0965707302093506 secs\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0678\n",
            "Epoch 186 Loss 0.0805\n",
            "Time taken for 1 epoch: 2.8156471252441406 secs\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0670\n",
            "Epoch 187 Loss 0.0883\n",
            "Time taken for 1 epoch: 2.8160860538482666 secs\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0602\n",
            "Epoch 188 Loss 0.0864\n",
            "Time taken for 1 epoch: 2.8262131214141846 secs\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0687\n",
            "Epoch 189 Loss 0.0873\n",
            "Time taken for 1 epoch: 2.8720321655273438 secs\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0709\n",
            "Saving checkpoint for epoch 190 at checkpoints/ckpt-39\n",
            "Epoch 190 Loss 0.0912\n",
            "Time taken for 1 epoch: 3.1302390098571777 secs\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0655\n",
            "Epoch 191 Loss 0.0814\n",
            "Time taken for 1 epoch: 2.833688259124756 secs\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0751\n",
            "Epoch 192 Loss 0.0776\n",
            "Time taken for 1 epoch: 2.820955514907837 secs\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0668\n",
            "Epoch 193 Loss 0.0679\n",
            "Time taken for 1 epoch: 2.831512928009033 secs\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0553\n",
            "Epoch 194 Loss 0.0665\n",
            "Time taken for 1 epoch: 2.812236785888672 secs\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0768\n",
            "Saving checkpoint for epoch 195 at checkpoints/ckpt-40\n",
            "Epoch 195 Loss 0.0686\n",
            "Time taken for 1 epoch: 3.082528829574585 secs\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0564\n",
            "Epoch 196 Loss 0.0787\n",
            "Time taken for 1 epoch: 2.8178882598876953 secs\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0492\n",
            "Epoch 197 Loss 0.0759\n",
            "Time taken for 1 epoch: 2.8324737548828125 secs\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0724\n",
            "Epoch 198 Loss 0.0724\n",
            "Time taken for 1 epoch: 2.85648775100708 secs\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0648\n",
            "Epoch 199 Loss 0.0742\n",
            "Time taken for 1 epoch: 2.8664586544036865 secs\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0755\n",
            "Saving checkpoint for epoch 200 at checkpoints/ckpt-41\n",
            "Epoch 200 Loss 0.0790\n",
            "Time taken for 1 epoch: 3.0956835746765137 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5D5cv2Jd8-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpdiW6wnmiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZoEHvIxrYKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3b3821be-27d1-48be-93ba-66611ac27f58"
      },
      "source": [
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'the paper studies factorizations of convolutional kernels the proposed kernels lead to theoretical and practical efficiency improvements but these improvements are very very limited for instance figure 5 it remains unclear how they compare to popular alternative approaches such as well done but the reviewers were made by the paper needs improvement is made it does not it is presented'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JADQ5_WOXPiZ",
        "colab_type": "text"
      },
      "source": [
        "NOW MAKING MAJOR SUMMERIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHr7t5eY5PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews= test['reviews'].tolist()\n",
        "auto_summary=[]\n",
        "for review in reviews:\n",
        "  text= summarize(review)\n",
        "  auto_summary.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFQnVkgZneO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eee852b9-bc60-45a9-da6c-778a73fb5d37"
      },
      "source": [
        "test['auto_summary']=auto_summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCIUyoVWDgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a6217ddb-5e2e-4d20-d617-32dae24d1ed0"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>real_summary</th>\n",
              "      <th>reviews</th>\n",
              "      <th>auto_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HkPCrEZ0Z</th>\n",
              "      <td>The paper has some potentially interesting ide...</td>\n",
              "      <td>The main idea of the paper is to improve off-p...</td>\n",
              "      <td>the paper studies factorizations of convolutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hy8hkYeRb</th>\n",
              "      <td>The paper attempts to develop a method for lea...</td>\n",
              "      <td>Quality\\n\\nThe authors introduce a deep networ...</td>\n",
              "      <td>this paper adapts nachum et al 2017 to continu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyMvJrdaW</th>\n",
              "      <td>This paper proposes a warp operator based on...</td>\n",
              "      <td>Motivated via Talor approximation of the Resid...</td>\n",
              "      <td>this paper presents a novel and interesting sk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hk2MHt-3</th>\n",
              "      <td>The paper studies end-to-end training of a mul...</td>\n",
              "      <td>This paper presents a deep network architectur...</td>\n",
              "      <td>this paper is intersting but has a few flaws t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B1QRgziT</th>\n",
              "      <td>This paper presents impressive results on scal...</td>\n",
              "      <td>The paper is motivated by the fact that in GAN...</td>\n",
              "      <td>this paper provides an important problem visua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HkjL6MiTb</th>\n",
              "      <td>Reviewers unanimous in assessment that manuscr...</td>\n",
              "      <td>The authors tackle the problem of estimating r...</td>\n",
              "      <td>the authors propose to use identity some weigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BJgPCveAW</th>\n",
              "      <td>The paper received weak scores: 4,4,5. R2 comp...</td>\n",
              "      <td>The paper seems to claims that\\n1) certain Con...</td>\n",
              "      <td>the paper studies factorizations of convolutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B1X4DWWRb</th>\n",
              "      <td>The submission provides an interesting way to ...</td>\n",
              "      <td>The paper proposes a novel way of causal infer...</td>\n",
              "      <td>the paper proposes a method for interpreting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B1KFAGWAZ</th>\n",
              "      <td>The authors present a centralized neural contr...</td>\n",
              "      <td>This paper investigates multiagent reinforceme...</td>\n",
              "      <td>the paper proposes a method for learning conne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SysEexbRb</th>\n",
              "      <td>I recommend acceptance based on the positive r...</td>\n",
              "      <td>This paper studies the critical points of shal...</td>\n",
              "      <td>the paper studies factorizations of convolutio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                real_summary  ...                                       auto_summary\n",
              "HkPCrEZ0Z  The paper has some potentially interesting ide...  ...  the paper studies factorizations of convolutio...\n",
              "Hy8hkYeRb  The paper attempts to develop a method for lea...  ...  this paper adapts nachum et al 2017 to continu...\n",
              "SyMvJrdaW  This paper proposes a warp operator based on...  ...  this paper presents a novel and interesting sk...\n",
              "Hk2MHt-3   The paper studies end-to-end training of a mul...  ...  this paper is intersting but has a few flaws t...\n",
              "B1QRgziT   This paper presents impressive results on scal...  ...  this paper provides an important problem visua...\n",
              "...                                                      ...  ...                                                ...\n",
              "HkjL6MiTb  Reviewers unanimous in assessment that manuscr...  ...  the authors propose to use identity some weigh...\n",
              "BJgPCveAW  The paper received weak scores: 4,4,5. R2 comp...  ...  the paper studies factorizations of convolutio...\n",
              "B1X4DWWRb  The submission provides an interesting way to ...  ...  the paper proposes a method for interpreting t...\n",
              "B1KFAGWAZ  The authors present a centralized neural contr...  ...  the paper proposes a method for learning conne...\n",
              "SysEexbRb  I recommend acceptance based on the positive r...  ...  the paper studies factorizations of convolutio...\n",
              "\n",
              "[63 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35MTN2frqJHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_excel(\"Neural_AUTO_Vs_MATA_onsame.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0zTPBOrS4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_summary= test['real_summary'].tolist()\n",
        "auto_summary= test['auto_summary'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ufo3meKwRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('ref.txt', 'w') as f:\n",
        "    for item in real_summary:\n",
        "      if(len(item)==0):\n",
        "        print(\"yes\")\n",
        "      else:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open('hyp.txt', 'w') as f:\n",
        "    for item in auto_summary:\n",
        "      if(len(item)==0):\n",
        "        print(\"yes\")\n",
        "      else:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhH6GnNQrTyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "def dump(hyp,ref,filename):\n",
        "  # Data to be written \n",
        "  dictionary ={ \n",
        "      \"hyp\" : hyp, \n",
        "      \"ref\" : ref\n",
        "  } \n",
        "    \n",
        "  with open(filename, \"a\") as outfile: \n",
        "      json.dump(dictionary, outfile) \n",
        "      outfile.write(',\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7qUR_9nrY3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(real_summary)):\n",
        "    dump(auto_summary[i],real_summary[i],\"Neural_AUTO_Vs_META_onsame.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXiW7Irh-d3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}