{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OUR_Review_wise_summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b9phL2qVFtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "327dbc89-9184-4699-d5a6-bbb1804715ed"
      },
      "source": [
        "import pre as pre\n",
        "from operator import add \n",
        "import numpy as np\n",
        "\n",
        "label=pre.create_label(1)\n",
        "corpus=pre.create_sentence_list(1)\n",
        "fileid= pre.fileid\n",
        "label_new= list (map (lambda x : x[0],label))\n",
        "print(label_new)\n",
        "\n",
        "fileidVSsentence={}\n",
        "\n",
        "for i in range(0,len(corpus)):\n",
        "  if (fileid[i] in fileidVSsentence.keys()):\n",
        "    fileidVSsentence[fileid[i]][0] = \" .\".join((fileidVSsentence[fileid[i]][0], corpus[i]))\n",
        "    if (label_new[i] == 'MAJ'):\n",
        "      if (len(fileidVSsentence[fileid[i]][1])<2):\n",
        "        fileidVSsentence[fileid[i]][1] = \" \".join((fileidVSsentence[fileid[i]][1], corpus[i]))\n",
        "      else:\n",
        "        fileidVSsentence[fileid[i]][1] = \". \".join((fileidVSsentence[fileid[i]][1], corpus[i]))\n",
        "\n",
        "  else:\n",
        "    fileidVSsentence[fileid[i]]= [corpus[i]]\n",
        "    fileidVSsentence[fileid[i]].append(\" \")\n",
        "    if (label_new[i] == 'MAJ'):\n",
        "        fileidVSsentence[fileid[i]][1] = \" \".join((fileidVSsentence[fileid[i]][1], corpus[i]))\n",
        "\n",
        "ids = fileidVSsentence.keys()\n",
        "reviews = list ( map(lambda x:fileidVSsentence[x][0],ids))\n",
        "real_summary = list ( map(lambda x:fileidVSsentence[x][1],ids))\n",
        "\n",
        "from pandas import DataFrame\n",
        "df = DataFrame(zip(ids,reviews,real_summary),columns = ['ids','reviews','real_summary'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'CNT', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'CNT', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'CNT', 'CNT', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'CNT', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'CNT', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'CNT', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'CNT', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'CNT', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'CNT', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'CNT', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'CNT', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'CNT', 'CNT', 'CNT', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'CNT', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'CNT', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'CNT', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'CNT', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'CNT', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'CNT', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'CNT', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'CNT', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'GEN', 'MIN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MIN', 'CNT', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MAJ', 'MAJ', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'GEN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'GEN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'GEN', 'GEN', 'GEN', 'MAJ', 'MIN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'GEN', 'GEN', 'MAJ', 'MAJ', 'MIN', 'MIN', 'GEN', 'GEN', 'GEN', 'MAJ', 'GEN', 'MAJ', 'MIN', 'MIN', 'MAJ', 'MIN', 'MIN', 'MIN', 'GEN', 'MAJ', 'MAJ', 'MAJ', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MIN', 'MAJ', 'MAJ', 'MAJ', 'MIN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Wgzn3SVMDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ZH1PfRVNF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document= train['reviews']\n",
        "summary= train['real_summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbxhyl_zFlWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z55AhpKIdK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "44bd0e5b-7de9-42a0-ae5e-c4484ee83140"
      },
      "source": [
        "document[30], summary[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"this paper presents advantage-based regret minimization somewhat similar to advantage actor-critic with reinforc .the main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature and without markov assumpt .the claim that arm is more robust to partially observable domains is supported by experiments where it outperforms dqn .there are several things to like about this paper- the authors do a good job of reviewing/referencing several papers in the field of regret minimization that would probably be of interest to the iclr community + provide non-obvious connections / summaries of these perspect .- the issue of partial observability is good to bring up rather than simply relying on the mdp framework that is often taken as a given in deep reinforcement learn .- the experimental results show that arm outperforms dqn on a suite of deep rl task .however there are also some negatives- reviewing so much of the cfr-literature in a short paper means that it ends up feeling a little rushed and confus .- the ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as ac trpo etc .at a high enough level these algorithms can be written the same way there are undoubtedly some key differences in how they behave but it's not spelled out to the reader and i think the connections can be miss .- the experiment/motivation i found most compelling was  since it clearly matches the issue of partial observ .but we only see results compared to dqn  it feels like you don't put a compelling case for the non-markovian benefits of arm vs other policy gradient method .yes ac and trpo seem like they perform very poorly compared to arm .but i'm left wondering how/whi .i feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a pap .a lot of the cited literature was also new to me so it could be that i'm missing something about why this is so interest .however i came away from this paper quite uncertain about the real benefits/differences of arm versus other similar policy gradient method .i also didn't feel the experimental evaluations drove a clear message except arm did better than all other methods on these experi .i'd want to understand how/why and whether we should expect this univers .the focus on regret minimization perspectives didn't really get me too excit .overall i would vote against acceptance for this vers\",\n",
              " \"  the claim that arm is more robust to partially observable domains is supported by experiments where it outperforms dqn. there are several things to like about this paper- the authors do a good job of reviewing/referencing several papers in the field of regret minimization that would probably be of interest to the iclr community + provide non-obvious connections / summaries of these perspect. - the issue of partial observability is good to bring up rather than simply relying on the mdp framework that is often taken as a given in deep reinforcement learn. - the experimental results show that arm outperforms dqn on a suite of deep rl task. however there are also some negatives- reviewing so much of the cfr-literature in a short paper means that it ends up feeling a little rushed and confus. - the experiment/motivation i found most compelling was  since it clearly matches the issue of partial observ. but we only see results compared to dqn  it feels like you don't put a compelling case for the non-markovian benefits of arm vs other policy gradient method. yes ac and trpo seem like they perform very poorly compared to arm. but i'm left wondering how/whi. i feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a pap. i also didn't feel the experimental evaluations drove a clear message except arm did better than all other methods on these experi. the focus on regret minimization perspectives didn't really get me too excit. overall i would vote against acceptance for this vers\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6LE4MrJjC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c0f9f35a-c0dc-4a50-b52f-92ca92c5810e"
      },
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      <go>   overall the results are a bit mix <stop>\n",
              "1    <go>   the paper presents an interesting idea ...\n",
              "2    <go>   while it is true that some image-indepe...\n",
              "3    <go>   the paper presents some conceptually in...\n",
              "4    <go>   the writing is clear concise and easy t...\n",
              "Name: real_summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqbpEyPMRqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHw2csoYImsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU9Xu7OKVab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ESm-aYR-tvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVyErXAei5_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6971192-d1ca-48b5-92d8-4af7f401b156"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[9, 3, 7, 177]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryx9qx90jwXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6173e82b-0c0b-4e34-eb7c-3a5b8b9970f5"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['being as not data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoizyBvLKv8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "782ab27a-9a6e-4240-c6e1-c38b000cbe5e"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12022, 6815)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr",
        "colab_type": "text"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4o2nGdK5Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXZlO99C-UXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "adb41ff2-8bad-4633-b56e-338ccec76c4d"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     801.00000\n",
              "mean     1897.97628\n",
              "std      1013.32399\n",
              "min       170.00000\n",
              "25%      1138.00000\n",
              "50%      1686.00000\n",
              "75%      2433.00000\n",
              "max      5725.00000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMwKMx--ZF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d1f85258-2e66-48c8-eba4-96ba528ea070"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     801.000000\n",
              "mean      697.186017\n",
              "std       589.820675\n",
              "min        13.000000\n",
              "25%       255.000000\n",
              "50%       561.000000\n",
              "75%       954.000000\n",
              "max      4006.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVeMilXr-bpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D",
        "colab_type": "text"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyUBeu7ACRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm",
        "colab_type": "text"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzO6l3-AB7hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZ5f4P4DurS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "#BATCH_SIZE = 64\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-fV7eABWN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Purv7oyhETDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40J2pc2NEXp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN1wVQAdMVYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmjAPLWuMREE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7",
        "colab_type": "text"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_B6M9OBNBKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuFrdXnNZEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-qoKuTNwKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNuoJoFWO335",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW",
        "colab_type": "text"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVmvs6dPMRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrbnTwijQJ-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmeqkZrIRbSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh",
        "colab_type": "text"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHRG-o4R9Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMTZJdIoSbuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj",
        "colab_type": "text"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiynCLlTL8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY",
        "colab_type": "text"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1-943kTXXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKwyvKtTvF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4LA_45T4Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze0u6xxXT7dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO",
        "colab_type": "text"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-RcxqFUCuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk",
        "colab_type": "text"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZxHuyZxU5Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d",
        "colab_type": "text"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOc1_3c-VGaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad8f8e07-8b9e-4577-84fb-f57690a6cbb6"
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c",
        "colab_type": "text"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVOMzkrczgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORKpv69dSW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d36520bb-f811-48d2-f92c-df2d6c9df6eb"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 25 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0232\n",
            "Epoch 1 Batch 25 Loss 0.0274\n",
            "Epoch 1 Batch 50 Loss 0.0271\n",
            "Epoch 1 Loss 0.0271\n",
            "Time taken for 1 epoch: 16.89800000190735 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0336\n",
            "Epoch 2 Batch 25 Loss 0.0252\n",
            "Epoch 2 Batch 50 Loss 0.0293\n",
            "Epoch 2 Loss 0.0293\n",
            "Time taken for 1 epoch: 3.781325578689575 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0133\n",
            "Epoch 3 Batch 25 Loss 0.0260\n",
            "Epoch 3 Batch 50 Loss 0.0261\n",
            "Epoch 3 Loss 0.0261\n",
            "Time taken for 1 epoch: 3.7565770149230957 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0247\n",
            "Epoch 4 Batch 25 Loss 0.0241\n",
            "Epoch 4 Batch 50 Loss 0.0239\n",
            "Epoch 4 Loss 0.0239\n",
            "Time taken for 1 epoch: 3.7684874534606934 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0279\n",
            "Epoch 5 Batch 25 Loss 0.0244\n",
            "Epoch 5 Batch 50 Loss 0.0239\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-69\n",
            "Epoch 5 Loss 0.0239\n",
            "Time taken for 1 epoch: 4.0214550495147705 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0087\n",
            "Epoch 6 Batch 25 Loss 0.0270\n",
            "Epoch 6 Batch 50 Loss 0.0259\n",
            "Epoch 6 Loss 0.0259\n",
            "Time taken for 1 epoch: 3.7867238521575928 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0272\n",
            "Epoch 7 Batch 25 Loss 0.0268\n",
            "Epoch 7 Batch 50 Loss 0.0273\n",
            "Epoch 7 Loss 0.0273\n",
            "Time taken for 1 epoch: 3.770102024078369 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0332\n",
            "Epoch 8 Batch 25 Loss 0.0285\n",
            "Epoch 8 Batch 50 Loss 0.0267\n",
            "Epoch 8 Loss 0.0267\n",
            "Time taken for 1 epoch: 3.767989158630371 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0289\n",
            "Epoch 9 Batch 25 Loss 0.0252\n",
            "Epoch 9 Batch 50 Loss 0.0261\n",
            "Epoch 9 Loss 0.0261\n",
            "Time taken for 1 epoch: 3.750826358795166 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0312\n",
            "Epoch 10 Batch 25 Loss 0.0261\n",
            "Epoch 10 Batch 50 Loss 0.0266\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-70\n",
            "Epoch 10 Loss 0.0266\n",
            "Time taken for 1 epoch: 3.9934804439544678 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0233\n",
            "Epoch 11 Batch 25 Loss 0.0260\n",
            "Epoch 11 Batch 50 Loss 0.0270\n",
            "Epoch 11 Loss 0.0270\n",
            "Time taken for 1 epoch: 3.7768661975860596 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0248\n",
            "Epoch 12 Batch 25 Loss 0.0245\n",
            "Epoch 12 Batch 50 Loss 0.0242\n",
            "Epoch 12 Loss 0.0242\n",
            "Time taken for 1 epoch: 3.757398843765259 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0235\n",
            "Epoch 13 Batch 25 Loss 0.0245\n",
            "Epoch 13 Batch 50 Loss 0.0266\n",
            "Epoch 13 Loss 0.0266\n",
            "Time taken for 1 epoch: 3.7578794956207275 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0201\n",
            "Epoch 14 Batch 25 Loss 0.0296\n",
            "Epoch 14 Batch 50 Loss 0.0295\n",
            "Epoch 14 Loss 0.0295\n",
            "Time taken for 1 epoch: 3.7433176040649414 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0304\n",
            "Epoch 15 Batch 25 Loss 0.0248\n",
            "Epoch 15 Batch 50 Loss 0.0336\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-71\n",
            "Epoch 15 Loss 0.0336\n",
            "Time taken for 1 epoch: 4.0141637325286865 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0247\n",
            "Epoch 16 Batch 25 Loss 0.0298\n",
            "Epoch 16 Batch 50 Loss 0.0283\n",
            "Epoch 16 Loss 0.0283\n",
            "Time taken for 1 epoch: 3.785459280014038 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0160\n",
            "Epoch 17 Batch 25 Loss 0.0222\n",
            "Epoch 17 Batch 50 Loss 0.0239\n",
            "Epoch 17 Loss 0.0239\n",
            "Time taken for 1 epoch: 3.763718605041504 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0245\n",
            "Epoch 18 Batch 25 Loss 0.0254\n",
            "Epoch 18 Batch 50 Loss 0.0255\n",
            "Epoch 18 Loss 0.0255\n",
            "Time taken for 1 epoch: 3.732797145843506 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0212\n",
            "Epoch 19 Batch 25 Loss 0.0204\n",
            "Epoch 19 Batch 50 Loss 0.0204\n",
            "Epoch 19 Loss 0.0204\n",
            "Time taken for 1 epoch: 3.721855401992798 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0346\n",
            "Epoch 20 Batch 25 Loss 0.0225\n",
            "Epoch 20 Batch 50 Loss 0.0221\n",
            "Saving checkpoint for epoch 20 at checkpoints/ckpt-72\n",
            "Epoch 20 Loss 0.0221\n",
            "Time taken for 1 epoch: 4.0346903800964355 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0158\n",
            "Epoch 21 Batch 25 Loss 0.0226\n",
            "Epoch 21 Batch 50 Loss 0.0232\n",
            "Epoch 21 Loss 0.0232\n",
            "Time taken for 1 epoch: 3.7632439136505127 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0210\n",
            "Epoch 22 Batch 25 Loss 0.0212\n",
            "Epoch 22 Batch 50 Loss 0.0231\n",
            "Epoch 22 Loss 0.0231\n",
            "Time taken for 1 epoch: 3.769923210144043 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0155\n",
            "Epoch 23 Batch 25 Loss 0.0219\n",
            "Epoch 23 Batch 50 Loss 0.0241\n",
            "Epoch 23 Loss 0.0241\n",
            "Time taken for 1 epoch: 3.729729413986206 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0380\n",
            "Epoch 24 Batch 25 Loss 0.0234\n",
            "Epoch 24 Batch 50 Loss 0.0239\n",
            "Epoch 24 Loss 0.0239\n",
            "Time taken for 1 epoch: 3.7512316703796387 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0331\n",
            "Epoch 25 Batch 25 Loss 0.0194\n",
            "Epoch 25 Batch 50 Loss 0.0219\n",
            "Saving checkpoint for epoch 25 at checkpoints/ckpt-73\n",
            "Epoch 25 Loss 0.0219\n",
            "Time taken for 1 epoch: 3.9793407917022705 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0177\n",
            "Epoch 26 Batch 25 Loss 0.0272\n",
            "Epoch 26 Batch 50 Loss 0.0247\n",
            "Epoch 26 Loss 0.0247\n",
            "Time taken for 1 epoch: 3.7632291316986084 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0241\n",
            "Epoch 27 Batch 25 Loss 0.0232\n",
            "Epoch 27 Batch 50 Loss 0.0218\n",
            "Epoch 27 Loss 0.0218\n",
            "Time taken for 1 epoch: 3.7562355995178223 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0171\n",
            "Epoch 28 Batch 25 Loss 0.0234\n",
            "Epoch 28 Batch 50 Loss 0.0231\n",
            "Epoch 28 Loss 0.0231\n",
            "Time taken for 1 epoch: 3.732517957687378 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0222\n",
            "Epoch 29 Batch 25 Loss 0.0238\n",
            "Epoch 29 Batch 50 Loss 0.0222\n",
            "Epoch 29 Loss 0.0222\n",
            "Time taken for 1 epoch: 3.778653621673584 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0139\n",
            "Epoch 30 Batch 25 Loss 0.0238\n",
            "Epoch 30 Batch 50 Loss 0.0233\n",
            "Saving checkpoint for epoch 30 at checkpoints/ckpt-74\n",
            "Epoch 30 Loss 0.0233\n",
            "Time taken for 1 epoch: 3.9650473594665527 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0145\n",
            "Epoch 31 Batch 25 Loss 0.0213\n",
            "Epoch 31 Batch 50 Loss 0.0233\n",
            "Epoch 31 Loss 0.0233\n",
            "Time taken for 1 epoch: 3.7129714488983154 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0135\n",
            "Epoch 32 Batch 25 Loss 0.0217\n",
            "Epoch 32 Batch 50 Loss 0.0228\n",
            "Epoch 32 Loss 0.0228\n",
            "Time taken for 1 epoch: 3.750718832015991 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0082\n",
            "Epoch 33 Batch 25 Loss 0.0261\n",
            "Epoch 33 Batch 50 Loss 0.0253\n",
            "Epoch 33 Loss 0.0253\n",
            "Time taken for 1 epoch: 3.7662887573242188 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0189\n",
            "Epoch 34 Batch 25 Loss 0.0229\n",
            "Epoch 34 Batch 50 Loss 0.0239\n",
            "Epoch 34 Loss 0.0239\n",
            "Time taken for 1 epoch: 3.757026195526123 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0220\n",
            "Epoch 35 Batch 25 Loss 0.0180\n",
            "Epoch 35 Batch 50 Loss 0.0214\n",
            "Saving checkpoint for epoch 35 at checkpoints/ckpt-75\n",
            "Epoch 35 Loss 0.0214\n",
            "Time taken for 1 epoch: 3.9976115226745605 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0133\n",
            "Epoch 36 Batch 25 Loss 0.0240\n",
            "Epoch 36 Batch 50 Loss 0.0250\n",
            "Epoch 36 Loss 0.0250\n",
            "Time taken for 1 epoch: 3.744366407394409 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0185\n",
            "Epoch 37 Batch 25 Loss 0.0263\n",
            "Epoch 37 Batch 50 Loss 0.0250\n",
            "Epoch 37 Loss 0.0250\n",
            "Time taken for 1 epoch: 3.7403557300567627 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0155\n",
            "Epoch 38 Batch 25 Loss 0.0218\n",
            "Epoch 38 Batch 50 Loss 0.0215\n",
            "Epoch 38 Loss 0.0215\n",
            "Time taken for 1 epoch: 3.7363357543945312 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0257\n",
            "Epoch 39 Batch 25 Loss 0.0225\n",
            "Epoch 39 Batch 50 Loss 0.0212\n",
            "Epoch 39 Loss 0.0212\n",
            "Time taken for 1 epoch: 3.7373766899108887 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0255\n",
            "Epoch 40 Batch 25 Loss 0.0193\n",
            "Epoch 40 Batch 50 Loss 0.0198\n",
            "Saving checkpoint for epoch 40 at checkpoints/ckpt-76\n",
            "Epoch 40 Loss 0.0198\n",
            "Time taken for 1 epoch: 3.9979543685913086 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0116\n",
            "Epoch 41 Batch 25 Loss 0.0219\n",
            "Epoch 41 Batch 50 Loss 0.0218\n",
            "Epoch 41 Loss 0.0218\n",
            "Time taken for 1 epoch: 3.733412265777588 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0181\n",
            "Epoch 42 Batch 25 Loss 0.0234\n",
            "Epoch 42 Batch 50 Loss 0.0223\n",
            "Epoch 42 Loss 0.0223\n",
            "Time taken for 1 epoch: 3.735161781311035 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0190\n",
            "Epoch 43 Batch 25 Loss 0.0230\n",
            "Epoch 43 Batch 50 Loss 0.0224\n",
            "Epoch 43 Loss 0.0224\n",
            "Time taken for 1 epoch: 3.7242672443389893 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0078\n",
            "Epoch 44 Batch 25 Loss 0.0213\n",
            "Epoch 44 Batch 50 Loss 0.0207\n",
            "Epoch 44 Loss 0.0207\n",
            "Time taken for 1 epoch: 3.7169392108917236 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0165\n",
            "Epoch 45 Batch 25 Loss 0.0191\n",
            "Epoch 45 Batch 50 Loss 0.0188\n",
            "Saving checkpoint for epoch 45 at checkpoints/ckpt-77\n",
            "Epoch 45 Loss 0.0188\n",
            "Time taken for 1 epoch: 3.96610689163208 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0115\n",
            "Epoch 46 Batch 25 Loss 0.0226\n",
            "Epoch 46 Batch 50 Loss 0.0224\n",
            "Epoch 46 Loss 0.0224\n",
            "Time taken for 1 epoch: 3.746066093444824 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0168\n",
            "Epoch 47 Batch 25 Loss 0.0244\n",
            "Epoch 47 Batch 50 Loss 0.0242\n",
            "Epoch 47 Loss 0.0242\n",
            "Time taken for 1 epoch: 3.726539134979248 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0156\n",
            "Epoch 48 Batch 25 Loss 0.0199\n",
            "Epoch 48 Batch 50 Loss 0.0194\n",
            "Epoch 48 Loss 0.0194\n",
            "Time taken for 1 epoch: 3.719144821166992 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0178\n",
            "Epoch 49 Batch 25 Loss 0.0201\n",
            "Epoch 49 Batch 50 Loss 0.0197\n",
            "Epoch 49 Loss 0.0197\n",
            "Time taken for 1 epoch: 3.724036693572998 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0191\n",
            "Epoch 50 Batch 25 Loss 0.0220\n",
            "Epoch 50 Batch 50 Loss 0.0211\n",
            "Saving checkpoint for epoch 50 at checkpoints/ckpt-78\n",
            "Epoch 50 Loss 0.0211\n",
            "Time taken for 1 epoch: 3.9789974689483643 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0096\n",
            "Epoch 51 Batch 25 Loss 0.0176\n",
            "Epoch 51 Batch 50 Loss 0.0197\n",
            "Epoch 51 Loss 0.0197\n",
            "Time taken for 1 epoch: 3.7416529655456543 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0107\n",
            "Epoch 52 Batch 25 Loss 0.0202\n",
            "Epoch 52 Batch 50 Loss 0.0200\n",
            "Epoch 52 Loss 0.0200\n",
            "Time taken for 1 epoch: 3.7118594646453857 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0144\n",
            "Epoch 53 Batch 25 Loss 0.0200\n",
            "Epoch 53 Batch 50 Loss 0.0204\n",
            "Epoch 53 Loss 0.0204\n",
            "Time taken for 1 epoch: 3.7002060413360596 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0066\n",
            "Epoch 54 Batch 25 Loss 0.0214\n",
            "Epoch 54 Batch 50 Loss 0.0215\n",
            "Epoch 54 Loss 0.0215\n",
            "Time taken for 1 epoch: 3.7118027210235596 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0137\n",
            "Epoch 55 Batch 25 Loss 0.0204\n",
            "Epoch 55 Batch 50 Loss 0.0227\n",
            "Saving checkpoint for epoch 55 at checkpoints/ckpt-79\n",
            "Epoch 55 Loss 0.0227\n",
            "Time taken for 1 epoch: 3.939309597015381 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0157\n",
            "Epoch 56 Batch 25 Loss 0.0236\n",
            "Epoch 56 Batch 50 Loss 0.0255\n",
            "Epoch 56 Loss 0.0255\n",
            "Time taken for 1 epoch: 3.7654786109924316 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0096\n",
            "Epoch 57 Batch 25 Loss 0.0254\n",
            "Epoch 57 Batch 50 Loss 0.0230\n",
            "Epoch 57 Loss 0.0230\n",
            "Time taken for 1 epoch: 3.7085750102996826 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0191\n",
            "Epoch 58 Batch 25 Loss 0.0188\n",
            "Epoch 58 Batch 50 Loss 0.0186\n",
            "Epoch 58 Loss 0.0186\n",
            "Time taken for 1 epoch: 3.7236411571502686 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0300\n",
            "Epoch 59 Batch 25 Loss 0.0183\n",
            "Epoch 59 Batch 50 Loss 0.0195\n",
            "Epoch 59 Loss 0.0195\n",
            "Time taken for 1 epoch: 3.7306723594665527 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0146\n",
            "Epoch 60 Batch 25 Loss 0.0208\n",
            "Epoch 60 Batch 50 Loss 0.0195\n",
            "Saving checkpoint for epoch 60 at checkpoints/ckpt-80\n",
            "Epoch 60 Loss 0.0195\n",
            "Time taken for 1 epoch: 3.957824230194092 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0154\n",
            "Epoch 61 Batch 25 Loss 0.0195\n",
            "Epoch 61 Batch 50 Loss 0.0189\n",
            "Epoch 61 Loss 0.0189\n",
            "Time taken for 1 epoch: 3.717712640762329 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0183\n",
            "Epoch 62 Batch 25 Loss 0.0213\n",
            "Epoch 62 Batch 50 Loss 0.0199\n",
            "Epoch 62 Loss 0.0199\n",
            "Time taken for 1 epoch: 3.742926597595215 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0153\n",
            "Epoch 63 Batch 25 Loss 0.0206\n",
            "Epoch 63 Batch 50 Loss 0.0209\n",
            "Epoch 63 Loss 0.0209\n",
            "Time taken for 1 epoch: 3.694190263748169 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0207\n",
            "Epoch 64 Batch 25 Loss 0.0203\n",
            "Epoch 64 Batch 50 Loss 0.0210\n",
            "Epoch 64 Loss 0.0210\n",
            "Time taken for 1 epoch: 3.7441928386688232 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0100\n",
            "Epoch 65 Batch 25 Loss 0.0174\n",
            "Epoch 65 Batch 50 Loss 0.0184\n",
            "Saving checkpoint for epoch 65 at checkpoints/ckpt-81\n",
            "Epoch 65 Loss 0.0184\n",
            "Time taken for 1 epoch: 3.9676096439361572 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0181\n",
            "Epoch 66 Batch 25 Loss 0.0157\n",
            "Epoch 66 Batch 50 Loss 0.0183\n",
            "Epoch 66 Loss 0.0183\n",
            "Time taken for 1 epoch: 3.76224422454834 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0138\n",
            "Epoch 67 Batch 25 Loss 0.0190\n",
            "Epoch 67 Batch 50 Loss 0.0200\n",
            "Epoch 67 Loss 0.0200\n",
            "Time taken for 1 epoch: 3.723606586456299 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0151\n",
            "Epoch 68 Batch 25 Loss 0.0185\n",
            "Epoch 68 Batch 50 Loss 0.0189\n",
            "Epoch 68 Loss 0.0189\n",
            "Time taken for 1 epoch: 3.7289042472839355 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0326\n",
            "Epoch 69 Batch 25 Loss 0.0196\n",
            "Epoch 69 Batch 50 Loss 0.0195\n",
            "Epoch 69 Loss 0.0195\n",
            "Time taken for 1 epoch: 3.7303829193115234 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0224\n",
            "Epoch 70 Batch 25 Loss 0.0167\n",
            "Epoch 70 Batch 50 Loss 0.0182\n",
            "Saving checkpoint for epoch 70 at checkpoints/ckpt-82\n",
            "Epoch 70 Loss 0.0182\n",
            "Time taken for 1 epoch: 3.949676513671875 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0168\n",
            "Epoch 71 Batch 25 Loss 0.0190\n",
            "Epoch 71 Batch 50 Loss 0.0238\n",
            "Epoch 71 Loss 0.0238\n",
            "Time taken for 1 epoch: 3.735762119293213 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0172\n",
            "Epoch 72 Batch 25 Loss 0.0208\n",
            "Epoch 72 Batch 50 Loss 0.0224\n",
            "Epoch 72 Loss 0.0224\n",
            "Time taken for 1 epoch: 3.720238447189331 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0233\n",
            "Epoch 73 Batch 25 Loss 0.0238\n",
            "Epoch 73 Batch 50 Loss 0.0230\n",
            "Epoch 73 Loss 0.0230\n",
            "Time taken for 1 epoch: 3.72751522064209 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0141\n",
            "Epoch 74 Batch 25 Loss 0.0204\n",
            "Epoch 74 Batch 50 Loss 0.0198\n",
            "Epoch 74 Loss 0.0198\n",
            "Time taken for 1 epoch: 3.717879295349121 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0117\n",
            "Epoch 75 Batch 25 Loss 0.0179\n",
            "Epoch 75 Batch 50 Loss 0.0185\n",
            "Saving checkpoint for epoch 75 at checkpoints/ckpt-83\n",
            "Epoch 75 Loss 0.0185\n",
            "Time taken for 1 epoch: 3.9769740104675293 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0147\n",
            "Epoch 76 Batch 25 Loss 0.0211\n",
            "Epoch 76 Batch 50 Loss 0.0195\n",
            "Epoch 76 Loss 0.0195\n",
            "Time taken for 1 epoch: 3.7217164039611816 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0147\n",
            "Epoch 77 Batch 25 Loss 0.0167\n",
            "Epoch 77 Batch 50 Loss 0.0177\n",
            "Epoch 77 Loss 0.0177\n",
            "Time taken for 1 epoch: 3.7197680473327637 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0063\n",
            "Epoch 78 Batch 25 Loss 0.0204\n",
            "Epoch 78 Batch 50 Loss 0.0200\n",
            "Epoch 78 Loss 0.0200\n",
            "Time taken for 1 epoch: 3.7109904289245605 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0212\n",
            "Epoch 79 Batch 25 Loss 0.0178\n",
            "Epoch 79 Batch 50 Loss 0.0182\n",
            "Epoch 79 Loss 0.0182\n",
            "Time taken for 1 epoch: 3.7039220333099365 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0142\n",
            "Epoch 80 Batch 25 Loss 0.0202\n",
            "Epoch 80 Batch 50 Loss 0.0206\n",
            "Saving checkpoint for epoch 80 at checkpoints/ckpt-84\n",
            "Epoch 80 Loss 0.0206\n",
            "Time taken for 1 epoch: 3.945098876953125 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0190\n",
            "Epoch 81 Batch 25 Loss 0.0226\n",
            "Epoch 81 Batch 50 Loss 0.0202\n",
            "Epoch 81 Loss 0.0202\n",
            "Time taken for 1 epoch: 3.7529830932617188 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0165\n",
            "Epoch 82 Batch 25 Loss 0.0193\n",
            "Epoch 82 Batch 50 Loss 0.0182\n",
            "Epoch 82 Loss 0.0182\n",
            "Time taken for 1 epoch: 3.7441701889038086 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0098\n",
            "Epoch 83 Batch 25 Loss 0.0167\n",
            "Epoch 83 Batch 50 Loss 0.0188\n",
            "Epoch 83 Loss 0.0188\n",
            "Time taken for 1 epoch: 3.7376508712768555 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0197\n",
            "Epoch 84 Batch 25 Loss 0.0184\n",
            "Epoch 84 Batch 50 Loss 0.0196\n",
            "Epoch 84 Loss 0.0196\n",
            "Time taken for 1 epoch: 3.7632968425750732 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0197\n",
            "Epoch 85 Batch 25 Loss 0.0180\n",
            "Epoch 85 Batch 50 Loss 0.0178\n",
            "Saving checkpoint for epoch 85 at checkpoints/ckpt-85\n",
            "Epoch 85 Loss 0.0178\n",
            "Time taken for 1 epoch: 3.9510512351989746 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0292\n",
            "Epoch 86 Batch 25 Loss 0.0179\n",
            "Epoch 86 Batch 50 Loss 0.0179\n",
            "Epoch 86 Loss 0.0179\n",
            "Time taken for 1 epoch: 3.7131893634796143 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0099\n",
            "Epoch 87 Batch 25 Loss 0.0227\n",
            "Epoch 87 Batch 50 Loss 0.0207\n",
            "Epoch 87 Loss 0.0207\n",
            "Time taken for 1 epoch: 3.7195260524749756 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0230\n",
            "Epoch 88 Batch 25 Loss 0.0149\n",
            "Epoch 88 Batch 50 Loss 0.0165\n",
            "Epoch 88 Loss 0.0165\n",
            "Time taken for 1 epoch: 3.7255101203918457 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0155\n",
            "Epoch 89 Batch 25 Loss 0.0174\n",
            "Epoch 89 Batch 50 Loss 0.0172\n",
            "Epoch 89 Loss 0.0172\n",
            "Time taken for 1 epoch: 3.716676950454712 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0224\n",
            "Epoch 90 Batch 25 Loss 0.0195\n",
            "Epoch 90 Batch 50 Loss 0.0192\n",
            "Saving checkpoint for epoch 90 at checkpoints/ckpt-86\n",
            "Epoch 90 Loss 0.0192\n",
            "Time taken for 1 epoch: 3.9534337520599365 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0152\n",
            "Epoch 91 Batch 25 Loss 0.0172\n",
            "Epoch 91 Batch 50 Loss 0.0166\n",
            "Epoch 91 Loss 0.0166\n",
            "Time taken for 1 epoch: 3.7639682292938232 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0095\n",
            "Epoch 92 Batch 25 Loss 0.0149\n",
            "Epoch 92 Batch 50 Loss 0.0157\n",
            "Epoch 92 Loss 0.0157\n",
            "Time taken for 1 epoch: 3.762004852294922 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0195\n",
            "Epoch 93 Batch 25 Loss 0.0165\n",
            "Epoch 93 Batch 50 Loss 0.0181\n",
            "Epoch 93 Loss 0.0181\n",
            "Time taken for 1 epoch: 3.7277657985687256 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0128\n",
            "Epoch 94 Batch 25 Loss 0.0161\n",
            "Epoch 94 Batch 50 Loss 0.0178\n",
            "Epoch 94 Loss 0.0178\n",
            "Time taken for 1 epoch: 3.7199831008911133 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0104\n",
            "Epoch 95 Batch 25 Loss 0.0187\n",
            "Epoch 95 Batch 50 Loss 0.0189\n",
            "Saving checkpoint for epoch 95 at checkpoints/ckpt-87\n",
            "Epoch 95 Loss 0.0189\n",
            "Time taken for 1 epoch: 3.934478998184204 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0190\n",
            "Epoch 96 Batch 25 Loss 0.0180\n",
            "Epoch 96 Batch 50 Loss 0.0165\n",
            "Epoch 96 Loss 0.0165\n",
            "Time taken for 1 epoch: 3.7398035526275635 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0217\n",
            "Epoch 97 Batch 25 Loss 0.0162\n",
            "Epoch 97 Batch 50 Loss 0.0166\n",
            "Epoch 97 Loss 0.0166\n",
            "Time taken for 1 epoch: 3.7344956398010254 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0073\n",
            "Epoch 98 Batch 25 Loss 0.0156\n",
            "Epoch 98 Batch 50 Loss 0.0161\n",
            "Epoch 98 Loss 0.0161\n",
            "Time taken for 1 epoch: 3.6985602378845215 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0122\n",
            "Epoch 99 Batch 25 Loss 0.0167\n",
            "Epoch 99 Batch 50 Loss 0.0181\n",
            "Epoch 99 Loss 0.0181\n",
            "Time taken for 1 epoch: 3.704615354537964 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0179\n",
            "Epoch 100 Batch 25 Loss 0.0188\n",
            "Epoch 100 Batch 50 Loss 0.0189\n",
            "Saving checkpoint for epoch 100 at checkpoints/ckpt-88\n",
            "Epoch 100 Loss 0.0189\n",
            "Time taken for 1 epoch: 3.96083927154541 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0126\n",
            "Epoch 101 Batch 25 Loss 0.0164\n",
            "Epoch 101 Batch 50 Loss 0.0167\n",
            "Epoch 101 Loss 0.0167\n",
            "Time taken for 1 epoch: 3.7089335918426514 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0269\n",
            "Epoch 102 Batch 25 Loss 0.0198\n",
            "Epoch 102 Batch 50 Loss 0.0190\n",
            "Epoch 102 Loss 0.0190\n",
            "Time taken for 1 epoch: 3.706794500350952 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0475\n",
            "Epoch 103 Batch 25 Loss 0.0185\n",
            "Epoch 103 Batch 50 Loss 0.0178\n",
            "Epoch 103 Loss 0.0178\n",
            "Time taken for 1 epoch: 3.7230236530303955 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.0164\n",
            "Epoch 104 Batch 25 Loss 0.0173\n",
            "Epoch 104 Batch 50 Loss 0.0175\n",
            "Epoch 104 Loss 0.0175\n",
            "Time taken for 1 epoch: 3.73069167137146 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.0209\n",
            "Epoch 105 Batch 25 Loss 0.0168\n",
            "Epoch 105 Batch 50 Loss 0.0174\n",
            "Saving checkpoint for epoch 105 at checkpoints/ckpt-89\n",
            "Epoch 105 Loss 0.0174\n",
            "Time taken for 1 epoch: 3.946077346801758 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.0236\n",
            "Epoch 106 Batch 25 Loss 0.0214\n",
            "Epoch 106 Batch 50 Loss 0.0199\n",
            "Epoch 106 Loss 0.0199\n",
            "Time taken for 1 epoch: 3.7184460163116455 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.0130\n",
            "Epoch 107 Batch 25 Loss 0.0174\n",
            "Epoch 107 Batch 50 Loss 0.0169\n",
            "Epoch 107 Loss 0.0169\n",
            "Time taken for 1 epoch: 3.7142226696014404 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.0111\n",
            "Epoch 108 Batch 25 Loss 0.0134\n",
            "Epoch 108 Batch 50 Loss 0.0144\n",
            "Epoch 108 Loss 0.0144\n",
            "Time taken for 1 epoch: 3.7192344665527344 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.0082\n",
            "Epoch 109 Batch 25 Loss 0.0150\n",
            "Epoch 109 Batch 50 Loss 0.0148\n",
            "Epoch 109 Loss 0.0148\n",
            "Time taken for 1 epoch: 3.7412400245666504 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0156\n",
            "Epoch 110 Batch 25 Loss 0.0145\n",
            "Epoch 110 Batch 50 Loss 0.0151\n",
            "Saving checkpoint for epoch 110 at checkpoints/ckpt-90\n",
            "Epoch 110 Loss 0.0151\n",
            "Time taken for 1 epoch: 3.949674129486084 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0126\n",
            "Epoch 111 Batch 25 Loss 0.0161\n",
            "Epoch 111 Batch 50 Loss 0.0152\n",
            "Epoch 111 Loss 0.0152\n",
            "Time taken for 1 epoch: 3.7129127979278564 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.0174\n",
            "Epoch 112 Batch 25 Loss 0.0158\n",
            "Epoch 112 Batch 50 Loss 0.0146\n",
            "Epoch 112 Loss 0.0146\n",
            "Time taken for 1 epoch: 3.7197725772857666 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0104\n",
            "Epoch 113 Batch 25 Loss 0.0140\n",
            "Epoch 113 Batch 50 Loss 0.0144\n",
            "Epoch 113 Loss 0.0144\n",
            "Time taken for 1 epoch: 3.7170963287353516 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0152\n",
            "Epoch 114 Batch 25 Loss 0.0145\n",
            "Epoch 114 Batch 50 Loss 0.0151\n",
            "Epoch 114 Loss 0.0151\n",
            "Time taken for 1 epoch: 3.702683687210083 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0104\n",
            "Epoch 115 Batch 25 Loss 0.0156\n",
            "Epoch 115 Batch 50 Loss 0.0157\n",
            "Saving checkpoint for epoch 115 at checkpoints/ckpt-91\n",
            "Epoch 115 Loss 0.0157\n",
            "Time taken for 1 epoch: 3.9177086353302 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0201\n",
            "Epoch 116 Batch 25 Loss 0.0152\n",
            "Epoch 116 Batch 50 Loss 0.0163\n",
            "Epoch 116 Loss 0.0163\n",
            "Time taken for 1 epoch: 3.6996519565582275 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.0087\n",
            "Epoch 117 Batch 25 Loss 0.0160\n",
            "Epoch 117 Batch 50 Loss 0.0152\n",
            "Epoch 117 Loss 0.0152\n",
            "Time taken for 1 epoch: 3.726321220397949 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.0081\n",
            "Epoch 118 Batch 25 Loss 0.0142\n",
            "Epoch 118 Batch 50 Loss 0.0137\n",
            "Epoch 118 Loss 0.0137\n",
            "Time taken for 1 epoch: 3.722869873046875 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0088\n",
            "Epoch 119 Batch 25 Loss 0.0145\n",
            "Epoch 119 Batch 50 Loss 0.0153\n",
            "Epoch 119 Loss 0.0153\n",
            "Time taken for 1 epoch: 3.731110095977783 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0151\n",
            "Epoch 120 Batch 25 Loss 0.0146\n",
            "Epoch 120 Batch 50 Loss 0.0150\n",
            "Saving checkpoint for epoch 120 at checkpoints/ckpt-92\n",
            "Epoch 120 Loss 0.0150\n",
            "Time taken for 1 epoch: 3.9608352184295654 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.0161\n",
            "Epoch 121 Batch 25 Loss 0.0145\n",
            "Epoch 121 Batch 50 Loss 0.0150\n",
            "Epoch 121 Loss 0.0150\n",
            "Time taken for 1 epoch: 3.7466354370117188 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.0110\n",
            "Epoch 122 Batch 25 Loss 0.0130\n",
            "Epoch 122 Batch 50 Loss 0.0144\n",
            "Epoch 122 Loss 0.0144\n",
            "Time taken for 1 epoch: 3.7031447887420654 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.0170\n",
            "Epoch 123 Batch 25 Loss 0.0164\n",
            "Epoch 123 Batch 50 Loss 0.0155\n",
            "Epoch 123 Loss 0.0155\n",
            "Time taken for 1 epoch: 3.700697183609009 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.0144\n",
            "Epoch 124 Batch 25 Loss 0.0140\n",
            "Epoch 124 Batch 50 Loss 0.0141\n",
            "Epoch 124 Loss 0.0141\n",
            "Time taken for 1 epoch: 3.724799156188965 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.0118\n",
            "Epoch 125 Batch 25 Loss 0.0139\n",
            "Epoch 125 Batch 50 Loss 0.0145\n",
            "Saving checkpoint for epoch 125 at checkpoints/ckpt-93\n",
            "Epoch 125 Loss 0.0145\n",
            "Time taken for 1 epoch: 3.955331802368164 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.0211\n",
            "Epoch 126 Batch 25 Loss 0.0154\n",
            "Epoch 126 Batch 50 Loss 0.0144\n",
            "Epoch 126 Loss 0.0144\n",
            "Time taken for 1 epoch: 3.737048387527466 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0167\n",
            "Epoch 127 Batch 25 Loss 0.0157\n",
            "Epoch 127 Batch 50 Loss 0.0167\n",
            "Epoch 127 Loss 0.0167\n",
            "Time taken for 1 epoch: 3.7171425819396973 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.0086\n",
            "Epoch 128 Batch 25 Loss 0.0134\n",
            "Epoch 128 Batch 50 Loss 0.0134\n",
            "Epoch 128 Loss 0.0134\n",
            "Time taken for 1 epoch: 3.7048351764678955 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0155\n",
            "Epoch 129 Batch 25 Loss 0.0120\n",
            "Epoch 129 Batch 50 Loss 0.0135\n",
            "Epoch 129 Loss 0.0135\n",
            "Time taken for 1 epoch: 3.7191364765167236 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.0109\n",
            "Epoch 130 Batch 25 Loss 0.0163\n",
            "Epoch 130 Batch 50 Loss 0.0144\n",
            "Saving checkpoint for epoch 130 at checkpoints/ckpt-94\n",
            "Epoch 130 Loss 0.0144\n",
            "Time taken for 1 epoch: 3.9226136207580566 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0062\n",
            "Epoch 131 Batch 25 Loss 0.0183\n",
            "Epoch 131 Batch 50 Loss 0.0179\n",
            "Epoch 131 Loss 0.0179\n",
            "Time taken for 1 epoch: 3.7126243114471436 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0173\n",
            "Epoch 132 Batch 25 Loss 0.0178\n",
            "Epoch 132 Batch 50 Loss 0.0187\n",
            "Epoch 132 Loss 0.0187\n",
            "Time taken for 1 epoch: 3.7047228813171387 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0073\n",
            "Epoch 133 Batch 25 Loss 0.0157\n",
            "Epoch 133 Batch 50 Loss 0.0145\n",
            "Epoch 133 Loss 0.0145\n",
            "Time taken for 1 epoch: 3.7077579498291016 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0109\n",
            "Epoch 134 Batch 25 Loss 0.0131\n",
            "Epoch 134 Batch 50 Loss 0.0135\n",
            "Epoch 134 Loss 0.0135\n",
            "Time taken for 1 epoch: 3.6966307163238525 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0130\n",
            "Epoch 135 Batch 25 Loss 0.0137\n",
            "Epoch 135 Batch 50 Loss 0.0147\n",
            "Saving checkpoint for epoch 135 at checkpoints/ckpt-95\n",
            "Epoch 135 Loss 0.0147\n",
            "Time taken for 1 epoch: 3.967940092086792 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0209\n",
            "Epoch 136 Batch 25 Loss 0.0144\n",
            "Epoch 136 Batch 50 Loss 0.0155\n",
            "Epoch 136 Loss 0.0155\n",
            "Time taken for 1 epoch: 3.7232182025909424 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0066\n",
            "Epoch 137 Batch 25 Loss 0.0129\n",
            "Epoch 137 Batch 50 Loss 0.0139\n",
            "Epoch 137 Loss 0.0139\n",
            "Time taken for 1 epoch: 3.7405617237091064 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0145\n",
            "Epoch 138 Batch 25 Loss 0.0135\n",
            "Epoch 138 Batch 50 Loss 0.0131\n",
            "Epoch 138 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.7397663593292236 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.0120\n",
            "Epoch 139 Batch 25 Loss 0.0121\n",
            "Epoch 139 Batch 50 Loss 0.0135\n",
            "Epoch 139 Loss 0.0135\n",
            "Time taken for 1 epoch: 3.727278232574463 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0067\n",
            "Epoch 140 Batch 25 Loss 0.0126\n",
            "Epoch 140 Batch 50 Loss 0.0131\n",
            "Saving checkpoint for epoch 140 at checkpoints/ckpt-96\n",
            "Epoch 140 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.939484119415283 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0093\n",
            "Epoch 141 Batch 25 Loss 0.0155\n",
            "Epoch 141 Batch 50 Loss 0.0168\n",
            "Epoch 141 Loss 0.0168\n",
            "Time taken for 1 epoch: 3.7559144496917725 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0166\n",
            "Epoch 142 Batch 25 Loss 0.0135\n",
            "Epoch 142 Batch 50 Loss 0.0130\n",
            "Epoch 142 Loss 0.0130\n",
            "Time taken for 1 epoch: 3.716005563735962 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0125\n",
            "Epoch 143 Batch 25 Loss 0.0132\n",
            "Epoch 143 Batch 50 Loss 0.0138\n",
            "Epoch 143 Loss 0.0138\n",
            "Time taken for 1 epoch: 3.6988353729248047 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.0148\n",
            "Epoch 144 Batch 25 Loss 0.0181\n",
            "Epoch 144 Batch 50 Loss 0.0197\n",
            "Epoch 144 Loss 0.0197\n",
            "Time taken for 1 epoch: 3.7170681953430176 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0107\n",
            "Epoch 145 Batch 25 Loss 0.0177\n",
            "Epoch 145 Batch 50 Loss 0.0176\n",
            "Saving checkpoint for epoch 145 at checkpoints/ckpt-97\n",
            "Epoch 145 Loss 0.0176\n",
            "Time taken for 1 epoch: 3.9385581016540527 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0120\n",
            "Epoch 146 Batch 25 Loss 0.0144\n",
            "Epoch 146 Batch 50 Loss 0.0134\n",
            "Epoch 146 Loss 0.0134\n",
            "Time taken for 1 epoch: 3.729342222213745 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0148\n",
            "Epoch 147 Batch 25 Loss 0.0152\n",
            "Epoch 147 Batch 50 Loss 0.0141\n",
            "Epoch 147 Loss 0.0141\n",
            "Time taken for 1 epoch: 3.748209238052368 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0121\n",
            "Epoch 148 Batch 25 Loss 0.0130\n",
            "Epoch 148 Batch 50 Loss 0.0132\n",
            "Epoch 148 Loss 0.0132\n",
            "Time taken for 1 epoch: 3.7331066131591797 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0068\n",
            "Epoch 149 Batch 25 Loss 0.0155\n",
            "Epoch 149 Batch 50 Loss 0.0154\n",
            "Epoch 149 Loss 0.0154\n",
            "Time taken for 1 epoch: 3.718780279159546 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0067\n",
            "Epoch 150 Batch 25 Loss 0.0169\n",
            "Epoch 150 Batch 50 Loss 0.0164\n",
            "Saving checkpoint for epoch 150 at checkpoints/ckpt-98\n",
            "Epoch 150 Loss 0.0164\n",
            "Time taken for 1 epoch: 3.9680027961730957 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0067\n",
            "Epoch 151 Batch 25 Loss 0.0132\n",
            "Epoch 151 Batch 50 Loss 0.0129\n",
            "Epoch 151 Loss 0.0129\n",
            "Time taken for 1 epoch: 3.73305082321167 secs\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.0098\n",
            "Epoch 152 Batch 25 Loss 0.0132\n",
            "Epoch 152 Batch 50 Loss 0.0173\n",
            "Epoch 152 Loss 0.0173\n",
            "Time taken for 1 epoch: 3.724343776702881 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0203\n",
            "Epoch 153 Batch 25 Loss 0.0181\n",
            "Epoch 153 Batch 50 Loss 0.0219\n",
            "Epoch 153 Loss 0.0219\n",
            "Time taken for 1 epoch: 3.7084250450134277 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.0075\n",
            "Epoch 154 Batch 25 Loss 0.0197\n",
            "Epoch 154 Batch 50 Loss 0.0177\n",
            "Epoch 154 Loss 0.0177\n",
            "Time taken for 1 epoch: 3.6825883388519287 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.0167\n",
            "Epoch 155 Batch 25 Loss 0.0129\n",
            "Epoch 155 Batch 50 Loss 0.0127\n",
            "Saving checkpoint for epoch 155 at checkpoints/ckpt-99\n",
            "Epoch 155 Loss 0.0127\n",
            "Time taken for 1 epoch: 3.938981533050537 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.0086\n",
            "Epoch 156 Batch 25 Loss 0.0109\n",
            "Epoch 156 Batch 50 Loss 0.0112\n",
            "Epoch 156 Loss 0.0112\n",
            "Time taken for 1 epoch: 3.7244319915771484 secs\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.0250\n",
            "Epoch 157 Batch 25 Loss 0.0131\n",
            "Epoch 157 Batch 50 Loss 0.0138\n",
            "Epoch 157 Loss 0.0138\n",
            "Time taken for 1 epoch: 3.7066497802734375 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.0123\n",
            "Epoch 158 Batch 25 Loss 0.0143\n",
            "Epoch 158 Batch 50 Loss 0.0141\n",
            "Epoch 158 Loss 0.0141\n",
            "Time taken for 1 epoch: 3.712174415588379 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.0205\n",
            "Epoch 159 Batch 25 Loss 0.0135\n",
            "Epoch 159 Batch 50 Loss 0.0128\n",
            "Epoch 159 Loss 0.0128\n",
            "Time taken for 1 epoch: 3.7342028617858887 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.0171\n",
            "Epoch 160 Batch 25 Loss 0.0116\n",
            "Epoch 160 Batch 50 Loss 0.0127\n",
            "Saving checkpoint for epoch 160 at checkpoints/ckpt-100\n",
            "Epoch 160 Loss 0.0127\n",
            "Time taken for 1 epoch: 3.9664947986602783 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0089\n",
            "Epoch 161 Batch 25 Loss 0.0144\n",
            "Epoch 161 Batch 50 Loss 0.0134\n",
            "Epoch 161 Loss 0.0134\n",
            "Time taken for 1 epoch: 3.692920207977295 secs\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.0110\n",
            "Epoch 162 Batch 25 Loss 0.0127\n",
            "Epoch 162 Batch 50 Loss 0.0126\n",
            "Epoch 162 Loss 0.0126\n",
            "Time taken for 1 epoch: 3.7125227451324463 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0116\n",
            "Epoch 163 Batch 25 Loss 0.0143\n",
            "Epoch 163 Batch 50 Loss 0.0139\n",
            "Epoch 163 Loss 0.0139\n",
            "Time taken for 1 epoch: 3.7401061058044434 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.0143\n",
            "Epoch 164 Batch 25 Loss 0.0149\n",
            "Epoch 164 Batch 50 Loss 0.0142\n",
            "Epoch 164 Loss 0.0142\n",
            "Time taken for 1 epoch: 3.6992595195770264 secs\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0124\n",
            "Epoch 165 Batch 25 Loss 0.0133\n",
            "Epoch 165 Batch 50 Loss 0.0138\n",
            "Saving checkpoint for epoch 165 at checkpoints/ckpt-101\n",
            "Epoch 165 Loss 0.0138\n",
            "Time taken for 1 epoch: 3.9926986694335938 secs\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.0066\n",
            "Epoch 166 Batch 25 Loss 0.0130\n",
            "Epoch 166 Batch 50 Loss 0.0127\n",
            "Epoch 166 Loss 0.0127\n",
            "Time taken for 1 epoch: 3.7505857944488525 secs\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0104\n",
            "Epoch 167 Batch 25 Loss 0.0114\n",
            "Epoch 167 Batch 50 Loss 0.0128\n",
            "Epoch 167 Loss 0.0128\n",
            "Time taken for 1 epoch: 3.759448289871216 secs\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.0261\n",
            "Epoch 168 Batch 25 Loss 0.0132\n",
            "Epoch 168 Batch 50 Loss 0.0134\n",
            "Epoch 168 Loss 0.0134\n",
            "Time taken for 1 epoch: 3.7024102210998535 secs\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0078\n",
            "Epoch 169 Batch 25 Loss 0.0155\n",
            "Epoch 169 Batch 50 Loss 0.0157\n",
            "Epoch 169 Loss 0.0157\n",
            "Time taken for 1 epoch: 3.7258174419403076 secs\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.0129\n",
            "Epoch 170 Batch 25 Loss 0.0148\n",
            "Epoch 170 Batch 50 Loss 0.0130\n",
            "Saving checkpoint for epoch 170 at checkpoints/ckpt-102\n",
            "Epoch 170 Loss 0.0130\n",
            "Time taken for 1 epoch: 3.9771690368652344 secs\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.0215\n",
            "Epoch 171 Batch 25 Loss 0.0120\n",
            "Epoch 171 Batch 50 Loss 0.0127\n",
            "Epoch 171 Loss 0.0127\n",
            "Time taken for 1 epoch: 3.754932403564453 secs\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.0188\n",
            "Epoch 172 Batch 25 Loss 0.0138\n",
            "Epoch 172 Batch 50 Loss 0.0139\n",
            "Epoch 172 Loss 0.0139\n",
            "Time taken for 1 epoch: 3.7063493728637695 secs\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0143\n",
            "Epoch 173 Batch 25 Loss 0.0128\n",
            "Epoch 173 Batch 50 Loss 0.0131\n",
            "Epoch 173 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.7360658645629883 secs\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.0063\n",
            "Epoch 174 Batch 25 Loss 0.0127\n",
            "Epoch 174 Batch 50 Loss 0.0140\n",
            "Epoch 174 Loss 0.0140\n",
            "Time taken for 1 epoch: 3.7132911682128906 secs\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.0118\n",
            "Epoch 175 Batch 25 Loss 0.0143\n",
            "Epoch 175 Batch 50 Loss 0.0135\n",
            "Saving checkpoint for epoch 175 at checkpoints/ckpt-103\n",
            "Epoch 175 Loss 0.0135\n",
            "Time taken for 1 epoch: 3.951350688934326 secs\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.0067\n",
            "Epoch 176 Batch 25 Loss 0.0133\n",
            "Epoch 176 Batch 50 Loss 0.0131\n",
            "Epoch 176 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.736490249633789 secs\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0179\n",
            "Epoch 177 Batch 25 Loss 0.0117\n",
            "Epoch 177 Batch 50 Loss 0.0125\n",
            "Epoch 177 Loss 0.0125\n",
            "Time taken for 1 epoch: 3.746311664581299 secs\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0081\n",
            "Epoch 178 Batch 25 Loss 0.0139\n",
            "Epoch 178 Batch 50 Loss 0.0131\n",
            "Epoch 178 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.7259950637817383 secs\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0107\n",
            "Epoch 179 Batch 25 Loss 0.0136\n",
            "Epoch 179 Batch 50 Loss 0.0141\n",
            "Epoch 179 Loss 0.0141\n",
            "Time taken for 1 epoch: 3.7127437591552734 secs\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.0066\n",
            "Epoch 180 Batch 25 Loss 0.0133\n",
            "Epoch 180 Batch 50 Loss 0.0125\n",
            "Saving checkpoint for epoch 180 at checkpoints/ckpt-104\n",
            "Epoch 180 Loss 0.0125\n",
            "Time taken for 1 epoch: 3.9420149326324463 secs\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0106\n",
            "Epoch 181 Batch 25 Loss 0.0138\n",
            "Epoch 181 Batch 50 Loss 0.0121\n",
            "Epoch 181 Loss 0.0121\n",
            "Time taken for 1 epoch: 3.727248430252075 secs\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0116\n",
            "Epoch 182 Batch 25 Loss 0.0126\n",
            "Epoch 182 Batch 50 Loss 0.0130\n",
            "Epoch 182 Loss 0.0130\n",
            "Time taken for 1 epoch: 3.752253770828247 secs\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0061\n",
            "Epoch 183 Batch 25 Loss 0.0123\n",
            "Epoch 183 Batch 50 Loss 0.0131\n",
            "Epoch 183 Loss 0.0131\n",
            "Time taken for 1 epoch: 3.7030327320098877 secs\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0062\n",
            "Epoch 184 Batch 25 Loss 0.0135\n",
            "Epoch 184 Batch 50 Loss 0.0136\n",
            "Epoch 184 Loss 0.0136\n",
            "Time taken for 1 epoch: 3.7141833305358887 secs\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0051\n",
            "Epoch 185 Batch 25 Loss 0.0120\n",
            "Epoch 185 Batch 50 Loss 0.0127\n",
            "Saving checkpoint for epoch 185 at checkpoints/ckpt-105\n",
            "Epoch 185 Loss 0.0127\n",
            "Time taken for 1 epoch: 3.9580209255218506 secs\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0184\n",
            "Epoch 186 Batch 25 Loss 0.0115\n",
            "Epoch 186 Batch 50 Loss 0.0121\n",
            "Epoch 186 Loss 0.0121\n",
            "Time taken for 1 epoch: 3.712505340576172 secs\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0218\n",
            "Epoch 187 Batch 25 Loss 0.0132\n",
            "Epoch 187 Batch 50 Loss 0.0121\n",
            "Epoch 187 Loss 0.0121\n",
            "Time taken for 1 epoch: 3.727339744567871 secs\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0171\n",
            "Epoch 188 Batch 25 Loss 0.0142\n",
            "Epoch 188 Batch 50 Loss 0.0130\n",
            "Epoch 188 Loss 0.0130\n",
            "Time taken for 1 epoch: 3.7216343879699707 secs\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0090\n",
            "Epoch 189 Batch 25 Loss 0.0105\n",
            "Epoch 189 Batch 50 Loss 0.0116\n",
            "Epoch 189 Loss 0.0116\n",
            "Time taken for 1 epoch: 3.7237143516540527 secs\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0193\n",
            "Epoch 190 Batch 25 Loss 0.0128\n",
            "Epoch 190 Batch 50 Loss 0.0116\n",
            "Saving checkpoint for epoch 190 at checkpoints/ckpt-106\n",
            "Epoch 190 Loss 0.0116\n",
            "Time taken for 1 epoch: 3.9747776985168457 secs\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0136\n",
            "Epoch 191 Batch 25 Loss 0.0115\n",
            "Epoch 191 Batch 50 Loss 0.0113\n",
            "Epoch 191 Loss 0.0113\n",
            "Time taken for 1 epoch: 3.731207847595215 secs\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0167\n",
            "Epoch 192 Batch 25 Loss 0.0092\n",
            "Epoch 192 Batch 50 Loss 0.0111\n",
            "Epoch 192 Loss 0.0111\n",
            "Time taken for 1 epoch: 3.7369256019592285 secs\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0140\n",
            "Epoch 193 Batch 25 Loss 0.0124\n",
            "Epoch 193 Batch 50 Loss 0.0134\n",
            "Epoch 193 Loss 0.0134\n",
            "Time taken for 1 epoch: 3.7141177654266357 secs\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0214\n",
            "Epoch 194 Batch 25 Loss 0.0105\n",
            "Epoch 194 Batch 50 Loss 0.0112\n",
            "Epoch 194 Loss 0.0112\n",
            "Time taken for 1 epoch: 3.7255003452301025 secs\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0047\n",
            "Epoch 195 Batch 25 Loss 0.0111\n",
            "Epoch 195 Batch 50 Loss 0.0121\n",
            "Saving checkpoint for epoch 195 at checkpoints/ckpt-107\n",
            "Epoch 195 Loss 0.0121\n",
            "Time taken for 1 epoch: 3.9822118282318115 secs\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0239\n",
            "Epoch 196 Batch 25 Loss 0.0143\n",
            "Epoch 196 Batch 50 Loss 0.0147\n",
            "Epoch 196 Loss 0.0147\n",
            "Time taken for 1 epoch: 3.714675188064575 secs\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0117\n",
            "Epoch 197 Batch 25 Loss 0.0142\n",
            "Epoch 197 Batch 50 Loss 0.0147\n",
            "Epoch 197 Loss 0.0147\n",
            "Time taken for 1 epoch: 3.7331461906433105 secs\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0082\n",
            "Epoch 198 Batch 25 Loss 0.0131\n",
            "Epoch 198 Batch 50 Loss 0.0123\n",
            "Epoch 198 Loss 0.0123\n",
            "Time taken for 1 epoch: 3.7121713161468506 secs\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0061\n",
            "Epoch 199 Batch 25 Loss 0.0113\n",
            "Epoch 199 Batch 50 Loss 0.0114\n",
            "Epoch 199 Loss 0.0114\n",
            "Time taken for 1 epoch: 3.7031521797180176 secs\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0103\n",
            "Epoch 200 Batch 25 Loss 0.0103\n",
            "Epoch 200 Batch 50 Loss 0.0117\n",
            "Saving checkpoint for epoch 200 at checkpoints/ckpt-108\n",
            "Epoch 200 Loss 0.0117\n",
            "Time taken for 1 epoch: 3.964681625366211 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5D5cv2Jd8-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpdiW6wnmiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZoEHvIxrYKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "565d0651-2446-4fcf-88dc-e6165c83c1fd"
      },
      "source": [
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'the main idea is to combine the approaches in han et al and ullrich et al to get a loss value constrained k means encoding method for network compress the reviewer would expect papers submitted for review to be of publishable qu however this manuscript is not polished enough for publication it has too many language errors and imprecisions which make the paper hard to follow in particular there is no clear definition of the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JADQ5_WOXPiZ",
        "colab_type": "text"
      },
      "source": [
        "NOW MAKING MAJOR SUMMERIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHr7t5eY5PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews= test['reviews'].tolist()\n",
        "auto_summary=[]\n",
        "for review in reviews:\n",
        "  text= summarize(review)\n",
        "  auto_summary.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFQnVkgZneO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f9bf886d-a2bc-4ebb-f3af-3fa695a22bb3"
      },
      "source": [
        "test['auto_summary']=auto_summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCIUyoVWDgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "b288d3ae-5717-4b3e-a97a-b26a2950cc97"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ids</th>\n",
              "      <th>reviews</th>\n",
              "      <th>real_summary</th>\n",
              "      <th>auto_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>801</th>\n",
              "      <td>ICLR2018-H196sainb-R2_annotated.txt</td>\n",
              "      <td>the paper proposes a method to learn bilingual...</td>\n",
              "      <td>the task is interesting and relevant especia...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>ICLR2018-B1mSWUxR--R3_annotated.txt</td>\n",
              "      <td>the authors claim three contributions in this ...</td>\n",
              "      <td>i found the first contribution is sound and ...</td>\n",
              "      <td>the main idea is to combine the approaches in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>803</th>\n",
              "      <td>ICLR2018-BJQRKzbA--R3_annotated.txt</td>\n",
              "      <td>the authors present a novel evolution scheme a...</td>\n",
              "      <td>the authors present a novel evolution scheme...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>ICLR2018-H1uP7ebAW-R2_annotated.txt</td>\n",
              "      <td>this paper presents an impressive set of resul...</td>\n",
              "      <td>this paper presents an impressive set of res...</td>\n",
              "      <td>the main idea of this paper is to be a good co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>805</th>\n",
              "      <td>ICLR2018-B1al7jg0b-R1_annotated.txt</td>\n",
              "      <td>the paper leaves me guessing which part is a n...</td>\n",
              "      <td>the paper leaves me guessing which part is a...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>ICLR2018-HyEi7bWR--R3_annotated.txt</td>\n",
              "      <td>the paper is clearly written with a good cover...</td>\n",
              "      <td>the paper is clearly written with a good cov...</td>\n",
              "      <td>the main idea is to be incremental over the mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>ICLR2018-Bys4ob-Rb-R3_annotated.txt</td>\n",
              "      <td>this paper derived an upper bound on adversari...</td>\n",
              "      <td>the main idea of  using upper bound as oppos...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>ICLR2018-By4HsfWAZ-R2_annotated.txt</td>\n",
              "      <td>in this paper the authors show how a deep lear...</td>\n",
              "      <td>pros- the paper is written in a clear and co...</td>\n",
              "      <td>the main idea of this paper is to be a good co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>ICLR2018-BJRZzFlRb-R3_annotated.txt</td>\n",
              "      <td>the authors proposed to compress word embeddin...</td>\n",
              "      <td>the proposed method achieved compression rat...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>ICLR2018-B1e5ef-C--R2_annotated.txt</td>\n",
              "      <td>the interesting paper provides theoretical sup...</td>\n",
              "      <td>the interesting paper provides theoretical s...</td>\n",
              "      <td>the main idea is to be incremental and the pap...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>201 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      ids  ...                                       auto_summary\n",
              "801   ICLR2018-H196sainb-R2_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "802   ICLR2018-B1mSWUxR--R3_annotated.txt  ...  the main idea is to combine the approaches in ...\n",
              "803   ICLR2018-BJQRKzbA--R3_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "804   ICLR2018-H1uP7ebAW-R2_annotated.txt  ...  the main idea of this paper is to be a good co...\n",
              "805   ICLR2018-B1al7jg0b-R1_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "...                                   ...  ...                                                ...\n",
              "997   ICLR2018-HyEi7bWR--R3_annotated.txt  ...  the main idea is to be incremental over the mo...\n",
              "998   ICLR2018-Bys4ob-Rb-R3_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "999   ICLR2018-By4HsfWAZ-R2_annotated.txt  ...  the main idea of this paper is to be a good co...\n",
              "1000  ICLR2018-BJRZzFlRb-R3_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "1001  ICLR2018-B1e5ef-C--R2_annotated.txt  ...  the main idea is to be incremental and the pap...\n",
              "\n",
              "[201 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35MTN2frqJHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_excel(\"Neural_AUTO_Vs_MAJ_onsame.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0zTPBOrS4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_summary= test['real_summary'].tolist()\n",
        "auto_summary= test['auto_summary'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ufo3meKwRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('ref.txt', 'w') as f:\n",
        "    for item in real_summary:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open('hyp.txt', 'w') as f:\n",
        "    for item in auto_summary:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhH6GnNQrTyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "def dump(hyp,ref,filename):\n",
        "  # Data to be written \n",
        "  dictionary ={ \n",
        "      \"hyp\" : hyp, \n",
        "      \"ref\" : ref\n",
        "  } \n",
        "    \n",
        "  with open(filename, \"a\") as outfile: \n",
        "      json.dump(dictionary, outfile) \n",
        "      outfile.write(',\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7qUR_9nrY3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(real_summary)):\n",
        "    dump(auto_summary[i],real_summary[i],\"Neural_AUTO_Vs_MAJ_onsame.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MreQDal7GVI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}