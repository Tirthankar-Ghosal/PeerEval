{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper generally presents a nice idea, and some of the modifications to searn/lols that the authors had to make to work with neural networks are possibly useful to others. Some weaknesses exist in the evaluation that everyone seems to agree on, but disagree about importance (in particular, comparison to things like BLS and Mixer on problems other than MT).\n\nA few side-comments (not really part of meta-review, but included here anyway):\n- Treating rollin/out as a hyperparameter is not unique to this paper; this was also done by Chang et al., NIPS 2016, \"A credit assignment compiler...\"\n- One big question that goes unanswered in this paper is \"why does learned rollin (or mixed rollin) not work in the MT setting.\" If the authors could add anything to explain this, it would be very helpful!\n- Goldberg & Nivre didn't really introduce the _idea_ of dynamic oracles, they simply gave it that name (e.g., in the original Searn paper, and in most of the imitation learning literature, what G&n call a \"dynamic oracle\" everyone else just calls an \"oracle\" or \"expert\")", "decision": "Accept (Poster)"}