{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The problem addressed here is an important one: What is a good evaluation metric for generative models?  A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs.  Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test.  This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns.\n\nFrom a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction.  One must read more than a few pages to get to the answer of why the metrics that are advocated were picked.  It need not read like a mystery.\n\nR4: \"The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification\"\n\nR2: \"First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions.\" - the first point of which is also related to a concern of R4.\n\nGiven the overall high selectivity of ICLR, the present submission falls short.", "decision": "Reject"}