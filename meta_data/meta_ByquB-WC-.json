{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The contribution of this paper basically consists of using MLPs in the attention mechanism of end-2-end memory networks. Though it leads to some improvements on bAbI (which may not be so surprising - MLP attention has been shown preferable in certain scenarious), it does not seem to be a sufficient contribution. The motivation is also confusing - the work is not really that related to relation networks, which were specifically designed to deal with situations where *relations* between objects matter. The proposed architecture does not model relations.\n\n+  improvement on bAbI over the baselines\n-  limited novelty (MLP attention is fairly standard)\n-  the presentation of the idea is confusing (if the claim is about relations -> other datasets need to be considered)\n\nThere is a consensus between reviewers. ", "decision": "Reject"}