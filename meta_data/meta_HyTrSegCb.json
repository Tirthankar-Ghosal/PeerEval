{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The pros and cons of this paper cited by the reviewers can be summarized below:\n\nPros:\n* Empirical results demonstrate decent improvements over other reasonable models\n* The method is well engineered to the task\n\nCons:\n* The paper is difficult to read due to grammar and formatting issues\n* Experiments are also lacking detail and potentially difficult to reproduce\n* Some of the experimental results are suspect in that the train/test accuracy are basically the same. Usually we would expect train to be much better in highly parameterized neural models\n* The content is somewhat specialized to a particular task in NLP, and perhaps of less interest to the ICLR audience as a whole (although I realize that ICLR is attempting to cast a broad net so this alone is not a reason for rejection of the paper)\n\nIn addition to the Cons cited by the reviewers above, I would also note that there is some relevant work on morphology in sequence-to-sequence models, e.g.:\n* \"What do Neural Machine Translation Models Learn about Morphology?\" Belinkov et al. ACL 2017.\n\nand that it is common in sequence-to-sequence models to use sub-word units, which allows for better handling of morphological phenomena:\n* \"Neural Machine Translation of Rare Words with Subword Units\" Sennrich et al. ACL 2016.\n\nWhile the paper is not without merit, given that the cons seem to significantly outweigh the pros, I don't think that it is worthy of publication at ICLR at this time, although submission to a future conference (perhaps NLP conference) seems warranted.", "decision": "Reject"}