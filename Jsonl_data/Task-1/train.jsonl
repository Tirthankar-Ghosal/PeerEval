{"abstract_id": 0, "sentences": ["in this paper the authors present an adaptation of space-by-time non-negative matrix factorization sbt-nmf that can rigorously account for the pre-stimulus baseline act", "the authors go on to compare their baseline-corrected bc method with several established methods for dimensionality reduction of spike train data", "overall the results are a bit mix", "the bc method often performs similarly to or is outperformed by non-bc sbt-nmf", "the authors provide a possible mechanism to explain these results by analyzing classification performance as a function of baseline firing r"], "labels": ["SMY", "SMY", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity however this doesn't fully address why non-bc sbt-nmf can strongly outperform the bc method in certain tasks eg the step of light fig b", "the authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity however this doesn't fully address why non-bc sbt-nmf can strongly outperform the bc method in certain tasks eg the step of light fig b", "finally while this method introduces a principled way to remove mean baseline activity from the sensory-driven response this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response destexhe et al nature reviews neuroscience   gutnisky da et al cerebral cortex", "the authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or batch", "a network is trained to learn the transformations that minimize the wasserstein distance between distribut"], "labels": ["QSN", "CRT", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the wasserstein distance is also called the earth mover distance and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to anoth", "in this paper they have a neural network compute the wasserstein distance using a different formulation that was used in arjovsky et al  finds a lipschitz function f which shows the maximal difference when evaluated on samples from the two distribut", "here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network", "results are examined mainly by looking at the first two pca components of the data", "the paper presents an interesting idea and is fairly well written"], "labels": ["SMY", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i have a few concerns most of the ideas presented in the paper rely on works by arjovsky et al  gulrajani et al  and gulrajani et ", "some selections which are presented in the papers are not explained for example the gradient penalty the choice of lambda and the choice of points for gradient comput", "some selections which are presented in the papers are not explained for example the gradient penalty the choice of lambda and the choice of points for gradient comput", "the experimental results are not fully convincing they simply compare the first two pc components on this broad bioimage benchmark collect", "this section could be improved by demonstrating the approach on more dataset"], "labels": ["CRT", "DFT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is a lack comparison to other methods such as shaham et ", "why is using earth mover distance better than mmd based dist", "they only compare it to a method named coral and to typicalvariation normalization tvn", "what about comparison to other batch normalization methods in biology such as seurat", "why is the affine transform assumption valid in biolog"], "labels": ["DFT", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there can definitely be non-linear effects that are different between batches such as ion detection efficiency differ", "only early stopping seems to constrain their model to be near ident", "doesn't this also prevent optimal result", "how does this compare to the near-identity constraints in resnets in shaham et ", "the paper makes a bold claim that deep neural networks are robust to arbitrary level of nois"], "labels": ["DIS", "DIS", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it also implies that this would be true for any type of noise and support this later claim using experiments on cifar and mnist with three noise types  uniform label noise  non-uniform but image-independent label noise which is named structured noise and  samples from out-of-dataset class", "the experiments show robustness to these types of nois", "review the claim made by the paper is overly general and in my own experience incorrect when considering real-world-nois", "this is supported by the literature on data cleaning partially by the authors a procedure which is widely acknowledged as critical for good object recognit", "while it is true that some image-independent label noise can be alleviated in some datasets incorrect labels in real world datasets can substantially harm classification accuraci"], "labels": ["SMY", "SMY", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to understand the source of the difference between the results in this paper and the more common results where label noise damages recognition qu", "the paper did not get a chance to test these differences and i can only raise a few hypothes", "first real-world noise depends on the image and classes in a more structured way for instance raters may confuse one bird species from a similar one when the bird is photographed from a particular angl", "this could be tested experimentally for example by adding incorrect labels for close species using the cub data for fine-grained bird species recognit", "another possible reason is that classes in mnist and cifar are already very distinctive so are more robust to nois"], "labels": ["DIS", "DFT", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["once again it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in gener", "without such an analysis i feel the paper should not be accepted to iclr because the way it states its claim may mislead read", "other specific comments -- section  the experimental setup should clearly state details of the optimization architecture and hyper parameter search", "for example for conv how many channels at each lay", "how was the net initi"], "labels": ["SUG", "FBK", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["which hyper parameters were tuned and with which valu", "were hyper parameters tuned on a separate validation set", "how was the train/val/test split done etc", "these details are useful for judging technical correct", "-- section  importance of large dataset"], "labels": ["QSN", "QSN", "QSN", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the recent paper by chen et al  would be relevant her", "-- figure  failed to show for m", "-- figure  need to specify which noise model was us", "the paper presents some conceptually incremental improvements over the models in ucneural statisticianud and ucgenerative matching networksud", "nevertheless it is well written and i think it is solid work with reasonable convincing experiments and good result"], "labels": ["SUG", "CRT", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the authors use powerful pixelcnn priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive compon", "although the authors use powerful pixelcnn priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive compon", "this paper defines building blocks for complex-valued convolutional neural networks complex convolutions complex batch normalisation several variants of the relu nonlinearity for complex inputs and an initialisation strategi", "the writing is clear concise and easy to follow", "an important argument in favour of using complex-valued networks is said to be the propagation of phase inform"], "labels": ["SMY", "DIS", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i feel that the observation that crelu works best out of the  proposed alternatives contradicts this somewhat", "crelu simply applies relu component-wise to the real and imaginary parts which has an effect on the phase information that is hard to conceptualis", "it definitely does not preserve phase like modrelu would", "this makes me wonder whether the complex numbers paradigm is applied meaningfully here or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well note that even completely random parameter tying can work well as shown in compressing neural networks with the hashing trick by chen et ", "some more insight into how phase information is used what it represents and how it is propagated through the network would help to make sense of thi"], "labels": ["DFT", "DIS", "SMY", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the image recognition results are mostly inconclusive which makes it hard to assess the benefit of this approach", "the improved performance on the audio tasks seems significant but how the complex nature of the networks helps achieve this is not really demonstr", "it is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network because i think it is implied that this is what happens this connection is a bit vagu", "once again a more in-depth analysis of this phase behavior would be very welcom", "i'm on the fence about this work i like the ideas and they are explained wel"], "labels": ["DIS", "DFT", "DFT", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i'm missing some insight into why and how all of this is actually helping to improve performance especially wrt how phase information is us", "comments- the related work section is comprehensive but a bit unstructured with each new paragraph seemingly describing a completely different type of work", "maybe some subsection titles would help make it feel a bit more cohes", "- page  cite a couple of them should be replaced by some actual refer", "- although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters doesn't the complex version always require more computation on account of there being more filters in each lay"], "labels": ["DFT", "DFT", "SUG", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be nice to discuss computational cost as wel", "revision i have decided to raise my rating from  to  as i feel that the authors have adequately addressed many of my com", "in particular i really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network", "regarding the cifar results i may have read over it but i think it would be good to state even more clearly that these experiments constitute a sanity check as both reviewer  and myself were seemingly unaware of thi", "with this in mind it is of course completely fine that the results are not better than for real-valued network"], "labels": ["SUG", "FBK", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper investigates the effect of adversarial train", "based on experiments using cifar the authors show that adversarial training is effective in protecting against shared adversarial perturbation in particular against universal perturb", "in contrast it is less effective to protect against singular perturb", "then they show that singular perturbation are less robust to image transformation meaning after image transformation those perturbations are no longer effect", "finally they show that singular perturbations can be easily detect"], "labels": ["SMY", "SMY", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i like the message conveyed in this pap", "however as the statements are mostly backed by experiments then i think it makes sense to ask how statistically significant the present results ar", "moreover is cifar  experiments conclusive enough", "this paper applies the boosting trick to deep learn", "the idea is quite straightforward and the paper is relatively easy to follow"], "labels": ["APC", "DIS", "QSN", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed algorithm is validated on several image classification dataset", "the paper is its current form has the following issues there is hardly any baseline compared in the pap", "the proposed algorithm is essentially an ensemble algorithm there exist several works on deep model ensemble eg boosted convolutional neural networks and snapshot ensemble should be compared against", "the proposed algorithm is essentially an ensemble algorithm there exist several works on deep model ensemble eg boosted convolutional neural networks and snapshot ensemble should be compared against", "i did not carefully check all the proofs but seems most of the proof can be moved to supplementary to keep the paper more concis"], "labels": ["SMY", "DFT", "DFT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in eq  tilde{d} is not defin", "under the assumption $epsilon_tl  frac{}{lambda}$ the definition of $beta_t$ in eq does not satisfy $  beta_t  $", "how many layers is the densenet-bc used in this pap", "why the error rate reported here is higher than that in the original pap", "typo in session  line  there is a missing refer"], "labels": ["DFT", "CRT", "QSN", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in session  line  uc object classesud should be uc object classesud", "in line  of the paragraph below equation  ucclasseud should be ucclassud", "the paper intends to show that complex and real valued neural network are different and lead to different results on similar tasks the complex valued network being more appropriate to 'difficult' problems and dataset", "the work seems to have been written in a rush leading to a big number of typos and quickly filled experiment tables  and  are full of zero", "the only valid conclusion is that real and complex valued neural network cannot be directly compared using the same number of paramet"], "labels": ["CRT", "CRT", "SMY", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some theoretical aspect or at least some intuition should be more in depth detailed to understand when one should be better than the oth", "concerning the novelty the paper is in the same spirit as https//arxivorg/abs/ but with weaker experiments theoretical justifications and no valid conclus", "the authors propose a strategy for compressing rnn acoustic models in order to deploy them for embedded appl", "the technique consists of first training a model by constraining its trace norm which allows it to be well-approximated by a truncated svd in a second fine-tuning stag", "overall i think this is interesting work"], "labels": ["DFT", "DFT", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i have a few concerns which iuve listed below section  which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit ucdisjointud from the rest of the pap", "i had a number of clarification questions spefically on this section- am i correct that the results in this section do not use the trace-norm regularization at al", "it would strengthen the paper significantly if the experiments presented on wsj in the first section were also conducted on the ucinternalud task with more data", "- how large are the training/test sets used in these experiments for test sets number of words for training sets amount of data in hours is this ~hrs whether any data augmentation such as multi-style training was done etc", "- what are the uctier-ud and uctier-ud models in this sect"], "labels": ["CRT", "QSN", "SUG", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would also aid readability if the various models were described more clearly in this section with an emphasis on structure output targets what lms are used how are the lms pruned for the embedded-size models etc", "also particularly given that the focus is on embedded speech recognition of which the acoustic model is one part i would like a few more details on how decoding was done etc", "- the details in appendix b are interesting and i think they should really be a part of the main pap", "that being said the results in section b as the authors mention are somewhat preliminari", "and i think the paper would be much stronger if the authors can re-run these experiments were models are trained to converg"], "labels": ["SUG", "DIS", "APC", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the paper focuses fairly heavily on speech recognition tasks and i wonder if it would be more suited to a conference on speech recognit", "could the authors comment on the relative training time of the models with the trace-norm regularizer l-regularizer and the unconstrained model in terms of convergence tim", "clarification question for the wsj experiments was the model decoded without an lm", "if no lm was used then the choice of reporting results in terms of only cer is reason", "but i think it would be good to also report wers on the wsj set in either cas"], "labels": ["SUG", "DIS", "QSN", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could the authors indicate the range of values of lambda_{rec} and lambda_{nonrec} that were examined in the work", "also on a related note in figure  does each point correspond to a specific choice of these regularization paramet", "figure  for the models in figure  it would be useful to indicate the starting cer of the stage- model before stage- training to get a sense of how stage- training impacts perform", "although the results on the wsj set are interest", "i would be curious if the same trends and conclusions can be drawn from a larger dataset -- eg the internal dataset that results are reported on later in the paper or on a set like switchboard"], "labels": ["QSN", "QSN", "SUG", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think these experiments would strengthen the pap", "the experiments in section  were interesting since they demonstrate that the model can be warm-started from a model that hasnut fully converg", "could the authors also indicate the cer of the model used for initialization in addition to the final cer after stage- training in figur", "in section  the authors mention that quantization could be used to compress models further although this is usually degrades wer by --% relative i think the authors should consider citing previous works which have examined quantization for embedded speech recognition [] [] in particular note that [] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after train", "references[] vincent vanhoucke andrew senior and mark mao ucimproving the speed of neural networks on cpusud in deep learning and unsupervised feature learning workshop nips [] raziel alvarez rohit prabhavalkar anton bakhtin ucon the efficient representation and execution of deep acoustic modelsud proc of interspeech pp  --   minor comment the authors use the term ucwarmstartingud to refer to the process of training nns by initializing from a previous model"], "labels": ["DIS", "APC", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be good to clarify this in the text", "this paper presents a method to search neural network architectures at the same time of train", "it does not require training from scratch for each architecture thus dramatically saves the training tim", "the paper can be understood with no problem", "moderate novelti"], "labels": ["SUG", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["network morphism is not novel", "applying it to architecture search is novel", "pros the required time for architecture searching is significantly reduc", "with the same number or less of parameters this method is able to outperform previous methods with much less tim", "however the method described is restricted in the following aspect"], "labels": ["CRT", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing this also makes the search greedy  which could be suboptim", "in addition the algorithm in this paper selects the best performing network at each step which also hampers the discover of the optimal model", "in addition the algorithm in this paper selects the best performing network at each step which also hampers the discover of the optimal model", "n strong human prior network morphism iv is more general than skip connection for example a two column structure belongs to type iv", "however in the implementation it is restricted to skip connection by addit"], "labels": ["APC", "APC", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this choice could be motivated from the success of residual network", "this choice could be motivated from the success of residual network", "this limits the method from discovering meaningful structur", "for example it is difficult to discover residual network denovo", "this is a common problem of architecture searching methods compared to handcrafted structur"], "labels": ["SUG", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the comparison with zoph & le is not fair because their controller is a meta-network and the training happens only onc", "for example the rnncell discovered can be fixed and used in other tasks and the rnn controller for cnn architecture search could potentially be applied to other tasks too though not report", "qualitythe paper is well-written and clear and includes relevant comparisons to previous work npi and recursive npi", "claritythe paper is clearly written", "originalityto my knowledge the method proposed in this work is novel"], "labels": ["CRT", "CRT", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is the first to study constructing minimal training sets for npi given a black-box oracl", "however as pointed out by the authors there is a lot of similar prior work in software test", "significancethe work could be potentially signific", "but there are some very strong assumptions made in the paper that could limit the impact", "if the npi has access to a black-box oracle it is not clear what is the use of training an npi in the first plac"], "labels": ["APC", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be very helpful to describe a potential scenario where the proposed approach could be us", "also it is assumed that the number of possible inputs is finite also true for the recursive npi paper and it is not clear what techniques or lessons of this paper might transfer to tasks with perceptual input", "the main technical contribution is the search procedure to find minimal training sets and pare down the observation size and the empirical validation of the idea on several algorithmic task", "pros- greatly improves the data efficiency of recursive npi", "- training and verification sets are automatically generated by the proposed method"], "labels": ["SUG", "DFT", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- requires access to a black-box oracle to construct the dataset", "- not clear that the idea will be useful in more complex domains with unbounded input", "this paper studies empirical risk in deep neural network", "results are provided in section  for linear networks and in section  for nonlinear network", "results for deep linear neural networks are puzzl"], "labels": ["DFT", "CRT", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["whatever the number of layers a deep linear nn is simply a matrix multiplication and minimizing the mse is simply a linear regress", "so results in section  are just results for linear regression and i do not understand why the number of layers come into play", "also this is never explicitly mentioned in the paper i guess the authors make an assumption that the samples x_iy_i are drawn iid from a given distribution d", "in such a case i am sure results on the population risk minimization can be found for linear regression and should be compare to results in sect", "i thank the authors for the thoughtful response and updated manuscript"], "labels": ["DIS", "QSN", "CRT", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after reading through both my review score remains unchang", "=================the authors describe a new variant of a generative adversarial network gan for generating imag", "this model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art gan model", "major comments spatial resolution what spatial resolution is the model generating images at", "the ac-gan work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the inception score versus naively resizing the imag"], "labels": ["FBK", "SMY", "SMY", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upsc", "it would be great to see the authors address this issue in a serious mann", "fid in real data the numbers in table  appear favorable to the projection model", "please add error bars based on figure  i would imagine they are quite larg", "additionally would it be possible to compute this statistic for *real* imag"], "labels": ["CRT", "SUG", "APC", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would be curious to know what the fid looks like as a 'gold standard'", "conditional batch normalization  i am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discrimin", "the former has been seen to be quite powerful in accomodating multi-modal tasks eg https//arxivorg/abs/ https//arxivorg/abs/", "if the authors could provide some evidence highlighting the marginal gains of one technique that would be extremely help", "minor comments- i believe you have the incorrect reference for conditional batch normalization on pag"], "labels": ["DIS", "CRT", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a learned representation for artistic styl", "dumoulin shlens and kudlur https//arxivorg/abs/", "- please enlarge images in figure - hard to see the detail of x imag", "- please add citations for figures a-b", "do these correspond with some known model"], "labels": ["SUG", "SUG", "SUG", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["depending on how the authors respond to the reviews i would consider upgrading the score of my review", "this paper presents a so-called cross-view training for semi-supervised deep model", "experiments were conducted on various data sets and experimental results were reportedpros* studying semi-supervised learning techniques for deep models is of practical signific", "cons* the novelty of this paper is margin", "the use of unlabeled data is in fact a self-training process"], "labels": ["FBK", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retriev", "* the proposed approach suffers from a technical weakness or flaw", "for the self-labeled data the prediction of each view is enforced to be same as the assigned self-label", "however since each view related to a sub-region of the image especially when the model is not so deep it is less likely for this region to contain the representation of the concepts eg some local region of an image with a horse may exhibit only grass enforcing the prediction of this view to be the same self-labeled concepts eguchorseud may drive the prediction away from what it should be  eg it will make the network to predict grass as hors", "such a flaw may affect the final performance of the proposed approach"], "labels": ["CRT", "CRT", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the word ucviewud in this paper is mislead", "the ucviewud in this paper is corresponding to actually sub-regions in the imag", "* the experimental results indicate that the proposed approach fails to perform better than the compared baselines in table  which reduces the practical significance of the proposed approach", "this paper proposes an unsupervised method called parallel checkpointing learners pcl to detect and defend adversarial exampl", "the main idea is essentially learning the manifold of the data distribution and using gaussian mixture models gmms and dictionary learning to train a reformer without seeing adversarial examples to detect and correct adversarial exampl"], "labels": ["CRT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with pcl one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attack", "although the motivation is well ground", "there are two major issues of this work i limited  novelty - the idea of unsupervised manifold projection method has been proposed in the previous work", "and ii insufficient attack evaluations - the defender performance is evaluated against weak attacks or attacks with improper paramet", "the details are as follows  limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in magnet a two-pronged defense against adversarial examples appeared in may"], "labels": ["SMY", "APC", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the details are as follows  limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in magnet a two-pronged defense against adversarial examples appeared in may", "instead of gmms and dictionary learning in pcl  magnet trains autoencoders for defense and provides sufficient experiments to claim its defense cap", "on the other hand the authors of this paper seem to be not aware of this pioneering work and claim to the best of our knowledge our proposed pcl methodology is the first unsupervised countermeasure that is able to detect dl adversarial samples generated by the existing state-of-the-art attacks which is obviously not tru", "more importantly magnet is able to defend the adversarial examples very well almost % success no matter the adversarial examples are close to the information manifold or not", "as a result the resulting roc and auc score are expected be better than pcl"], "labels": ["CRT", "DIS", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition the authors of magnet also compared their performance in white-box attacker knowing the reformer gray-box having multiple independent reformers and black-box attacker not knowing the reformer scenarios whereas this paper only considers the last cas", "insufficient attack evaluations - the attacks used in this paper to evaluate the performance of pcl are either weak no longer state-of-the-art or incorrectly impl", "for fgsm the iterative version proposed by kurakin iclr  should be us", "jsma and deep fool are not considered strong attacks now see carlini's bypassing  detection methods pap", "carlini-wagner attack is still strong but the authors only use  iterations should be at least  and setting the confidence= which is known to be producing non-transferable adversarial exampl"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in comparison magnet has shown to be effective against different confidence paramet", "in summary this paper has limited novelty incremental contributions and lacks convincing experimental results due to weak attack implement", "in summary this paper has limited novelty incremental contributions and lacks convincing experimental results due to weak attack implement", "paper examines the use of skip connections including residual layers in deep networks as a way of alleviating two perceived difficulties in training  when a neuron does not contain any information and", "when two neurons in a layer compute the same funct"], "labels": ["CRT", "DFT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["both of these cases lead to singularities in the hessian matrix and this work includes a number of experiments showing the effect of skip connections on the hessian during train", "this is a significant and timely top", "while i may not be the best one to judge the originality of this work i appreciated how the authors presented clear and concise arguments with experiments to back up their claim", "the paper attempts to study model meta parameter inference eg model architecture optimization etc using a supervised learning approach", "they take three approaches one whereby the target models are evaluated on a fixed set of inputs one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combin"], "labels": ["SMY", "CRT", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors also show that these inferred quantities can be used to generate more effective attacks against the target", "the paper is generally well written and most details for reproducibility are seem enough", "i also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intrigu", "it is of course not entirely surprising that the system can be trained but that there is some form of generalization happen", "aside that i think most system in practical use will be much more different than any a priori enumeration/brute force search for model paramet"], "labels": ["SMY", "APC", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing ensembling non-standard data and a number of optimization and architectural tricks that are developer depend", "it is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that cas", "i have found it hard to understand what table  in section  actually mean", "it seems to say for instance that a model is trained on  and  layers then queried with  and the accuracy only slightly drop", "accuracy of what  is it the other attributes  is it somehow that attribute  if so how can that poss"], "labels": ["DIS", "DIS", "DFT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my main main concern is extrapolation out of the training set which is particularly important here i don't find enough evidence in  for that point", "one experiment that i would find compelling is to train for instance a meta model on svbr but not d on imagenet predict all the attributes except architecture and see how that changes when d is ad", "if these are better than random and the perturbations are more successful it would be a much more compelling stori", "after reading the rebuttalthe authors addressed some of my theoretical quest", "i think the paper is borderline leaning towards accept"], "labels": ["DIS", "SUG", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do want to note my other concernsi suspect the theoretical results obtained here are somewhat restricted to the least-squares autoencoder loss", "and note that the authors show that the proposed algorithm performs comparably to sgd but not significantly bett", "the classification result table  was obtained on the autoencoder features instead of training a classifier on the original input", "so it is not clear if the proposed algorithm is better for training the classifier which may be of more interest", "=============================================================this paper presents an algorithm for training deep neural network"], "labels": ["CRT", "CRT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead of computing gradient of all layers and perform updates of all weight parameters at the same time the authors propose to perform alternating optimization on weights of individual lay", "the theoretical justification is obtained for single-hidden-layer auto-encod", "motivated by recent work by hazan et al  the authors developed the local-quasi-convexity of the objective wrt the hidden layer weights for the generalized relu activ", "as a result the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of hazan et ", "this itself can be a small nice contribut"], "labels": ["DIS", "DIS", "DIS", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what concerns me is the extension to multiple lay", "some questions are not clear from sect", "do we still have local-quasi-convexity for the weights of each layer when there are multiple nonlinear layers above it", "a negative answer to this question will somewhat undermine the significance of the single-hidden-layer result", "practically even if the authors can perform efficient optimization of weights in individual layers when there are many layers the alternating optimization nature of the algorithm can possibly result in overall slower converg"], "labels": ["CRT", "CRT", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also since the proposed algorithm still uses gradient based optimizers for each layer computing the gradient wrt lower layers closer to the inputs are still done by backdrop which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same tim", "as a result i am not sure if the proposed algorithm is on par with / faster than the regular sgd algorithm in actual runtim", "in the experiments the authors plotted the training progress wrt the minibatch iterations i do not know if the minibatch iteration is a proxy for actual runtime or number of floating point oper", "in the experiments the authors found the network optimized by the proposed algorithm generalize better than regular sgd", "is this result consistent across dataset random initializations etc and can the authors elaborate the intuition behind"], "labels": ["DIS", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper identifies and proposes a fix for a shortcoming of the deep information bottleneck approach namely that the induced representation is not invariant to monotonic transform of the marginal distributions as opposed to the mutual information on which it is bas", "the authors address this shortcoming by applying the dib to a transformation of the data obtained by a copula transform", "this explicit approach is shown on synthetic experiments to preserve more information about the target yield better reconstruction and converge faster than the baselin", "the authors further develop a sparse extension to this deep copula information bottleneck dcib which yields improved representations in terms of disentangling and sparsity on a uci dataset", "significance this is a promising idea"], "labels": ["SMY", "SMY", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper builds on the information theoretic perspective of representation learning and makes progress towards characterizing what makes for a good represent", "invariance to transforms of the marginal distributions is clearly a useful property and the proposed method seems effective in this regard", "unfortunately i do not believe the paper is ready for publication as it stands as it suffers from lack of clarity and the experimentation is limited in scop", "unfortunately i do not believe the paper is ready for publication as it stands as it suffers from lack of clarity and the experimentation is limited in scop", "clarity while section  clearly defines the explicit form of the algorithm where data and labels are essentially pre-processed via a copula transform details regarding the ucimplicit formud are very scarc"], "labels": ["SMY", "SMY", "DFT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from section  it seems as though the authors are optimizing the form of the gaussian information bottleneck ixt in the hopes of recovering an encoder $f_betax$ which gaussianizes the input thus emulating the explicit transform", "could the authors clarify whether this interpretation is correct or alternatively provide additional clarifying detail", "there are also many missing details in the experimental section how were the number of ucactiveud components select", "which versions of the algorithm explicit/implicit were used for which experi", "i believe explicit was used for section  and implicit for  but again this needs to be spelled out more clearli"], "labels": ["QSN", "QSN", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would also like to see a discussion and perhaps experimental comparison to standard preprocessing techniques such as pca-whiten", "quality the experiments are interesting and seem well execut", "unfortunately i do not think their scope single synthetic plus a single uci dataset is suffici", "while the gap in performance is significant on the synthetic task this gap appears to shrink significantly when moving to the uci dataset", "while the gap in performance is significant on the synthetic task this gap appears to shrink significantly when moving to the uci dataset"], "labels": ["DFT", "APC", "DFT", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how does this method perform for more realistic data even eg mnist", "i think it is crucial to highlight that the deficiencies of dib matter in practice and are not simply a theoretical consider", "i think it is crucial to highlight that the deficiencies of dib matter in practice and are not simply a theoretical consider", "similarly the representation analyzed in figure  is promisingbut again the authors could have targeted other common datasets for disentangling eg the simple sprites dataset used in the beta-vae pap", "similarly the representation analyzed in figure  is promisingbut again the authors could have targeted other common datasets for disentangling eg the simple sprites dataset used in the beta-vae pap"], "labels": ["QSN", "SUG", "DFT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would have also liked to see a more direct and systemic validation of the claims made in the pap", "for example the shortcomings of dib identified in section   could have been verified more directly by plotting iyt for various monotonic transformations of x", "a direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion", "pros* theoretically well motivated* promising results on synthetic task* potential for impact", "cons* paper suffers from lack of clarity method and experimental sect"], "labels": ["DFT", "SMY", "SUG", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* lack of ablative / introspective experiments* weak empirical results small or toy datasets onli", "summaryin this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free dna cfdna", "the idea is that in the sample being sequenced there would also be circulating tumor dna ctdna so such mutations could be captured in the sequencing read", "the issue is that the ctdna are expected to be found with low abundance in such samples and therefore are likely to be hit by few or even single read", "this makes the task of differentiating between sequencing errors and true variants due to ctdna hard"], "labels": ["DFT", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mut", "to this they add channels based on low base quality low mapping qu", "the algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered cnn with /bp long filters to capture di and trinucleotide frequencies and a fully connected layer to a softmax function at the top", "the data is based on mutations in  patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region", "one more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate perform"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["prosthe paper tackles what seems to be both an important and challenging problem", "we also liked the thoughtful construction of the network and way the reference the read the cigar and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context", "using matched samples of tumor and normal from the patients is also a nice idea to mimic cfdna data", "conswhile we liked both the challenge posed and the idea to solve it we found several major issues with the work", "first the writing is far from clear there are typos and errors all over at an unacceptable level"], "labels": ["APC", "APC", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["many terms are not defined or defined after being introduced eg cigar mf bqmq", "a more reasonable cs style of organization is to first introduce the methods/model and then the results but somehow the authors flipped it and started with results first lacking many definitions and experimental setup to make sense of those  yet sec  ucresultsud p  is not really results but part of the method", "the ucpipelineud is never well defined only implicitly in p top and then it is hard to relate the various figures/tables to bottom line results having the labels wrong does not help that", "the filters by themselves seem trivial and as such do not offer much novelti", "moreover the authors filter the ucnormalud samples using those p top which makes the entire exercise a possible circular argu"], "labels": ["DFT", "CRT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations if more than a single read for that mutation is available - but the authors do not discuss/try that", "if the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations if more than a single read for that mutation is available - but the authors do not discuss/try that", "the entire dataset is based on  pati", "it is not clear what is the source of the other cancer control cas", "the authors claim the reduced performance show they are learning lung cancer-specific context what evidence do they have for that can they show a context they learned and make sense of it"], "labels": ["DFT", "DIS", "SMY", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how does this relate to the original papers they cite to motivate this direction alexandrov", "since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of  pati", "as such this may have very little relevance for the actual problem of cfdna", "finally performance itself did not seem to improve significantly compared to previous methods/simple filters and the novelty in terms of ml and insights about learning representations seemed limit", "albeit the above caveats we iterate the paper offers a nice construction for an important problem"], "labels": ["QSN", "DIS", "DIS", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ismb/recomb", "we believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ismb/recomb", "the authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation instead of in the original input data spac", "the details of their proposed method are covered in algorithm  on page  where an additional gan generative adversarial network i_{gamma} which can be regarded as the inverse function of the original gan g_{theta} is trained to learn a map from the original input data space to the latent z-spac", "the authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural legible grammatical and semantically similar to the input"], "labels": ["SUG", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["generally i think that the paper is written well except some issues listed at the end", "the intuition of the proposed approach is clearly explained and it seems very reasonable to m", "my main concern however is in the current sampling-based search algorithm in the latent z-space which the authors have already admitted in the paper the efficiency of such a search method decreases very fast when the dimensions of the z-space increas", "furthermore such an approximation solution based on the sampling may be not close to the original optimal solution z* in equ", "this makes me feel that there is large room to further advance the pap"], "labels": ["APC", "APC", "DIS", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method such as fgsm in generating the adversari", "the example in table  is very good but more examples especially involving the quantitative comparison are needed to demonstrate the claimed advantag", "for example could the authors add such a comparison in human evaluation in section  to support the claim that the adversaries generated by their method are more natur", "other issues are listed as follows could you explicitly specify the dimension of the latent z-space in each example in image and text domain in sect", "in tables  and  the human beings agree with the lenet in = % of cases could you still say that your generated ucadversariesud leading to the wrong decision from lenet"], "labels": ["DFT", "APC", "SUG", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are these really ucadversariesud", "how do you choose the parameter lambda in equ", "the paper introduces a new memory mechanism specifically tailored for agent navigation in d environ", "the memory consists of a d array and includes trainable read/write mechan", "the rl agent's policy is a function of the context read read and next step write vectors which are functions of the observ"], "labels": ["QSN", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the effectiveness of the proposed architecture is evaluated via reinforcement learning % of mazes solv", "the evaluation included  test mazes--which sets a good precedent for evaluation in this subfield", "my main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architectur", "the downsampling experiment in section  seems to indicate the contrary downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own loc", "the downsampling experiment in section  seems to indicate the contrary downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own loc"], "labels": ["SMY", "APC", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm concerned whether the proposed agent is actually employing a navigation strategy as seems to be suggested or is simply a good agent architecture for this task eg for optimization reason", "the short experiment in appendix e seems to try and answer this question but it's results are anecdotal at best", "if good rl performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation world centric into memory and use something like a value iteration network or shortest path planning to plan rout", "my point is that there are classical algorithms to solve navigation even in partially observable d grid worlds why bother with deep rl her", "this paper examines ways of producing word embeddings for rare words on demand"], "labels": ["CRT", "CRT", "DIS", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key real-world use case is for domain specific terms but here the techniques are demonstrated on rarer words in standard data set", "the strength of this paper is that it both gives a more systematic framework for and builds on existing ideas character-based models using dictionary definitions to implement them as part of a model trained on the end task", "the contribution is clear", "but not hug", "in general for the scope of the paper it seems like what is here could fairly easily have been made into a short paper for other conferences that have that categori"], "labels": ["SMY", "APC", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the basic method easily fits within  pages and while the presentation of the experiments would need to be much briefer this seems quite poss", "more things could have been consid", "some appear in the paper and there are some fairly natural other ones such as mining some use contexts of a word such as just from google snippets rather than only using textual definitions from wordnet", "the contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task and the idea of adding a learned linear transformation matrix inside the mean pooling model p", "however it is not made very clear why this matrix is needed or what the qualitative effect of its addition i"], "labels": ["CRT", "DFT", "DIS", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is clearly written", "a paper that should be referred to is the short paper of dhingra et al  a comparative study of word embeddingsfor reading comprehension https//arxivorg/pdf/pdf", "while it in no way covers the same ground as this paper it is relevant as follows this paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to unk", "however they point out that at least for matching tasks like qa and nli that one can do better by assigning random vectors on the fly to unknown word", "that method could also be considered as a possible approach to compare against her"], "labels": ["APC", "SUG", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["other comments - the paper suggests a couple of times including at the end of the nd intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words which are not morphological derivations etc", "while this argument has intuitive appeal it seems to fly in the face of the fact that actually spelling models including in this paper seem to do surprisingly well at learning such arbitrary semant", "- p you use pretrained glove vectors that you do not upd", "my impression is that people have had mixed results sometimes better sometimes worse with updating pretrained vectors or not did you try it both ways - fn  perhaps slightly exaggerates the point being made since people usually also get good results with the glove or wordvec model trained on only  billion words u  orders of magnitude less data", "- p when no definition is available is making e_dw a zero vector worse than or about the same as using a trained unk vector"], "labels": ["APC", "APC", "DFT", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- table  the baseline seems reasonable near enough to the quality of the original salesforce model from   f", "but well below current best single models of around - f", "the difference between d and d does well illustrate that better definition learning is done with backprop from end object", "this model shows the rather strong performance of spelling models u at least on this task u which he again benefit from training in the context of the end object", "- fig  it's weird that only the +dict left model learns to connect in and wher"], "labels": ["APC", "CRT", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the point made in the text between where and overseas is perfectly reasonable but it is a mystery why the base model on the right doesn't learn to associate the common words where and in both commonly expressing a loc", "- table  these results are interestingly differ", "dict is much more useful than spelling her", "i guess that is because of the nature of nli but it isn't % clear why nli benefits so much more than qa from definitional knowledg", "- p i was slightly surprised by how small vocabs k and k words are said to be optimal for nli and similar remarks hold for squad"], "labels": ["CRT", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my impression is that most papers on nli use much larger vocabs no", "- fig  this could really be drawn considerably better make the dots bigger and their colors more distinct", "- table  the differences here are quite small and perhaps the least compelling but the same trends hold", "- this paper is not well written and incomplet", "there is no clear explanation of what exactly the authors want to achieve in the paper what exactly is their approach/contribution experimental setup and analysis of their result"], "labels": ["QSN", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the paper is hard to read due to many abbreviations eg the last paragraph in pag", "- the format is inconsistent section  is numbered but not the other sect", "- in page  what do the numbers mean at the end of each sentence probably the figur", "- in page  in this figure which figure is this referring to", "comments on prior workp  authors write vanilla backpropagation vbp was proposed around  rumelhart et "], "labels": ["CRT", "CRT", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not true a main problem with the  paper is that it does not cite the inventors of backpropag", "the vbp that everybody is using now is the one published by  linnainmaa in  extending kelley's work of", "the first to publish the application of vbp to nns was werbos in  please correct", "p  authors write almost at the same time biologically inspired convolutional networks was also introduced as well using vbp lecun et ", "here one must cite the person who really invented this biologically inspired convolutional architecture but did not apply backprop to it fukushima  he is cited later but in a misleading way please correct"], "labels": ["CRT", "DIS", "SUG", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["here one must cite the person who really invented this biologically inspired convolutional architecture but did not apply backprop to it fukushima  he is cited later but in a misleading way please correct", "p  authors write deep learning dl was introduced as an approach to learn deep neural network architecture using vbp lecun et al   krizhevsky et ", "not true deep learning was introduced by ivakhnenko and lapa in  the first working method for learning in multilayer perceptrons of arbitrary depth please correctthe term deep learning was introduced to ml in  by dechter for something els", "p authors write extremely deep networks learning reached  layers of representation with residual and highway networks he et al  srivastava et ", "highway networks were published half a year earlier than resnets and reached many hundreds of layers before resnets please correct"], "labels": ["DIS", "DIS", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["highway networks were published half a year earlier than resnets and reached many hundreds of layers before resnets please correct", "general recommendation clear rejection for now", "but perhaps the author want to resubmit this to another conference taking into account the reviewer com", "this paper proposes maskgan a gan-based generative model of text based onthe idea of recovery from masked text", "for this purpose authors employed a reinceforcement learning approach tooptize a prediction from masked text"], "labels": ["DFT", "FBK", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover authors argue that the quality of generated texts is not appropriately measured by perplexitiesthus using another criterion of a diversity of generated n-grams as well asqualitative evaluations by examples and by human", "while basically the approach seems plausible the issue is that the result isnot compared to ordinary lstm-based baselin", "while it is better than a conterpart of mle maskedmle whether the result is qualitatively better thanordinary lstm is still in quest", "in fact this is already appearent both from the model architectures and thegenerated examples because the model aims to fill-in blanks from the textaround up to that time generated texts are generally locally valid but notalways valid globally this issue is also pointed out by authors in appendixa", "while the idea of using mask is interesting and important i think if thisidea could be implemented in another way because it resembles gibbs samplingwhere each token is sampled from its sorrounding context while its objectiveis still global sentence-wis"], "labels": ["SMY", "APC", "DIS", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as argued in section  the ability of obtaining signals token-wise looks beneficial at first but it will actuallybreak a global validity of syntax and other sentence-wise phenoma", "based on the arguments above i think this paper is valuable at leastconceptually but doubt if it is actually usable in place of ordinary lstmor rnn-based gener", "more arguments are desirable for the advantage of this paper ie quantitativeevaluation of diversity of generated text as opposed to lstm-based method", "*based on the rebuttals and thorough experimental results i modified the global r", "this paper proposes a new method to train residual networks in which one starts by training shallow resnets doubling the depth and warm starting from the previous smaller model in a certain way and iter"], "labels": ["CRT", "DFT", "SUG", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors relate this idea to a recent dynamical systems view of resnets in which residual blocks are viewed as taking steps in an euler discretization of a certain differential equ", "the authors relate this idea to a recent dynamical systems view of resnets in which residual blocks are viewed as taking steps in an euler discretization of a certain differential equ", "this interpretation plays a role in the proposed training method by informing how the ucstep sizesud in the euler discretization should change when doubling the depth of the network", "the punchline of the paper is that the authors are able to achieve similar performance as ucfull resnet trainingud but with significantly reduced training tim", "the punchline of the paper is that the authors are able to achieve similar performance as ucfull resnet trainingud but with significantly reduced training tim"], "labels": ["SMY", "DIS", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the proposed method is novel u even though this idea of going from shallow to deep is natural for residual networks tying the idea to the dynamical systems perspective is eleg", "moreover the paper is clearly written", "experimental results are decent u there are clear speedups to be had based on the authors' experi", "however it is unclear if these gains in training speed are significant enough for people to flock to using this more complicated method of train", "i only have a few small questions/comments* a more naive way to do multi-level training would be to again iteratively double the depth but perhaps not halve the step s"], "labels": ["APC", "APC", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint", "* one thing ium unclear on is how convergence was assessedu my understanding is that the training proceeds for a fixed number of epochs  - but shouldnut this also depend on the depth in some way", "* would the speedups be more dramatic for a larger dataset like imagenet", "* finally not being very familiar with multigrid methods from the numerical methods literature u i would have liked to hear about whether there are deeper connections to these method", "the authors revised the paper according to all reviewers suggestions i am satisfied with the current vers"], "labels": ["DIS", "QSN", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graph", "it build upon several previous works successively introducing convolutional networks gated edges convnets on graphs and lstms on tre", "the authors extend the tree lstms formulation to perform graph labeling on arbitrary graphs merge convnets with residual connections and edge gating mechan", "they apply the  proposed models to  baselines also based on graph neural networks on two problems sub-graph matching expressing the problem of sub-graph matching as a node classification problem and semi supervised clust", "main commentsit would strengthen the paper to also compare all these network learning based approaches to variational on"], "labels": ["SMY", "APC", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance to a spectral clustering method for the semi supervised clustering orsolving the combinatorial dirichlet problem as in grady random walks for image segment", "the abstract and the conclusion should be revised they are very vagu", "the abstract and the conclusion should be revised they are very vagu", "- the abstract should be self contained and should not contain cit", "- the abstract should be self contained and should not contain cit"], "labels": ["SUG", "SUG", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors should clarify which problem they are dealing with", "- instead of the numerical result show the performance of the new model give some numerical results here otherwise this sentence is useless", "- instead of the numerical result show the performance of the new model give some numerical results here otherwise this sentence is useless", "- we propose  as propose - unclear what do you propos", "minor comments- you should make sentences when using references with the author names format"], "labels": ["CRT", "SUG", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["example  graph theory chung  - graph theory by chung", "example  graph theory chung  - graph theory by chung", "- as eq  - as the minimization of eq  same with eq - don't start sentences with and or but", "- as eq  - as the minimization of eq  same with eq - don't start sentences with and or but", "summary the paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an svm model"], "labels": ["SUG", "DIS", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the svm is trained for the final classification task at hand using the last layer features of the deep network", "the motivation behind all this is to learn the input features to the svm as opposed to hand-crafting them and use the generalization ability of the svm to do well on tasks which have only a handful of training exampl", "the authors apply their technique to two datasets namely the omniglot dataset and the timit dataset and show that their model does a reasonable job in these two task", "while the paper is reasonably clearly written and easy to read", "i have a number of objections to it"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first i did not see any novel idea presented in this pap", "lots of people have tried pre-training a neural network on auxiliary tasks and using the features from it as input to the final svm classifi", "people have also specifically tried to train a siamese network and use its features as input to the svm", "these works go way back to the years  -  when deep learning was not called deep learn", "unless i have missed something completely i did not see any novel idea proposed in this pap"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["second the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach", "for example the authors compare their model against rather weak baselin", "while the approach as has been shown in the past is very reason", "i would have liked the experiments to be more thorough with comparison to the state of the art models for the two dataset", "summarythis paper proposes an extension to the rwa model by introducing the discount gates to computed discounted averages instead of the undiscounted attent"], "labels": ["APC", "CRT", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the problem with the rwa is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t", "pros- addresses an issue of rwa", "cons-the paper addresses a problem with an issue with rwa", "but it is not clear to me why would that be an important contribut", "-the writing needs more work"], "labels": ["DIS", "APC", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-the experiments are lacking and the results are not good enough", "general commentsthis paper addresses an issue regarding to rwa which is not really widely adopted and well-known architecture because it seems to have some have some issues that this paper is trying to address", "i would still like to have a better justification on why should we care about rwa and fixing that model", "the writing of this paper seriously needs more work", "the lemma  doesn't make sense to me i think it has a typo in it it should have been -^t c instead of -^t c"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are only on toyish and small scale task", "according to the results the model doesn't really do better than a simple lstm or gru", "learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this pap", "a projected sub-gradient descent algorithm is us", "the ups optimizer by itself is not new"], "labels": ["CRT", "CRT", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["graph polynomial signal gps neural network is proposed to address two shortcomings of gsp using linear polynomial graph filt", "first a nonlinear function sigma in  is used and second weights are shared among neighbors of every data point", "there are some concerns about this network that need to be clarified sigma is never clarified in the main context or experi", "there are some concerns about this network that need to be clarified sigma is never clarified in the main context or experi", "the shared weights should be relevant to the ordering of neighbors instead of the set of neighbors without ordering in which case the sharing looks random"], "labels": ["SMY", "SMY", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another explanation about the weights as the rescaling to matrix a needs to further clarifi", "as authors mentioned that the magnitude of |a| from l norm might be detrimental for the predict", "what is the disagreement between l penalty and prediction qu", "why not apply these weights to l norm as a weighted l norm to control the scaling of a", "why not apply these weights to l norm as a weighted l norm to control the scaling of a"], "labels": ["DFT", "SMY", "QSN", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors stated that the last step is to build a mapping from the gps features into the response i", "they mentioned that linear fully connected layer or a more complex neural network can be build on top of the gps featur", "however no detailed information is given in the pap", "in the experiments authors only stated that ucwe fit the gps architecture using ups optimizer for varying degree of the neighborhood of the graphud and then the graph is used to train existing models as the input of the graph", "which architecture is used for building the map"], "labels": ["SMY", "SMY", "DFT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the experimental results detailed definition or explanation of the compared methods and different settings should be clarifi", "in the experimental results detailed definition or explanation of the compared methods and different settings should be clarifi", "for example what is gps  gcn_ eq  in table  and gcn_  and gps_ gps_ gps_ and so on", "more explanations of figure  and the visualization method can be great helpful to understand the advantages of the proposed algorithm", "this paper presents advantage-based regret minimization somewhat similar to advantage actor-critic with reinforc"], "labels": ["DFT", "CRT", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature and without markov assumpt", "the claim that arm is more robust to partially observable domains is supported by experiments where it outperforms dqn", "there are several things to like about this paper- the authors do a good job of reviewing/referencing several papers in the field of regret minimization that would probably be of interest to the iclr community + provide non-obvious connections / summaries of these perspect", "- the issue of partial observability is good to bring up rather than simply relying on the mdp framework that is often taken as a given in deep reinforcement learn", "- the experimental results show that arm outperforms dqn on a suite of deep rl task"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are also some negatives- reviewing so much of the cfr-literature in a short paper means that it ends up feeling a little rushed and confus", "- the ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as ac trpo etc", "at a high enough level these algorithms can be written the same way there are undoubtedly some key differences in how they behave but it's not spelled out to the reader and i think the connections can be miss", "- the experiment/motivation i found most compelling was  since it clearly matches the issue of partial observ", "but we only see results compared to dqn  it feels like you don't put a compelling case for the non-markovian benefits of arm vs other policy gradient method"], "labels": ["CRT", "CRT", "SUG", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["yes ac and trpo seem like they perform very poorly compared to arm", "but i'm left wondering how/whi", "i feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a pap", "a lot of the cited literature was also new to me so it could be that i'm missing something about why this is so interest", "however i came away from this paper quite uncertain about the real benefits/differences of arm versus other similar policy gradient method"], "labels": ["CRT", "QSN", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also didn't feel the experimental evaluations drove a clear message except arm did better than all other methods on these experi", "i'd want to understand how/why and whether we should expect this univers", "the focus on regret minimization perspectives didn't really get me too excit", "overall i would vote against acceptance for this vers", "the paper studies the problem of dnn loss function design for reducing intra-class variance in the output feature spac"], "labels": ["CRT", "DIS", "CRT", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class", "the proposed loss has been compared extensively against a number of closely related approaches in methodolog", "numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss wen et al  when applied to distance-based classifiers such as k-nn and k-mean", "pros - the idea of isotropic normalization for enhancing compactness of class is well motiv", "- the paper is mostly clearly organized and pres"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- numerical study shows some promise of the proposed method", "cons-  the novelty of method is mostly incremental given the prior work of wen et al  which has provided a slightly different isotropic variant of softmax loss", "- the training procedure of the proposed method remains unclear in this pap", "the paper proposes to learn a custom translation or rotation invariant kernel in the fourier representation to maximize the margin of svm", "instead of using monte carlo approximation as in the traditional random features literature the main point of the paper is to learn these fourier features in a min-max sens"], "labels": ["APC", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this perspective leads to some interesting theoretical results and some new interpret", "synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bas", "i like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boost", "as for the related work it seems the authors have missed some very relevant pieces of work in learning these fourier features through gradient descent [ ]", "it would be interesting to compare these algorithms as wel"], "labels": ["APC", "APC", "APC", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] zichao yang marcin moczulski misha denil nando de freitas alex smola le song ziyu wang deep fried convnets iccv", "[] zichao yang alexander j smola le song andrew gordon wilson a la carte u learning fast kernels aistat", "the key argument authors present against relu+bn is the fact that using relu after bn skews the values resulting in non-normalized activ", "although the bn paper suggests using bn before non-linearity many articles have been using bn after non-linearity which then gives normalized activations https//githubcom/ducha-aiki/caffenet-benchmark/blob/master/batchnormmd and also better overall perform", "the approach of using bn after non-linearity is termed standardization layer https//arxivorg/pdf/pdf"], "labels": ["DIS", "DIS", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i encourage the authors to validate their claims against simple approach of using bn after non-linear", "this paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding spac", "this is an interesting idea which from a robustness point of view xu et al  makes sens", "note that a similar strategy has been used in the recent past under the name of stability train", "the proposed method works well on cifar and mnist dataset"], "labels": ["FBK", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my main concerns aret- the adversarial objective and the stability objective are potentially conflict", "indeed when the network misclassifies an example its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version that of the ground truth label", "have the authors considered this issu", "can they elaborate more on how they with thi", "t- it may be significantly more difficult to make this work in such setting due to the dimensionality of the data"], "labels": ["DIS", "DIS", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["did the authors try such experi", "it would be interesting to see these result", "lastly the insights regarding label leaking are not compel", "label leaking is not a mysterious phenomenon", "an adversarially trained model learns on two different distribut"], "labels": ["QSN", "DIS", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the fixed size of the hypothesis space explored ie same architecture used for vanilla and adversarial training it is natural that the statistics of the simpler distribution are captured better by the model", "overall the paper contains valuable information and a method that can contribute to the quest of more robust model", "i lean on accept sid", "strengths* very simple approach amounting to coupled training of e identical copies  of a chosen net architecture whose predictions are fused during train", "this forces the different model instances to become more complementari"], "labels": ["DIS", "DIS", "FBK", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* perhaps counterintuitively experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate train", "* perhaps counterintuitively experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate train", "* the practical advantages of the proposed approach are twofold given a fixed parameter budget coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained net", "* the practical advantages of the proposed approach are twofold given a fixed parameter budget coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained net", "for the same accuracy coupled ensembling yields significant parameter sav"], "labels": ["APC", "DIS", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weaknesses* although results are very strong the proposed models do not outperform the state-of-the-art except for the models reported in table  which however were obtained by *traditional* ensembling of coupled ensembl", "weaknesses* although results are very strong the proposed models do not outperform the state-of-the-art except for the models reported in table  which however were obtained by *traditional* ensembling of coupled ensembl", "weaknesses* although results are very strong the proposed models do not outperform the state-of-the-art except for the models reported in table  which however were obtained by *traditional* ensembling of coupled ensembl", "* coupled ensembling requires joint training of all nets in the ensemble and thus is limited by the size of the model that can be fit in memori", "conversely traditional ensembling involves separate training of the different instances and this enables the learning of an arbitrary number of individual net"], "labels": ["SMY", "DFT", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* i am surprised by the results in table  which suggest that the optimal number of nets in the ensemble is remarkably low only !", "it'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net", "* strictly speaking it is correct to refer to the individual nets in the ensembles as branches and basic block", "nevertheless i find the use of these terms confusing in the context of the proposed approach since they are commonly used to denote concepts different from those represented her", "i would recommend refraining from using these terms her"], "labels": ["CRT", "SMY", "SMY", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the paper provides limited technical novelti", "overall the paper provides limited technical novelti", "yet it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensembl", "the authors propose a decoupled backpropagation method called continuous propagation through the interpretation of backpropagation as a continuous differential system", "because the layer-wise decoupling it can easily be applied for distributed training of the model"], "labels": ["APC", "FBK", "APC", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment result", "although i found the proposed method is interesting enough to investigate more thoroughli", "it is a shame to see the overall quality of the paper very weak", "the writing requires a significant improvement in addition to the overall unclarity of the exposition it sometimes use unexplained abbreviation eg cpgd cp", "the experiments are also very weak"], "labels": ["SMY", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["important information on the experiment settings are missing eg how the model is parallel", "- mini-batch gradient descent mbgd is unfamiliar concept compared to sgd", "it needs to be better defin", "this paper analyzes the loss function and properties of cnns with one wide layer ie a layer with number of neurons greater than the train sample s", "under this and some additional technique conditions the paper shows that this layer can extract linearly independent features and all critical points are local minimum"], "labels": ["DFT", "CRT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i like the presentation and writing of this pap", "however i find it uneasy to fully evaluate the merit of this paper mainly because the wide-layer assumption seems somewhat artificial and makes the corresponding results somewhat expect", "the mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easi", "the mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easi", "this is not surpris"], "labels": ["APC", "CRT", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to make the results more quantitive eg to quantify the tradeoff between having local minimums and having nonzero training error", "the main contribution of the paper seems to be the application to this problem plus minor algorithmic/problem-setting contributions that consist in considering partial observability and to balance multiple object", "on one hand fleet management is an interesting and important problem", "on the other hand although the experiments are well designed and illustr", "the approach is only tested in a small x grid and  agents and in a x grid with  ag"], "labels": ["SUG", "DIS", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in spirit these simulations are similar to those in the original paper by m egorov", "since the main contribution is to use an existing algorithm to tackle a practical application it would be more interesting to tweak the approach until it is able to tackle a more realistic scenario mainly larger scale but also more realistic dynamics with traffic models real data etc", "simulation results compare madqn with dijkstra's algorithm as a baseline which offers a myopic solution where each agent picks up the closest custom", "again since the main contribution is to solve a specific problem it would be worthy to compare with a more extensive benchmark including state of the art algorithms used for this problem eg heuristics and metaheurist", "the paper is clear and well written"], "labels": ["DIS", "DIS", "DIS", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are several minor typos and formatting errors eg at the end of sec  the authors mention figure  which seems to be missing also references [egorov maxim] and [palmer gregory] are bad format", "-- comments and questions to the authors in the introduction please could you add references to what is called traditional solut", "n regarding the partial observability each agent knows the location of all agents including itself and the location of all obstacles and charging locations but it only knows the location of customers that are in its vision rang", "this assumption seems reasonable if a central station broadcasts all agents' positions and customers are only allowed to stop vehicles in the street without ever contacting the central station otherwise if agents order vehicles in advance eg by calling or using an app the central station should be able to communicate customers locations too", "on the other hand if no communication with the central station is allowed then positions of other agents may be also partial observ"], "labels": ["CRT", "DFT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in other words the proposed partial observability assumption requires some further motiv", "moreover in sec  it is said that agents can see around them + spaces away however experiments are run in x and x grid worlds meaning that the agents are able to observe the grid complet", "the fact that partial observability helped to alleviate the credit-assignment noise caused by the missing customer penalty might be an artefact of the set", "for instance since the reward has been designed arbitrarily it could have been defined as giving a penalty for those missing customers that are at some distance of an ag", "please could you explain the last sentence of sec  that says the drawback here is that the agents will not be able to generalize to other unseen maps that may have very different geographies in particular how is this sentence related to partial observ"], "labels": ["DFT", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes sensor transformation attention network stan which dynamically select appropriate sequential sensor inputs based on an attention mechan", "prosone of the main focuses of this paper is to apply this method to a real task multichannel speech recognition based on chime- by providing its reasonable sensor selection function in real data especially to avoid audio data corrupt", "this analysis is quite intuitive and also shows the effectiveness of the proposed method in this practical setup", "consthe idea seems to be simple and does not have significant origin", "also the paper does not clearly mention the attention mechanism part and needs some improv"], "labels": ["SMY", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments-tthe paper mainly focuses on the soft sensor select", "however in an array signal processing context and its application to multichannel speech recognition it would be better to mention beamforming techniques where the compensation of the delays of sensors is quite import", "-tin addition there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by ochiai et al a unified architecture for multichannel end-to-end speech recognition with neural beamforming ieee journal of selected topics in signal process", "this paper uses the same chime- database and also showing a similar analysis of channel select", "itus better to discuss about this paper as well as a refer"], "labels": ["SMY", "SUG", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tsection  better to explain about how to obtain attention scores z in more detail", "-tsection  better to explain about how to obtain attention scores z in more detail", "-tfigure  experiments of double audio/video clean conditions i cannot understand why they are improved from single audio/video clean conditionsneed some explan", "-tsection  -dimensional mel-frequency cepstral coefficients mfccs -  -dimensional mel-frequency cepstral coefficients mfccs with st and nd order delta featur", "-tsection  dataset ucas for tidigitud ucas for gridud"], "labels": ["SUG", "DFT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tsection  models ucthe parameters of the attention modules are either shared across sensors stan-shared or not shared across sensors stan- default", "ud itus better to explain this part in more details possibly with some equations it is hard to understand the differ", "ud itus better to explain this part in more details possibly with some equations it is hard to understand the differ", "this paper provides visualizations of different deep network loss surfaces using d contour plots both at minima and along optimization trajectori", "they mention some subtle details that must be taken into account such as scaling the plot axes by the filter magnitudes in order to obtain correctly scaled plot"], "labels": ["CRT", "SUG", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think there is potential with this work but it feels preliminari", "the visualizations are interesting and provide some general intuition but they don't yield any clear novel insights that could be used in practic", "the visualizations are interesting and provide some general intuition but they don't yield any clear novel insights that could be used in practic", "the visualizations are interesting and provide some general intuition but they don't yield any clear novel insights that could be used in practic", "also several parts of the paper spend too much time on describing other work or on implementation details which could be moved to the appendix"], "labels": ["SMY", "SMY", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["general comments- i think sections    are too long we only start getting to the results section at the end of pag", "i suggest shortening section  and it should be possible to combine sections  and  into a page at most", "d interpolations and d contour plots can be described in a few sentences each", "- i think section  can be put in the appendix - it's essentially an illustration of why the weight scaling is import", "once these details are done correctly the experiments support the relatively well-accepted hypothesis that flat minima generalize bett"], "labels": ["DFT", "SUG", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the plots in section  are interesting it would be nice if the authors had an explanation of why the loss surface changes the way it does when skip connections are ad", "- in section  it's less useful to spend time describing what happens when the visualization is done wrong ie projecting along random directions rather than pca vectors -  this can be put in the appendix", "i would suggest just including the visualizations of the optimization trajectories which are done correctly and focus on deriving interesting/useful conclusions from them", "i would suggest just including the visualizations of the optimization trajectories which are done correctly and focus on deriving interesting/useful conclusions from them", "the primary intellectual point the authors make is that previous networks for machine comprehension are not fully attent"], "labels": ["APC", "SUG", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that is they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level", "the network proposed here fusionhet fixes problem", "importantly the model achieves state-of-the-art performance of the squad dataset", "the paper is very well-written and easy to follow", "i found the architecture very intuitively laid out even though this is not my area of expertis"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover i found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work!", "what most impressed me however was the literature review", "perhaps this is facilitated by the squad leaderboard which makes it simple to list related work", "nevertheless i am not used to seeing comparison to as many recent systems as are presented in t", "all in all it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset"], "labels": ["APC", "APC", "APC", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary the authors present a simple variation of vanilla recurrent neural networks which use relu hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix", "this identity connection acts as a ucsurrogate memoryud component preserving hidden activations over time step", "the experiments demonstrate that this architecture reliably solves the addition task for up to  input fram", "it also achieves a very good performance on sequential and permuted mnist and achieves sota performance on babi", "the authors observe that the proposed recurrent identity network rin is relatively robust to hyperparameter choic"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after le et al  the paper presents another convincing case for the application of relus in rnn", "review i very much like the pap", "the motivation and architecture is presented very clearly and i am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!", "i have a few comments and questions clarification in section  do you really mean bit-wise multiplication or element-wis", "if bit-wise can you elaborate whi"], "labels": ["APC", "APC", "APC", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i might have missed someth", "why does the learning curve of the irnn stop around epoch  in figure c", "also some curves in the appendix stop abruptly without visible explos", "were these experiments run until complet", "if so would it be possible to plot the complete curv"], "labels": ["CNT", "QSN", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think for a fair comparison with lstms and irnns a limited hyperparameter search should be performed separately on all three architectures at least for the addition task", "optimal hyperparameters are usually model-specif", "admittedly the authors mention that they do not intend to make claims about superior performance to lstms however the competitive performance of small rins is mentioned a couple of times in the manuscript", "le et al  for instance perform a coarse grid search for each model", "i wouldn't say that resnets are gated neural networks as the branches are just summed up"], "labels": ["SUG", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no multiplicative gating as in highway network", "i think what enables the training of very deep networks or lstms on long sequences is the presence of a close-to-identity component in forward/backward propagation not the g", "the use of relu activations in irnns with identity initialization of the hidden-to-hidden weights and rins effectively initialized with identity plus some noise makes the recurrence more linear than with squashing activation funct", "regarding the absence of gating in rins what is your intuition on how the model would perform in tasks for which conditional forgetting is us", "consider for example a task with long sequences outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activ"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would rins readily learn to reset parts of the hidden st", "henaff et al  might be related as they are also looking into the addition task with long sequencesoverall the presented idea is novel to the best of my knowledge and the manuscript is well-written i would recommend it for acceptance but would like to see the above points addressed especially - and some comments on -", "after a revision i would consider to increase the scor", "referenceshenaff mikael arthur szlam and yann lecun", "recurrent orthogonal networks and long-memory task"], "labels": ["QSN", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in international conference on machine learning pp -", "le quoc v navdeep jaitly and geoffrey e hinton", "a simple way to initialize recurrent networks of rectified linear units arxiv preprint arxiv", "summarythis paper- provides a compehensive review of existing techniques for verifying properties of neural network", "- introduces a simple branch-and-bound approach"], "labels": ["DIS", "DIS", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- provides fairly extensive experimental comparison of their method and  others reluplex planet mip on  existing benchmarks and a new synthetic on", "relevance although there isn't any learning going on the paper is relevant to the confer", "clarity writing is excellent the content is well presented and the paper is enjoyable read", "soundness as far as i can tell the work is sound", "novelty this is in my opinion the weakest point of the pap"], "labels": ["SMY", "FBK", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there isn't really much novelty in the work", "the branch&bound method is fairly standard two benchmarks were already existing and the third one is synthetic with weights that are not even trained so not clear how relevant it i", "the main novel result is the experimental comparison which does indeed show some surprising results like the fact that bab works so wel", "significance there is some value in the experimental results and it's great to see you were able to find bugs in existing method", "unfortunately there isn't much insight to be gained from them"], "labels": ["CRT", "CRT", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i couldn't see any emerging trend/useful recommendations like if your problem looks like x then use algorithm b", "this is unfortunately often the case when dealing with combinatorial search/optim", "this paper introduces minimax curriculum learning as an approach for adaptively train models by providing it different subsets of data", "the authors formulate the learning problem as a minimax problem which tries to choose diverse example and hard examples where the diversity is captured via a submodular loss function and the hardness is captured via the loss funct", "the authors formulate the problem as an iterative technique which involves solving a minimax objective at every iter"], "labels": ["CRT", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors argue the convergence results on the minimax objective subproblem but do not seem to give results on the general problem", "the ideas for this paper are built on existing work in curriculum learning which attempts to provide the learner easy examples followed by harder examples later on", "the belief is that this learning style mimics human learn", "pros- the analysis of the minimax objective is novel and the proof technique introduces several interesting idea", "- this is a very interesting application of joint convex and submodular optimization and uses properties of both to show the final convergence result"], "labels": ["CRT", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- even through the submodular objective is only approximately solvable it still translates into a convergence result", "- the experimental results seem to be complete for the most part", "they argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tri", "cons- the main algorithm mcl is only a huerist", "though the minimax subproblem can converge the authors use this in somewhat of a hueristic mann"], "labels": ["DIS", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it seems somewhat hand wavy in the way the authors describe the hyper parameters of mcl and it seems unclear when the algorithm converge and how to increase/decrease it over iter", "- the objective function also seems somewhat non-intuit", "though the experimental results seem to indicate that the idea works i think the paper does not motivate the loss function and the algorithm wel", "- it seems to me the authors have experimented with smaller datasets cifar mnist newsgroup", "this being mainly an empirical paper i would have expected results on a few larger datasets eg imagenet celebfaces etc particularly to see if the idea also scales to these more real world larger dataset"], "labels": ["CRT", "SMY", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i would like to see if the paper could have been stronger empir", "nevertheless i do think there are some interesting ideas theoretically and algorithm", "for this reason i vote for a borderline accept", "this paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tre", "the contributions of the work are very limit"], "labels": ["SUG", "APC", "FBK", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["basic attention models which have been shown to help model structures are not included or compar", "method-wise the encoder is not novel and decoder is rather straightforward", "the contributions of the work are in general very limit", "moreover this manuscript contains many grammatical error", "in general it is not ready for publ"], "labels": ["CRT", "CRT", "CRT", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- investigating the ability of distributed representation in encoding input structured is in general interest", "although there have been much previous work this paper is along this lin", "cons- the contributions of the work are very limit", "for example attention which have been widely used and been shown to help capture structures in many tasks are not included and compared in this pap", "- evaluation is not very convinc"], "labels": ["APC", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the baseline performance in mt is too low", "it is unclear if the proposed model is still helpful when other components are considered eg attent", "- for the objective function defined in the paper it may be hard to balance the structure loss and content loss in different problems and moreover the loss function may not be even useful in real tasks eg in mt which often have their own objectives as discussed in this pap", "earlier work on tree kernels in terms of defining tree distances may be related to this work", "- the manuscript is full of grammatical errors and the following are some of themencoder only only need tofor for tree reconstruction task"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the socher et al b propose a basic form", "experiments and theroy analysis are don", "proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the sam", "as authors note this constraint can be converted into a kkt style penalty with kkt multiplier lambda", "thus this is very  similar to other regularizers that increase smoothness of the function such as total variation or a graph laplacian defined with graph edges connecting the examples in each group as well as manifold regularization see eg belkin niyogi et al jmlr"], "labels": ["DIS", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["heck in practie ridge regularization will also do something similar for many function class", "experiments didn't compare to any similar smoothness regularization and my preferred would have been a comparison to graph laplacian or total variation on graphs formed by the same clustered exampl", "it's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same result", "that made it hard to get excited about the results in a vacuum", "would this proposed strategy have thwarted the russian tank legend problem"], "labels": ["SMY", "DIS", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it have fixed the google gorilla problemwhy or why not", "overall i found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of exampl", "page  calling additional instances of the same person uccounterfactual observationsud didnut seem consistent with the usual definition of that termu maybe i am just missing the semantic link here but this isn't how we usually use the term counterfactual in my corner of the field", "re ucone creates additional samples by modifyinguud be nice to quote more of the early work doing this i believe the first work of this sort was scholkopfus he called it ucvirtual examplesud and ium pretty sure he specifically did it for rotation mnist images and if not exactly that it was impli", "i think the right citation is ucincorporating invariances in support vector learning machinesuc scholkopf burges vapnik  but also see decoste * scholkopf  uctraining invariant support vector machinesud"], "labels": ["QSN", "APC", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of using cross-task transfer performance to do task clustering is not new", "please refer to the paper ucdiscovering structure in multiple learning tasks the tc algorithmud published in icml", "one issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks which is useful for learning from multiple task", "for example in binary classification tasks a very small s_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each oth", "so the use of cross-task transfer performance and the task clustering approach can only capture positive correlations between tasks but ignore the negative task relations which are also important to the sharing among tasks in multi-task learn"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["problem  is identical to robust pca and theorem  is common in matrix completion literatur", "i donut see much novelti", "appendix a seems obvious but it cannot prove the validity of the assumption made in problem", "based on previous works such as ucmulti-task sparse structure learning with gaussian copula modelsud and uclearning sparse task relations in multi-task learningud when the number of tasks is large the task relation exhibits the sparse structur", "i donut know whether the low-rank structure does exist in the cross-task transfer performance or not"], "labels": ["CRT", "CRT", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the two parts in this paper are not new", "the combination of the two parts seems a bit incremental and does not bring much novelti", "pros a new dna structure gan is utilized to manipulate/disentangle attribut", "non attribute part z is explicitly modeled in the framework", "based on the experiment results this proposed method outperformed previous methods td-gan icgan"], "labels": ["CRT", "CRT", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons it assumes that each individual piece represents an independent factor of variation which can not hold all the tim", "the authors also admit that when two factors are dependent this method might fail", "in lreconstruct only min difference between a and a is consid", "how about a and a her", "it seems that a should also be similar with a since only one bit in a and a is differ"], "labels": ["CRT", "CRT", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["only one attribute can be manipulated each tim", "is it possible to change more than one attribute each time in this method", "update following the author's response i've increased my score from  to", "the revised paper includes many of the additional references that i suggested and the author response clarified my confusion over the charades experiments their results are indeed close to state-of-the-art on charades activity localization slightly outperformed by [] which i had mistakenly confused with activity classification from []", "the paper proposes the skip rnn model which allows a recurrent network to selectively skip updating its hidden state for some inputs leading to reduced computation at test-tim"], "labels": ["QSN", "QSN", "FBK", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at each timestep the model emits an update probability if this probability is over a threshold then the next input and state update will be skip", "the use of a straight-through estimator allows the model to be trained with standard backpropag", "the number of state updates that the model learns to use can be controlled with an auxiliary loss funct", "experiments are performed on a variety of tasks demonstrating that the skip-rnn compares as well or better than baselines even when skipping nearly half its state upd", "pros- task of reducing computation by skipping inputs is interest"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- model is novel and interest", "- experiments on multiple tasks and datasets confirm the efficacy of the method", "- skipping behavior can be controlled via an auxiliary loss term", "- paper is clearly written", "cons- missing comparison to prior work on sequential mnist"], "labels": ["APC", "APC", "DIS", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- low performance on charades dataset no comparison to prior work", "- low performance on charades dataset no comparison to prior work", "- no comparison to prior work on imdb sentiment analysis or ucf- activity classif", "the task of reducing computation by skipping rnn inputs is interesting and the proposed method is novel interesting and clearly explain", "experimental results across a variety of tasks are convincing in all tasks the skip-rnns achieve their goal of performing as well or better than equivalent non-skipping vari"], "labels": ["DFT", "CRT", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the use of an auxiliary loss to control the number of state updates is interest", "since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuraci", "however where possible experiments should compare directly with prior published results on these tasks none of the experiments from the main paper or supplementary material report any numbers from any other published work", "however where possible experiments should compare directly with prior published results on these tasks none of the experiments from the main paper or supplementary material report any numbers from any other published work", "on permuted mnist table  could include results from [-]"], "labels": ["APC", "APC", "DFT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["of particular interest is [] which reports % accuracy with a -unit lstm initialized with orthogonal and identity weight matrices this is significantly higher than all reported results for the sequential mnist task", "for charades all reported results appear significantly lower than the baseline methods reported in [] and [] with no explan", "all methods work on ucfc features from the rgb stream of a two-stream cnn provided by the organizers of the [charades] challengeud and the best-performing method skip gru achieves  map", "this is significantly lower than the two-stream results from []  map and  map and also lower than pretrained alexnet features averaged over  frames and classified with a linear svm which [] reports as achieving  map", "i donut expect to see state-of-the-art performance on charades the point of the experiment is to demonstrate that skip-rnns perform as well or better than their non-skipping counterparts which it do"], "labels": ["APC", "CRT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am surprised at the low absolute performance of all reported results and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something els", "in a similar vein from the supplementary material sentiment analysis on imdb and action classification on ucf- are well-studied problems but the authors do not compare with any previously published results on these task", "though experiments may not show show state-of-the-art performance i think that they still serve to demonstrate the utility of the skip-rnn architecture when compared side-by-side with a similarly tuned non-skipping baselin", "however i feel that the authors should include some discussion of other published result", "on the whole i believe that the task and method are interesting and experiments convincingly demonstrate the utility of skip-rnns compared to the authorus own baselin"], "labels": ["CRT", "CRT", "APC", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experi", "references[] le et al uca simple way to initialize recurrent networks of rectified linear unitsud arxiv [] arjovsky et al ucunitary evolution recurrent neural networksud icml [] cooijmans et al ucrecurrent batch normalizationud iclr [] zhang et al ucarchitectural complexity measures of recurrent neural networksud nips [] sigurdsson et al uchollywood in homes crowdsourcing data collection for activity understandingud eccv [] sigurdsson et al ucasynchronous temporal fields for action recognitionud cvpr", "this is an interesting paper exploring gan dynamics using ideas from online learning in particular the pioneering sparring follow-the-regularized leader analysis of freund and schapire using what is listed here as lemma", "by restricting the discriminator to be a single layer the maximum player plays over a concave parameter space which stabilizes the full sequence of losses so that lemma  can be proved allowing proof of the dynamics' convergence to a nash equilibrium", "the analysis suggests a practical heuristic algorithm incorporating two features which emerge from the theory l regularization and keeping a history of past model"], "labels": ["FBK", "DIS", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a very simple queue for the latter is shown to do quite competitively in practic", "this paper merits acceptance on theoretical merits alone because the ftrl analysis for convex-concave games is a very robust tool from theory see also the more recent sequel [syrgkanis et al  fast convergence of regularized learning in games] that is natural to employ to gain insight on the much more brittle gan cas", "the practical aspects are also interesting because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains these ideas could clearly be developed in future work", "summarythis paper proposes a data augmentation method for one-shot learning of image class", "this is the problem where given just one labeled image of a class the aim is to correctly identify other images as belonging to that class as wel"], "labels": ["SMY", "APC", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea presented in this paper is that instead of performing data augmentation in the image space it may be useful to perform data augmentation in a latent space whose features are more discriminative for classif", "one candidate for this is the image feature space learned by a deep network", "however they advocate that a better candidate is what they refer to as semantic space formed by embedding the word labels of the images according to pre-trained language models like wordvec", "the reasoning here is that the image feature space may not be semantically organized so that we are not guaranteed that a small perturbation of an image vector will yield image vectors that correspond to semantically similar images belonging to the same class", "on the other hand in this semantic space by construction we are guaranteed that similar concepts lie near by each oth"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus this space may constitute a better candidate for performing data augmentation by small perturbations or by nearest neighbour search around the given vector since  the augmented data is more likely to correspond to features of similar images as the original provided image and  it is more likely to thoroughly capture the intra-class variability in the augmented data", "the authors propose to first embed each image into a feature space and then feed this learned representation into a auto-encoder that handles the projection to and from the semantic space with its encoder and decoder respect", "specifically they propose to perform the augmentation on the semantic space representation obtained from the encoder of this autoencod", "this involves producing some additional data points either by adding noise to the projected semantic vector or by choosing a number of that vector's nearest neighbour", "the decoder then maps these new data points into feature space obtaining in this way the image feature representations that along with the feature representation of the original real image will form the batch that will be used to train the one-shot classifi"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they conduct experiments in  datasets where they experiment with augmentation in the image feature space by random noise as well as the two aforementioned types of augmentation in the semantic spac", "they claim that these augmentation types provide orthogonal benefits and can be combined to yield superior result", "overall i think this paper addresses an important problem in an interesting way", "but there is a number of ways in which it can be improved detailed in the comments below", "comments-- since the authors are using a pre-trained vgg for to embed each image i'm wondering to what extent they are actually doing one-shot learning her"], "labels": ["DIS", "DIS", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in other words the test set of a dataset that is used for evaluation might contain some classes that were also present in the training set that vgg was originally trained on", "it would be useful to clarify whether this is happen", "can the vgg be instead trained from scratch in an end-to-end way in this model", "-- a number of things were unclear to me with respect to the details of the training process the feature extractor vgg is pre-train", "is this finetuned during train"], "labels": ["DIS", "DIS", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if so is this done jointly with the training of the auto-encod", "further is the auto-encoder trained separately or jointly with the training of the one-shot learning classifi", "-- while the authors have convinced me that data augmentation indeed significantly improves the performance in the domains considered based on the results in table  and figure a", "i am not convinced that augmentation in the proposed manner leads to a greater improvement than just augmenting in the image feature domain", "in particular in table  where the different types of augmentation are compared against each other we observe similar results between augmenting only in the image feature space versus augmenting only in the semantic feature space ie we observe that featg performs similarly as semg and as semn"], "labels": ["QSN", "QSN", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when combining multiple types of augmentation the results are bett", "but i'm wondering if this is because more augmented data is used overal", "specifically the authors say that for each image they produce  additional virtual data points but when multiple methods are combined does this mean  from each method or  overall if it's the former the increased performance may merely be attributed to using more data", "it is important to clarify this point", "-- comparison with existing work there has been a lot of work recently on one-shot and few-shot learning that would be interesting to compare against"], "labels": ["APC", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular mini-imagenet is a commonly-used benchmark for this task that this approach can be applied to for comparison with recent methods that do not use data augment", "some examples are- model-agnostic meta-learning for fast adaptation of deep network", "finn et al- prototypical networks for few-shot learning snell et al- matching networks for one-shot learning vinyals et ", "- few-shot learning through an information retrieval lens triantafillou et ", "-- a suggestion as future work i would be very interested to see if this method can be incorporated into common few-shot learning models to on-the-fly generate additional training examples from the support set of each episode that these approaches use for train"], "labels": ["DIS", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes a regularizer that encourages a gan discriminator to focus its capacity in the region around the manifolds of real and generated data points even when it would be easy to discriminate between these manifolds using only a fraction of its capacity so that the discriminator provides a more informative signal to the gener", "the regularizer rewards high entropy in the signs of discriminator activ", "experiments show that this helps to prevent mode collapse on synthetic gaussian mixture data and improves inception scores on cifar", "the high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge though i am not an expert in this spac", "figure ` is a fantastic illustration that presents the core idea very clearli"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that said i found the intuitive story a little bit difficult to follow", "-- it's true that in figure b the discriminator won't communicate the detailed structure of the data manifold to the generator but it's not clear why this would be a problem", "-- the gradients should still pull the generator *towards* the manifold of real data and as this happens and the manifolds begin to overlap the discriminator will naturally be forced to allocate its capacity towards finer-grained detail", "is the implicit assumption that for real high-dimensional data the generator and data manifolds will *never* overlap", "but in that case much of the theoretical story goes out the window"], "labels": ["CRT", "CRT", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'd also appreciate further discussion of the relationship of this approach to wasserstein gans which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap", "i'd also appreciate further discussion of the relationship of this approach to wasserstein gans which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap", "more generally i'd like to better understand what effect we'd expect this regularizer to hav", "it appears to be motivated by improving training dynamics which is understandably a significant concern", "does it also change the location of the nash equilibria"], "labels": ["APC", "DIS", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or equivalently the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https//arxivorg/abs/", "i'd expect that it would but the effects of this changed objective are not discussed in the pap", "the experimental results seem promising although not earthshatt", "i would have appreciated a comparison to other methods for guiding discriminator representation capacity eg autoencoding i'd also imagine that learning an inference network eg bigan might serve as a useful auxiliary task", "overall this feels like an cute hack supported by plausible intuition but without deep theory or compelling results on real tasks yet"], "labels": ["SUG", "CRT", "SMY", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as such i'd rate it as borderline though perhaps interesting enough to be worth presenting and discuss", "a final note this paper was difficult to read due to many grammatical errors and unclear or misleading constructions as well as missing citations eg sec", "from the second paragraph aloneimpede their wider applications in new data domain - domainsextreme collapse and heavily oscillation - heavy oscillationmodes of real data distribution - modes of the real data distributionwhile d fails to exploit the failure to provide better training signal to g - should be this failure to refer to the previously-described generator mode collapse or rewrite entirelyeven when they are their jensen-shannon divergence - even when their jensen-shannon divergence i'm sympathetic to the authors who are presumably non-native english speak", "many good papers contain mistakes but in my opinion the level in this paper goes beyond what is appropriate for published work", "i encourage the authors to have the work proofread by a native speaker clearer writing will ultimately increase the reach and impact of the pap"], "labels": ["FBK", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper studies the generalization properties of -layer neural networks based on fourier analysi", "studying the generalization property of neural network is an important problem and fourier-based analysis is a promising direction as shown in lee et ", "however i am not satisfied with the results in the current vers", "the main theoretical results are on the sin activation functions instead of commonly used relu funct", "even if for sin activation functions the analysis is not complet"], "labels": ["SMY", "SMY", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors claimed in the abstract that gradient-based methods will converge to generalizable local minima", "however corollary  is only a concentration bound on the gradi", "there is a gap that how this corollary implies gener", "the paragraph below this corollary is only a high level intuit", "the paper proposes to improve the kernel approximation of random features by using quadratures in particular stochastic spherical-radial rul"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the quadrature rules have smaller variance given the same number of random features and experiments show its reconstruction error and classification accuracies are better than existing algorithm", "it is an interesting paper but it seems the authors are not aware of some existing works [ ] on quadrature for random featur", "given these previous works the contribution and novelty of the paper is limit", "[] francis bach on the equivalence between kernel quadrature rules and random feature expansions jmlr [] tri dao christopher de sa christopher rue gaussian quadrature for kernel features nip", "the paper aims tackles the problem of generate vectorized sketch drawings by using a rnn-variational autoencod"], "labels": ["DIS", "CRT", "DFT", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["each node is represented with dx dy along with one-hot representation of three different drawing statu", "a bi-directional lstm is used to encode latent space in the training stag", "auto-regressive vae is used for decod", "similar to standard vaes log-likelihood has bee used as the data-term and the kl divergence between latent space and gaussian prior is the regularisation term", "pros- good solution to an interesting problem"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- very interesting dataset to be releas", "- intensive experiments to validate the perform", "cons- i am wondering whether the dataset contains biases regarding dx di", "in the data collection stage how were the points lists generated from pen strok", "did each points are sampled from same travelling distance or according to the same time interv"], "labels": ["APC", "APC", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are there any other potential biases brought because the data collection tool", "- is log-likelihood a good loss her", "think about the case where the sketch is exactly the same but just more points are densely sampled along the pen strok", "how do you deal with this cas", "- does the dataset contain more meta-info that could be used for other tasks beyond generation eg segmentation classification identification etc"], "labels": ["QSN", "QSN", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interfac", "similar to image captioning systems the image is processed with a cnn and an lstm is used to output tokens one at a tim", "experiments are performed on three new synthetic datasets of user interfaces for ios android and html/css which will be publicly releas", "experiments are performed on three new synthetic datasets of user interfaces for ios android and html/css which will be publicly releas", "pros- generating programs with neural networks is an exciting direct"], "labels": ["SMY", "DIS", "SMY", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- novel task of generating ui code from ui screenshot", "- three new datasets of ui images and corresponding cod", "n- paper is clearly written", "ncons- limited technical novelt", "y- limited experi"], "labels": ["APC", "APC", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research", "generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest", "the main novelty of the paper is the task itself and the three synthetic datasets created to study the task", "my main concern with this paper is a lack of technical novelti", "the model combines a cnn with an lstm and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now"], "labels": ["APC", "APC", "APC", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ideally i would have liked to see cnn+lstm as a baseline together with some technical innovations that specialize this general model to the particular task at hand", "the experiments in this paper are also lack", "given that the main contribution of the paper is the pixcode task and datasets i would have liked to see more thorough experi", "the only model tested is cnn+lstm with various beam sizes and performance is only demonstrated through overall accuracy and qualitative exampl", "i would have liked to see comparisons with other methods such as nearest neighbor or other retrieval-based method"], "labels": ["DFT", "DFT", "DFT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would have also liked to see more innovation in evalu", "are there metrics other than overall accuracy that could be used to measure perform", "compared to other tasks like image captioning can you design metrics that capture the particular challenges involved in the pixcode task", "in general in what types of circumstances does your model succeed or fail and can you capture this quantitatively through carefully designed metr", "since the data is synthetic could you generate different datasets of increasing complexity and measure performance as complexity increas"], "labels": ["DFT", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how does performance change with different amounts of training data", "would it be possible to somehow transfer knowledge of ui across datasets where you pretrain on one dataset and somehow finetune on anoth", "i donut expect the authors to answer any of these questions in particular", "i list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset", "on the whole i appreciate the novelty of the task and dataset"], "labels": ["QSN", "QSN", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the paper suffers from a lack of technical novelty in the model and limited experimental valid", "this paper presents some reviews on clustering methods with deep learn", "based on the review taxonomy the authors presents a mixed objective which aims for bretter clustering perform", "the proposed method is then tested on two image data set", "the claimed main contribution of the paper is the taxonomi"], "labels": ["DFT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are no new things in such kind of review", "there are no new things in such kind of review", "the taxonomy gives no scientific axiom", "therefore the impact or actual contribution to the iclr community is very limit", "the proposed clustering method is problemat"], "labels": ["DFT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is hard to set the paramter alpha", "the experimental results are also disappoint", "for example the coil accuracy is only  much worse than the state of the art", "moreover results on only two image data sets are not sufficient for convinc", "summary the authors take two pages to describe the data they eventually analyze - chinese license plates sections  with the aim of predicting auction price based on the luckiness of the license plate numb"], "labels": ["CRT", "CRT", "CRT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors mentions other papers that use nn's to predict prices contrasting them with the proposed model by saying they are usually shallow not deep and only focus on numerical data not str", "then the paper goes on to present the model which is just a vanilla rnn with standard practices like batch normalization and dropout", "the proposed pipeline converts each character to an embedding with the only sentence of description being each character is converted by a lookup table to a vector representation known as character embed", "specifics of the data  rnn training and the results as well as the stability of the network to hyperparameters is also examin", "finally they find a a feature vector for each plate by summing up the output of the last recurrent layer overtim"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and the use knn on these features to find other plates that are grouped together to try to explain how the rnn predicts the prices of the pl", "in section   the rnn is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally bett", "specific comments on sections comments sec in these sections the author has somewhat odd references to specific economists that seem a little off topic and spends a little too much time in my opinion setting up this specific data", "sec the author does not mention the following reference deep learning for stock prediction using numerical and textual information by akita et al that does incorporate non-numerical info to predict stock prices with deep network", "sec what are the characters embedded with this is important to specifi"], "labels": ["SMY", "DIS", "CRT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sec what are the characters embedded with this is important to specifi", "is it wordvec or something els", "what does the lookup table consist of", "references should be added to the relevant method", "sec i feel like there are many regression models that could have been tried here with wordvec embeddings that would have been an interesting comparison"], "labels": ["QSN", "QSN", "QSN", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["lstms as well could have been a point of comparison", "sec  nothing too insightful is said about the rnn model", "sec  nothing too insightful is said about the rnn model", "sec the ensembling was a strange extension especially with the woo model given that the other mlp architecture gave way better results in their t", "overall this is a unique nlp problem and it seems to make a lot of sense to apply an rnn here considering that wordvec is an rnn"], "labels": ["SUG", "DFT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however comparisons are lacking and the paper is not presented very scientif", "however comparisons are lacking and the paper is not presented very scientif", "the lack of comparisons made it feel like the author cherry picked the rnn to outperform other approaches that obviously would not do wel", "this paper investigates the impact of noisy input on machine translation and tests simple ways to make nmt models more robust", "overall the paper is a clearly written well described report of several experi"], "labels": ["DFT", "CRT", "CRT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it shows convincingly that standard nmt models completely break down on both natural noise and various types of input perturb", "it then tests how the addition of noise in the input helps robustify the charcnn model somewhat", "the extent of the experiments is quite impressive three different nmt models are tried and one is used in extensive experiments with various noise combin", "this study clearly addresses an important issue in nmt and will be of interest to many in the nlp commun", "the outcome is not entirely surprising noise hurts and training and the right kind of noise help"], "labels": ["APC", "DIS", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the impact may b", "i wonder if you could put this in the context of training with input noise which has been studied in neural network for a while at least since the ", "ie it could be that each type of noise has a different regularizing effect and clarifying what these regularizers are may help understand the impact of the various types of nois", "also the bit of analysis in sections  and  is promising if maybe not so conclusive yet", "a few constructive criticismsthe way noise is included in training sec  could be clarified unless i missed it eg are you generating a fixed noisy training set and adding that to clean data"], "labels": ["APC", "DIS", "DIS", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or introducing noise on-line as part of the train", "if fixed what sizes were tri", "more information on the experimental design would help", "table  is highly suspect some numbers seem to have been copy-pasted in the wrong cells eg the rand line for german or the swap/mid/rand lines for czech", "it's highly unlikely that training on noisy swap data would yield a boost of + bleu points on czech -- or you have clearly found a magical way to improve perform"], "labels": ["QSN", "QSN", "SUG", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the amount of experiment is already important it may be interesting to check whether all seseq models react similarly to training with noise it could be that some architecture are easier/harder to robustify in this basic way", "[response read -- thanks]i agree with authors that this paper is suitable for iclr although it will clearly be of interest to acl/mt-minded folk", "the authors present a method to enable robust generation of adversarial visualinputs for image classif", "they develop on the theme that 'real-world' transformations typically provide acountermeasure against adversarial attacks in the visual domain to show thatcontextualising the adversarial exemplar generation by those verytransformations can still enable effective adversarial example gener", "they adapt an existing method for deriving adversarial examples to act under aprojection space effectively a latent-variable model which is defined througha transformations distribut"], "labels": ["DIS", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they demonstrate the effectiveness of their approach in the d and dsimulated and real domain", "the paper is clear to follow and the objective employed appears to be sound", "ilike the idea of using d generation and particularly d printing as a meansof generating adversarial examples -- there is definite novelty in thatparticular exploration for adversarial exampl", "i did however have some concerns what precisely is the distribution of transformations used for each", "experi"], "labels": ["SMY", "APC", "APC", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is it a pcfg", "are the different components quantised such that   they are discrete rvs or are there still continuous rvs for example is   lighting discretised to particular locations or taken to be say a d   gaussian", "and on a related note how were the number of sampl", "transformations chosen", "knowing the distribution and the extent of it's support can help situate   the effectiveness of the number of samples taken to derive the adversarial   input"], "labels": ["QSN", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while choosing the distance metric in transformed space lab is used but   for the experimental results l_ is measured in rgb space -- showing the   rgb distance is perhaps not all that useful given it's not actually being   used in the object", "i would perhaps suggest showing lab maybe in   addition to rgb if requir", "quantitative analysis i would suggest reporting confidence intervals   perhaps just the st standard deviation over the accuracies for the true and   'adversarial' labels -- the min and max don't help too much in understand", "quantitative analysis i would suggest reporting confidence intervals   perhaps just the st standard deviation over the accuracies for the true and   'adversarial' labels -- the min and max don't help too much in understand", "n   what effect the monte-carlo approximation of the objective has on th"], "labels": ["CRT", "SUG", "SUG", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover the min and max are only reported for the d and rendered d   experiments -- it's missing for the d printing experi", "experiment power while the experimental setup seems well thought out and   structured the sample size ie the number of entities considered seems a   bit too small to draw any real conclusions from", "there are  exemplar   objects for the d rendering experiment and only  for the d printing on", "while i understand that d printing is perhaps not all that scalable to be   able to rattle off many models the d rendering experiment surely can be   extended to include more model", "were the turtle and baseball models chosen   randomly or chosen for some particular reason"], "labels": ["DFT", "DFT", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["similar questions for the    models in the d rendering experi", "d printing experiment transformations while the d and d rendering   experiments explicitly state that the sampled transformations were random   the d printing one says over a variety of viewpoint", "were these   viewpoints chosen randomli", "most of these concerns are potentially quirks in the exposition rather than anyissues with the experiments conducted themselv", "for now i think thesubmission is good for a weak accept"], "labels": ["QSN", "QSN", "QSN", "QSN", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u- if the authors address my concerns and/orcorrect my potential misunderstanding of the issues i'd be happy to upgrade myreview to an accept", "the papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density funct", "a monte carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead predict", "the approach is tested on two artificially generated datasets and on two real-world datasets and compared with standard approaches such as the autoregressive model the kalman filter and a regression lstm", "nthe paper is quite dense and quite difficult to follow also due to the complex notation used by the author"], "labels": ["FBK", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe comparison with other methods is very week the authors compare their approach with two very simple alternatives namely a first-order autoregressive mode and the kalman filt", "more sophisticated should have been employ", "summaryauthors propose a method which uses a q-learning-based high-level policy which is combined with a contextual mask derived from safety-contraints and low-level controllers which disable certain actions from being selectable at certain st", "the high-level policy is learnt via fairly standard q-learning epsilon-greedy exploration policy and a nn function approxim", "experiments in a simple car simulator on a task which requires the car to take a certain exit while navigating through traffic are presented with two baselines  a greedy policy which navigates to the right-most lane asap and then follows traffic till the exit is reach"], "labels": ["CRT", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["human subjects driving cars in the simul", "comments- why use a model-free technique like q-learning especially when one knows the model of the car in autonomous driving setting and can simply run model-predictive control mpc convolve forward the model to get candidate trajectories of certain reasonable horizon evaluate and pick the best trajectory execute selected trajectory for a few time-steps and then rinse-and-repeat", "this is a very well-accepted method actually used in real-world autonomous car", "see the urmson et al  paper in the bibliography at the very least this technique should be a baselin", "this method is not learning-based doesn't need training data in a simulator generalizes to **any** exit and lane configuration and variants of this basic technique continue to be used on real-world autonomous car"], "labels": ["SMY", "QSN", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what kind of safety constraints cannot be expressed by masking act", "it seems that most safety constraints can be expressed via mask", "but certain kinds of safety constraints like 'do not drive in the blindspot of other vehicles' sometimes require the ego car to speed up for a bit beyond the speed limit to pass the blindspot area and then slow down", "this is an example of a constraint which cannot be expressed by masking actions and in fact requires breaking the top speed limit for a bit in order to be safer in the longer term", "- this work also assumes that other cars in the vicinity can be simply observed without any perception uncertainty or even through occlus"], "labels": ["QSN", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure c is pretty unrealistic to obtain for a real vehicle especially for the four cars near the top where the topmost vehicles would be occluded at least partially from the vantage point of the ego-car", "the authors propose a penalization term that enforces decorrelation between the dimensions of the represent", "they show that it can be included as additional term in cost functions to train generic model", "the idea is simple and it seems to work for the presented exampl", "however they talk about gradient descent using this extra term but i'd like to see the derivatives of the proposed term depending on the parameters of the model and this depends on the model!"], "labels": ["CRT", "SMY", "SMY", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the other hand given the expression of the proposed regulatizationit seems to lead to non-convex optimization problems which are hard to solve any comment on that", "on the other hand given the expression of the proposed regulatizationit seems to lead to non-convex optimization problems which are hard to solve any comment on that", "moreover its results are not quantitatively compared to other non-linear generalizations of pca/ica designed for similar goals eg those cited in the related work section or others which have been proved to be consistent non-linear generalizations of pca such as principal polynomial analysis dimensionality reduction via regression that follow the family introduced in the book of jolliffe principal component analysi", "minor points fig conveys not that much inform", "the paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network"], "labels": ["DIS", "QSN", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead of distillation from a pre-trained network the paper proposes to train both teacher and student network jointli", "the paper shows an interesting result that the distilled low precision network actually performs better than high precision network", "i found the paper interest", "but the contribution seems quite limit", "but the contribution seems quite limit"], "labels": ["SMY", "APC", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros the paper is well written and easy to read", "the paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network and that training jointly outperforms the traditional distillation method fixing the teacher network margin", "cons the name apprentice seems a bit confusing with apprenticeship learn", "the experiments might be further improved by providing a systematic study about the effect of precisions in this work eg producing more samples of precisions on activations and weight", "it is unclear how the proposed method outperforms other methods based on fine-tun"], "labels": ["APC", "APC", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model", "this paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embed", "this paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embed", "it also adopts an interesting multicodebook approach for encoding than binary embed", "the paper presents the proposed approach to a few nlp problems and have shown that this is able to significant reduce the size increase compression ratio and still achieved good accuraci"], "labels": ["DIS", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are convincing and solid", "overall i am weakly inclined to accept this pap", "my review reflects more from the compressive sensing perspective instead that of deep learn", "in general i find many of the observations in this paper interest", "however this paper is not strong enough as a theory paper rather the value lies perhaps in its fresh perspect"], "labels": ["APC", "FBK", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper studies text embeddings through the lens of compressive sensing theori", "the authors proved that for the proposed embedding scheme certain lstms with random initialization are at least as good as the linear classifiers the theorem is almost a direction application of the rip of random rademacher matric", "several simplifying assumptions are introduced which rendered the implication of the main theorem vagu", "but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow", "the second contribution of the paper is the empirical observation that in terms of sparse recovery of embedded words the pretrained embeddings are better than random matrices the latter being the main focus of compressive sensing theori"], "labels": ["SMY", "SMY", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["partial explanations are provided again using results in compressive sensing theori", "in my personal opinion the explanations are opaque and unsatisfactori", "an alternative route is suggested in my detailed review", "finally extensive experiments are conducted and they are in accordance with the theori", "my most criticism regarding this paper is the narrow scope on compressive sensing and this really undermines the potential contribution in sect"], "labels": ["DIS", "CRT", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically the authors considered only basis pursuit estimators for sparse recovery and they used the rip of design matrices as the main tool to argue what is explainable by compressive sensing and what is not", "this seems to be somewhat of a tunnel-visioning for me there are a variety of estimators in sparse recovery problems and there are much less restrictive conditions than rip of the design matrices that guarantee perfect recoveri", "in particular in section  instead of invoking [donoho&tanner ] i believe that a more plausible approach is through [chandrasekaran et al ]", "there a simple deterministic condition the null space property for successful recovery is prov", "it would be of direct interest to check whether such condition holds for a pretrained embedding say glove given some bow"], "labels": ["CRT", "CRT", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore it is proved in the same paper that restricted strong convexity rsc alone is enough to guarantee successful recovery rip is not required at al", "while as the authors argued in section  it is easy to see that pretrained embeddings can never possess rip they do not rule out the possibility of rsc", "exactly the same comments above apply to many other common estimators lasso dantzig selector etc in compressive sensing which might be more tolerant to nois", "several minor comments please avoid the use of ucinformation theoryud especially ucclassical information theoryud in the current context", "these words should be reserved to studies of channel capacity/source coding `a la shannon"], "labels": ["DIS", "DIS", "DIS", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these words should be reserved to studies of channel capacity/source coding `a la shannon", "i understand that in recent years people are expanding the realm of information theory but as compressive sensing is a fascinating field that deserves its own name thereus no need to mention information theory her", "in theorem  please be specific about how the l-regularization is chosen", "in section  please briefly describe why you need to extend previous analysis to the lipschitz cas", "i understood the necessity only through reading proof"], "labels": ["CRT", "SUG", "DIS", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can the authors briefly comment on the two assumptions in section  especially the second one on n- cooccurrence is this pract", "page  there is a typo in the sentence preceding [radfors et al ]", "page  first paragraph of related work the sentence ucour method also closely related to ud is incomplet", "page  second paragraph of related work ucpagliardini also introduced a linear ud", "page  conclusion the beginning sentence of the second paragraph is erron"], "labels": ["QSN", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] venkat chandrasekaran benjamin recht pablo a parrilo alan s willsky ucthe convex geometry of linear inverse problemsud foundations of computational mathemat", "the authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperatur", "in particular they use a cnn encoder-decoder to learn a motion field and a warping function from the last component to provide forecast", "i like the idea of using deep learning for physical equ", "i would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method"], "labels": ["DIS", "SMY", "SMY", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i got confused at several points because it was not clear what was exactly being estimated with the cnn", "having an algorithmic environment would make the description easi", "i know that authors are going to publish the code but this is not enough at this point of the revis", "physical processes in machine learning have been studied from the perspective of gaussian processes just to mention a couple of references uclinear latent force models using gaussian processesud and numerical gaussian processes for time-dependent and non-linear partial differential equ", "in theorem  do you need to care about boundary conditions for your equ"], "labels": ["CRT", "SUG", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i didnut see any mention to those in the definition for ixt you only mention initial condit", "how do you estimate the diffusion parameter d", "are you assuming isotropic diffusion is that realist", "can you provide more details about how you run the data assimilation model in the experiments did you use your own cod", "the authors propose a new defense against security attacks on neural network"], "labels": ["DFT", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the attack model involves a standard l_inf norm constraint", "remarkably the approach outputs a security certificate security guarantee on the algorithm which makes it appealing for security use in practic", "remarkably the approach outputs a security certificate security guarantee on the algorithm which makes it appealing for security use in practic", "furthermore the authors include an approximation of the certificate into their objective function thus training networks that are more robust against attack", "the approach is evaluated for several attacks on mnist data"], "labels": ["SMY", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first of all the paper is very well written and structur", "as standard in the security community the attack model is precisely formalized i find this missing in several other ml papers on the top", "the certificate is derived with rigorous and sound math", "an innovative approximation based on insight into a relation to the maxcut algorithm is shown", "an innovative approximation based on insight into a relation to the maxcut algorithm is shown"], "labels": ["APC", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["an innovative training criterion based on that certificate is propos", "an innovative training criterion based on that certificate is propos", "both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attack", "in summary this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evalu", "for me it is a clear accept"], "labels": ["SMY", "APC", "APC", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the only drawback i see is the missing theoretical and empirical comparison to the recent nips  paper by hein et ", "this paper suggests an rnn reparametrization of the recurrent weights with a skew-symmetric matrix using cayley transform to keep the recurrent weight matrix orthogon", "they suggest that they reparametrization leads to superior performance compare to other forms of unitary recurrent network", "i think the paper is well-written", "authors have discussed previous works adequately and provided enough insight and motivation about the proposed method"], "labels": ["DFT", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have two questions from authors- what are the hyperparameters that you optimized in experi", "- how sensitive is the results to the number of - in the diagonal matrix", "- ince the paper is not about compression it might be unfair to limit the number of hidden units in lstms just to match the number of parameters to rnn", "in mnist experiment for example better numbers are reported for larger lstm", "i think matching the number of hidden units could be help"], "labels": ["QSN", "QSN", "DIS", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also one might want to know if the scornn is still superior in the regime where the number of hidden units is about", "i appreciate if authors can provide more results in these set", "the paper proposes to extend the usual ppmi matrix factorization levy and goldberg  to a rd-order ppmi tensor factorization the paper chooses symmetric cp decomposition so that word representations are tied across all three view", "the mse objective optionally interpolated with a nd-order tensor is optimized incrementally by sgd", "the paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors which indeed does not seem to hold in cbow"], "labels": ["DIS", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the paper reports superior performance the empirical claims are not well substanti", "it is *not* true that given cbow it's not important to compare with sgns and glov", "in fact in certain cases such as unsupervised word analogy sgns is clearly and vastly superior to other techniques stratos et ", "the word similarity scores are also generally low it's easy to achieve  on men using the plain ppmi matrix factorization on wikipedia", "so it's hard to tell if it's real improv"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality borderlin", "the proposed approach is simple and has an appealing compositional featur", "but the work is not adequately validated and the novelty is somewhat limit", "clarity clear", "originality low-rank tensors have been used to derive features in many prior works in nlp eg lei et "], "labels": ["DIS", "APC", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper's particular application to learning word embeddings ppmi factorization however is new although perhaps not particularly origin", "the observation on multiplicative compositionality is the main strength of the pap", "significance moder", "for those interested in word embeddings this work suggests an alternative training technique but it has some issues described abov", "summary this paper introduces a model that combines the rotation matrices with the lstm"], "labels": ["APC", "APC", "DIS", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they apply the rotations before the final tanh activation of the lstm and before applying the output g", "the rotation matrix is a block-diagonal one where each block is a x rotations and those rotations are parametrized by another neural network that predicts the angle of the rot", "the paper only provides results on the babi task", "questionshave you compared against to the other parametrizations of the lstms and rotation matrices ablation studi", "have you tried on other task"], "labels": ["SMY", "SMY", "SMY", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why did you just apply the rotations only on d_{t}", "prosuses a simple parametrization of the rotation matric", "consnot clear justification and motiv", "the experiments are really lackingno ablation studi", "the results are only limited to single toy task"], "labels": ["QSN", "APC", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["general commentsthis paper proposes to use the rotation matrices with lstm", "however there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output g", "the experiments are seriously lacking an ablation study should have been made and the results are not good enough", "the experiments are only limited to babi task which doesnut tell you much", "this paper is not ready for publication and really feels like it is rush"], "labels": ["SMY", "CRT", "CRT", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor commentthis paper needs more proper proof-read", "there are some typos in it egst page senstence -- sentenceth page the th", "it is hard to interpret this work as the authors do not mention the original work by gutmann and his colleague on the nce in the required detail", "their paper provides a proof that in the non-parametric case the optimum on nce objective function is at the data distribution with normalisation constant either learned or held fixed  or any value you lik", "what exactly is the purpose of this pap"], "labels": ["DFT", "DFT", "DFT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are a number of minor issues as well in language modelling we do not compute normalisation term during nce training or testing as explicitly stated by the authors you are referring to chen  - that is the whole point of using nc", "what is pc in equation  and where it comes from", "this paper dives deeper into understand reward augmented maximum likelihood train", "overall i feel that the paper is hard to understand and that it would benefit from more clarity eg section  states that decoding from the softmax q-distribution is similar to the bayes decision rul", "please elaborate on thi"], "labels": ["DFT", "QSN", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["did you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candid", "section  says that ranzato et al and bahdanau et al require sampling from the model distribut", "section  says that ranzato et al and bahdanau et al require sampling from the model distribut", "however the methods analyzed in this paper also require sampling cf appendix d where you mention a sample size of", "please explain the differ"], "labels": ["QSN", "SMY", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the fundamental contribution of the article is the explicit use of compositionality in the definition of the search spac", "instead of merely defining an architecture as a directed acyclic graph dag with nodes corresponding to feature maps and edges to primitive operations the approach in this paper introduces a hierarchy of architectures of this form", "each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the dag", "as a result this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the dag corresponding to the final architecture which is located at the top level in the hierarchi", "manually designing novel neural architectures is a laborious time-consuming process"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore exploring new approaches to automatise this task is a problem of great relevance for the field", "overall the paper is well-written clear in its exposition and technically sound", "while some hyperparameter and design choices could perhaps have been justified in greater detail the paper is mostly self-contained and provides enough information to be reproduc", "the fundamental contribution of this article when put into the context of the many recent publications on the topic of automatic neural architecture search is the introduction of a hierarchy of architectures as a way to build the search spac", "compared to existing work this approach should emphasise modularity making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model"], "labels": ["DIS", "APC", "APC", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["exploiting compositionality in model design is not novel per se eg []", "but it is to the best of my knowledge the first explicit application of this idea in neural architecture search", "nevertheless while the idea behind the proposed approach is definitely interest", "i believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical flat representation of architectures used in other publ", "in particular the results highlighted in figure  and table  seem to indicate that the difference in performance between both paradigms is rather smal"], "labels": ["CRT", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover the performance gap between the flat and hierarchical representations of the search space as reported in table  remains smaller than the performance gap between the best performing of the approaches proposed in this article and nasnet-a zoph et al  as reported in tables  and", "another concern i have is regarding the definition of the mutation operators in sect", "while not explicitly stated i assume that all sampling steps are performed uniformly at random otherwise please clarify it", "if that was indeed the case there is a systematic asymmetry between the probability to add and remove an edge making the former considerably more lik", "this could bias the architectures towards fully-connected dags as indeed seems to occur based on the motifs reported in appendix a"], "labels": ["CRT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally while the main motivation behind neural architecture search is to automatise the design of new models the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow", "this includes for instance the number of levels in the hierarchy l the number of motifs at each level in the hierarchy m_l the number of nodes in each graph at each level in the hierarchy | g^{l} | as well as the set of primitive oper", "i believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters and/or provided users with a principled approach to select reasonable valu", "references[] grosse roger et al exploiting compositionality to explore a large space of model structures uai [] duvenaud david et al structure discovery in nonparametric regression through compositional kernel search icml", "the paper is well motivated and written"], "labels": ["DIS", "DIS", "SUG", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are several issu", "as the regularization constant increases the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problem", "further the sudden drop in performance also follows from vanishing gradients problem in deep network", "the description for relus in section  follows from these two arguments directly hence not novel", "several of the key aspects here not addressed are a is the time-delayed regularization equivalent to reducing the value and there by bringing it back to the 'good' regime before the cliff in the example plot"], "labels": ["CRT", "DIS", "SMY", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b why should we keep increasing the regularization constant beyond a limit", "is this for compressing the networks for which there are alternate procedures or anything els", "in other words for a non-convex problem about whose landscape we know barely anything if there are regimes of regularizers that work well see point  -- why should we ask for more stronger regular", "is there any optimization-related motivation here beyond the single argument that networks are overparameter", "the proposed experiments are not very conclus"], "labels": ["QSN", "DIS", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["firstly the authors need to test with modern state-of-the-art architectures including inception and residual network", "secondly more datasets including imagenet needs to be test", "unless these two are done we cannot assertively say that the proposal seems to do interesting th", "thirdly it is not clear what figure  means in terms of goodness of learn", "and lastly although confidence intervals are reported for figures  and table  statistical tests needs to be performed to report p-values so as to check if one model significantly beats the oth"], "labels": ["SUG", "SUG", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a method for learning new tasks without interfering previous tasks using conceptor", "this method originates from linear algebra where a the network tries to algebraically infer the main subspace where previous tasks were learned and make the network learn the new task in a new sub-space which is unused until the present task in hand", "the paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem", "in section  the authors review conceptor", "this method is algebraic method closely related to spanning sub spaces and svd"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main advantage of using conceptors is their trait of boolean logics ie their ability to be added and multiplied natur", "in section  the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to sgd with back-propag", "the authors provide a version with batch sgd as wel", "in section  the authors show their method on permuted mnist", "they compare the method to ewc with the same architectur"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they show that their method more efficiently suffers on permuted mnist from less degrad", "also they compared the method to ewc and imm on disjoint mnist and again got the best perform", "in general unlike what the authors suggest i do not believe this method is how biological agents perform their tasks in real lif", "nevertheless the authors show that their method indeed reduce the interference generated by a new task on the old learned task", "i think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous on"], "labels": ["SMY", "APC", "CRT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is missing is the following i think that without any additional effort a network can learn a new task in parallel to other task or some other techniques may be used which are not bound to any algebraic method", "therefore my only concern is that in this comparison the work bounded to very specific group of methods and the question of what is the best method for continual learning remained open", "the paper introduces svd parameterization and uses it mostly for controlling the spectral norm of the rnn", "my concerns with the paper include a the paper says that the same method works for convolutional neural networks but i couldn't find anything about convolut", "b the theoretical analysis might be misleading --- clearly section  shouldn't have title all critical points are global minimum because  is a critical point but it's not a global minimum"], "labels": ["DFT", "DIS", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theorem  should be phrased as all critical points of the population risk that is non-singular are global minima", "c the paper should run some experiments on language applications where rnn is widely us", "d i might be wrong on this point but it seems that the gpu utilization of the method would be very poor so that it's kind of impossible to scale to large dataset", "the paper proposes new rnn training method based on the searn learning to search ls algorithm and named as searnn", "it proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by l"], "labels": ["SUG", "DFT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it can consider different classifiers and loss functions and a sampling strategy for making the optimization problem scalable is propos", "searnn improves the results obtained by mle training in three different problems including a large-vocabulary machine transl", "in summary a very nice pap", "in summary a very nice pap", "quality searnn is a well rooted and successful application of the ls strategy to the rnn training that combines at the same time global optimization and scalable complex"], "labels": ["SMY", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarity the paper is well structured and written with a nice and well-founded literature review", "originality the paper presents a new algorithm for training rnn based on the ls methodology and it has been proven to be competitive in both toy and real-world problem", "significance although the application of ls to rnn training is not new", "the parameterized clipping activation pact idea is very clear extend clipping activation by learning the clipping paramet", "then  pact is combined with quantizing the activ"], "labels": ["APC", "APC", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed technique sound", "the performance improvement is expected and validated by experi", "but i am not sure if the novelty is strong enough for an iclr pap", "i find this paper not suitable for iclr", "all the results are more or less direct applications of existing optimization techniques and not provide fundamental new understandings of the learning represent"], "labels": ["APC", "APC", "FBK", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a small modification to the monotonic attention in [] by adding a soft attention to the segment predicted by the monotonic attent", "this paper proposes a small modification to the monotonic attention in [] by adding a soft attention to the segment predicted by the monotonic attent", "the paper is very well written and easy to follow", "the paper is very well written and easy to follow", "the experiments are also convincing here are a few suggestions and questions to make the paper strong"], "labels": ["SMY", "DIS", "APC", "FBK", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are also convincing here are a few suggestions and questions to make the paper strong", "the first set of questions is about the monotonic attent", "training the monotonic attention with expected context vectors is intuitive but can this be justified furth", "training the monotonic attention with expected context vectors is intuitive but can this be justified furth", "for example how far does using the expected context vector deviate from marginalizing the monotonic attent"], "labels": ["QSN", "DIS", "DFT", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example how far does using the expected context vector deviate from marginalizing the monotonic attent", "the greedy step described in the first paragraph of page  also has an effect on the produced attent", "how does the greedy step affect training and decod", "it is also unclear how tricks in the paragraph above section  affect training and decod", "it is also unclear how tricks in the paragraph above section  affect training and decod"], "labels": ["QSN", "SMY", "QSN", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is also unclear how tricks in the paragraph above section  affect training and decod", "these questions should really be answered in []", "since the authors are extending their work and since these issues might cause training difficulti", "since the authors are extending their work and since these issues might cause training difficulti", "it might be useful to look into these design choic"], "labels": ["FBK", "QSN", "DFT", "FBK", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it might be useful to look into these design choic", "the second question is about the window size $w$", "instead of imposing a fixed window size which might not make sense for tasks with varying length segments such as the two in the pap", "instead of imposing a fixed window size which might not make sense for tasks with varying length segments such as the two in the pap", "why not attend to the entire segment ie from the current boundary to the previous boundari"], "labels": ["DFT", "QSN", "DFT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is pretty clear that the model is discovering the boundaries in the utterance shown in figur", "the spectrogram can be made more visible by removing the delta and delta-delta in the last subplot", "how does the mocha attention look like for words whose orthography is very nonphonemic for example aaa and www", "how does the mocha attention look like for words whose orthography is very nonphonemic for example aaa and www", "for the experiments it is intriguing to see that $w=$ works best for speech recognit"], "labels": ["APC", "DFT", "DIS", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if that's the case would it be easier to double the hidden layer size and use the vanilla monotonic attent", "if that's the case would it be easier to double the hidden layer size and use the vanilla monotonic attent", "the latter should be a special case of the former and in general you can always increase the size of the hidden layer to incorporate the windowed inform", "the latter should be a special case of the former and in general you can always increase the size of the hidden layer to incorporate the windowed inform", "would the special cases lead to worse performance and if so why is there a differ"], "labels": ["DIS", "QSN", "SMY", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n[] c raffel m luong p liu r weiss d eck online and linear-time attention by enforcing monotonic align", "pros- the proposed model is a nice way of multiplicatively combining two features   one which determines which classes to pay attention to and other thatprovides useful features for discrimin", "- the adaptive component seems to provide improvements for small dataset s", "and large number of class", "cons- one can easily see that if o_tx w =  then class t becomes neutral in the  classification and the gradients are not back-propagated from it"], "labels": ["FBK", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this doesnot seem to be tru", "even if the logits are zero the class would have anon-zero probability and would receive gradi", "do the authors meanexpo_txw =", "- related to the above it should be clarified what is meant by dropping a  class", "is its logit set to zero or -infti"], "labels": ["CRT", "DIS", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["excluding a class from thesoftmax is equivalent to having a logit of -infty not zero", "however from theequations in the paper it seems that the logit is set to zero", "this would notresult in excluding the unit", "the overall effect would just be to raise themagnitude of logits across the entire softmax", "- it seems that the model benefits from at least two separate effects - one is  the attention mechanism provided by the sigmoids and the other is thestochasticity during train"], "labels": ["DIS", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["presently it is not clear if only one of thecomponents is providing most of the benefits or if both things are us", "itwould be great to compare this model to a non-stochastic one which just has themultiplicative effects applied in a deterministic way during both training andtest", "- the objective of the attention mechanism that sets the dropout mask seems to  be the same as the primary objective of classifying the input and theattention mechanism is prevented from solving the task by adding an extraentropy regular", "it would be useful to explain more why this is need", "would it not be fine if the attention mechanism did a perfect job of selectingthe class"], "labels": ["DIS", "DIS", "DIS", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["qualitythe paper makes relevant comparisons and is overall well-motiv", "howeversome aspects of the paper can be improved by adding more explan", "claritysome crucial aspects of the paper are unclear as mentioned abov", "originalitythe main contribution of the paper is similar to multiplicative g", "theadded stochasticity and the model ensembling interpretation is probably novel"], "labels": ["APC", "SUG", "CRT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however experiments are insufficient to determine whether it is this noveltythat contributes to improved performance or just the g", "significancethis paper makes incremental improvements and would be of moderate interest tothe machine learning commun", "typos - in eq  the numerator has z_t should that be z_i", "- in eq  the denominator has z_i", "should that be z_t"], "labels": ["CRT", "APC", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposed an extension to the fast weights from ba et al to include additional gating units for changing the fast weights learning rate adapt", "the authors empirically demonstrated the gated fast weights outperforms other baseline methods on the associative retrieval task", "comment- i found the paper very hard to follow", "the authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest", "the authors should emphasize the first half of the method section are from existing works and should go into a separate background sect"], "labels": ["SMY", "SMY", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- overall the only contribution of the paper seems to be the modification to ba et al is the eq", "the authors have only evaluated the method on a synthetic associative retrieval task", "without additional experiments on other datasets it is hard for the reader to draw any meaningful conclusion about the proposed method in gener", "the authors try to combine the power of gans with hierarchical community structure detect", "while the idea is sound"], "labels": ["SMY", "DFT", "DFT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["many design choices of the system is question", "the problem is particularly aggravated by the poor presentation of the paper creating countless confusions for read", "i do not recommend the acceptance of this draft", "compared with gan traditional graph analytics is model-specific and non-adaptive to training data", "this is also the case for hierarchical community structur"], "labels": ["CRT", "CRT", "FBK", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by building the whole architecture on the louvain method the proposed method is by no means truly model-agnost", "in fact if the layers are fine enough a significant portion of the network structure will be captured by the sum-up module instead of the gan modules rendering the overall behavior dominated by the community detection algorithm", "the evaluation remains superficial with minimal quantitative comparison", "treating degree distribution and clustering coefficient appeared as cluster coefficient in draft as global features is problemat", "they are merely global average of local topological features which is incapable of capturing true long-distance structures in graph"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the writing of the draft leaves much to be desir", "the description of the architecture is confusing with design choices never clearly explain", "multiple concepts needs better introduction including the very name of their model gti and the idea of stage identif", "not to mention numerous grammatical errors i suggest the authors seek professional english writing servic", "this paper can be seen as an extension of the paper attention is all you need that will be published at nips in a few weeks at the time i write this review"], "labels": ["CRT", "CRT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the goal here is to make the target sentence generation non auto regress", "the authors propose to introduce a set of latent variables to represent the fertility of each source word", "the number of target words can be then derived and they're all predicted in parallel", "the idea is interesting and trendi", "however the paper is not really stand alon"], "labels": ["SMY", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a lot of tricks are stacked to reduce the performance degrad", "however they're sometimes to briefly described to be understood by most read", "the training process looks highly elaborate with a lot of hyper paramet", "maybe you could comment on thi", "for instance the use fertility supervision during training could be better motivated and explain"], "labels": ["CRT", "DIS", "DIS", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance the use fertility supervision during training could be better motivated and explain", "your choice of ibm  is wired since it doesn't include fertil", "why not ibm  for inst", "how you use ibm model for supervis", "this a simple example but a lot of things in this paper is too briefly described and their impact not really evalu"], "labels": ["DFT", "CRT", "QSN", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this a simple example but a lot of things in this paper is too briefly described and their impact not really evalu", "this paper utilizes acol algorithm for unsupervised learn", "acol can be considered a type of semi-supervised learning where the learner has access only to parent-class information for example in digit recognition whether a digit is bigger than  or not and not the sub-class information number between -", "acol can be considered a type of semi-supervised learning where the learner has access only to parent-class information for example in digit recognition whether a digit is bigger than  or not and not the sub-class information number between -", "given that in many applications such parent-class supervised information is not avail"], "labels": ["CRT", "SMY", "SMY", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors of this paper propose domain specific pseudo parent-class labels for example transformed images of digits to adapt acol for unsupervised learn", "the authors of this paper propose domain specific pseudo parent-class labels for example transformed images of digits to adapt acol for unsupervised learn", "the authors also modified affinity and balance term utilized in gar as part of acol algorithm to improve it", "the authors use multiple data sets to study different aspects of the proposed approach", "the authors use multiple data sets to study different aspects of the proposed approach"], "labels": ["SMY", "DIS", "SUG", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i updated my scores based on the reviewers respons", "it turned out that acol and gar are also originally proposed by the same authors and was only published in arxiv!", "it turned out that acol and gar are also originally proposed by the same authors and was only published in arxiv!", "because of the double-blind review nature of iclr i didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue iclr", "so my main problem with this paper lack of novelty is addressed and my score has chang"], "labels": ["CNT", "SMY", "DIS", "FBK", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thanks to the reviewer for clarifying thi", "this paper provides a new method for learning representations of preposit", "the basic idea is to count word pairs which co-occur with a preposition rather than single words which co-occur as in standard word vector models such as wordvec", "this seems to work quite well and i speculate that it is because prepositions often function to indicate grammatical relations between different arguments rather than being content-bearing words themselv", "the paper counts up these word-pair co-occurrences in a tensor then applies a tensor decomposition and low-rank approximation method to produce word and preposition represent"], "labels": ["FBK", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments show that the method helps to find paraphrases of phrasal verbs as well as improve downstream performance on preposition selection and preposition attachment disambigu", "this paper was quite interesting and clearly written for the most part", "i enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sent", "i think that this work could be quite useful to the field but that a number of frustrating weaknesses prevent me from recommending it without qualif", "the main weaknesses of the paper are in the soundness of some of its qualitative analyses and claim"], "labels": ["APC", "APC", "APC", "FBK", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first i found the cosine similarity scores in table  largely uninterpret", "the claim is that different prepositions should have representations that are sufficiently distinct from each oth", "even if we accept this premise and why should we they are of the same syntactic category after all using the similarity scores to make this argument is not reasonable as there is no absolute interpretation or calibration of the similarity scores that can be applied across model", "is  in similarity high or low", "the other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verb"], "labels": ["CRT", "CRT", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this was not done systematically but a broad general claim that the tensor multiplication models does the best cannot be verifi", "to me the wordvec addition paraphrases also look quite good", "it seems to me that a human subject experiment to somehow compare the two methods is requir", "i wonder whether this approach could be generalized to other classes of words or morphem", "for example you could imagine that in a morphologically rich language this method would work well to learn the representation of certain morphemes such as case endings or verbal conjug"], "labels": ["QSN", "APC", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the problem addressed in the paper seems interest", "but there lacks of evidence to support some of the arguments that the authors mak", "and the paper does not contribute novelty to representation learning therefore it is not a good fit for the confer", "detailed critiques are as following the idea proposed by the authors seems too quite simpl", "it is just performing random projections for  times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class svm"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it says in the experiments part that the authors have used  different s_{attack} values but they only present results for s_{attack} =", "it would be nicer if they include results for all s_{attack} values that they have used in their experiments which would also give the reader insights on how the anomaly detection performance degrades when the s_attack value chang", "it would be nicer if they include results for all s_{attack} values that they have used in their experiments which would also give the reader insights on how the anomaly detection performance degrades when the s_attack value chang", "the paper claims that the nonlinear random projection is a defence against adversary due to the randomness but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that", "and pca as a non-random projection would a nice baseline to compare against"], "labels": ["DIS", "SUG", "DFT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper seems to misuse the term ucfalse positive rateud as the y label of figure d/e/f", "the definition of false positive rate is fp/fp+tn so if the fpr= it means that all negative samples are labeled as posit", "so it is surprising to see fpr= in figure d when feature dimension= while the f score is still high in figure a", "from what i understand the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassifi", "the paper should come up with a better term for that evalu"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the conclusion that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower cannot be drawn from figure d", "need more experiment on more dimensionality to prove that  in the appendix b results part sometimes the word us_attacku is typed wrong and the values in  ucdistorted/distortedud columns in table  do not match up with the ones in figure c", "this paper proposes replacing fully connected layers with block-diagonal fully connected layers and proposes two methods for doing so", "it also make some connections to random matrix theori", "the parameter pruning angle in this paper is fairly weak"], "labels": ["CRT", "DFT", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the networks it is demonstrated on are not particularly large largeness usually being the motivation for pruning and the need for making them smaller is not well motiv", "additionally mnist is a uniquely bad dataset for evaluating pruning methods since they tend to work uncharacteristically well on mnist this can be seen in some of the references the paper cit", "the random matrix theory part of this paper is intriguing but left me wondering and then what", "it is presented as a collection of observations with no synthesis or context for why they are import", "i'm usually quite happy to see connections being made to other field"], "labels": ["CRT", "CRT", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it is not clear at all how this particular connection is more than a curios", "this paper would be much stronger if it offered some way to exploit this connect", "there are two half-papers here one on parameter pruning and one on applying insights from random matrix theory to neural networks but i don't see a strong connection between them", "this paper presents a method based on gans for visualizing how humans represent visual categori", "authors perform experiments on two datasets asian faces dataset and imagenet large scale recognition challenge dataset"], "labels": ["DIS", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["positive aspects+ the idea of using gans for this goal is smart and interest", "+ the results seem interesting too", "weaknesses- some aspects of the paper are not clear and presentation needs improv", "- i miss a clearer results comparison with previous methods like vondrick et ", "specific comments and questions-  figure  is not clear"], "labels": ["APC", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors should clarify how they use the inference network and what the two arrows from this inference network repres", "- figure  is also not clear", "just the fld projections of the mcmcp chains are difficult to interpret", "the legend of the figure is too tini", "the right part of the figure should be better described in the text or in the caption i don't understand well what this illustr"], "labels": ["DIS", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the right part of the figure should be better described in the text or in the caption i don't understand well what this illustr", "- regarding to the human experiments with amt how do the authors deal with noise on the workers perform", "is any qualification task us", "what are the instructions given to the work", "- in section  the authors state we also simultaneously learn a corresponding inference network  granular human biases captur"], "labels": ["CRT", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in section  the authors state we also simultaneously learn a corresponding inference network  granular human biases captur", "this seems interest", "but i didn't find any result on that in the pap", "can you give more details or refer to where in the paper it is discussed/test", "- figure  shows most interpretable mixture compon"], "labels": ["DIS", "APC", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how this most interpretable were select", "- in second paragraph section  it should be table  instead of figur", "- it would be interesting to see a discussion on why mcmcp density is better for group  and mcmcp mean is better for group", "to see the confusion matrixes could be us", "i like this pap"], "labels": ["QSN", "SUG", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the addressed problem is challenging and the proposed idea seems interest", "however the aspects mentioned make me think the paper needs some improvements to be publish", "this paper proposes a method for learning from noisy labels particularly focusing on the case when data isn't redundantly labeled ie the same sample isn't labeled by multiple non-expert annot", "the authors provide both theoretical and experimental validation of their idea", "pros+ the paper is generally very clearly written"], "labels": ["APC", "FBK", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the motivation notation and method are clear", "+ plentiful experiments against relevant baselines are included validating both the no-redundancy and plentiful redundancy cas", "+ the approach is a novel twist on an existing method for learning from noisy data", "cons - all experiments use simulated workers this is probably common but still not very convinc", "- the authors missed an important related work which studies the same problem and comes up with a similar conclusion lin mausam and weld to re label or not to re label hcomp"], "labels": ["APC", "APC", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors should have compared their approach to the base approach of natarajan et ", "- it seems too simplistic too assume all workers are either hammers or spammers the interesting cases are when annotators are neither of thes", "- the resnet used for each experiment is different and there is no explanation of the choice of architectur", "questions - how would the model need to change to account for example difficulti", "- why are joulin  krause  not relev"], "labels": ["SUG", "SMY", "SMY", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- best to clarify what the weights in the weighted sum of natarajan ar", "- large training error on wrongly labeled examples -- how do we know they are wrongly labeled ie do we have a ground truth available apart from the crowdsourced label", "where does this ground truth come from", "- not clear what ensure means in the algorithm descript", "- in sec  why is it important that the samples are fresh"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--------------summary and evaluation--------------this work present a novel multi-agent reference game designed to train monolingual agents to perform translation between their respective languages -- all without parallel corpora", "the proposed approach closely mirrors that of nakayama and nishida  in that image-aligned text is encouraged to map to similarly to the grounded imag", "unlike in this previous work the approach proposed here induces this behavior though a multi-agent reference gam", "the key distinction being that in this gamified setting the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth", "the authors demonstrate that this change results in significantly improved bleu scores across a number of translation tasks furthermore increasing the number of agents/languages in this setting seems to overall i think this is an interesting pap"], "labels": ["APC", "SMY", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the technical novelty is somewhat limited to a minor but powerful change in approach from nakayama and nishida  however the resulting translators outperform this previous method", "i have a few things listed in the weaknesses section that i found unclear or think would make for a stronger submiss", "--------------strengths--------------- the paper is fairly clearly written and the figures appropriately support the text", "- learning translation without parallel corpora is a useful task and leveraging a pragmatic reference game to induce additional semantically valid samples of a source language is an interesting approach to do so", "- i'm also excited by the result that multi-agent populations tend to improve the rate of convergence and final translation abilities of these models though i'm slightly confused about some of the results here see weak"], "labels": ["DFT", "CRT", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--------------weaknesses--------------- perhaps i'm missing something but shouldn't the single en-de/de-en results in table  match the not pretrained en-de/de-en multik task  results i understand that this is perhaps on a different data split into m/ but why is there such a drastic differ", "- i would have liked to see some context as how these results compare to an approach trained with aligned corpora", "perhaps a model trained on the human-translated pairs from task  of multik obviously outperforming such a model is not necessary for this approach to be interesting but it would provide useful context on how well this is do", "n- a great deal of the analysis and qualitative examples are pushed to the supplement which is a bit of a sham", "given they are quite interest"], "labels": ["DIS", "DIS", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a new way of sampling data for updates in deep-q network", "the basic principle is to update q values starting from the end of the episode in order to facility quick propagation of rewards back along the episod", "the paper is interesting but it lacks the proper comparisons to previously published techniqu", "the results presented by this paper shows improvement over the baselin", "but the atari results is still significantly worse than the current sota"], "labels": ["SMY", "SMY", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the non-tabular case the authors have actually moved away from q learning and defined an objective that is both on and off-polici", "some theoretical analysis would be nic", "it is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular cas", "there has been a number of highly relevant pap", "prioritized replay for example could have a very similar effect to proposed approach in the tabular cas"], "labels": ["DIS", "SUG", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the non-tabular case the retrace algorithm tree backup watkin's q learning all bear significant resemblance to the proposed method", "although the proposed algorithm is different from all  the authors should still have compared to at least one of them as a baselin", "the retrace algorithm specifically has also been shown to help significantly in the atari case and it defines a convergent update rul", "summarythis paper addresses the cybersecurity problem of domain generation algorithm dga  detect", "a class of malware uses algorithms to automatically generate artificial domain names for various purposes eg to generate large numbers of rendezvous point"], "labels": ["DIS", "CRT", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["dga detection concerns the automatic distinction of actual and artificially generated domain nam", "in this paper a basic problem formulation and general solution approach is investigated namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of charact", "a set of five deep learning architectures both cnns and rnns are compared empirical on the text classification task", "a domain name data set with two million instances is used for the experi", "the main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures since training time and the likelihood for overfitting can potentially be reduc"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["commentsthe introduction is well-written clear and concis", "it describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem", "the introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used one suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solut", "the methods section section  provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experi", "in the beginning of the section it is not clarified why if a  character string is encoded as a  byte ascii sequence the content has to be stored in a  x  matrix instead of a vector of s"], "labels": ["APC", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach", "section  describes the experiment settings the results and discusses the learned representations and the possible implications of using either the deep architectures or the ucbaselineud random forest classifi", "perhaps the authors could elaborate a little bit more on why random forests were trained on a completely different set of features than the deep architectur", "the data is stated to be randomly divided into training % validation % and testing %", "how many times is this procedure rep"], "labels": ["CRT", "DIS", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that is how many experimental runs were averaged or was the experiment run onc", "in summary this is an interesting and well-written paper on a timely top", "the main conclusion is intuit", "perhaps the conclusion is even regarded as obvious by some but in my opinion the result is important since it was obtained from new rather extensive experiments on a large data set and through the comparison of several existing earlier proposed architectur", "since the main conclusion is that simple models should be prioritised over complex ones due to that their accuracy is very similar it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclus"], "labels": ["QSN", "APC", "APC", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor commentsabstract uclittle studiesud - ucfew studiesud", "table  ucapproachud - ucapproachesud", "figure  use the same y-axis scale for all subplots if possible to simplify comparison", "also try to move figure  so that it appears closer to its inline reference in the text", "section  ucbased their on popularityud - ucbased on their popularityud"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper introduces smoothed q-values defined as the value of drawing an action from a gaussian distribution and following a given policy thereaft", "it demonstrates that this formulation can still be optimized with policy gradients and in fact is able to dampen instability in this optimization using the kl-divergence from a previous policy unlike preceding techniqu", "experiments are performed on an simple domain which nicely demonstrates its properties as well as on continuous control problems where the technique outperforms or is competitive with ddpg", "the paper is very clearly written and easy to read and its contributions are easy to extract", "the appendix is quite necessary for the understanding of this paper as all proofs do not fit in the main pap"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the appendix is quite necessary for the understanding of this paper as all proofs do not fit in the main pap", "the inclusion of proof summaries in the main text would strengthen this aspect of the pap", "on the negative side the paper fails to make a strong case for significant impact of this work the solution to this of course is not overselling benefits but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniqu", "on the negative side the paper fails to make a strong case for significant impact of this work the solution to this of course is not overselling benefits but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniqu", "in other words the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact"], "labels": ["CRT", "SMY", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall however i found the paper interesting readable and the technique worth thinking about so i recommend its accept", "overall however i found the paper interesting readable and the technique worth thinking about so i recommend its accept", "summary this paper proposes a different approach to deep multi-task learning using ucsoft ord", "ud  multi-task learning encourages the sharing of learned representations across tasks thus using less parameters and tasks help transfer useful knowledge across", "thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen task"], "labels": ["APC", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of ucsoft orderingud enforces the idea that there shall not be a rigid structure for all the tasks but a soft structure would make the models more generalizable and modular", "the methods reviewed prior work which the authors refer to as ucparallel orderud which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldnut be the cas", "they authors then extend the approach to ucpermuted orderud and finally present their proposed ucsoft orderingud approach", "the authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the perform", "the ucsoft orderingud approach simply enable task specific selection of layers scaled with a learned scaling factor to be combined in which order to result for the best performance for each task"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors evaluate their approach on mnist uci omniglot and celeba datasets and compare their approach to ucparallel orderingud and ucpermuted orderingud and show the performance gain", "positives - the paper is clearly written and easy to follow", "- the idea is novel and impactful if its evaluated properly and consist", "- the authors did a great job summarizing prior work and motivating their approach", "negatives - multi-class classification problem is one incarnation of multi-task learning there are other problems where the tasks are different classification and localization or auxiliary depth detection for navig"], "labels": ["SMY", "APC", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["celeba dataset could have been a good platform for testing different tasks attribute classification and landmark detect", "utodo i would recommend that the authors test their approach on such set", "- figure  is a bit confusing the authors do not explain why the ucpermuted orderud performs worse than ucparallel orderud", "their assumptions and results as of this section should be consistent that soft orderpermuted orderparallel ordersingle task", "utodo i would suggest that the authors follow up on this result which would be beneficial for the read"], "labels": ["SUG", "SUG", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- figure a and b the results shown on validation loss how about testing error similar to figure a", "how about results for celeba dataset it could be useful to visualize them as was done for mnist omniglot and ucl", "utodo i would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compar", "notation and typos- figure  is a bit confusing how come the accuracy decreases with increasing number of training samples please clarifi", "- if i assume that the y-axis is incorrectly labeled and it is training error instead then the permuted order is doing worse than the parallel ord"], "labels": ["QSN", "SUG", "SUG", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u- if i assume that the x-axis is incorrectly labeled and the numbering is reversed start from max and ending at  then i think it would make sens", "- figure  is very small and not easy to read the text", "does single task mean average performance over the task", "- in eq choosing sigma_i for a task-specific permutation of the network is a bit confusing since it could be thought of as a sigmoid function i suggest using a different symbol", "uconclusion i would suggest that the authors address the concerns mentioned abov"], "labels": ["DIS", "CRT", "QSN", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their approach and idea is very interesting and relevant and addressing these suggestions will make the paper strong for publ", "this paper considers a new way to incorporate episodic memory with shallow-neural-nets rl using reservoir sampl", "the authors propose a reservoir sampling algorithm for drawing samples from the memori", "some theoretical guarantees for the efficiency of reservoir sampling are provid", "the whole algorithm is tested on a toy problem with  repeat"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the comparisons between this episodic approach and recurrent neural net with basic gru memory show the advantage of proposed algorithm", "the paper is well written and easy to understand", "typos didn't influence read", "it is a novel setup to consider reservoir sampling for episodic memori", "the theory part focuses on effectiveness of drawing samples from the reservoir"], "labels": ["APC", "APC", "DIS", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["physical meanings of theorem  are not well repres", "what are the theoretical advantages of using reservoir sampl", "four simple shallow neural nets are built as query write value and policy network", "the proposed architecture is only compared with a recurrent baseline with -unit gru network", "it is not clear the better performance comes from reservoir sampling or other differ"], "labels": ["DFT", "QSN", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover the hyperparameters are not optimized on different architectures it is hard to justify the empirically better performance without hyperparameter tun", "the authors mentioned that the experiments are done on a toy problem only three repeats for each experimentthe technically soundness of this work is weakened by the experi", "summary the paper introduces phase conductor which consists of two phases context-question attention phase and context-context self attention phas", "each phase has multiple layers of attention for which the paper uses a novel way to fuse the layers and context-question attention uses different question embedding for getting the attention weight and getting the attention vector", "the paper shows that the model achieves state of the art on squad among published papers and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention while it is not so helpful for context-question attent"], "labels": ["DFT", "DFT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["note while i will mostly try to ignore recently archived non-published papers when evaluating this paper i would like to mention that the paper's ensemble model currently stands th on squad leaderboard", "pros- the model achieves sota on squad among published pap", "- the sequential fusing gru-like of the multiple layers of attention is interesting and novel", "visual analysis of the attention map is convinc", "- the paper is overall well-written and clear"], "labels": ["DIS", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models and should cite relevant pap", "for instance memory networks [] uses different embedding for key computing attention weight and value computing attended vector", "- while ablations for number of attention layers  or  were visually convincing numerically there is a very small difference even for selfatt", "for instance in table  having two layers of selfatt with two layers of question-passage only increases max f by  where the standard deviation is  for the one lay", "while this may be statistically significant it is a very small gain nonetheless"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- given the above two cons the main contribution of the paper is % improvement over previous state of the art", "i think this is a valuable engineering contribut", "but i feel that it is not well-suited / sufficient for iclr audi", "questions- page  first para why have you not tried glove d if you think it is a critical factor", "errors- page  last para gives an concrete - gives a concrete- page  last para matching - matchedfigure  i think passage embedding h and question embedding v boxes should be switched- page   first para evidence fully - evidence to be "], "labels": ["APC", "APC", "FBK", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] jason weston sumit chopra and antoine bordes memory networks iclr", "the paper describes the problem of continual learning the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learn", "the work defends the point of view that bayesian inference is the right approach to attack this problem and address difficulties in past implement", "the paper is well written", "the problem is described neatly in conjunction with the past work"], "labels": ["FBK", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and the proposed algorithm is supported by experi", "the work is a useful addition to the communit", "ymy main concern focus on the validity of the proposed model in harder tasks such as the atari experiments in kirkpatrick et al  or the split cifar experiments in zenke et ", "even though the experiments carried out in the paper are important they fall short of justifying a major step in the direction of the solution for the continual learning problem", "this paper applies recently developed ideas in the literature of robust optimization in particular distributionally robust optimization with wasserstein metr"], "labels": ["APC", "APC", "DIS", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and showed that under this framework for smooth loss functions when not too much robustness is request", "then the resulting optimization problem is of the same difficulty level as the original one where the adversarial attack is not concern", "i think the idea is intuitive and reasonable the result is nic", "although it only holds when light robustness are impos", "but in practice this seems to be more of the case than say large deviation/adversary exist"], "labels": ["DFT", "DFT", "APC", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as adversarial training is an important topic for deep learning i feel this work may lead to promising principled ways for adversarial train", "this paper presented a multi-modal extension of variational autoencoder vae for the task visually grounded imagin", "in this task  the model learns a joint embedding of the images and the attribut", "the proposed model is novel but incremental comparing to existing framework", "the author also introduced new evaluation metrics to evaluate the model performance concerning correctness coverage and composition"], "labels": ["APC", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros the paper is well-written and the contribution both the model and the evaluation metric potentially can to be very useful in the commun", "the discussion comparing the related work/baseline methods is insight", "the proposed model addresses many important problems such as attribute learning disentanged representation learning learning with missing values and proper evaluation method", "cons/questions the motivation of the model choice of q is not clear", "comparing to bivcca apart from the differences that the author discussed a big difference is the choice of q"], "labels": ["APC", "APC", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["bivcca uses two inference networks qz|x and qz|y while the proposed method uses three qz|x qz|y and qz|xi", "bivcca uses two inference networks qz|x and qz|y while the proposed method uses three qz|x qz|y and qz|xi", "how does such model choice affect the final perform", "baselines are not necessarily suffici", "the paper compared the vanilla version of bivcca but not the one with factorized representation vers"], "labels": ["SMY", "DIS", "QSN", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper compared the vanilla version of bivcca but not the one with factorized representation vers", "in the original vaecca paper the extension of using factorized representation private and shared improved the performance]", "the author should also compare this extension of vaecca", "some details are not clear", "a how to set/learn the scaling parameter lambda_y and beta_i"], "labels": ["DFT", "APC", "APC", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if it is set as hyper-parameter how does the performance change concerning them", "b discussion of the experimental results is not suffici", "for example why jmvae performs much better than the proposed model when all attributes are given", "for example why jmvae performs much better than the proposed model when all attributes are given", "what is the conclusion from figure b"], "labels": ["QSN", "QSN", "SMY", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the jmvae seems to generate more diverse better coverage results which are not consistent with the claims in the related work", "the same applies to figur", "the authors address two important issues semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples and visual rationale generation explaining the outputs of the classifiier by overlaing a visual rationale on the original imag", "this focus is mainly on medical image classification but the approach could potentially be useful in many more area", "the main idea is to train a gan on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately gaussian to the space of images and then to train an encoder to map the original images into this space minimizing reconstruction error with the gan weights fix"], "labels": ["DFT", "DFT", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the encoder is then used as a feature extractor for classification and regression of targets eg heard diseas", "the visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class", "this gives an image that is similar to the original but with features that caused the classification of the disease remov", "the resulting image can be subtracted from the original encoding to highlight problematic area", "the approach is evaluated on an in-house dataset and a public nih dataset demonstrating good performance and illustrative visual rationales are also given for mnist"], "labels": ["SMY", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea in the paper is to my knowledge novel and represents a good step toward the important task of generating interpretable visual rational", "there are a few limitations eg the difficulty of evaluating the rationales and the fact that the resolution is fixed to x which means discarding many pixels collected via ionizing radiation but these are readily acknowledged by the authors in the conclus", "comments there are a few details missing like the batch sizes used for training it is difficult to relate epochs to iterations without thi", "also the number of hidden units in the  layer mlp from para  in sec", "it would be good to include psnr/mse figures for the reconstruction task fig  to have an objective measure of error"], "labels": ["APC", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sec  para  the reconstruction loss on the validation set was similar to the reconstruction loss on the validation set -- perhaps you could be a little more precise her", "eg learning curves would be useful sec  para  paired with a bnp blood test that is correlated with heart failure i suspect many readers of iclr like myself will not be well versed in this test correlation with hf diagnostic capacity etc so a little further explanation would be helpful her", "the term correlated is a bit too broad and it is difficult for a non-expert to know exactly how correlated this i", "it is also a little confusing that you begin this paragraph saying that you are doing a classification task but then it seems like a regression task which may be postprocessed to give a classif", "anyway a clearer explanation would be help"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also if this test is diagnostic why use x-rays for diagnosis in the first plac", "i would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale as this would have practical impl", "sec  para  l_target is a target objective which can be a negative class probability or in the case of heart failure predicted bnp level -- for predicted bnp level are you treating this as a probability and using cross entropy here or mean squared error", "as always it would be illustrative if you could include some examples of failure cases which would be helpful both in suggesting ways of improving the proposed technique and in providing insight into where it may fail in practical situ", "this paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learn"], "labels": ["QSN", "DIS", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the algorithm then chooses optimistically over the distribution induced by the ensembl", "this leads to improved learning / exploration notably better than the similar approach bootstrapped dqn", "there are several things to like about this paper- it is a clear paper with a simple message and experiments that back up the claim", "- the proposed algorithm is simple and could be practical in a lot of settings and even non-dqn vari", "- it is interesting that bootstrapped dqn gets such poor performance this suggests that it is very important in the original paper https//arxivorg/abs/ that ensemble voting is applied to the test evaluation why do you think this is by the way do you think it has something to do with the data being *more* off-policy / diverse under a ts vs ucb schem"], "labels": ["SMY", "SMY", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it is interesting that bootstrapped dqn gets such poor performance this suggests that it is very important in the original paper https//arxivorg/abs/ that ensemble voting is applied to the test evaluation why do you think this is by the way do you think it has something to do with the data being *more* off-policy / diverse under a ts vs ucb schem", "on the other hand- the novelty/scope of this work is somewhat limited this is more likely valuable incremental work than a game-chang", "- something feels wrong/hacky/incomplete about just doing ensemble for uncertainty without bootstrapping/randomization if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - i think that you should mention that you are heavily reliant on random initialization + sgd/adam + specific network architecture to maintain this idea of uncertainti", "- something feels wrong/hacky/incomplete about just doing ensemble for uncertainty without bootstrapping/randomization if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - i think that you should mention that you are heavily reliant on random initialization + sgd/adam + specific network architecture to maintain this idea of uncertainti", "for example this wouldn't work for linear value functions!- i think the original bootstrapped dqn used ensemble voting at test time so maybe you should change the labels or the way this is introduced/discuss"], "labels": ["QSN", "DFT", "DFT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's definitely very interesting that *essentially* the learning benefit is coming from ensembling rather than raw bootstrapped dqn and ucb still looks like it does bett", "- i'm not convinced that page  and the bayesian derivation really add too much value to this paper alternatively maybe you could introduce the actual algorithm first train k models in parallel and then say this is similar to particle filter and add the mathematical derivation after rather than as if it was some complex formula deriv", "if you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https//arxivorg/pdf/pdf instead", "- i think this paper might miss the point of the bigger problem of efficient exploration in rl or even how to get deep exploration with deep rl", "yes this algorithm sees improvements across atari but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate  actually i do believe this algorithm can demonstrate deep exploration but it looks like we're not seeing the big improvements on the sub-human games you might hop"], "labels": ["DFT", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i do think this is a pretty good short paper/evaluation of ucb-ensembles on atari", "the scope/insight of the paper isn't groundbreaking but i think it delivers a clear short message on the atari benchmark", "perhaps this will encourage people to dig deeper into some of these issues i vote accept", "this paper presents a tensor decomposition method called tensor ring tr decomposit", "the proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensor"], "labels": ["APC", "DIS", "FBK", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is in contrast with another popular approach based on tensor train tt decomposition which requires several constraints on the core tensors such as the rank of the first and last core tensor to b", "to learn tr representations the paper presents a non-iterative tr-svd algorithm that is similar to tt-svd algorithm", "to find the optimal lower tr-ranks a block-wise als algorithms is presented and an sgd algorithm is also presented to make the model scal", "the proposed method is compared against the tt method on some synthetic high order tensors and on an image completion task and shown to yield better result", "this is an interesting work"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["tt decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limit", "this seems to be a good direct", "the experimental results are somewhat limited but the overall framework looks app", "the experimental results are somewhat limited but the overall framework looks app", "this paper demonstrate that by freezing all the penultimate layers at the end of regular training improves gener"], "labels": ["SMY", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the results do not convince this reviewer to switch to using 'post-training'", "learning features and then use a classifier such as a softmax or svm is not new and were actually widely used  years ago", "however freezing the layers and continue to train the last layer is of a minor novelti", "the results of the paper show a generalization gain in terms of better test time performance however it seems like the gain could be due to the lambda term which is added for post-training but not added for the baselin", "the results of the paper show a generalization gain in terms of better test time performance however it seems like the gain could be due to the lambda term which is added for post-training but not added for the baselin"], "labels": ["DFT", "CRT", "DFT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cf eq  and eq therefore it's unclear whether the gain in generalization is due to an additional lambda term or from the post-training training itself", "a way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwis", "other notes remark  while it is true that dropout would change the feature function to say that dropout 'should not be' applied it would be good to support that statement with some experi", "for table  please use decimal points instead of comma", "d data processing is very important topic nowadays since it has a lot of applications robotics ar/vr etc"], "labels": ["DFT", "SUG", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["d data processing is very important topic nowadays since it has a lot of applications robotics ar/vr etc", "current approaches to d image processing based on deep neural networks provide very accurate results and a wide variety of different architectures for image modelling generation classification retriev", "the lack of dl architectures for d data is due to complexity of representation of d data especially when using d point cloud", "considered paper is one of the first approaches to learn gan-type generative model", "using pointnet architecture and latent-space gan the authors obtained rather accurate generative model"], "labels": ["DIS", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written results of experiments are convincing the authors provided the code on the github realizing their architectur", "thus i think that the paper should be publish", "the paper proposes a piecewise linear activation function that is build on elu", "in general it was an ok paper and there are many to be improved+ novelty seems minor", "in my sense the authors do not provide any evidence theoretically or analysis on why the shifted version of elu which does not pass the origin is more favor"], "labels": ["APC", "FBK", "SMY", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea proposed in the paper is just a stack of better experi", "why the ultimate shape irrelevant of their initialization relu lrelu etc results in the same shap", "section  seems to provide a breakdown of how they formulate the piecewise linear function which the difference from alostinelli et al  is not clearly st", "in  section  the shifted version delta is abruptly proposed only based on results presented in  could improve learn", "this is not a professional ml paper looks lik"], "labels": ["APC", "QSN", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ experiment not strong to support the idea", "experiments are only conducted in cifar", "this is obviously not enough", "in table  i see svelu is better for lenet and shelu is better for clevert- network which form sh or sv do you use as final candidate via the title name sh win", "and the performance seems to be trivial among each other elu  shelu  the current sotas for cifar could reach below %"], "labels": ["CRT", "CRT", "DFT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the paper needs to be re-organized in a better way eg state clearly the difference from previous methods how to formulate the story etc for now i don't think it is ready to iclr", "also the paper needs to be re-organized in a better way eg state clearly the difference from previous methods how to formulate the story etc for now i don't think it is ready to iclr", "the authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogu", "in their evaluations this approach and one that additionally uses policy gradient rl with discourse-level objectives to fine-tune the dialogue act predictions outperform past models for human-scored response quality and conversation engag", "while this is a fairly straightforward idea with a long history the authors claim to be the first to use dialogue act prediction for open-domain rather than task-driven dialogu"], "labels": ["CRT", "FBK", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if that claim to originality is not contested and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue system", "some points the introduction uses scalability throughout to mean something closer to ability to generalize consider revising the wording her", "the dialogue act tag set used in the paper is not original to ivanovic  but derives with modifications from the tag set constructed for the damsl project jurafsky et al  stolcke et ", "it's probably worth citing some of this early work that pioneered the use of dialogue acts in nlp since they discuss motivations for building da corpora", "in section  the authors don't explicitly mention existing da-annotated corpora or discuss specifically why they are not sufficient is there eg a dataset that would be ideal for the purposes of this paper except that it isn't large enough"], "labels": ["DIS", "SUG", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors appear to consider only one option selecting the top predicted dialogue act then conditioning the response generator on this da among many for inference-time search over the joint da-response spac", "a more comprehensive search strategy eg selecting the top k dialogue acts then evaluating several responses for each da might lead to higher response divers", "the description of the rl approach in section  was fairly terse and included a number of ad-hoc choic", "if these choices like the dialogue termination conditions are motivated by previous work they should be cit", "examples perhaps in the appendix might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reason"], "labels": ["SMY", "SUG", "SMY", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the comparison against previous work is missing some assurances i'd like to se", "while directly citing the codebases you used or built off of is fantastic it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to", "without that there could always be the chance that something is missing from the implementation of eg rl-ss that you're using for comparison", "table  is not described in the main text so it isn't clear what the different potential outputs of eg the rl-dagm system result from my guess conditioning the response generation on the top  predicted dialogue act", "a simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs especially in later sect"], "labels": ["SUG", "SUG", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's fine if that pushes the paper somewhat over the th pag", "a consistent focus on human evaluation as found in this paper is probably the right approach for contemporary dialogue research", "the examples provided in the appendix are great", "it would be helpful to have confirmation that they were selected randomly rather than cherry-pick", "this paper gives an elaboration on the gated attention reader gar adding gates based on answer elimination in multiple choice reading comprehens"], "labels": ["SUG", "APC", "APC", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compel", "in my opinion the main weakness of the paper is the focus on the race dataset", "this dataset has not attracted much attention and most work in reading comprehension has now moved to the squad dataset for which there is an active leader board", "i realize that squad is not explicitly multiple choice and that this is a challenge for an answer elimination architectur", "however it seems that answer elimination might be applied to each choice of the initial position of a possible answer span"], "labels": ["APC", "CRT", "CRT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in any case competing with an active leader board would be much more compel", "this paper aims to learn a single policy that can perform a variety of tasks that were experienced sequenti", "the approach is to learn a policy for task  then for each task k+ copy distilled policy that can perform tasks -k finetune to task k+ and distill again with the additional task", "the results show that this plaid algorithm outperforms a network trained on all tasks simultan", "questions- when distilling the policies do you start from a randomly initialized policy or do you start from the expert policy network"], "labels": ["DIS", "SMY", "SMY", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what data do you use for the distil", "section  stateswe use a method similar to the dagger algorithm but what is your method", "if you generate trajectories form the student network and label them with the expert actions does that mean all previous expert policies need to be kept in memori", "- i do not understand the purpose of input injection nor where it is used in the pap", "strengths- the method is simple but novel"], "labels": ["APC", "DIS", "QSN", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results support the method's util", "- the testbed is nice the tasks seem significantly different from each oth", "it seems that no reward shaping is us", "- figure  is helpful for understanding the advantage of plaid vs multitask", "weaknesses- figure  the plots are too smal"], "labels": ["APC", "APC", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- distilling may hurt performance  figure d", "- the method lacks details see questions abov", "- no comparisons with prior work are provid", "the paper cites many previous approaches to this but does not compare against any of them", "- a second testbed such as navigation or manipulation would bring the paper up a notch"], "labels": ["CRT", "DFT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- a second testbed such as navigation or manipulation would bring the paper up a notch", "in conclusion the paper's approach to multitask learning is a clever combination of prior work", "the method is clear", "but not precisely describ", "the results are promis"], "labels": ["DIS", "APC", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think that this is a good approach to the problem that could be used in real-world scenario", "with some filling out this could be a great pap", "overall authors defined a new learning task that requires a dnn to predict mixing ratio between sounds from two different class", "previous approaches to training data mixing are  from random classes or  from the same class", "the presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network"], "labels": ["APC", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results look like significant improvements over standard learning setup", "detailed evaluation the approach presented is simple clearly presented and looks effective on benchmark", "in terms of originality it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative pow", "the authors have also provided extensive analysis from the point of views  network architecture  mixing method  number of labels / classes in mix  mixing layers -- really well done due-diligence across different model and task paramet", "minor asks clarification on how the error rates are defin"], "labels": ["APC", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["especially since the standard learning task could be - loss and this new bc learning task could be based on distribution divergence if we're not using argmax as class label", "#class_pairs targets as analysis - the number of epochs needed is naturally going to be higher since the bc-dnn has to train to predict mixing ratios between pairs of class", "since pairs of classes could be huge if the total number of classes is large it'll be nice to see how this scal", "ie are we talking about a space of  total classes or  total class", "how does num required epochs get impacted as we increase this class spac"], "labels": ["DIS", "CRT", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarify how g_/ and g_/ is important / derived - i assume it's unit conversion from decibel", "please explain why it is important to use the smoothed average of  softmax predictions in evalu", "what happens if you just randomly pick one of the  crops for predict", "the authors suggest using a variational autoencoder to infer binary relationships between medical ent", "the model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed befor"], "labels": ["CRT", "CRT", "QSN", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the paper is very well-written", "i have certain concerns regarding the motivation model and evaluation methodology follow", "a stronger motivation for this model is requir", "having a generative model for causal relationships between symptoms and diseases is intriguing yet i am really struggling with the motivation of getting such a model from word co-occurences in a medical corpu", "i can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the word embeddings used seem to be sufficient to capture the knowledge included in the corpu", "an ablation study of the impact of word embeddings on this model is requir", "the authors do not describe how the data from xywycom were annot", "were they annotated by experts in the medical domain or random us", "the metric of quality is particularly ad-hoc"], "labels": ["APC", "CRT", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[main comments]* i would advice the authors to explain in more details in the introwhat's new compared to li & malik  and andrychowicz et ", "it took me until section  to figure it out", "* if i understand correctly the only new part compared to li & malik  issection  where block-diagonal structure is imposed on the learned matric", "is that correct", "* in the experiments why not comparing with li & malik  ie without  block-diagonal structur"], "labels": ["SUG", "DIS", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* please clarify whether the objective value shown in the plots is wrt the training  set or the test set", "reporting the training objective value makes littlesense to me unless the time taken to train on mnist is taken into account inthe comparison", "* please clarify what are the hyper-parameters of your meta-training algorithm  and how you chose them", "i will adjust my score based on the answer to these quest", "[other comments]* given this state of affairs perhaps it is time for us to start practicing  what we preach and learn how to learnthis is in my opinion too casual for a scientific publ"], "labels": ["SUG", "DIS", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* aim to learn what parameter values of the base-level learner are useful  across a family of related tasksif this is essentially multi-task learning why not calling it so  learningwhat to learn does not mean anyth", "i understand that the authors wanted tohave what which and how sections but this is not clear at al", "what is a base-level learner i think it would be useful to define it moreprecisely early on", "what is a base-level learner i think it would be useful to define it moreprecisely early on", "* i don't see the difference between what is described in section   learning which model to learn and usual machine learning searching forthe best hypothesis in a hypothesis class"], "labels": ["CRT", "CRT", "SUG", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* typo p captures the how - p captures how", "* the l-bfgs results reported in all figures looked suspicious to m", "how do you  explain that it converges to a an objective value that is so much wors", "moreover the fact that there are huge oscillations makes me think that theauthors are measuring the function value during the line search rather thanthat at the end of each iter", "summary========the authors present clever an algorithm which consists in evaluating the local lipschitz constant of a trained network around a data point"], "labels": ["CRT", "CRT", "QSN", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network", "the method proposed in the paper already exists for classical function they only transpose it to neural network", "moreover the lower bound comes from basic results in the analysis of lipschitz continuous funct", "clarity=====the paper is clear and well-written", "originality=========this idea is not new if we search for lipschitz constant estimation in google scholar we get for examplewood g r and b p zhang estimation of the lipschitz constant of a function which presents a similar algorithm ie estimation of the maximum slope with reverse weibul"], "labels": ["SMY", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["technical quality==============the main theoretical result in the paper is the analysis of the lower-bound on delta the smallest perturbation to apply ona data point to fool the network", "this result is obtained almost directly by writing the bound on lipschitz-continuous function | fy-fx |  l || y-x ||where x = x_ and y = x_ + delta", "comments- lemma  why citing paulavicius and zilinskas for the definition of lipschitz continu", "moreover a lipschitz-continuous function does not need to be differentiable at all eg |x| is lipschitz with constant  but sharp at x=", "indeed this constant can be easier obtained if the gradient exists but this is not a requir"], "labels": ["SMY", "CRT", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- flaw theorem   this theorem works for fixed target-class since g = f_c - f_j for fixed g however once g = min_j f_c - f_j this theorem is not clear with the constant lq indeed the function g should be gx = min_{k eq c} f_cx - f_kx", "thus its lipschitz constant is different potentially equal tol_q = max_{k} | l_q^k | where l_q^k is the lipschitz constant of f_c-f_k", "if the theorem remains unchanged after this modification you should clarify the proof", "otherwise the theorem will work with the maximum over all lipschitz constants but the theoretical result will be weaken", "- theorem  i do not see the purpose of this result in this paper this should be better motiv"], "labels": ["CRT", "DIS", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["numerical experiments====================globally the numerical experiments are in favor of the presented method", "the authors should also add information about the time it takes to compute the bound the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial exampl", "moreover the numerical experiments look to be realized in the context of targeted attack", "to show the real effectiveness of the approach the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack", "#######################################################post-rebuttal review---------------------------given the details the authors provided to my review i decided to adjust my scor"], "labels": ["APC", "SUG", "DIS", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is simple and shows to be extremely effective/accurate in practic", "detailed answers indeed i was not aware that the paper only focuses on one dimensional funct", "however they still work with less assumption ie with no differential funct", "i was pointing out the similarities between their approach and your the two algorithms clever and slope are basically the same and using a limit you can go from slope to gradient norm", "in any case i have read the revision and the additional numerical experiment to compare clever with their method is a good point"], "labels": ["APC", "DIS", "DFT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall our analysis is simple and more intuitive and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work", "this is right i am just surprised is has not been done before since it requires only few lines of deriv", "i searched a bit but it is not possible to find any kind of similar result", "moreover this leads to good performances so there is no needs to have something more complex", "the usual lipschitz continuity is defined in terms of l norm and the extension to an arbitrary lp norm is not straightforwardindeed people usually use the lipschitz continuity using the lnorm but the original definition is wid"], "labels": ["SMY", "DIS", "DIS", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quickly if you have a differential scalar function from a space e - r then the gradient is a function from space e to e* the dual of the space ", "let ||  || the norm of space e then ||  ||* is the dual norm of |||| and also the norm of e*in that case lipschitz continuity writesfx-fy = l || x-y || with l = max_{x in e*} || f'x ||*in the case where ||  || is an ell-p norm then ||  ||* is an ell-q norm with /p+/q =", "if you are interested there is a clear and concise explanation in the introduction of this paper accelerating the cubic regularization of newtonus method on convex problems by yurii nesterov", "i have no additional remarks for  -  since everything is fixed in the new version of the pap", "this paper proposes a method to automatically tuning the momentum parameter in momentum sgd methods which achieves better results and fast convergence speed than state-of-the-art adam algorithm"], "labels": ["DIS", "DIS", "SUG", "FBK", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the results are promis", "i found the presentation of this paper almost inaccessible to m", "first though a minor point but where does the name *yellowfin* come from", "for the presentation the motivation in introduction is fin", "but the following section about momentum operator is hard to follow"], "labels": ["APC", "CRT", "QSN", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are a lot of undefined not", "for example what does the *convergence rate* mean what is the measurement for converg", "and is the *optimal accelerated rate* the same as *convergence rate* mentioned abov", "also what do you mean by *all directions* in the sentence below eq", "then the paper talks about robustness properties of the momentum oper"], "labels": ["CRT", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then the paper talks about robustness properties of the momentum oper", "but first i am not sure why the derivative of fx is defined as in eq how is that related to the original definition of deriv", "in the following paragraph what is *contraction*", "does it have anything to do with the paper as i didn't see it in the remaining text", "lemma  seems to use the spectral radius of the momentum operator as the *robustness*"], "labels": ["DIS", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but how can it describe the robust", "more details are needed to understand thi", "what it comes to section  it seems to me that the authors try to use a local quadratic approximation for the original function fx and use the results in last section to find the optimal momentum paramet", "i got confused in this section because eq defines fx as a quadratic funct", "is this fx the original function non quadratic or just the local quadratic approxim"], "labels": ["QSN", "DFT", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if it is the local quadratic approximation how is it correlated to the original funct", "it seems to me that the authors try to say if h and c are calculated from the original function then this fx is a local quadratic approxim", "if what i think is correct i think it would be important to show thi", "also the objective function in singlestep algorithm seems to come from eq but i failed to get the exact reason", "overall i think this is an interesting pap"], "labels": ["QSN", "QSN", "DIS", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the presentation is too fuzzy to get it evalu", "this paper adds source side dependency syntax trees to an nmt model without explicit supervis", "exploring the use of syntax in neural translation is interesting but i am not convinced that this approach actually works based on the experimental result", "the paper distinguishes between syntactic and semantic objectives th paragraph in section  attention and head", "please define what semantic attention i"], "labels": ["CRT", "SMY", "CRT", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["you just introduce this concept without any explan", "i believe you mean standard attention if so please explain why standard attention is semant", "clarity what is shared attention exactli", "section  says that you share attention weights from the decoder with encoder please explain this a bit mor", "also the example in figure  is not very clear and did not help me in understanding this concept"], "labels": ["CRT", "SUG", "QSN", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results a good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structur", "flat attention seems to add a self-attention model and is somewhat comparable to two mechan", "the results show hardly any improvement over the flat attention baseline at most  bleu which is well within the variation of different random initi", "it looks as if the improvement comes from adding additional capacity to the model", "equation  please define h"], "labels": ["APC", "SMY", "CRT", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper was fairly easy to follow", "but i would not say it was well written", "these are minor annoyances there were some typos and a strange citation format", "there is nothing wrong with the fundamental idea itself", "but given the experimental results it just is not clear that it is work"], "labels": ["APC", "CRT", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe bot performance significantly better than the fully trained ag", "this leads to a few questions what was the performance of the regression policy that was learned during the supervised pretraining phas", "n given enough time would the basic rl agent reach similar performance guessing no why not", "considering the results of figure  right shouldn't the conclusion be that the rl portion is essentially contributing noth", "prosthe regularization of the q-values wrt the policy of another agent is interest"], "labels": ["DIS", "QSN", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consnot very well setup experimentsperformance is lower than you would expect just using supervised trainingnot clear what parts are working and what parts are not", "the paper introduces a modified actor-critic algorithm where a ucguide actorud uses approximate second order methods to aid comput", "the experimental results are similar to previously proposed method", "the paper is fairly well-written provides proofs of detailed properties of the algorithm and has decent experimental result", "however the method is not properly motiv"], "labels": ["CRT", "SMY", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i can tell the paper never answers the questions why do we need a guide actor", "what problem does the guide actor solv", "the paper argues that the guide actor allows to introduce second order methods but  there are other ways of doing so and", "itus not clear why we should want to use second-order methods in reinforcement learning in the first plac", "using second order methods is not an end in itself"], "labels": ["QSN", "QSN", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental results show the authors have found a way to use second order methods without making performance *worse*", "given the high variability of deep rl they have not convincingly shown it performs bett", "the paper does not discuss the computational cost of the method", "how does it compare to other method", "my worry is that the method is more complicated and slower than existing methods without significantly improved perform"], "labels": ["APC", "APC", "DFT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm", "this paper propose a variant of generative replay buffer/memory to overcome catastrophic forget", "they use multiple copy of their model dgmn as short term memories and then consolidate their knowledge in a larger dgmn as a long term memori", "the main novelty of this work are -balancing mechanism for the replay memori", "-using multiple models for short and long term memori"], "labels": ["SUG", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the most interesting aspect of the paper is using a generate model as replay buffer which has been introduced befor", "as explained in more detail below it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forget", "the paper claims using the task id either from oracle or from a hmm is an advantage of the model", "it is not clear to me as why is the case if anything it should be the opposit", "humans and animal are not given task id and it's always clear distinction between task in real world"], "labels": ["APC", "CRT", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["deep generative replay section and description of dgdmn are written poorly and is very incomprehens", "it would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechan", "using the sttm during testing means essentially number of sttm +  models are used which is not same as preventing one network from catastrophic forget", "baselines why is shin et al  not included as one of the baselin", "as it is the closet method to this paper it is essential to be compared against"], "labels": ["CRT", "SUG", "DIS", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i disagree with the argument in section   a good robust model against catastrophic forgetting would be a model that still can achieve close to sota", "overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capac", "pages is very long  pages was the suggested page limit itus understandable if the page limit is extend by one page but  pages is over stretch", "*summary*the paper proposes using batch normalisation at test time to get the predictive uncertainti", "the stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each lay"], "labels": ["CRT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is justified by an argument that using batch norm is doing variational inference so one should use the approximate posterior provided by batch norm at prediction tim", "several experiments show monte carlo prediction at test time using batch norm is better than dropout", "*originality and significance*as far as i understand almost learning algorithms similar to equation  can be recast as variational inference under equ", "however the critical questions are what is the corresponding prior what is the approximating density what are the additional approximations to obtain  and whether the approximation is a good approximation for getting closer to the posterior/obtain better predict", "it is not clear to me from the presentation what the qw density is -- whether this is explicit as in vanilla gaussian vi or mc dropout or implicit the stochasticity on the activity h due to batch norm induces an equivalence q on w"], "labels": ["APC", "APC", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from a bayesian perspective it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small lambda] -- what is the rationale of thi", "can this be explained by comparing the variational free-energi", "the experiments also do not compare to modern variational inference methods using the reparameterisation trick with gaussian variational approximations see blundell et al  or richer variational families see eg louizos and wel", "the vi method included in the pbp paper hernandez-lobato and adams  does not use the reparameterisation trick which has been found to reduce variance and improve over graves' vi method", "*clarity*the paper is in general well written and easy to understand"], "labels": ["CRT", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["*additional comments*page  monte carlo droput -- dropoutpage  related work adams  should be hernandez-lobato and adam", "this paper looks at a specific aspect of the learning-to-teach problem where the learner is assumed to have a teacher that selects training examples for the student according to a strategi", "the teacher's strategy should also be  learned from data", "in this case the authors look at finding interpretable teaching strategi", "the authors define the good strategies as similar to intuitive strategies based on human intuition about the structure of the domain or strategies that are effective for teaching human"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the suggested method follow an iterative process in which the student and teacher are interchangeably us", "at each iteration the teacher generates  examples based on the students current concept", "i found it very difficult to follow the claims in the pap", "why is it assumed that human intuition is necessarily good", "the experiments do not answer these questions but are designed to show that the suggested approach follows human intuit"], "labels": ["SMY", "SMY", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are not enough details to get a good grasp of the suggested method and the different choices for it  and similarly the experiments are not described in a very convincing way", "specifically - the domains picked seem very contrived  there actual results are not reported the size of the data seems minimal so it's not clear what is actually learn", "how would you analyze the teaching strategy in realistic cases where there is no simple intuitive strategy this would be more convinc", "well written and appropriately structur", "well within the remit of the confer"], "labels": ["DIS", "DIS", "QSN", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not much technical novelty to be found", "but the original contributions are adequately identified and they are interesting on their own", "my main concern and complaint is not technical but application-bas", "this study is unfortunately typical in that it focuses on and provides detail of the technical modeling issues but ignores the medical applicability of the model and result", "this is exemplified by the fact that the data set is hardly described at all and the  abnormalities/pathologies the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint"], "labels": ["CRT", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if i were a medical expert i would not have a clue about how these results and models could be applied in practice or about what medical insight i could achiev", "the bottom line seems to be my model and approach works better than the other guys' model and approach but one is left with the impression that these experiments could have been made with other data other problems other fields of application and they would not have not changed much", "the authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural network", "experimental results on cifar- and cifar- were reported using alexnet and vgg-", "the results seem to show that a delayed application of the regularization parameter leads to improved classification perform"], "labels": ["DIS", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed scheme which delays the application of regularization parameter seems to be in contrast of the continuation approach used in sparse learn", "in the latter case a stronger parameter is applied followed by reduced regularization paramet", "one may argue that the continuation approach is applied in the convex optimization case while the one proposed in this paper is for non-convex optim", "it would be interesting to see whether deep networks can benefit from the continuation approach and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on", "one limitation of the work as pointed by the authors is that experimental results on big data sets such as imagenet is not report"], "labels": ["DIS", "SMY", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["an unsupervised approach is proposed to build bilingual dictionaries without parallel corpora by aligning the monolingual word embeddings spaces ia via adversarial learn", "the paper is very well-written and makes for a rather pleasant read save for some need for down-toning the claims to novelty as voiced in the comment re ravi & knight  or simply in gener", "the paper is very well-written and makes for a rather pleasant read save for some need for down-toning the claims to novelty as voiced in the comment re ravi & knight  or simply in gener", "the paper is very well-written and makes for a rather pleasant read save for some need for down-toning the claims to novelty as voiced in the comment re ravi & knight  or simply in gener", "it's a very nice paper i enjoy reading it *in spite* and not *because* of the text sales-pitching itself at tim"], "labels": ["SMY", "SUG", "DFT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction eg the work by vulic & moen", "there are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction eg the work by vulic & moen", "nthe evaluation is for the most part intrinsic and it would be nice to see the approach applied downstream beyond the simplistic task of english-esperanto translation plenty of outlets out there for applying multilingual word embed", "nthe evaluation is for the most part intrinsic and it would be nice to see the approach applied downstream beyond the simplistic task of english-esperanto translation plenty of outlets out there for applying multilingual word embed", "would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest"], "labels": ["SUG", "DFT", "SMY", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in my view to conclude this is still a very nice paper so i vote clear accept in hope to see these minor flaws filtered out in the revis", "in my view to conclude this is still a very nice paper so i vote clear accept in hope to see these minor flaws filtered out in the revis", "this work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or rl", "the experimentation and basic results are probably sufficient for acceptance but to this reviewer the paper spins the actual experiments and results a too strongli", "the biggest two nitpicks in our work we pursue an alternative approach instead of restricting the search space directly we allow the architectures to have flexible network topologies arbitrary directed acyclic graph"], "labels": ["APC", "FBK", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a gross overstatement the architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution separable convolution and pooling lay", "only the topology of the cells themselves are design", "the work is still great", "but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertast", "as an exercise to the authors count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire vgg- network"], "labels": ["CRT", "DIS", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems fewer hyperparameters are needed to describe vgg- making this paper hardly an alternative to the [common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search", "table why is the second best method on cifar uchier repr-n random search  samplesud never tested on imagenet", "the omission is conspicu", "just test it and report", "smaller nitpicks ucnew state of the art for evolutionary strategies on this task"], "labels": ["CRT", "QSN", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["uducevolutionary strategiesud at least as used in salimans  has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do", "it may be more clear to change this phrase to ucevolutionary methodsud or similar", "our evolution algorithm is similar but more generic than the binary tournament selection k =  used in a recent large-scale evolutionary method real et ", "a k=% tournament does not seem more generic than a binary k= tournament theyure just differ", "this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rul"], "labels": ["QSN", "DIS", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have a few questions on the motivation and the result", "- in the section open problems in raml both i and ii are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q", "this is not tru", "the globally optimal solution is related to both the underlying data distribution p and q and not the same as q", "it is given by q'y | x tau = sum_{y'} py' | x qy | y' tau"], "labels": ["DIS", "DIS", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- both theorem  and theorem  do not directly justify that raml has similar reward as the bayes decision rul", "can anything be said about thi", "can anything be said about thi", "are the kl divergence small enough to guarantee similar predictive reward", "- in theorem  when does the exponential tail bound assumption hold"], "labels": ["CRT", "QSN", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in table  the differences between raml and sqdml do not seem to support the claim that sqdml is better than raml", "are the differences actually signific", "are the differences between sqdml/raml and ml signific", "in addition how should tau be chosen in these experi", "in addition how should tau be chosen in these experi"], "labels": ["CRT", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper is an application paper on detecting when a face is disguis", "however it is poorly written and do not contribute much in terms of novelty of the approach", "the application domain is interest", "however it is simply a classification problem", "the paper is written clearli"], "labels": ["SMY", "CRT", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with mistakes in an equation however it does not contribute much in terms of novelty or new idea", "to make the paper better more empirical results are need", "to make the paper better more empirical results are need", "in addition it would be useful to investigate how this particular problem is different than a binary classification problem using cnn", "in this paper the authors present a computational framework for the active vision problem"], "labels": ["CRT", "SUG", "DFT", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["motivating the study biologically the authors explain how the control policy can be learned to reduce the entropy of the posterior belief and present an application mnist digit classification to substantiate their propos", "i am not convinced about the novelty and contribution of the work", "the active vision/sensing problem has been well studied and both the information theory and bayes risk formulations have already been considered in previous works see najemnik and geisler  butko and movellan  ahmad and yu", "the paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision examples foveate inspection the data abstract may allow to motivation tu put it clear motivation on contrary to animals retina footnote  minimize at most the current uncertainty perception-driven control center an keep fovea-based implementation degrade te recognition outlook and perspect", "the citations are in non-standard format section  kalman"], "labels": ["SMY", "CRT", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think the paper considers an important problem but the contribution to the state of the art is minimal and editing highly lack", "j najemnik and w s geisler optimal eye movement strategies in visual search nature u  n j butko and j r movellan infomax control of eye movements ieee transactions on autonomous mental development u  s ahmad and a j yu active sensing as bayes-optimal sequential decision-making uncertainty in artificial intellig", "in this paper the authors consider symmetric rd order cp decomposition of a ppmi tensor m from neighboring triplets which they call cp-", "additionally they propose an extension jcp-s for n-order tensor decomposit", "this is then compared with random wordvec and nnse the latter of two which are matrix factorization based or interpretable method"], "labels": ["CRT", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is shown to be superior in tasks of -way outlier detection supervised analogy recovery and sentiment analysi", "additionally it is evaluated over the men and mturk dataset", "for the jcp-s model the loss function is unclear to m", "l is defined for rd order tensors only  how is the extended to n", "intuitively it seems that l is redefined and for say n =  the model is mijkn = sum_^r u_ir u_jr u_kr u_nr"], "labels": ["SMY", "SMY", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the statement since we are using at most third order tensors in this work i am further confus", "is it just that jcp-s also incorporates nd order embed", "i believe this requires clarification in the manuscript itself", "for the evaluations there are no other tensor-based methods evaluated although there exist several well-known tensor-based word embedding models existingpengfei liu xipeng qiuu and xuanjing huang learning context-sensitive word embeddings with neural tensor skip-gram model  ijcai jingwei zhang and jeremy salwen michael glass and alfio gliozzo", "word semantic representations using bayesian probabilistic tensor factorization emnlp mo yu mark dredze raman arora matthew r gormley embedding lexical features via low-rank tensorsto name a few via quick googl"], "labels": ["CRT", "QSN", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["additionally since it seems the main benefit of using a tensor-based method is that you can use rd order cooccurance information multisense embedding methods should also be evalu", "there are many such methods see for example jiwei li dan jurafsky do multi-sense embeddings improve natural language understand", "and citations within plus quick googling for more recent work", "i am not saying that these works are equivalent to what the authors are doing or that there is no novelty but the evaluations seem extremely unfair to only compare against matrix factorization techniques when in fact many higher order extensions have been proposed and evaluated and especially so on the tasks proposed in particular the -way outlier detect", "observe also that in table  nnse gets the highest performance in both men and mturk"], "labels": ["SUG", "QSN", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["frankly this is not very surprising matrix factorization is very powerful and these simple word similarity tasks are well-suited for matrix factor", "so statements like as we can see our embeddings very clearly outperform the random embedding at this task is  an unnecessary inflation of a result that  is not good", "and  is reasonable to not be good", "overall i think for a more sincere evaluation the authors need to better pick tasks that clearly exploit -way information and compare against other methods proposed to do the sam", "the multiplicative relation analysis is interest"], "labels": ["CRT", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model", "in conclusion because the novelty is also not that big cp decomposition for word embeddings is a very natural idea i believe the evaluation and analysis must be significantly strengthened for accept", "this manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the cayley transform and a scaling weight matrix", "this scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniqu", "comments-- itus not clear to me how d is determined for each test"], "labels": ["CRT", "CRT", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the definition in theorem  it seems like you would have to have some knowledge of how many eigenvalues in w you expect to be close to -", "-- for the copying and adding problem test cases it might be useful to clarify or cite something clarifying that the failure mode rnns run into with temporal ordering problems is an exploding gradient rather than any other pathological training condition just to make it clear why these experiments are relev", "-- the ylabel in figure  is uctest lossud which i didnut see defin", "is this test loss the cross entropi", "if so i think it would be more effective to label the plot with that"], "labels": ["CRT", "SUG", "DFT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- the plots in figure  and  have different colors to represent the same set of techniqu", "i would suggest keeping a  consistent color schem", "-- it looks like in figure  the scornn is outperformed by the urnn in the long run in spite of the scornn convergence being smoother which should be clarifi", "-- it looks like in figure  the scornn is outperformed by the lstm across the board which should be clarifi", "-- how is test set accuracy defined in sect"], "labels": ["SUG", "SUG", "SUG", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["classifying digits recreating digit", "-- when discussing table  the manuscript mentions scornn and restricted-capacity urnn have similar performance for k parameters and then state that scornn has the best test accuracy at %", "-- when discussing table  the manuscript mentions scornn and restricted-capacity urnn have similar performance for k parameters and then state that scornn has the best test accuracy at %", "however there is no example for restricted-capacity urnn with k parameters to show that the performance of restricted-capacity urnn doesn't also increase similarly with more paramet", "-- overall itus unclear to me how to completely determine the benefit of this technique over the others because for each of the tests different techniques may have superior perform"], "labels": ["QSN", "SMY", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance lstm performs best in  and in  for the mnist test accuraci", "scornn and restricted-capacity urnn perform similarly for permuted mnist test accuracy in", "finally scornn seems to far outperform the other techniques in table  on the timit speech dataset", "i donut understand the significance of each test and why the relative performance of the techniques vary from one to the oth", "-- for example the manuscript seems to be making the case that the scornn gradients are more stable than those of a urnn but all of the results are presented in terms of network accuracy and not gradient st"], "labels": ["DIS", "DIS", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["you can sort of see that generally the convergence is more gradual for the scornn than the urnn from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training as in figure  of the arjovsky  paper being compared to for instance just to make it really clear", "you can sort of see that generally the convergence is more gradual for the scornn than the urnn from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training as in figure  of the arjovsky  paper being compared to for instance just to make it really clear", "this paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be appli", "while the attack model is quite general the current bound is only valid for linear and nn with one hidden layer model so the result is quite restrict", "however the new bound is an upper bound of the worst-case performance which is very different from the conventional sampling based lower bound"], "labels": ["SUG", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifi", "this paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization loss + upper bound", "in conclusion i recommend this paper to be accepted since it presents a new and feasible direction of a principled approach to train a robust classifi", "in conclusion i recommend this paper to be accepted since it presents a new and feasible direction of a principled approach to train a robust classifi", "and the paper is clearly written and easy to follow"], "labels": ["APC", "DIS", "APC", "FBK", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are possible future directions to be develop", "apply the sum-of-squares sos methodthe paper's sdp relaxation is the straightforward relaxation of quadratic program qp and in terms of sos relaxation hierarchy it is the first hierarchi", "one can increase the complexity going beyond the first hierarchy and this should provides a computationally more challenging but tighter upper bound", "the paper already mentions about this direction and it would be interesting to see the experimental result", "develop a similar relaxation for deep neural networksthe author already mentioned that they are pursuing this direct"], "labels": ["DIS", "SUG", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["develop a similar relaxation for deep neural networksthe author already mentioned that they are pursuing this direct", "while developing the result to the general deep neural networks might be hard residual networks maybe fine thanks to its structur", "this paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors and representing each embedding as a list of the basis indic", "the proposed method can reduce more than % memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation task", "overall the paper is well-written"], "labels": ["DIS", "DIS", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the motivation is clear the idea and approaches look suitable and the results clearly follow the motiv", "i think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding lay", "i think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding lay", "for example the model does not guarantee to be able to convert resulting indices to actual words ie there are multiple words that have completely same indices such as rows  and  in table  and also there is no trivial method to restore the original indices from the composite vector", "for example the model does not guarantee to be able to convert resulting indices to actual words ie there are multiple words that have completely same indices such as rows  and  in table  and also there is no trivial method to restore the original indices from the composite vector"], "labels": ["APC", "DFT", "DIS", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a result the model couldn't be used also as the proxy of the word prediction softmax layer which is another but usually more critical bottleneck of the machine translation task", "for reader's comprehension it would like to add results about whole memory consumption of each model as wel", "also although this paper is focused on only the input embeddings authors should refer some recent papers that tackle to reduce the complexity of the softmax lay", "there are also many studies and citing similar approaches may help readers to comprehend overall region of these studi", "furthermore i would like to see two additional analysi"], "labels": ["CRT", "DIS", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first if we trained the proposed model with starting from zero eg randomly settling each index value what results are obtained second what kind of information is distributed in each trained basis vector are there any common/different things between bases trained by different task", "i think the first intuition is interest", "however i think the benefits are not clear enough", "maybe finding better examples where the benefits of the proposed regularization are stressed could help", "there is a huge amount of literature about ica unmixing pca infomax based on this principle that go beyond of the propos"], "labels": ["QSN", "SMY", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do not see a clear novelty in the propos", "for instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on pca", "as shown in [szegedy et al  intriguing properties of neural networks] adding an extra linear transformation does not change the expressive power of the represent", "- inspired by this we consider a simpler objective a representation disentangles the data well when its components do not correl", "the first paragraph is confusing since jumps from total correlation to correlation without making clear the differ"], "labels": ["CRT", "DIS", "DIS", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although correlation is a second oder approach to total correlation are not the same this is extremely important since the whole proposal is based on that", "- sec  what prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covari", "i think the definition needs to enforce the out-diagonal terms in c to be small with respect to the terms in the diagon", "- all the evaluation measures are based on linear relations some of them should take into account non-linear relations ie total correlation mutual information in order to show that the method gets something interest", "- the first experiment dim red is not clear to m"], "labels": ["DIS", "QSN", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the original dimensionality of the data is  and only a linear relation is introduced i do not understand the dimensionality reduction if the dimensionality of the transformed space i", "also the data problem is extremely simple and it is not clear the didactic benefit of using it", "i think a much more complicated data would be more interest", "besides l_ is not well defin", "if it is l_ norm on the output coefficients the comparison is mislead"], "labels": ["CRT", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- sec  as in general the model needs to be compared with other regularization techniques to stress its benefit", "- sec  here the comparison makes clear that not a real benefit is obtained with the propos", "the idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future sampl", "however the mse obtained when not using regularization is the same or even smaller than when using it", "the authors investigate knowledge distillation as a way to learn low precision network"], "labels": ["DIS", "CRT", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they propose three training schemes to train a low precision student network from a teacher network", "they conduct experiments on imagenet-k with variants of resnets and multiple low precision regimes and compare performance with previous work", "pros+ the paper is well written the schemes are well explained+ ablations are thorough and comparisons are fair", "cons- the gap with full precision models is still large -", "transferability of the learned low precision models to other tasks is not discuss"], "labels": ["SMY", "SMY", "APC", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors tackle a very important problem the one of learning low precision models without comprosiming perform", "for scheme-a the authors show the performance of the student network under many low precision regimes and different depths of teacher network", "one observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks see table   &", "as a matter of fact under some scenarios performance even decreas", "the authors do not discuss the gains of their best low-precision regime in terms of computation and memori"], "labels": ["APC", "SMY", "DFT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally the true applications for models with a low memory footprint are not necessarily related to image classification models eg imagenet-k", "how good are the low-precision models trained by the authors at transferring to other task", "is it possible to transfer student-teacher training practices to other task", "interesting ideas that extend lstm to produce probabilistic forecasts for univariate time series experiments are okay", "unclear if this would work at all in higher-dimensional time seri"], "labels": ["DIS", "QSN", "QSN", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is also unclear to me what are the sources of the uncertainties captur", "nthe author proposed to incorporate  different discretisation techniques into lstm in order to produce probabilistic forecasts of univariate time seri", "the proposed approach deviates from the bayesian framework where there are well-defined priors on the model and the parameter uncertainties are subsequently updated to incorporate information from the observed data and propagated to the forecast", "instead the conditional density py_t|y_{t-| theta} was discretised by  of the  proposed schemes and parameterised by a lstm", "the lstm was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised label"], "labels": ["DIS", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore the uncertainties produced by the model appear to be a black-box", "it is probably unlikely that the discretisation method can be generalised to high-dimensional set", "quality the experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth", "in the experiments with real data it's unclear how good the uncertainties produced by the model ar", "it may be useful to compare to the uncertainty produced by a gp with suitable kernel"], "labels": ["CRT", "QSN", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fig c the pct ci looks more or less constant over time is there an explanation for that", "in fig c the pct ci looks more or less constant over time is there an explanation for that", "clarity the paper is well-written", "the presentations of the ideas are pretty clear", "originality above averag"], "labels": ["CRT", "QSN", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clev", "significance averag", "it would be excellent if the authors can extend this to higher dimensional time seri", "i'm unsure about the correctness of algorithm  as i don't have knowledge in smc", "summary this work proposes a way to create d objects to fool the classification of their pictures from different view points by a neural network"], "labels": ["APC", "APC", "SUG", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rather than optimizing the log-likelihood of a single example the optimization if performed over a the expectation of a set of transformations of sample imag", "using an inception v net they create adversarial attacks on a subset of the imagenet validation set transformed by translations lightening conditions rotations and scalings among others and observe a drop of the classifier accuracy performance from % to less than %", "they also create two d printed objects which most pictures taken from random viewpoints are fooling the network in its class predict", "main comments- the idea of building d adversarial objects is novel so the study is interest", "however the paper is incomplete with a very low number of references only  conference papers if we assume the list is up to d"], "labels": ["SMY", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["see for instance cisse et al houdini fooling deep structured prediction models nips  for a recent list of related work in this research area", "- the presentation of the results is not very clear", "see specific comments below- it would be nice to include insights to improve neural nets to become less sensitive to these attack", "minor commentsfig  a bug with color seems to have been fixedmodel section be consistent with the not", "bold everywhere or nowher"], "labels": ["CRT", "CRT", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results the tables are difficult to read and should be clarifi", "what does the l metric stands for", "how about min max", "accuracy - classification accuraci", "models - d modelsdescribe each metric adversarial miss-classified correct"], "labels": ["CRT", "QSN", "QSN", "CNT", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall strengthin this paper the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attent", "this work has a well-established motivation traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predict", "to solve this problem the authors proposed five formulations in the final prediction lay", "the illustration about the problem is clear as well as the explanation for the formul", "major concernstthis work brings some modifications to the prediction layer which is a bit trivi"], "labels": ["APC", "APC", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the effect has been shown the model is too specific to a narrow area and is not general to be applied in a broad sens", "it could have more contribution if the authors model the interactions within the attention model itself instead of a simple prediction layer which is problem-depend", "tthe experiments are insufficient to show the effect", "it would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in t", "only two examples are not convinc"], "labels": ["CRT", "SUG", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["tin section  the authors claimed that  models the target and context independ", "however in section  in  the authors claimed the target vector v_t will affect the context shifting their representation to cu_i", "this should also work for", "tthere are too many typos in the paper eg alpha is replaced by a etc", "other concernstit seems that one needs to train at least three embedding matrices a c d which represent input embeddings output embeddings and interactive embeddings respect"], "labels": ["DIS", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wonder if this brings redundant parameters that do not guarantee converg", "why not use one matrix instead", "did the authors try experiments with less embedding matric", "tthere is another work that also considers the target-context interaction using interactive attention model", "please refer to this paper ucinteractive attention networks for aspect-level sentiment classificationud"], "labels": ["CRT", "QSN", "QSN", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a comparison is need", "tit is better to provide results in terms of accuracy for both datasets as previous methods usually use accuracy for comparison", "howus the score of the proposed model compared with the above paper as well as [tang et al ]", "qualitythis paper demonstrates that human category representations can be inferred by sampling deep feature spac", "the idea is an extension of the earlier developed mcmc with people approach where samples are drawn in the latent space of a dcgan and a bigan"], "labels": ["SUG", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach is thoroughly validated using two online behavioural experi", "claritythe rationale is clear and the results are straightforward to interpret", "in section  statements on resemblance and closeness to mean faces could be test", "last sentences on page  are hard to pars", "the final sentence probably relates back to the ci approach"], "labels": ["APC", "APC", "SUG", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a few typosoriginalitythe approach is a straightforward extension of the mcmcp approach using generative model", "significance the approach improves on previous category estimation approaches by embracing the expressiveness of recent generative model", "extensive experiments demonstrate the usefulness of the approach", "prosuseful extension of an important technique backed up by behavioural experi", "consdoes not provide new theory but combines existing ideas in a new mann"], "labels": ["APC", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary this paper proposes a multi-agent communication task where the agents learn to translate as a side-product to solving the communication task", "authors use the image modality as a bridge between two different languages and the agents learn to ground different languages to same image based on the similar", "this is achieved by learning to play the game in both directions authors show results in a word-level translation task and also a sentence-level translation task they also show that having more languages help the agent to learn bett", "my commentsthe paper is well-written and i really enjoyed reading this pap", "while the idea of pivot based common representation learning for language pairs with no parallel data is not new adding the communication aspect as an additional supervision is novel"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i would encourage authors to rephrase their claim of emergent translation the title is misleading as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages bridged by the image and hence there is no autonomous emergence of translation out of ne", "i see this work as adding communication to improve the translation learn", "is your equation  correct i understand that your logits are reciprocal of mean squared error", "is your equation  correct i understand that your logits are reciprocal of mean squared error", "but donut you need a softmax before applying the nll loss mentioned in equation  in current form of equation  i think you are not including the distractor images into account while computing the loss please clarifi"], "labels": ["CRT", "DIS", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but donut you need a softmax before applying the nll loss mentioned in equation  in current form of equation  i think you are not including the distractor images into account while computing the loss please clarifi", "what is the size of the vocabulary used in all the experi", "because gumbel softmax doesnut scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experi", "are you willing to release the code for reproducing the result", "minor commentsin appendix c table  caption you say target sentence is uctrgud but it is ucrefud in the t"], "labels": ["QSN", "QSN", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also is the reference sentence for skateboard example typo-fre", "this paper focuses on the learning-from-crowds problem when there is only one or very few noisy label per item", "the main framework is based on the dawid-skene model", "by jointly update the classifier weights and the confusion matrices of workers the predictions of the classifier can help on the estimation problem with rare crowdsourced label", "the paper discusses the influence of the label redundancy both theoretically and empir"], "labels": ["QSN", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results show that with a fixed budget itus better to label many examples once rather than fewer examples multiple tim", "the model and algorithm in this paper are simple and straightforward", "however i like the motivation of this paper and the discussion about the relationship between training efficiency and label redund", "the problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discuss", "the conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourc"], "labels": ["DIS", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["about the technique details this paper is clearly written", "but some experimental comparisons and claims are not very convinc", "here i list some of my questions+about the mbem algorithm itus better to make clear the difference between mbem and a standard em will it always converge whatus its object", "+the setting of theorem  seems too simple can the results be extended to more general settings such as when workers are not ident", "+when n = om log m the result that epslon_ is constant is counterintuitive people usually think large redundancy r can bring benefits on estimation can you explain more on thi"], "labels": ["APC", "DFT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+during cifar- experiments when r= each example only have one label for the baselines weighted-mv and weighted-em they can only be directly trained using the same noisy labels so can you explain why their performance is slightly different in most settings is it due to the randomly chosen procedure of the noisy label", "+for imagenet and ms-coco experiments with a fixed budget you reduced the training set when increasing the redundancy which is unfair", "the reduction of performance could mainly cause by seeing fewer raw images but not the label", "itus better to train some semi-supervised model to make the settings more compar", "the paper proposes a method which jointly learns the label embedding in the form of class similarity and a classification model"], "labels": ["QSN", "CRT", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the motivation of the paper makes sense the model is not properly justified and i learned very little after reading the pap", "there are  terms in the proposed objective function there are also several other parameters associated with them for example the label temperature of z_uu and and parameter alpha in the second last term etc", "for all the experiments the same set of parameters are used and it is claimed that ucthe method is robust in our experiment and simply works without fine tuningud", "while i agree that a robust and fine-tuning-free model is ideal  this has to be justified by experiment  showing the experiment with different parameters will help us understand the role each component play", "this is perhaps more important than improving the baseline method by a few point especially given that the goal of this work is not to beat the state-of-the-art"], "labels": ["DFT", "SMY", "SMY", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a method for learning representations for preposit", "they first take co-occurrence counts counts of pairs of words in a local window of each preposition and then factorize the matrix to find low dimensional word represent", "the main difference from previous work is restricting the context to be close to a preposit", "the authors report improved paraphrasing of phrasal verbs and state-of-the-art accuracy in correcting grammatical errors involving prepositions and good results on prepositional phrase attach", "- the paper frequently overclaim"], "labels": ["SMY", "SMY", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for one example weure told that ucpreposition selection [is] a major area of study in both syntactic and semantic computational linguisticsud but at best itus quite a specialized nich", "the paper would be much improved if it was generally toned down", "- the authors claim their method is ucvastlyud better at paraphrasing phrasal verbs than baselines based on qualitative comparison", "however i couldnut find any details on how the phrasal verbs were chosen or what if any held out data was used for tuning the model", "even assuming this is a meaningful task surely the natural baseline would be to treat these phrasal verbs as non-compositional eg extend the vocab with words like ucsparked_offud  and train wordvec"], "labels": ["APC", "CRT", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the other experiments are lacking important detail", "for example weure just told some values that hyperparameters were fixed at for both tasks - how were these chosen including for the baselin", "was the model architecture tuned based on the proposed represent", "were the word representations fixed or fine tuned during train", "- despite the authorus expectations that their representations will be uwidely usedu i am struggling to think of cases where they would be useful outside of the very specific tasks involving prepositions that they us"], "labels": ["CRT", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that is because almost all tasks require good representations for all words not just preposit", "the authors should add more justification for the where/how these representations will be us", "overall i think the technical contributions of the paper are quite limited and the experiments are not well enough described for publ", "the paper presents an interesting framework for babi qa", "essentially the argument is that when given a very long paragraph the existing approaches for end-to-end learning becomes very inefficient linear to the number of the sent"], "labels": ["CRT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed alternative is to encode the knowledge of each sentence symbolically as n-grams which is thus easy to index", "while the argument makes sense it is not clear to me why one cannot simply index the original text", "the additional encode/decode mechanism seems to introduce unnecessary nois", "the framework does include several components and techniques from latest recent work which look pretty sophist", "however as the dataset is generated by simulation with a very small set of vocabulary the value of the proposed framework in practice remains largely unproven"], "labels": ["SMY", "CRT", "CRT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros   an interesting framework for babi qa by encoding sentence to n-gram", "cons   the overall justification is somewhat unclear", "the approach could be over-engineered for a special lengthy version of babi and it lacks evaluation using real-world data", "the paper proposes to use the start-end rank to measure the long-term dependency in rnn", "it shows that deep rnn is signficantly better than shallow one in this metr"], "labels": ["APC", "CRT", "SUG", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theory part seems to be technical enough and interest", "though i haven't checked all the detail", "though i haven't checked all the detail", "the main concern with the paper is that i am not sure whether the rac studied by the paper is realistic enough for practic", "the main concern with the paper is that i am not sure whether the rac studied by the paper is realistic enough for practic"], "labels": ["APC", "SMY", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main concern with the paper is that i am not sure whether the rac studied by the paper is realistic enough for practic", "certain gating in rnn is very useful but i don't know whether one can train any reasonable rnn with all multiplicative g", "certain gating in rnn is very useful but i don't know whether one can train any reasonable rnn with all multiplicative g", "the paper will be much stronger if it has some experiments along this lin", "the paper will be much stronger if it has some experiments along this lin"], "labels": ["FBK", "SMY", "DIS", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper will be much stronger if it has some experiments along this lin", "this paper revisits the idea of exponentially weighted lambda-returns at the heart of td algorithm", "the basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value funct", "the paper empirically evaluates this idea on atari games with deep non-linear state representations compared to state-of-the-art baselin", "this paper is below the threshold because there are issues with the   motivation  the technical details and  the empirical result"], "labels": ["FBK", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper begins by stating that the exponential weighting of lambda returns is ad-hoc and unjustifi", "i would say the idea is well justified in several way", "first the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line vers", "second decades of empirical results illustrating good performance of td compared with mc method", "and an extensive literature of theoretical result"], "labels": ["CRT", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper claims that the exponential has been noted to be ad-hoc please provide a reference for thi", "there have been several works that have noted that lambda can and perhaps should be changed as a function of state sutton and barto white and white [] td-gammon", "in fact such works even not that lambda should be related to confid", "the paper should work harder to motivate why adapting lambda as a function of state---which has been studied---is not suffici", "i don't completely understand the object"], "labels": ["DIS", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["returns with higher confidence should be weighted higher according to the confidence estimate around the value function estimate as a function of st", "with longer returns n the role of the value function in the target is down-weighted by gamma^n---meaning its accuracy is of little relevance to the target", "how does your formalism take this into account", "the basic idea of the lambda return assumes td targets are better than mc targets due to variance which place more weight on shorter return", "i addition i don't understand how learning confidence of the value function has a realizable target"], "labels": ["QSN", "CRT", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we do not get supervised targets of the confidence of our value estim", "what is your network updating toward", "the work of konidaris et al [] is a more appropriate reference for this work rather than the thomas reference provid", "your paper does not very clearly different itself from konidaris's work her", "please expand on thi"], "labels": ["CRT", "QSN", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments have some issu", "one issue is that basic baselines could more clearly illustrate what is going on", "there are two such baselines random fixed weightings of the n-step returns and persisting with the usual weighting but changing lambda on each time step either randomly or according to some decay schedul", "the first baseline is a sanity check to ensure that you are not observing some random effect", "the second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or st"], "labels": ["CRT", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would say the current results indicate the conventional approach to td is working well if not better than the new on", "looking at fig  its clear the kangaroo is skewing the results and that overall the new method is performing wors", "this is further conflated by fig which attempts to illustrate the quality of the learned value funct", "in kangaroo the domain where your method does best the l error is wors", "on the other hand in sea quest and space invaders where your method does worse the l error is bett"], "labels": ["APC", "CRT", "DIS", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these results seem conflicting or at least raise more questions than they answ", "[] a greedy approach to adapting the trace parameter for temporal difference learn", "adam white and martha whit", "autonomous agents and multi-agent systems aama", "[] g d konidaris s niekum and p s thomas tdub re-evaluating complex backups in temporal difference learn"], "labels": ["CRT", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in advances in neural information processing systems  pages u", "this paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transform", "pipeline -data are augmented with domain-specific transform", "for instance in the case of mnist rotations with different degrees are appli", "all data are then labelled as original or transformed by specific transformation -classification task is performed with a neural network on augmented dataset according to the pseudo-label"], "labels": ["DIS", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["all data are then labelled as original or transformed by specific transformation -classification task is performed with a neural network on augmented dataset according to the pseudo-label", "-in parallel of the classification the neural network also learns the latent representation in an unsupervised fashion", "-k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax lay", "-k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax lay", "detailed comments* pros-the method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on mnist"], "labels": ["DIS", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["detailed comments* pros-the method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on mnist", "-use of acol and gar is interesting also the idea to make labeled data from unlabelled ones by using data augment", "-use of acol and gar is interesting also the idea to make labeled data from unlabelled ones by using data augment", "* cons-minor in the title i find the expression unsupervised clustering uselessly redundant since clustering is by definition unsupervis", "-choice of datasets we already obtained very good accuracy for the classification or clustering of handwritten digit"], "labels": ["DIS", "SMY", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is not a very challenging task", "and just because something works on mnist does not mean it works in gener", "what are the performances on more challenging datasets like colored images cifar- labelme imagenet etc", "-this is not clear what is novel here since acol and gar already exist", "the novelty seems to be in the adaptation to gar from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not"], "labels": ["SMY", "SMY", "QSN", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the novelty seems to be in the adaptation to gar from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not", "my main problem  was about the lack of novelti", "the authors clarified this point and it turned out that acol and gar have never published elsewhere except in arxiv", "the authors clarified this point and it turned out that acol and gar have never published elsewhere except in arxiv", "the other issue concerned the validation of the approach on databases other than mnist"], "labels": ["DIS", "DFT", "SUG", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the other issue concerned the validation of the approach on databases other than mnist", "the author also addressed this point and i changed my scores accordingli", "the author also addressed this point and i changed my scores accordingli", "the author also addressed this point and i changed my scores accordingli", "quality the work has too many gaps for the reader to fill in"], "labels": ["DIS", "SMY", "DIS", "FBK", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the generator reconstructed matrix is supposed to generate a - matrix adjacency matrix and allow backpropagation of the gradients to the gener", "i am not sure how this is achieved in this work", "the matrix is not isomorphic invariant and the different clusters donut share a common model", "even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clust", "how can such a limited technique be gener"], "labels": ["SMY", "CRT", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no metric in the results showing how the model generalizes it may be just overfitting the data", "nclarity the paper organization needs work there are also some missing pieces to put the nn training togeth", "it is only in section  that the nature of g_i^prime becomes clear", "although it is used in section  equation  is rather vague for a mathematical equ", "from what i understood from the text equation  creates a binary matrix from the softmax output using an indicator funct"], "labels": ["CRT", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the output is binary how can the gradients backpropagate is it backpropagating with a trick like the gumbel-softmax trick of jang gu and poole  or bengious path derivative estim", "this is a key point not discussed in the manuscript", "and if i misunderstood the sentence ucturn re_g into a binary matrixud and the values are continuous wouldnut the discriminator have an easy time distinguishing the generated data from the real data", "and wouldnut the generator start working towards vanishing gradients in its quest to saturate the re_g output", "originality the work proposes an interesting approach first cluster the network then learning distinct gans over each clust"], "labels": ["QSN", "DFT", "CRT", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are many such ideas now on arxiv but it would be unfair to contrast this approach with unpublished work", "there is no contribution in the gan / neural network aspect", "it is also unclear whether the model gener", "i donut think this is a good fit for iclr", "significance generating graphs is an important task in in relational learning tasks drug discovery and in learning to generate new relationships from knowledge bas"], "labels": ["DIS", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work itself however falls short of the go", "at best the generator seems to be working but i fear it is overfit", "the contribution for iclr is rather minimal unfortun", "minorgti was not introduced before it is first mentioned in the into", "y bengio n leonard and a courville estimating or propagating gradients through stochastic neurons for conditional computation arxiv"], "labels": ["CRT", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposed the new approach for feature upsampling called pixel deconvolution which aims to resolve checkboard artifact of conventional deconvolut", "by sequentially applying a series of decomposed convolutions the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some ext", "overall the paper is clearly written and easy to understand the main motivation and method", "however the checkboard artifact is a well-known problem of deconvolution network and has been addressed by several approaches which are simpler than the proposed pixel deconvolut", "for example it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent and bilinear additive upsampling proposed in wonja et al  also demonstrated its effectiveness as an alternative for deconvolut"], "labels": ["APC", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comparisons against these approaches would make the paper strong", "besides comparisons/discussions based on extensive analysis on various deconvolution architectures presented in wonja et al  would also be interest", "wonja et al the devil is in the decoder in bmvc", "the work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous controlproblems both in deterministic and randomized case whiling coping with non-convexity of the object", "i found that the paper suffers many shortcomings that must be addressed the writing and organization is quite cumbersome and should be improv"], "labels": ["SUG", "SUG", "DIS", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors state in the abstract and elsewhere  showing that model free policy gradient methods globally converge to the optimal solution  this is misleading and not tru", "the authors show the convergence of the objective but not of the iterates sequ", "this should be rephrased elsewher", "an important literature on convergence of descent-type methods for semialgebraic objectives is available but not discuss", "the authors have addressed my concerns and clarified a misunderstanding of the baseline that i had which i appreci"], "labels": ["CRT", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do think that it is a solid contribution with thorough experi", "i still keep my original rating of the paper because the method presented is heavily based on previous works which limits the novelty of the pap", "it uses previously proposed clipping activation function for quantization of neural networks adding a learnable parameter to this funct", "_______________original reviewthis paper proposes to use a clipping activation function as a replacement of relu to train a neural network with quantized weights and activ", "it shows empirically that even though the clipping activation function obtains a larger training error for full-precision model it maintains the same error when applying quantization whereas training with quantized relu activation function does not work in practice because it is unbound"], "labels": ["APC", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are thorough and report results on many datasets showing that pact can reduce down to  bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model", "related to that it seams a bit an over claim to state that the accuracy decrease of quantizing the dnn with pact in comparison with previous quantization methods is much less because the decrease is smaller or equal than % when competing methods accuracy decrease compared to the full-precision model is more than %", "also it is unfair to compare to the full-precision model using clipping because relu activation function in full-precision is the standard and gives much better results and this should be the reference accuraci", "also previous methods take as reference the model with relu activation function so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using pact for quantizing dnn", "other comments- the list of contributions is a bit strang"], "labels": ["APC", "CRT", "DFT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seams that the true contribution is number  on the list which is to introduce the parameter alpha in the activation function that is learned with back-propagation which reduces the quantization error with respect to using relu as activation funct", "to provide an analysis of why it works and quantitative results is part of the same contribution i would say", "this paper studies active learning for convolutional neural network", "authors formulate the active learning problem as core-set selection and present a novel strategi", "experiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselin"], "labels": ["DIS", "DIS", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theoretical analysis is presented to show the performance of any selected subset using the geometry of the data point", "authors are suggested to perform experiments on more datasets to make the results more convinc", "the initialization of the cnn model is not clearly introduced which however may affect the performance significantli", "the paper proposes an extension to a previous monotonic attention model raffel et al  to attend to a fixed-sized window up to the alignment posit", "both the soft attention approximation used for training the monotonic attention model and the online decoding algorithm is extended to the chunkwise model"], "labels": ["SMY", "SUG", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in terms of the model this is a relatively small extention of raffel et ", "results show that for online speech recognition the model matches the performance of an offline soft attention baseline doing significantly better than the monotonic attention model", "results show that for online speech recognition the model matches the performance of an offline soft attention baseline doing significantly better than the monotonic attention model", "is the offline attention baseline unidirectional or bidirect", "is the offline attention baseline unidirectional or bidirect"], "labels": ["DIS", "SMY", "DIS", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model", "my concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model", "my concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model", "especially given that w= works best for speech recognition it not clear that the model extension is actually help", "especially given that w= works best for speech recognition it not clear that the model extension is actually help"], "labels": ["QSN", "SMY", "SUG", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary so possibly a similar effect could be obtained by doubling the time frame through the convolutional lay", "while the empirical result is strong it is not clear that the proposed model is the best way to obtain the improv", "for document summarization the paper presents a strong result for an online model but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of thi", "for document summarization the paper presents a strong result for an online model but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of thi", "if the contribution is in terms of speed as shown with the synthetic benchmark in appendix b more emphesis should be placed on this in the pap"], "labels": ["SMY", "APC", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the contribution is in terms of speed as shown with the synthetic benchmark in appendix b more emphesis should be placed on this in the pap", "sentence summarization tasks do exhibit mostly monotonic alignment and most previous models with monotonic structure were evaluated on that so why not test that her", "i like the fact that the model is truely online but that contribution was made by raffel et al  and this paper at best proposes a slightly better way to train and apply that model", "--- the additional experiments in the new version gives stronger support in favour of the proposed model architecture vs the effect of hyperparameter choic", "while i'm still on the fence on whether this paper is strong enough to be accepted for iclr this version is certainly improves the quality of the pap"], "labels": ["DIS", "QSN", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a genetic algorithm inspired policy optimization method which mimics the mutation and the crossover operators over policy network", "the title and the motivation about the genetic algorithm are missing leading and improp", "the genetic algorithm is a black-box optimization method however the proposed method has nothing to do with black-box optim", "the mutation is a method to sample individual independence of the objective function which is very different with the gradient step", "mimicking the mutation by a gradient step is very unreason"], "labels": ["SMY", "CRT", "CRT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the crossover operator is the policy mixing method employed in game context eg deep reinforcement learning from self-play in imperfect-information games https//arxivorg/abs/", "it is straightforward if two policies are to be mixed although the mixing method is more reasonable than the genetic crossover operator it is strange to compare with that operator in a method far away from the genetic algorithm", "it is highly suggested that the method is called as population-based method as a set of networks is maintained instead of as genetic method", "another drawback perhaps resulted from the genetic algorithm motivation is that the proposed method has not been well explain", "the only explanation is that this method mimics the genetic algorithm however this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high prob"], "labels": ["DIS", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the baseline methods result in rewards much lower than those in previous experimental pap", "it is problemistic that if the baselines have bad paramet", "benchmarking deep reinforcement learning for continuous control deep reinforcement learning that matt", "in this paper a model is built for reading comprehension with multiple choic", "the model consists of three modules encoder interaction module and elimination modul"], "labels": ["CRT", "SUG", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the major contributions are two folds firstly proposing the interesting option elimination problem for multi-step reading comprehension  and secondly proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document represent", "intuitively one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignor", "the elimination module is interest", "but the usefulness of uceliminationud is not well justified for two reason", "first the improvement of the proposed model over the previous state of the art is limit"], "labels": ["SMY", "DIS", "APC", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first the improvement of the proposed model over the previous state of the art is limit", "second the model is built upon gar until the elimination module then according to table  it seems to indicate that the elimination module does not help significantly % improv", "in order to show the usefulness of the elimination module the model should be exactly built on the gar with an additional elimination module ie after removing the elimination module the performance should be similar to gar but not something significantly worse with a % accuraci", "in order to show the usefulness of the elimination module the model should be exactly built on the gar with an additional elimination module ie after removing the elimination module the performance should be similar to gar but not something significantly worse with a % accuraci", "in order to show the usefulness of the elimination module the model should be exactly built on the gar with an additional elimination module ie after removing the elimination module the performance should be similar to gar but not something significantly worse with a % accuraci"], "labels": ["CRT", "CRT", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then we can explicitly compare the performance between gar and the gar w/ elimination module to tell how much the new module help", "other issues is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $tilde{x}_i$", "even though the authors find the orthogonal vectors theyure gated summed together very soon", "it would be better to show how much uceliminationud and ucsubtractionud effect the final performance besides the effect of subtraction g", "a figure showing the model architecture and the corresponding qa process will better help the readers understand the proposed model"], "labels": ["DIS", "QSN", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["$c_i$ in page  is not defin", "whatus the performance of only using $s_i$ for answer selection or replacing $x^l$ with $s_i$ in score funct", "it would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance besides the case study in figur", "minor issues in eqn  it would be better to use a vector as the input of softmax", "it would be easier for discussion if the authors could assign numbers to every equ"], "labels": ["DFT", "QSN", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be easier for discussion if the authors could assign numbers to every equ", "this paper introduces a generative approach for d point cloud", "more specifically two generative adversarial approaches are introduced raw point cloud gan and latent-space gan r-gan and l-gan as referred to in the pap", "in addition a gmm sampling + gan decoder approach to generation is also among the experimented vari", "the results look convincing for the generation experiments in the paper both from class-specific figure  and multi-class generators figur"], "labels": ["DIS", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the quantitative results also support the visu", "one question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approach", "especially octree based approaches [-below] show very convincing and high-resolution shape generation result", "whereas the details seem to be washed out for the point cloud results presented in this pap", "i would like to see comparison experiments with voxel based approaches in the next update for the pap"], "labels": ["APC", "QSN", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see comparison experiments with voxel based approaches in the next update for the pap", "[]@article{tatarchenkooctree  title={octree generating networks efficient convolutional architectures for high-resolution d outputs}  author={tatarchenko maxim and dosovitskiy alexey and brox thomas}  journal={arxiv preprint arxiv}  year={}}in light of the authors' octree updates score is upd", "[]@article{tatarchenkooctree  title={octree generating networks efficient convolutional architectures for high-resolution d outputs}  author={tatarchenko maxim and dosovitskiy alexey and brox thomas}  journal={arxiv preprint arxiv}  year={}}in light of the authors' octree updates score is upd", "i expect these updates to be reflected in the final version of the paper itself as wel", "the authors propose using piecewise linear activation functions with contraints to make it contin"], "labels": ["DIS", "SUG", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they report that during training tuning piecewise versions of the multiple activation functions such as relu elu lrelu converge to shifted elu termed shelu in this articl", "authors claim to achive better performance when using shelu  while learning an individual bias shift for each neuron", "authors claim to achive better performance when using shelu  while learning an individual bias shift for each neuron", "given a prelu learnable alpha or elu is applied on pre-activation wx+b at each neuron one can achieve the same shift as that reported in shelu if requir", "given a prelu learnable alpha or elu is applied on pre-activation wx+b at each neuron one can achieve the same shift as that reported in shelu if requir"], "labels": ["SMY", "SMY", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors present no clear explanation on why the shift should result in improved perform", "the topic discussed in this paper is interest", "dialogue acts das or some other semantic relations between utterances are informative to increase the diversity of response gener", "it is interesting to see how das are used for conversational model", "however this paper is difficult for me to follow"], "labels": ["CRT", "APC", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the caption of section  is about supervised learning however the way of describing the model in this section sounds like reinforcement learn", "not sure whether it is necessary to formulate the problem with a rl framework since the data have everything that the model needs as for a supervised learn", "the formulation in equation  seems to be problemat", "simplify prri|siai as prri|aiuiuuiu since decoding natural language responses from long conversation history is challeng", "to my understanding the only difference between the original and simplified model is the encoder part not the decoder part did i miss someth"], "labels": ["CRT", "DIS", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["about section  again i didn't get whether the model needs rl for train", "we train mub ub with the  million crawled data through negative sampling not sure i understand the connection between training $mcdot cdot$ and the entire model", "the experiments are not convinc", "at least it should show the generation texts were affected about das in a systemic way", "only a single example in table  is not enough"], "labels": ["CRT", "CRT", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper studies the local optima of certain types of deep network", "it uses the notion of a locally open map to draw equivalences between local optima and global optima", "the basic idea is that for fitting nonlinear models with a convex loss if the mapping from the weights to the outputs is open then every local optimum in weight space corresponds to a local optimum in output space by convexity in output space every local optimum is glob", "this is mostly a uctheory buildingud work", "with an appropriate fix lemma  gives a cleaner set of assumptions than previous work in the same space nguyen + hein u but yields essentially the same conclus"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the notion of local openness seems very well adapted to deriving these type of results in a clean mann", "the result in section  on local openness of matrix multiplication on its range which is substantially motivated by behrends  may be of independent interest", "i did not check the proof of this result in detail but it appears to be correct", "for the linear deep case the paper corrects imprecisions in the previous work lu + kawaguchi", "for deep nonlinear networks the results require the ucpyramidalud assumption that the dimensionality is nonincreasing with respect to layer and more restrictively the feature dimension in the first layer is larger than the number of input point"], "labels": ["APC", "DIS", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this seems to differ from typical practice in the sense that it does not allow for wide intermediate lay", "this seems to be a limitation of the methodology unless i'm missing something this situation cannot be addressed using locally open map", "there are some imprecisions in the writ", "for example lemma  is not correct as written u an invertible mapping sigma is not necessarily locally open", "take $sigma_kt = t for t rational and -t for t irrational$ as an exampl"], "labels": ["SMY", "DFT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is easy to fix but not correct as written", "despite mentioning matrix completion in the introduction and comparing to work of ge et al the paper does not seem to have strong implications for matrix complet", "it extends results of ge and collaborators for the fully observed symmetric case to non-symmetric problem", "but the main interest in matrix completion is in the undersampled case u in the full observed case there is nothing to complet", "this paper studies the problem of one-shot and few-shot learning using the graph neural network gnn architecture that has been proposed and simplified by several author"], "labels": ["CRT", "CRT", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the data points form the nodes of the graph with the edge weights being learned using ideas similar to message passing algorithms similar to kearnes et al and gilmer et ", "this method generalizes several existing approaches for few-shot learning including siamese networks prototypical networks and matching network", "the authors also conduct experiments on the omniglot and mini-imagenet data sets improving on the state of the art", "there are a few typos and the presentation of the paper could be improved and polished mor", "i would also encourage the authors to compare their work to other unrelated approaches such as attentive recurrent comparators of shyam et al and the learning to remember rare events approach of kaiser et al both of which achieve comparable performance on omniglot"], "labels": ["QSN", "DIS", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as gnmt", "this paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial exampl", "the idea is to train the model to perform well not only with respect to the unknown population distribution but to perform well on the worst-case distribution in some ball around the population distribut", "in particular the authors adopt the wasserstein distance to define the ambiguity set", "this allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized erm with a different cost"], "labels": ["SUG", "SMY", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theoretical results in the paper are supported by experi", "overall this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem", "the authors propose a new exploration algorithm for deep rl", "they maintain an ensemble of q-values based on different initialisations to model uncertainty over q the ensemble is then used to derive a confidence interval at each step which is used to select actions ucb-styl", "there is some attempt at a bayesian interpretation for the bellman update but to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - ium not sure this is justified and necessary here moreover the ucb strategy is generally not considered a bayesian strategy so i wasnut convinced by the link to bayesian rl in this pap"], "labels": ["SMY", "APC", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ni liked the actual proposed method otherwise and the experimental results on atari seem good but see also latest sota atari results for example the rainbow pap", "some questions about the results-how does it perform compared to epsilon-greedy added on top of alg or is there evidence that this produces any meaningful exploration versus nois", "-how does the distribution of q values look like during different phases of learn", "-was epsilon-greedy used in addition to ucb exploration question for both alg  and alg", "-whatus different between alg  and bootstrapped dqn other than the action select"], "labels": ["APC", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor things-missing propto in eq", "-maybe mention that the leftarrows are not hard updates maybe you already do somewhereu-it looks more a bellman residual update as written in", "the authors propose a generative method that can produce images along a hierarchy of specificity ie both when all relevant attributes are specifi", "and when some are left undefined creating a more abstract generation task", "pros+ the results demonstrating the method's ability to generate results for  abstract and  novel/unseen attribute descriptions are generally convinc"], "labels": ["DFT", "DFT", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["both quantitative and qualitative results are provid", "+ the paper is fairly clea", "rcons- it is unclear how to judge diversity qualitatively eg in fig b", "- fig  could be more convincing bushy eyebrows is a difficult attribute to judg", "and in the abstract generation when that is the only attribute specified it is not clear how good the results ar"], "labels": ["APC", "APC", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and in the abstract generation when that is the only attribute specified it is not clear how good the results ar", "this paper introduces a fairly elaborate model for reading comprehension evaluated on the squad dataset", "the model is shown to improve on the published results but not as-of-submission leaderboard numb", "the main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principl", "a less significant issue is that the english is often disflu"], "labels": ["QSN", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specific comments i would remove the significance daggers from table  as the standard deviations are already reported and the null hypothesis for which significance is measured seems unclear", "i am also concerned to see test performance significantly better than development performance in t", "other systems seem to have development and test performance closer togeth", "have the authors been evaluating many times on the test data", "a large margin  end to end language model that uses a discriminative objective function is propos"], "labels": ["SUG", "SUG", "SMY", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the impost", "a variant on this which also incorporates the ranks of the imposters sorted by a metric such as edit distance or bleu metric with respect to the ground truth is also introduc", "the paper is missing some of the original references to a discriminative lm dlm as well as  references to the use of a nn lm directly in decoding presented in icassp and interspeech conferences over the last  year", "for example h-k j kuo e fosler-lussier h jiang and c-h lee ucdiscriminative training of language models for speech recognitionud in proc icasspvol   pp u", "have you considered the widely-used nce method to handle the large vocabulari"], "labels": ["SMY", "SMY", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the dev perplexity quoted in section  for a  gram lm is very high", "also table  and table  on wsj and  fisher  show baseline experiments that are quite far away from the state-of-the-art in these task", "even if you assume that you use the simplest possible acoustic model and/or an open source tool kit for the decoder  these error rates are high wsj error rates are lower than % not %", "even if the lm is trained on the common-crawl corpus it has  a very low oov rate and fine tuning on the tasks only lowers it b t %", "for reference  please see papers from saon et al seide et al povey et al yajie miao et al in various icassp interspeech and arxiv papers comparisons with weak baselines can significantly color the conclus"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the fisher test set the interpolated lm offers very little over the baseline lm in t", "this is contrary to what is observed in the literatur", "there is not much difference between ranklm and lmlm as well to draw a clear conclusion between the two", "given that this is n-best rescoring how are the n-best lists gener", "you state that they are extracted from   beam candidates are they unique n-best list"], "labels": ["CRT", "CRT", "CRT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can this method be applied to lattic", "what is the perplexity of all the language models corresponding to  tables  and", "this would have been useful to study in itself", "in the smt tasks the baselines reported seem to be far away from results presented in the literature on the iwslt task see http//workshopiwsltorg/downloads/iwslt__ep_pdf", "while the proposed objective is interesting and meaningful for several conversational applications as well as sentence modeling the presented experimental results are not convinc"], "labels": ["QSN", "QSN", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes an interesting  approach to prune a deep model from a computational point of view", "the idea is quite simple as pruning using the connection in the batch norm lay", "it is interesting to add the memory cost per channel into the optimization process", "it is interesting to add the memory cost per channel into the optimization process", "the paper suggests normal pruning does not necessarily preserve the network funct"], "labels": ["APC", "SMY", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper suggests normal pruning does not necessarily preserve the network funct", "i wonder if this is also applicable to the proposed method and how can this be evidenc", "i wonder if this is also applicable to the proposed method and how can this be evidenc", "as strong points the paper is easy to follow and does a good review of existing method", "then the proposal is simple and easy to reproduce and leads to interesting result"], "labels": ["DIS", "DFT", "QSN", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is clearly written there are some typos / grammar error", "as weak points the paper claims the selection of alpha is critical but then this is fixed empirically without proper sensitivity analysi", "i would like to see proper discussion her", "why is alpha set to  in the first experiment while set to a different number elsewher", "how is the pruning as post processing performed for the base model the so called model a"], "labels": ["APC", "DFT", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  in the algorithmic step", "how does the th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network", "results for cifar are nice although not really impressive as the main benefit comes from the fully connected layer as expect", "the paper presents a multi-task architecture that can perform multiple tasks across multiple different domain", "the authors design an architecture that works on image captioning image classification machine translation and pars"], "labels": ["DIS", "DIS", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe proposed model can maintain performance of single-task models and in some cases show slight improv", "this is the main take-away from this pap", "there is a factually incorrect statement - depthwise separable convolutions were not introduced in chollet", "section  of the same paper also notes it depthwise convolutions can be traced back to at least", "this paper proposes a new approach for multi-task learn"], "labels": ["APC", "APC", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while previous approaches assumes the order of shared layers are the same between tasks this paper assume the order can vary across tasks and the soft order is learned during train", "they show improved performance on a number of multi-task learning problem", "my primary concern about this paper is the lack of interpretation on permuting the lay", "for example in standard vision systems low level filters v learn edge detectors gabor filters and higher level filters learn angle detectors []", "it is confusing why permuting these filters make sens"], "labels": ["SMY", "SMY", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they accept different inputs raw pixels vs edg", "moreover if the network contains pooling layers different locations of the pooling layer result in different shapes of the feature map and the soft ordering strategy eq  does not work", "it makes sense that the more flexible model proposed by this paper performs better than previous model", "the good aspect of this paper is that it has some performance improv", "but i still wonder the effect of permuting the lay"], "labels": ["SMY", "CRT", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper also needs more clarifications in the writ", "for example in section  how each s_i j k is sampled from ", "the parallel ordering terminology also seems to be arbitrari", "[] lee honglak chaitanya ekanadham and andrew y ng sparse deep belief net model for visual area v advances in neural information processing system", "the paper proposes a modified approach to rl where an additional episodic memory is kept by the ag"], "labels": ["SUG", "QSN", "CRT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what this means is that the agent has a reservoir of n states in which states encountered in the past can be stor", "there are then of course two main questions to address i which states should be stored and how", "ii how to make use of the episodic memory when deciding what action to tak", "for the latter question the authors propose using a query network that based on the current state pulls out one state from the memory according to certain probability distribut", "this network has many tunable parameters but the main point is that the policy then can condition on this state drawn from the memori"], "labels": ["QSN", "QSN", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["intuitively one can see why this may be advantageous as one gets some information from the past", "as an aside the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success", "the first question had a quite an interesting and cute answer there is a non-negative importance weight associated with each state and a collection of states has weight that is simply the product of the weight", "the authors claim with some degree of mathematical backing that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired and they give a cute online algorithm for this purpos", "however the weights themselves are given by a network and so weights may change even for states that have been observed in the past"], "labels": ["SMY", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immut", "there is also a toy example created to show that this approach works well compared to the rnn based approach", "positives- an interesting new idea that has potential to be useful in rl", "- an elegant algorithm to solve at least part of the problem properly the rest of course relies on standard sgd methods to train the various network", "negatives- the math is fudged around quite a bit with approximations that are not always justifi"], "labels": ["DFT", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- while overall the writing is clear in some places i feel it could be improv", "i had a very hard time understanding the set-up of the problem in figur", "[in general i also recommend against using figure captions to describe the setup", "]- the experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach", "i think i should understand the gist of the paper which is very interesting where the action of tilde qsa is drawn from a distribut"], "labels": ["SMY", "DFT", "SUG", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the author also explains in detail the relation with pgq/soft q learning and the recent paper expected policy gradient by ciosek & whiteson", "all these seems very sound and interest", "weakness the major weakness is that throughout the paper i do not see an algorithm formulation of the smoothie algorithm which is the major algorithmic contribution of the paper i think the major contribution of the paper is on the algorithmic side instead of theoret", "such representation style is highly discouraging and brings about un-necessary readability difficulti", "sec  and  is a little bit abbreviated from the major focus of the paper and i guess they are not very important and novel just educational guess because i can only guess what the whole algorithm smoothie i"], "labels": ["SMY", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so i suggest moving them to the appendix and make the major focus more narrowed down", "this paper applies several nn architectures to classify urlus between benign and malware related url", "the baseline is random forests and feature engin", "this is clearly an application pap", "no new method is being proposed only existing methods are applied directly to the task"], "labels": ["SUG", "SMY", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not familiar with the task at hand so i cannot properly judge the quality/accuracy of the results obtained but it seems ok", "for evaluation data was split randomly in % train % test and % valid", "given the amount of data *** samples this seems suffici", "i think the evaluation could be improved by using malware urls that were obtained during a larger time window", "specifically it would be nice if train test and validation urls would be operated chronologically ie all train url precede the validation and test url"], "labels": ["DIS", "DIS", "APC", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ideally the train and test urls would also be different in tim", "this would enable a better test of the generalization capabilities in what is essentially a continuously changing environ", "this paper is a very difficult for me to assign a final r", "there is no obvious technical mistake  and the paper is written reasonably wel", "there is however a lack of technical novelty or insight in the models themselv"], "labels": ["SUG", "SUG", "FBK", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think that the paper should be submitted to a journal or conference in the application domain where it would be a better fit", "for this reason i will give the score marginally below the acceptance threshold now", "but if the other reviewers argue that the paper should be accepted i will change my scor", "the authors devise and explore use of the hessian of theapproximate/learned value function the critic to update the policyactor in the actor-critic  approach to rl", "they connect theirtechnique 'guide actor-critic' or gac to existing actor-criticmethods authors claim only two published work use st orderinformation on the crit"], "labels": ["CRT", "FBK", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they show that the nd order informationcan be useful in several of the  tasks their gac techniques werebest or competitive and in only one performed poorly compared to best", "the paper has a technical focu", "pros- strict generalization of an existing up to st order actor-critic approach", "- compared to many existing techniques on  task", "cons- no mention of time costs except that for more samples s   for taylor approximation it can be very expens"], "labels": ["SMY", "DIS", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- one would expect more information to strictly improve performance  but the results are a bit mixed perhaps due to convergence to local optima and both actor and critic being learned at same time   or the gaussian assumptions etc", "- one would expect more information to strictly improve performance  but the results are a bit mixed perhaps due to convergence to local optima and both actor and critic being learned at same time   or the gaussian assumptions etc", "- relevance the work presents a new approach to actor-critique strategy for  reinforcement learning remotely related to 'representation  learning' unless value and policies are deemed a form of  represent", "other comments/questions- why does the performance start high on ant  then goes to all approach", "- how were the tasks select"], "labels": ["DFT", "CRT", "APC", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are they all the continuous control tasks available in open ai", "this paper proposes an approximate method to construct bayesian uncertainty estimates in networks trained with batch norm", "there is a lot going on in this pap", "although the overall presentation is clean", "there are few key shortfalls see below"], "labels": ["QSN", "SMY", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the reported functionality is nic", "although the experimental results are difficult to intepret despite laudable effort by the authors to make them intuit", "some open questions that i find crucial* how exactly is the ucstochastic forward-passud performed that gives rise to the moment estim", "this step is the real meat of the paper yet i struggle to find a concrete definition in the text", "is this really just an average over a few recent weights during optim"], "labels": ["APC", "CRT", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if so how is this method specific to batch norm", "maybe ium showing my own lack of understanding here but itus worrying that the actual sampling technique is not explained anywher", "this relates to a larger point about the paper's main point what exactly is the bayesian interpretation of batch normalization proposed her", "in bayesian dropout there is an explicit variational object", "here this is replaced by an implicit regular"], "labels": ["QSN", "CRT", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the argument in section  seems rather weak to m", "to paraphrase it if the prior vanishes so does the regularizer fin", "but what's the regularizer that's vanish", "the sentence that the influence of the prior diminishes as the size of the training data increases is debatable for something as over-parametrized as a dnn", "i wouldn't be surprised that there are many directions in the weight-space of a trained dnn along which the posterior is dominated by the prior"], "labels": ["CRT", "CRT", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* ium confused about the statements made about the ucconstant uncertaintyud baselin", "first off how is this constant width of the predictive region chosen", "did i miss this or is it not explained anywher", "unless i misunderstand the definition of crps and pll that width should matter no", "then the paragraph at the end of page  is worrying the authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is ucquite a reasonable baselineud"], "labels": ["CRT", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that can hardly be true if it is then it puts the entire paper into question! if trivial uncertainty is almost as good as this method isn't the method trivial too", "on a related point what would figure  look like for the constand uncertainty set", "just a horizontal line in blue and r", "but at which level", "i like this pap"], "labels": ["CRT", "QSN", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is presented well modulo the above problems and it makes some strong point", "but ium worried about the empirical evaluation and the omission of crucial algorithmic detail", "they may hide serious problem", "this paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regim", "known task segmentation is assumed and task specific input generators are learned in parallel with label predict"], "labels": ["APC", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is tested on standard sequential mnist variants as long as a class incremental vari", "superior performance to recent baselines eg ewc is reported in several cas", "interesting parallels with human cortical and hippocampal learning and memory are discuss", "unfortunately the paper does not go beyond the relatively simplistic setup of sequential mnist in contrast to some of the methods used as baselin", "the proposed architecture implicitly reduces the continual learning problem to a classical multitask learning mtl setting for the ltm where in the best case scenario iid data from all encountered tasks is available during training this setting is not ideal though"], "labels": ["SMY", "APC", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are several example of successful multitask learning but it does not follow that a random grouping of several tasks immediately leads to successful mtl", "indeed there is good reason to doubt this in both supervised and reinforcement learning domain", "in the latter case it is well known that mtl with arbitrary sets of task does not guarantee superior or even comparable performance to plain single-task learning due to unegative interferenceu between tasks [ ]", "i agree that problems can be constructed where these assumptions hold but this core assumption is limit", "the requirement of task labels also rules out important use cases such as following a non-stationary objective function which is important in several realistic domains including deep rl"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] parisotto emilio lei ba jimmy salakhutdinov ruslan tactor-mimic deep multitask and transfer reinforcement learning iclr [] andrei a rusu sergio gomez colmenarejo ucaglar gufclueehre guillaume desjardins james kirkpatrick razvan pascanu volodymyr mnih koray kavukcuoglu raia hadsell policy distillation iclr", "the work claims a measure of robustness of networks that is attack-agnost", "robustness measure is turned into the problem of finding a local lipschitz constant which is given by the maximum of the norm of the gradient of the associated funct", "that quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those sampl", "such a maximum process is then described by the reverse weibull distribution which is used in the estim"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper closely follows hein and andriushchenko", "there is a slight modification that enlarges the class of functions for which the theory is applicable lemma", "as far as i know the contribution of the work starts in section  where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of sampl", "this is a rather simple idea that is shown to be effective in figur", "the following section the part starting from  presents the key to the success of the proposed measur"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an important problem and the paper attempts to tackle it in a computationally efficient way", "the fact that the norms of attacks are slightly above the proposed score is promising however there is always the risk of finding a lower bound that is too small zeros and large gaps in figur", "it would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed scor", "finally a minor point definition  has a confusing notation f is a k-valued vector throughout the paper but it also denotes the number that represents the prediction in definition  i believe this is just a typo", "edit thanks for the fixes and clarification of essential parts in the pap"], "labels": ["APC", "DIS", "SUG", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper describes a method to induce source-side dependency structures in service to neural machine transl", "the idea of learning soft dependency arcs in tandem with an nmt objective is very similar to recent notions of self-attention vaswani et al  cited or previous work on latent graph parsing for nmt hashimoto and tsuruoka  cit", "this paper introduces three innovations  they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities  they explicitly specify how the dependencies are to be used meaning that rather than simply attending over dependency representations with a separate attention they select a soft word to attend to through the traditional method and then attend to that wordus soft head called shared attention in the paper and  they gate when attention is us", "i feel that the first two ideas are particularly interest", "unfortunately the results of the nmt experiments are not particularly compelling with overall gains over baseline nmt being between  and  bleu"], "labels": ["SMY", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however they include a useful ablation study that shows fairly clearly that both ideas  and  contribute equally to their modest gains and that without them fa-nmt shared=no in table  there would be almost no gains at al", "interesting side-experiments investigate their accuracy as a dependency parser with and without a hard constraint on the systemus latent dependency decis", "this paper has some very good ideas and asks questions that are very much worth ask", "in particular the question of whether a tree constraint is useful in self-attention is very worthwhil", "unfortunately this is mostly a negative result with gains over ucflat attentionud being relatively smal"], "labels": ["SMY", "APC", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also like the ucshared attentionud - it makes a lot of sense to say that if the ucsemanticud attention mechanism has picked a particular word one should also attend to that wordus head it is not something i would have thought of on my own", "the paper is also marred by somewhat weak writing with a number of disfluencies and awkward phrasings making it somewhat difficult to follow", "in terms of specific criticismsi found the motivation section to be somewhat weak", "we need a better reason than morphology to want to do source-side dependency pars", "all published error analyses of strong nmt systems bentivogli et al emnlp  toral and sanchez-cartagena eacl  isabelle et al emnlp  have shown that morphology is a strength not a weakness of these systems and the sorts of head selection problems shown in figure  are in my experience handled capably by existing lstm-based system"], "labels": ["APC", "CRT", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper mentions ucsignificant improvementsud in only two places the introduction and the conclus", "with bleu score differences being so low the authors should specify how statistical significance is measur", "ideally using a technique that accounts for the variance of random restarts ie clark et al acl", "equation  i couldnut find the definition for h anywher", "sentence before equation  i believe there is a typo here ucf takes z_iud should be ucf takes u_tud"], "labels": ["SMY", "SUG", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first section of section  please cite the previous work you are talking about in this sent", "my understanding was that the dependency marginals in pz_{ij}=|xphi in equation  are directly used as beta_{ij}", "if ium correct thatus probably worth spelling out explicitly in equation  beta_{ij} = pz_{ij}=|xphi = u", "i donut donut feel like the clause between equations  and  ucwhen sharing attention weights from the decoder with the encoderud is a good description of your clever ucshared attentionud idea", "in general i found this region of the paper including these two equations and the text between them very difficult to follow"], "labels": ["SUG", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  itus very very good that you compared to ucflat attentionud", "but itus too bad for everyone cheering for linguistically-informed syntax that the results werenut bett", "table  i had a hard time understanding table  and the corresponding discuss", "what are ucproduction percentagesud", "finally it would have been interesting to include the fa system in the dependency accuracy experiment table  to see if it made a big difference ther"], "labels": ["APC", "CRT", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper explores momentum sgd and an adaptive version of momentum sgd which the authors name yf yellow fin", "they compare yf to hand tuned momentumsgd and to adam in several deep learning appl", "i found the first part which discusses the theoretical motivation behind yf to be very confusing and misleadingbased on the analysis of -dimensional problems the authors design a framework and an algorithm that  supposedly ensures accelerated converg", "there are two major problems with this approach", "-first exploring -dim functions is indeed a nice way to get some intuit"], "labels": ["SMY", "SMY", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["yet  algorithms that work in the -dim case do not trivially generalize to high dimensions and such reasoning might lead to very bad solut", "-second accelerated gd does not benefit over gd in the -dim case and therefore this is not an appropriate setting to explore accelerationconcretely the definition of the generalized condition number $u$ and relating it to the standard definition of the condition number $kappa$ is very mislead", "this is since $kappa =$ for -dim problems and therefore accelerated gd does not have any benefits over non accelerated gd in this cas", "however $u$ might be much larger than  even in the -dim cas", "regarding the algorithm itself there are too many hyper-parameters which depend on each other that are tuned per-dimens"], "labels": ["CRT", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and as i have mentioned the design of the algorithm is inspired by the analysis of -dim quadratic funct", "thus it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employ", "the authors mention that their experiments were done without tuning or with very little tuning which is very mysterious for m", "in contrast to the theoretical part the experiments seems very encourag", "showing yf to perform very well on several deep learning tasks without or with very little tun"], "labels": ["DIS", "CRT", "CRT", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["again this seems a bit magical or even too good to be truth", "i suggest the authors to perform a experiment with say a qaudratic high dimensional function which is not aligned with the axes in order to illustrate how their method behaves and try to give intuit", "the paper proposes a general neural network structure that includes tc temporal convolution blocks and attention blocks for meta-learning specifically for episodic task learn", "through intensive experiments on various settings including few-shot image classification on omniglot and mini-imagenet and four reinforcement learning applications the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art method", "the experiment results seem solid and the proposed structure is with simple design and highly generaliz"], "labels": ["CRT", "SUG", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the concern is that the contribution is quite incremental from the theoretical sid", "though it involves large amount of experimental efforts which could be impact", "please see the major comment belowone major comment- despite that the work is more application oriented the paper would have been stronger and more impactful if it includes more work on the theoretical sid", "please see the major comment belowone major comment- despite that the work is more application oriented the paper would have been stronger and more impactful if it includes more work on the theoretical sid", "specifically for two folds  in general some more work in investigating the task space would be nic"], "labels": ["DIS", "APC", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper assumes the tasks are ucrelatedud or ucsimilarud and thus transferrable also particularly in section  the authors define that the tasks follow the same distribut", "but what exactly should the distribution be like to be learnable and how to quantify such ucrelatedud or ucsimilarud relationship across task", "in particular for each of the experiments that the authors conduct it would be nice to investigate some more on when the proposed tc + attention network would work better and thus should be used by the community some questions to answer include when should we prefer the proposed combination of tc + attention blocks over the other method", "in particular for each of the experiments that the authors conduct it would be nice to investigate some more on when the proposed tc + attention network would work better and thus should be used by the community some questions to answer include when should we prefer the proposed combination of tc + attention blocks over the other method", "the result from the paper seems to answer with ucin all casesud but then that always brings the issue of ucoverfittingud or parameter tuning issu"], "labels": ["DIS", "QSN", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe the paper would have been much stronger if either of the two above are further investig", "more detailed comments- on page  ucthe optimal strategy for an arbitrary range of tasksud lacks definition of ucrangeud also in the setting in this paper these tasks should share ucsimilarityud or follow the same ucdistributionud and thus such ucarbitrarinessud is actually constrain", "- on page  the notation and formulation for the meta-learning could be more mathematically rigid the distribution over tasks is not defin", "it is understandable that the authors try to make the paradigm very generalizable but the ambiguity or the abstraction over the uctask distributionud is too large to be meaning", "one suggestion would be to split into two sections one for supervised learning and one for reinforcement learning but both share the same design paradigm which is generaliz"], "labels": ["SUG", "CRT", "DFT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- for results in table  and table  how are the confidence intervals comput", "is it over multiple runs or within the same run", "it would be nice to make clear in addition i personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified test", "the confidence intervals may not be clear without elaboration such is also concerning in the caption for table  about claiming ucnot statistically-significantly differentud because no significance test is report", "- at last some more details in implementation would be nice package availability run time analysis i suppose the package or the source code would be publicly available afterward"], "labels": ["QSN", "QSN", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposed a reinforcement learning rl based method to learn an optimal optimization algorithm for training shallow neural network", "this work is an extended version of [] aiming to address the high-dimensional problem", "strengthsthe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithm", "the proposed method has better robustess in different tasks and different batch size set", "the invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of lqg"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weaknesses since the batch size is small in each experiment it is hard to compare convergence rate within one epoch", "more iterations should be taken and the log-scale style figure is suggest", "in figure b llbgdbgd converges to a lower objective value while the other figures are difficult to compare the convergence value should be reported in all experi", "ucthe average recent iterateuc described in section  uses recent  iterations to compute the average the reason to choose ucud and the effectiveness of different choices should be discussed as well as the ucud used in state featur", "since the block-diagonal structure imposed on a_t b_t and f_t how to choose a proper block s"], "labels": ["DFT", "SUG", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or how to figure out a coordinate group", "the caption in figure  ucwith  input and hidden unitsud should clarify clearli", "the curves of different methods are suggested to use different lines eg dashed lines to denote different algorithms rather than colors onli", "typo sec  parg  uccurrent iterateud - uccurrent iterationud", "conclusionsince rl based framework has been proposed in [] by li & malik this paper tends to solve the high-dimensional problem"], "labels": ["QSN", "CRT", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with the new observation of invariant in coordinates permutation in neural networks this paper imposes the block-diagonal structure in the model to reduce the complexity of lqg algorithm", "sufficient experiment results show that the proposed method has better convergence rate than []", "but comparing to [] this paper has limited contribut", "[] ke li and jitendra malik learning to optimize corr abs/", "this paper proposes a distributed architecture for deep reinforcement learning at scale specifically focusing on adding parallelization in actor algorithm in prioritized experience replay framework"], "labels": ["SMY", "SMY", "CRT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it has a very nice introduction and literature review of prioritized experience replay", "and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel so that the experience replay can obtain more data for the learner to sample and learn", "not surprisingly as this framework is able to learn from way more data eg in atari it outperforms the baselines and figure  clearly shows the more actors we have the better performance we will hav", "while the strength of this paper is clearly the good writing as well as rigorous experiment", "the main concern i have with this paper is novelti"], "labels": ["APC", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is in my opinion a somewhat trivial extension of the previous work of prioritized experience replay in literature hence the challenge of the work is not quite clear", "hence i feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper for exampl", "this paper propose an adaptive dropout strategy for class logit", "they learn a distribution qz | x y that randomly throw class logit", ""], "labels": ["DFT", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by doing so they ensemble predictions of the models between different set of classes and focuses on more difficult discrimination task", "they learn the dropout distribution by variational inference with concrete relax", "overall i think this is a good pap", "the technique sounds the presentation is clear and i have not seen similar paper elsewher", "not % sure about the originality of the work though"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pro* general algorithmcon* the experiment is a little weak", "only on cifar the proposed approach is much better than other approach", "i would like to see the results on more dataset", "i would like to see the results on more dataset", "maybe should also compare with more dropout algorithms such as dropconnect and maxout"], "labels": ["CRT", "APC", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["maybe should also compare with more dropout algorithms such as dropconnect and maxout", "summarythe paper presents a variational autoencoder for generating entity pairs given a relation in a medical set", "the model strictly follows the standard vae architecture with an encoder that takes as input an entity pair and a relation between the ent", "the encoder maps the input to a probabilistic latent spac", "the latent variables plus a one-hot-encoding representation of the relation is used to reconstruct the input ent"], "labels": ["DIS", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally a generator is used to generate entity pairs give a rel", "----------overall judgmentthe paper presents a clever use of vaes for generating entity pairs conditioning on rel", "my main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set", "if this is the case all the analysis and results obtained are almost meaningless", "i suggest the authors make clear if they used the split training validation test"], "labels": ["SMY", "APC", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["until then it is not possible to draw any conclusion from this work", "assuming the experimental setting is correct it is not clear to me the reason of having the representation of r one-hot-vector of the relation also in the decoding/generation part", "the hidden representation obtained by the encoder should already capture information about the rel", "is there a specific reason for doing so", "this manuscript proposes a method to improve the performance of a generic learning method by generating in between class bc training sampl"], "labels": ["CRT", "CRT", "DIS", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the manuscript motivates the necessity of such technique and presents the basic intuit", "the authors show how the so-called bc learning helps training different deep architectures for the sound recognition task", "my first remark regards the presentation of the techniqu", "the authors argue that it is not a data augmentation technique but rather a learning method", "i strongly disagree with this statement not only because the technique deals exactly with augmenting data but also because it can be used in combination to any learning method including non-deep learning methodolog"], "labels": ["SMY", "SMY", "QSN", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["naturally the literature review deals with data augmentation technique which supports my point of view", "in this regard i would have expected comparison with other state-of-the-art data augmentation techniqu", "the usefulness of the bc technique is proven to a certain extent see paragraph below but there is not comparison with state-of-the-art", "in other words the authors do not compare the proposed method with other methods doing data augment", "this is crucial to understand the advantages of the bc techniqu"], "labels": ["CRT", "DIS", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is a more fundamental question for which i was not able to find an explicit answer in the manuscript", "intuitively the diagram shown in figure  works well for  classes in dimens", "if we add another class no matter how do we define the borders there will be one pair of classes for which the transition from one to another will pass through the region of a third class", "the situation worsens with more class", "however this can be solved by adding one dimension  classes and  dimensions seems something feas"], "labels": ["DIS", "APC", "DIS", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one can easily understand that if there is one more class than the number of dimensions the assumption should be feasible but beyond it starts to get problemat", "this discussion does not appear at all in the manuscript and it would be an important limitation of the method specially when dealing with large-scale data set", "this discussion does not appear at all in the manuscript and it would be an important limitation of the method specially when dealing with large-scale data set", "overall i believe the paper is not mature enough for publ", "some minor comments-  we introduce -- we discussion- pieczak a did not propose the extraction of mfcc"], "labels": ["CRT", "DFT", "CRT", "FBK", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the x_i and t_i of section  should not be denoted with the same letters as in", "- the correspondence with a semantic feature space is too pretentious specially since no experiment in this direction is shown", "- i understand that there is no mixing in the test phase perhaps it would be useful to recall it", "this paper describes plaid a method for sequential learning and consolidation of behaviours via policy distillation the proposed method is evaluated in the context of bipedal motor control across several terrain types which follow a natural curriculum", "pros- plaid masters several distinct tasks in sequence building up ucskillsud by learning ucrelatedud tasks of increasing difficulti"], "labels": ["SUG", "DFT", "SUG", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- although the main focus of this paper is on continual learning of ucrelatedud tasks the authors acknowledge this limitation and convincingly argue for the chosen task domain", "cons- plaid seems designed to work with task curricula or sequences of deeply related tasks for this regime classical transfer learning approaches are known to work well eg finetunning and it is not clear whether the method is applicable beyond this well understood cas", "- are the experiments single run", "due to the high amount of variance in single rl experiments it is recommended to perform several re-runs and argue about mean behaviour", "clarifications- what is the zero-shot performance of policies learned on the first few tasks when tested directly on subsequent task"], "labels": ["APC", "CRT", "QSN", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- how were the network architecture and network size chosen especially for the multitask", "would policies generalize to later tasks better with larger or smaller network", "- was any kind of regularization used how does it influence task performance vs transf", "- i find figure  c somewhat confus", "is performance maintained only on the last  tasks or all previously seen task"], "labels": ["QSN", "QSN", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thatus what the figure suggests at first glance but thatus a different goal compared to the learning strategies described in figures  a and b", "the paper proposes a variance reduction technique for policy gradient method", "the proposed approach justifies the utilization of action-dependent baselines and quantifies the gains achieved by it over more general state-dependent or static baselin", "the writing and organization of the paper is very well don", "it is easy to follow and succinct while being comprehens"], "labels": ["DIS", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the baseline definition is well-motivated and the benefits offered by it are quantified intuit", "there is only one mostly minor issues with the algorithm development and the experiments need to be more polish", "for the algorithm development there is an relatively strong assumption that z_i^t z_j =", "this assumption is not completely unrealistic for example it is satisfied if completely separate parts of a feature vector are used for act", "however it should be highlighted as an assumption and it should be explicitly stated as z_i^t z_j =  rather than z_i^t z_j approx"], "labels": ["APC", "FBK", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further because it is relatively strong of an assumption it should be discussed more thoroughly with some explicit examples of when it is satisfi", "otherwise the idea is simple and yet effective which is exactly what we would like for our algorithm", "the paper would be a much stronger contribution if the experiments could be improv", "- more details regarding the experiments are desirable - how many runs were done the initialization of the policy network and action-value function the deep architecture used etc", "- the experiment in figure  seems to reinforce the influence of lambda as concluded by the schulman et al pap"], "labels": ["SUG", "APC", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while that is interesting it seems unnecessary/non-relevant here unless performance with action-dependent baselines with each value of lambda is contrasted to the state-dependent baselin", "what was the goal her", "- in general the graphs are difficult to read fonts should be improved and the graphs polish", "- the multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baselin", "- it'd be great if plot a and b in figure  are swap"], "labels": ["DIS", "QSN", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think the idea proposed in the paper is benefici", "better discussing the strong theoretical assumption should be incorpor", "adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear mann", "particularly with some clarity on the experiments i would be willing to increase the scor", "minor comments in equation  how is the optimal-state dependent baseline obtain"], "labels": ["APC", "SUG", "SUG", "FBK", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be explicitly shown at least in the appendix", "the listed site for videos and additional results is not act", "some typos- section  - st para - last line these methods are therefore usually more sample efficient but can be less stable than critic-based method", "- section  - equation  - missing subscript i for bs_ta_t^{-i}", "- section  - hat{q} is just q in many plac"], "labels": ["SUG", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i hate to say that the current version of this paper is not ready as it is poorly written", "the authors present some observations of the weaknesses of the existing vector space models and list a -step approach for refining existing word vectors glove in this work and test the refined vectors on  toefl questions and  esl quest", "in addition to the incoherent presentation the proposed method lacks proper justif", "in addition to the incoherent presentation the proposed method lacks proper justif", "given the small size of the datasets it is also unclear how generalizable the approach i"], "labels": ["CRT", "SMY", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros   experimental study on retrofitting existing word vectors for esl and toefl lexical similarity dataset", "consub   the paper is poorly written and the proposed methods are not well justifi", "results on tiny dataset", "results on tiny dataset", "the paper proposes a simple modification to conditional gans obtaining impressive results on both the quality and diversity of samples on imagenet dataset"], "labels": ["APC", "CRT", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator d as in the literature the authors propose to project the condition y onto a penultimate feature space v of d by simply taking an inner product between y and v", "this implementation basically restricts the conditional distribution py|x to be really simple and seems to be posing a good prior leading to great empirical result", "+ quality- simple method leading to great results on imagenet!", "- while the paper admittedly leaves theoretical work for future work the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work", "one experiment could be sticking y to every hidden layer of d before the current projection layer and removing these y's increasingly and seeing how performance chang"], "labels": ["SMY", "DIS", "APC", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- appropriate comparison with existing conditional models ac-gans and ppgn", "- appropriate extensive metrics were used inception score/accuracy ms-ssim fid", "+ clarity- should explicitly define p q r upfront before equation  or between eq and eq- ppg should be ppgn", "n+ originalitythis work proposes a simple method that is original compared existing gan", "+ significancewhile the contribution is significant more experiments providing more intuition into why this projection works so well would make the paper much strong"], "labels": ["APC", "APC", "SUG", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ significancewhile the contribution is significant more experiments providing more intuition into why this projection works so well would make the paper much strong", "overall i really enjoy reading this paper and recommend for acceptance!", "this paper provides the analysis of empirical risk landscape for general deep neural networks dnn", "assumptions are comparable to existing results for oversimplifed shallow neural network", "the main results analyzed  correspondence of non-degenerate stationary points between empirical risk and the population counterpart"], "labels": ["APC", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["uniform convergence of the empirical risk to population risk", "generalization bound based on st", "the theory is first developed for linear dnns and then generalized to nonlinear dnns with sigmoid activ", "here are two detailed comments for deep linear networks with squared loss kawaguchi  has shown that the global optima are the only non-degerenate stationary point", "thus the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model y=xw"], "labels": ["SMY", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["should the risk bound only depends on the dimensions of the matrix w", "the comparison with bartlett & maassus bm work is a bit unfair because their result holds for polynomial activations while this paper handles linear activ", "thus the authors need to refine bm's result for comparison", "this paper proposes a new dataset for reading comprehension rc", "different from other existing rc datasets the authors claim that this new dataset requires background and common-sense knowledge  and across sentences reasoning in order to answer the questions correctli"], "labels": ["QSN", "CRT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think this dataset is very useful for rc", "the collection process is also carefully designed to reduce the lexical overlap between question and answer pair", "i have the questions as followsi in the abstract authors mentioned the workers set one only takes care of creating questions from version one of the plots and workers set two is in charge of generating answers from another version of plot", "however in bullet  of section  it seems that the workers set one is also required to answer the questions in selfrc", "is there any mistake in the description of the abstract"], "labels": ["APC", "APC", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ii what is the standard for creating the quest", "i noticed that the time and location information was used to generate questions sometime but sometimes these kinds of questions are ignor", "iii why the selfrc is about qa pairs but for paraphraserc you need to include docu", "iv what is the average length of the answers in both paraphraserc and selfrc", "i found that the answers are usually very short which is more like factoid qa"], "labels": ["QSN", "CRT", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be great if the authors could design some non-factoid qa pairs which require more reasoning and background knowledg", "v during nlp pre-processing section  how do you prune the irrelevant docu", "this paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on cifar and takes much less computational resourc", "the high-level idea is similar to the evolution method of [real et al ] but the mutation preserves netnet properties which means the mutated network does not need to retrain from scratch", "the high-level idea is similar to the evolution method of [real et al ] but the mutation preserves netnet properties which means the mutated network does not need to retrain from scratch"], "labels": ["SUG", "QSN", "APC", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["compared to other papers on neural architecture search the required computational resource is impressively small close to state-of-the-art result in one day on a single gpu", "however it is not clear to me what contribute to the massive improvement of spe", "is it due to the network morphing that preserve equ", "is it due to a good initial network structur", "is it due to the well designed mutation oper"], "labels": ["CRT", "DIS", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is it due to the simple hill climbing procedure basically evolution that only preserve the elit", "is it due to a well crafted search space that is potentially easi", "the experiments in this paper does not provide enough evidence to tease apart the possible causes of this dramatic reduction on computational resourc", "and the comparisons to other papers seems not fair since they all operate on different search spac", "in summary getting netnet to work for architecture search is interest"], "labels": ["QSN", "QSN", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary getting netnet to work for architecture search is interest", "and i love the result", "these are very impressive numbers for neural architecture search", "however i am not convinced that the improve is resulted from a better algorithm", "i would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resourc"], "labels": ["APC", "APC", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes an improvement in the speed of training/inference with structured prediction energy networks spens by replacing the inner optimization loop with a network trained to predict its output", "spens are an energy-based structured prediction method where the final prediction is obtained by optimizing min_y e_thetaf_phix y ie finding the label set y with the least energy as computed by the energy function e using a set of computed features f_phix which comes from a neural network", "the key innovation in spens was representing the energy function e as an arbitrary neural network which takes the features fx and candidate labels y and outputs a value for the energi", "at inference time y can be optimized by gradient descent step", "spens are trained using maximum-margin loss functions so the final optimization problem is max -lossy y' where y' = argmin_y efx i"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key idea of this paper is to replace the minimization of the energy function min_y efx y with a neural network which is trained to predict the resulting output of this minim", "the resulting formulation is a min-max problem at training time with a striking similarity to the gan min-max problem where the y-predicting network learns to predict labels with low energy according to the e-computing network and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels ie the y-predicting network acts as a generator and the e-predicting network acts as a discriminatorthe paper explores multiple loss functions and techniques to train these model", "they seem rather finnicky and the experimental results aren't particularly strong when it comes to improving the quality over spens but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based model", "the improved understanding of spens and potential for further work justify accepting this pap", "in this paper the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracl"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular they aim at generating a complete set that fully specifies the behavior of the oracl", "the authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpret", "the technique systematically interacts with the oracle using observations which are abstractions of environment states and it is guaranteed to produce a data set that completely specifies the oracl", "the technique systematically interacts with the oracle using observations which are abstractions of environment states and it is guaranteed to produce a data set that completely specifies the oracl", "the authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class"], "labels": ["SMY", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their experiments show that this improve technique can produce complete training sets for three program", "it is nice to see the application of ideas from different areas for learning-related quest", "however there is one thing that bothers me again and again why do we need a data-generation technique in the paper at al", "typically we are given a set of data not an oracle that can generate such data and our task is to learn something from the data", "if we have an executable oracle it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpret"], "labels": ["DIS", "APC", "QSN", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one thing that i can see is that the technique in the paper can be used when we do research on the neural programmer-interpret", "during research we have multiple executable oracles and need to produce good training data from them", "the authors' technique may let us do this data-generation easili", "but this benefit to the researchers does not seem to be strong enough for the acceptance at iclr'", "but this benefit to the researchers does not seem to be strong enough for the acceptance at iclr'"], "labels": ["APC", "DIS", "APC", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the problem considered in the paper is of compressing large networks grus for faster inference at test tim", "the proposed algorithm uses a two step approach   use trace norm regularization expressed in variational form on dense parameter matrices at training time without constraining the number of paramet", "b initializing from the svd of parameters trained in stage  learn a new network with reduced number of paramet", "the experiments on wsj dataset are promising towards achieving a trade-off between number of parameters and accuraci", "i have the following questions regarding the experiments could the authors confirm that the reported cers are on validation/test dataset and not on train/dev data"], "labels": ["SMY", "SMY", "SMY", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not explicitly st", "i hope it is indeed the former else i have a major concern with the efficacy of the algorithm as ultimately we care about the test performance of the compressed models in comparison to uncompressed model", "in b the authors use an increasing number units in the hidden layers of the grus as opposed to a fixed size like in deep speech  an obvious baseline that is missing from the  experiments is the comparison with *exact* same gru with      hidden units *without any compression*", "what do different points in fig  and  repres", "what are the values of lamdas that were used to train the l and trace norm regularization the stage  of models shown in fig"], "labels": ["DFT", "CRT", "DFT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i want to understand what is the difference in the  two types of  behavior of orange points some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stag", "it is interesting that although l regularization does not lead to low u parameters in stage  the compression stage does have comparable performance to that of trace norm minim", "the authors point it out but a further investigation might be interest", "writing the gru model for which the algorithm is proposed is not introduced until the appendix", "while it is a standard network i think the details should still be included in the main text to understand some of the notation referenced in the text like uclambda_recud and uclambda_norecud"], "labels": ["DIS", "APC", "DIS", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper presents an extensive framework for complex-valued neural network", "related literature suggests a variety of motivations for complex valued neural networks biological evidence richer representation capacity easier optimization faster learning noise-robust memory retrieval mechanisms and mor", "the contribution of the current work does not lie in presenting significantly superior results compared to the traditional real-valued neural networks but rather in developing an extensive framework for applying and conducting research with complex-valued neural network", "indeed the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization regularization activation function convolutions etc", "in this work the complex equivalent of many of these basics tools are developed such as a number of complex activation functions complex batch normalization complex convolution discussion of complex differentiability strategies for complex weight initialization complex equivalent of a residual neural network"], "labels": ["SMY", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts on a variety of different task", "then again the major contribution of this work is not advancing the state-of-the-art on many benchmark task", "but constructing a solid framework that will enable stable and solid application and research of these well-motivated model", "this work re-evaluates complex-valued neural networks complex weights complex activation funct", "the paper acknowledges that complex networks are not new and that the findings of previous authors is that complex networks perform less well than real-valued altern"], "labels": ["DIS", "DFT", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper reports a comparison of real-valued and complex-valued neural networks controlling for storage capacity with an interesting discussion of controlling for capacity in terms of computational infer", "the paper concludes with overall the complex-valued neural networks do not perform as well as expect", "i didn't understand this conclusion because previous work found complex-valued neural networks to be inferior which is consistent with the results reported her", "i did not see support in this paper for the claim in the abstract that special architectures make complex networks work better or that they are well suited to particular data set", "the empirical results are only presented in table-of-numbers format graphical comparisons would be easier to understand and tables - are all zero which doesn't make sense for these classification task"], "labels": ["SMY", "DFT", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["prosthe paper is easy to read", "logic flows naturally within the pap", "cons experimental results are neither enough nor convinc", "only one set of data is used throughout the paper the cifar dataset and the architecture used is only a  layered mlp", "even though lcw performs better than others in this circumst"], "labels": ["APC", "APC", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it does not prove its effectiveness in general or its elimination of the gradient vanishing problem", "for the  layer mlp it's very hard to train a simple mlp and the training/testing accuracy is very low for all the method", "more experiments with different number of layers and different architecture like resnet should be tried to show better result", "in figure  lcw seems to avoid gradient vanishing but introduces gradient exploding problem", "the proposed concept is only analyzed in mlp with sigmoid activation funct"], "labels": ["CRT", "CRT", "SUG", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the experimental parts the authors claim they use both relu and sigmoid function but no comparisons are reflected in the figur", "the whole standpoint of the paper is quite vague and not very convinc", "in section  the authors introduce angle bias and suggest its effect in mlps that with random weights showing that different samples may result in similar output in the second and deeper lay", "however the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connect", "the whole analysis of the connection is built solely on this one sentence at the same time the output does not change if we adjust the weight vectors in layer  which is nowhere verifi"], "labels": ["CRT", "CRT", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further the phenomenon is only tested on random initialization when the network is trained for several iterations and becomes more settled it is not clear how angle affect affects gradient vanishing problem", "minors theorem  are direct conclusions from the definitions and are mis-stated as theorem", "'patters' - 'patterns'", "in section  reasons  and  state the similar thing that output of mlp has relatively small change with different input data when angle bias occur", "only reason  mentions the gradient vanishing problem even though the title of this section is relation to vanishing gradient problem"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors study the effect of label noise on classification task", "they perform experiments of label noise in a uniform setting structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch s", "although the observations are interesting especially the one on mnist where the network performs well even with correct labels slightly above chance the overall contributions are increment", "most of the observations of label noise such as training with structured noise importance of larger datasets have already been archived in prior work such as in sukhbataar etal  and van horn et ", "agreed that the authors do a more detailed study on simple mnist classification but these insights are not transferable to more challenging domain"], "labels": ["SMY", "SMY", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main limitation of the paper is proposing a principled way to mitigate noise as done in sukhbataar etal  or an actionable trade-off between data acquisition and training schedul", "the authors contend that the way they deal with noise keeping number of training samples constant is different from previous setting which use label flip", "however the previous settings can be reinterpreted in the authors set", "i found the formulation of the alpha to be non-intuitive and confusing at tim", "the graphs plot number of noisy labels per clean label so a alpha of  would imply  right label and  noisy labels for total  label"], "labels": ["CRT", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact this depends on the task at hand for mnist it is  clean labels for  label", "this can be improved to help readers understand bett", "there are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting and also devise architectures depending on the level of expected noise in the label", "overall i feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable set", "missing citation training deep neural networks on noisy labels with bootstrapping reed et "], "labels": ["DIS", "SUG", "DFT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a -way factorization thereby identifying spatial and temporal modules as well as their signed activ", "the method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data", "the method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data", "prosthe paper is well written the analysis interesting and the application of the tucker framework sound", "removing baseline is a reasonable step and the paper includes analysis of several spike-train dataset"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the analysis of the approaches in terms of their ability to decode is also sound and interest", "consi find the novelty of the paper limited the authors extend the work by onken et al  to subtract baseline a rather marginal innovation of this approach", "to use a semi-nmf type of update rule as proposed by ding et al  and apply the approach to new spike-train datasets evaluating performance by their decoding ability decoding also considered in onken et ", "multiplicative update-rules are known to suffer from slow-convergence and i would suspect this also to be an issue for the semi-nmf update rul", "it would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted hals which admit negative values in the optimization see also the review by n gileshttps//arxivorg/abs/as well as for instancenielsen sufren fufns vind and morten mufrup"], "labels": ["APC", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain", "machine learning for signal processing mlsp  ieee international workshop on iee", "it would improve the paper to also discuss that the non-negativity constrained tucker model may be subject to local minima solutions and have issues of non-uniqueness ie rotational ambigu", "it would improve the paper to also discuss that the non-negativity constrained tucker model may be subject to local minima solutions and have issues of non-uniqueness ie rotational ambigu", "at least local minima issues could be assessed using multiple random initi"], "labels": ["SUG", "SUG", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results are in general only marginally improved by the baseline corrected non-negativity constrained approach", "for comparison the existing methods ica tucker should also be evaluated for the baseline corrected data to see if it is the constrained representation or the preprocessing influencing the perform", "finally how performance is influenced by dimensionality p and l should also be clarifi", "it seems that it would be naturally to model the baseline by including mean values in the model rather than treating the baseline as a preprocessing step", "this would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the tucker representation to be removed by the preprocess"], "labels": ["APC", "SUG", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor the approach corresponds to a tucker decomposition with non-negativity constrained factor matrices and unconstrained core - please clarify this as you also compare to tucker in the paper with orthogonal factor matricesding et al in their semi-nmf work provide elaborate derivation with convergence guarante", "in the present paper these details are omitted and it is unclear how the update rules are derived from the kkt conditions and the lagrange multiplier and how they differ from standard semi-nmf this should be better clarifi", "the paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological sign", "a loss function based on the wasserstein distance is used the paper is interest", "but could certainly do with more explan"], "labels": ["CRT", "DIS", "SMY", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but could certainly do with more explan", "comments it is difficult for the reader to understand a why wasserstein is us", "and b how exactly the nuisance variation is reduc", "a dedicated section on motivation is missing`", "does the deep metric network always return a '-dim' vector"], "labels": ["CRT", "CRT", "DIS", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["have you checked your model using different length vector", "label the y-axis in fig", "the fact that you have early-stopping as opposed to a principled regularizer also requires further substanti", "this paper presents an experimental study on the behavior of the units of neural network", "in particular authors aim to show that units behave as binary classifiers during training and test"], "labels": ["QSN", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i found the paper unnecessarily longer than the suggested  pag", "the focus of the paper is confusing while the introduction discusses about works on cnn model interpretability the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier without analyzing anything in relation to interpret", "i think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the pap", "also quantitative figures would be useful to get the big pictur", "for example in figures  and  the authors show the behavior of some specific units as examples but it would be nice to see a graph showing quantitatively the behavior of all the units at each lay"], "labels": ["CRT", "CRT", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be also useful to see a comparison of different cnns and see how the observation holds more or less depending on the performance of the network", "this paper considers a special deep learning model and shows that in expectation there is only one unique local minim", "as a result a gradient descent algorithm converges to the unique solut", "this works address a conjecture proposed by tian", "while it is clearly written my main concern is whether this model is significant enough"], "labels": ["SUG", "SMY", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the assumptions k= and v=v= reduces the difficulty of the analysi", "but it makes the model considerably simpler than any practical set", "the paper proposes an adaptation of existing graph convnets and evaluates this formulation on a several existing benchmarks of the graph neural network commun", "in particular a tree structured lstm is taken and modifi", "the authors describe this as adapting it to general graphs stacking followed by adding edge gates and residu"], "labels": ["APC", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my biggest concern is novelty as the modifications are minor", "in particular the formulation can be seen in a different way", "as i see it instead of adapting tree lstms to arbitary graphs it can be seen as taking the original formulation by scarselli and replacing the rnn by a gated version ie adding the known lstm gates input output forget g", "this is a minor modif", "adding stacking and residuality are now standard operations in deep learning and edge-gates have also already been introduced in the literature as described in the pap"], "labels": ["CRT", "DIS", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a second concern is the presentation of the paper which can be confusing at some point", "a major example is the mathematical description of the method", "when reading the description as given one should actually infer that graph convnets and graph rnns are the same thing which can be seen by the fact that equations  and  are equival", "another example after  the important point to raise is the difference to classical sequential rnns namely the fact that the dependence graph of the model is not a dag anymore which introduces cyclic depend", "generally a clear introduction of the problem is also miss"], "labels": ["CRT", "CRT", "CRT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["generally a clear introduction of the problem is also miss", "what are the inputs what are the outputs what kind of problems should be solv", "the update equations for the hidden states are given for all models but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph", "the model has been evaluated on standard datasets with a performance which seems to be on par or a slight edge which could probably be due to the newly introduced residu", "a couple of details - the length of a graph is not defin"], "labels": ["CRT", "QSN", "QSN", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the size of the set of nodes might be m", "the size of the set of nodes might be m", "- at the beginning of section  i do not understand the reference to word prediction and natural language process", "rnns are not restricted to nlp and i think there is no need to introduce an application at this point", "- it is unclear what does the following sentence means convnets are more pruned to deep networks than rnn"], "labels": ["SUG", "CRT", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what are heterogeneous graph domain", "the paper falls far short of the standard expected of an iclr submiss", "the paper has little to no cont", "there are large sections of blank page throughout", "the algorithm iterative temporal differencing is introduced in a figure -- there is no formal descript"], "labels": ["CRT", "FBK", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are only performed on mnist", "the subfigures are not label", "the paper over-uses acronyms sentences like ucin this figure vbp vbp with fba and itd using fba for vbpuud are painful to read", "the paper proposes action-dependent baselines for reducing variance in policy gradient through the derivation based on steinus identity and control funct", "the method relates closely to prior work on action-dependent baselin"], "labels": ["DFT", "DFT", "CRT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but explores in particular on-policy fitting and a few other design choices that empirically improve the perform", "a criticism of the paper is that it does not require steinus identity/control functionals literature to derive eq  since it can be derived similarly to linear control variate and it has also previously been discussed in ipg [gu et al ] as reparameterizable control vari", "the derivation through steinus identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick", "the empirical results appear promising and in particular in comparison with q-prop which fits q-function using off-policy td learn", "however the discussion on the causes of the difference should be elaborated much more as it appears there are substantial differences besides on-policy/off-policy fitting of the q such as-fitlinear fits linear q through parameterization based on linearization of q using on-policy learning rather than fitting nonlinear q and then at application time linearize around the mean act"], "labels": ["APC", "CRT", "DFT", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the discussion on the causes of the difference should be elaborated much more as it appears there are substantial differences besides on-policy/off-policy fitting of the q such as-fitlinear fits linear q through parameterization based on linearization of q using on-policy learning rather than fitting nonlinear q and then at application time linearize around the mean act", "a closer comparison would be to use same locally linear q function for off-policy learning in q-prop-the use of on-policy fitted value baseline within q-function parameterization during on-policy fitting is nic", "similar comparison should be done with off-policy fitting in q-prop", "i wonder if on-policy fitting of q can be elaborated more specifically on-policy fitting of v seems to require a few design details to have best performance [gae schulman et al ] fitting on previous batch instead of current batch to avoid overfitting  this is expected for your method as well since by fitting to current batch the control variate then depends nontrivially on samples that are being applied and possible use of trust-region regularization to prevent v from changing too much across iter", "the paper presents promising results with direct on-policy fitting of action-dependent baseline which is promising since it does not require long training iterations as in off-policy fitting in q-prop"], "labels": ["DIS", "SUG", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as discussed above it is encouraged to elaborate other potential causes that led to performance differ", "the experimental results are presented well for a range of mujoco task", "pros-simple effective method that appears readily available to be incorporated to any on-policy pg methods without significantly increase in computational time-good empirical evalu", "cons-]the name stein control variate seems misleading since the algorithm/method does not rely on derivation through steinus identity etc", "and does not inherit novel insights due to this deriv"], "labels": ["SUG", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythe contribution of this paper is an alternative activation function which is faster to compute than the exponential linear unit yet has similar characterist", "the paper first presents the mathematical form of the proposed activation function isrlu and then shows the similarities to elu graph", "it then argues that speeding up the activation function may be important since the convolution operations in cnns are becoming heavily optimized and may form a lesser fraction of the overall comput", "it then argues that speeding up the activation function may be important since the convolution operations in cnns are becoming heavily optimized and may form a lesser fraction of the overall comput", "it then argues that speeding up the activation function may be important since the convolution operations in cnns are becoming heavily optimized and may form a lesser fraction of the overall comput"], "labels": ["DFT", "SMY", "SMY", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the isrlu is then reported to be x faster compared to elu using avx instruct", "the isrlu is then reported to be x faster compared to elu using avx instruct", "the possibility of computing a faster approximation of isrlu is also ment", "preliminary experimental results are reported which demonstrate that isrlu can perform similar to elu", "quality and significancethe paper proposes an interesting direction for optimizing the computational cost of training and inference using neural network"], "labels": ["SMY", "DIS", "DIS", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however on one hand the contribution is rather narrow and on the other the results presented do not clearly show that the contribution is of significance in practic", "however on one hand the contribution is rather narrow and on the other the results presented do not clearly show that the contribution is of significance in practic", "the paper does not present clear benchmarks showing a what is the fraction of cpu cycles spent in evaluating the activation function in any reasonably practical neural network", "the paper does not present clear benchmarks showing a what is the fraction of cpu cycles spent in evaluating the activation function in any reasonably practical neural network", "b and what is the percentage of cycles saved by employing the isrlu"], "labels": ["SMY", "QSN", "SMY", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the presented results using small networks on the mnist dataset only show that networks with isrlu can perform similar to those with other activation functions but not the speed advantages of isrluthe effect of using the faster approximation on performance also remains to be investig", "claritythe content of the paper is unclear in certain area", "- it is not clear what table  is show", "what is performance measured in in general the table captions need to be clearer and more descript", "what is performance measured in in general the table captions need to be clearer and more descript"], "labels": ["SMY", "DFT", "QSN", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the acronym pkeep in later tables should be clarifi", "- why is the final cross-entropy loss so high even though the accuracy is % for the mnist experi", "it looks like the loss at initialization was reported instead", "the paper was a good contribution to domain adaptation it provided a new way of looking at the problem by using the cluster assumpt", "the experimental evaluation was very thorough and shows that vada and dirt-t performs really wel"], "labels": ["DFT", "QSN", "DFT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i found the math to be a bit problematic for example l_d in  involves a max operator although i understand what the authors mean i don't think this is the correct way to write this  should discuss the min-max object", "i found the math to be a bit problematic for example l_d in  involves a max operator although i understand what the authors mean i don't think this is the correct way to write this  should discuss the min-max object", "this will probably involve an explanation of the gradient reversal etc speaking of grl it's mentioned on p that they replaced grl with the traditional gan object", "this will probably involve an explanation of the gradient reversal etc speaking of grl it's mentioned on p that they replaced grl with the traditional gan object", "this is actually pretty important to discuss in detail did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional gan training why was that important to the author"], "labels": ["DFT", "DIS", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the literature review could also include shrivastava et al and bousmalis et al from cvpr  the latter also had mnist/mnist-m experi", "this paper presents a nearest-neighbor based continuous control polici", "two algorithms are presented nn- runs open-loop trajectories from the beginning state and nn- runs a state-condition policy that retrieves nearest state-action tuples for each st", "the overall algorithm is very simple to implement and can do reasonably well on some simple control tasks but quickly gets overwhelmed by higher-dimensional and stochastic environ", "it is very similar to learning to steer on winding tracks using semi-parametric control policies and is effectively an indirect form of tile coding each could be seen as a fixed voronoi cel"], "labels": ["SUG", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am sure this idea has been tried before in the s but i am not familiar enough with all the literature to find it a quick google search brings this up reinforcement learning of active recognition behaviors with a chapter on nearest-neighbor lookup for policies https//peopleeecsberkeleyedu/~trevor/papers/-/nodehtml", "although i believe there is work to be done in the current round of rl research using nearest neighbor policies i don't believe this paper delves very far into pushing new ideas even a simple adaptive distance metric could have provided some interesting results nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domain", "and for that reason i don't think it has a place as a conference paper at iclr", "i would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area", "this paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments assuming near perfect* relative localization provided by veloc"], "labels": ["CRT", "CRT", "FBK", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the model is more general than the cognitive maps gupta et al  and builds on the ntm/dnc or related architectures graves et al   rae et al  thanks to the d spatial structure of the associative memori", "the model is more general than the cognitive maps gupta et al  and builds on the ntm/dnc or related architectures graves et al   rae et al  thanks to the d spatial structure of the associative memori", "basically it consists of a d-indexed grid of features the map m_t that can be summarized at each time point into read vector r_t and used for extracting a context c_t for the current agent state s_t compute thanks to an lstm/gru an updated write vector w_{t+}^{xy} at the current position and update the map using that write vector", "the position {xy} is a binned representation of discrete or continuous coordin", "the absolute coordinate map can be replaced by a relative ego-centric map that is shifted just like in gupta et al  as the agent mov"], "labels": ["SMY", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the absolute coordinate map can be replaced by a relative ego-centric map that is shifted just like in gupta et al  as the agent mov", "the experiments are exhaustive and include remembering the goal location with or without cues similarly to mirowski et al  not cited in simple mazes of size x up to x in the d doom environ", "the most important aspect is the capability to build a feature map of previously unseen environ", "this paper showing excellent and important work has already been published on arxiv  months ago and widely cit", "it has been improved since through different sets of experiments and apparently a clearer present"], "labels": ["DIS", "APC", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the ideas are the sam", "i wonder how it is possible that the paper has not been accepted at icml or nips assuming that it was actually submitted ther", "what are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research or are they ignorant and is the peer review system broken", "i quite like the formulation of the nips ratings if this paper does not get accepted i am considering boycotting the confer", "* the noise model experiment in appendix d is commend"], "labels": ["CRT", "DIS", "QSN", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the noise model is somewhat unrealistic very small variance zero mean gaussian and assumes only drift in x and y not along the orient", "while this makes sense in grid world environments or rectilinear mazes it does not correspond to realistic robotic navigation scenarios with wheel skid missing measurements etc", "perhaps showing examples of trajectories with drift added would help convince the reader there is no space restriction in the appendix", "the authors propose an alternating minimization framework for training autoencoders and encoder-decoder network", "the central idea is that a single encoder-decoder network can be cast as an alternating minimization problem"], "labels": ["CRT", "CRT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize wrt each vari", "this leads to the proposed algorithm called dante which simply minimizes wrt each variable using stochastic normalized gradient algorithm to minimize wrt each variable the authors start with this idea and introduce a generalized relu which is specified via a subgradient function only whose local quasi-convexity properties are establish", "they then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each lay", "the ideas are interest", "but i have some concerns regarding this work"], "labels": ["SMY", "SMY", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["major comments when dealing with a  layer network where there are  matrices w_ w_ to optimize over it is not clear to me why optimizing over w_ is a quasi-convex optimization problem", "the authors seem to use the idea that solving a glm problem is a quasi-convex optimization problem", "however optimizing wrt w_ is definitely not a glm problem since w_ undergoes two non-linear transformations one via phi_ and another via phi_", "could the authors justify why minimizing wrt w_ is still a quasi-convex optimization problem", "theorem   establish  slqc properties with generalized relu activ"], "labels": ["QSN", "DIS", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an interesting result and useful in its own right", "however it is not clear to me why this result is even relevant here the main application of this paper is autoencoders which are functions from r^d - r^d however glms are functions from r^d --- r so it is not at all clear to me how theorem   and eventually  are useful for the autoencoder problem that the authors care about", "yes they are useful if one was doing -layer neural networks for binary classification but it is not clear to me how they are useful for autoencoder problem", "experimental results for classification are not convincing enough", "if one looks at table  sgd outperforms dante on ionosphere dataset and is competent with dante on mnist and usp"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results on reconstruction do not show any benefits for dante over sgd figur", "i would recommend the authors to rerun these experiments but truncate the iterations early enough", "if dante has better reconstruction performance than sgd with fewer iterations then that would be a positive result", "this paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-gaussian information bottleneck and solved via variational auto-encoder stochastic optim", "the authors suggest gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual inform"], "labels": ["CRT", "SUG", "SUG", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments include both artificial data and real data", "the paper is unclear at some places and writing gets confus", "for example it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments and the discussion at the end of section  also sounds confus", "it would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practic", "it would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practic"], "labels": ["SMY", "CRT", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover what is the form of f_beta and how beta is optim", "in the first equation on page  is tilde y involv", "how to choose lambda", "if mi is invariant to monotone transformations and information curves are determined by mis why uctransformations basically makes information curve arbitraryud", "can you elabor"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves more compact representation and better reconstruction quality it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attack", "minor comments - what is the meaning of the dashed lines and the solid lines respectively in figur", "- section  at the bottom of page  what is tilde t_j and x in the second term", "is there a typo", "- typo find the ucmost orthogonalud representation if the inputs - of the input"], "labels": ["SUG", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the main idea of this paper is interesting and well motivated and but the technical contribution seems increment", "overall the main idea of this paper is interesting and well motivated and but the technical contribution seems increment", "the paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough", "***************updates ***************the authors have clarified some questions that i had and further demonstrated the benefits of copula transform with new experiments in the revised pap", "***************updates ***************the authors have clarified some questions that i had and further demonstrated the benefits of copula transform with new experiments in the revised pap"], "labels": ["APC", "FBK", "DFT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["***************updates ***************the authors have clarified some questions that i had and further demonstrated the benefits of copula transform with new experiments in the revised pap", "the new results are quite informative and addressed some of the concerns raised by me and other review", "the new results are quite informative and addressed some of the concerns raised by me and other review", "i have updated my score to  accordingli", "the authors describe a mechanism for defending against adversarial learning attacks on classifi"], "labels": ["FBK", "SUG", "FBK", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they first consider the dynamics generated by the following procedur", "they begin by training a classifier generating attack samples using fgsm then hardening the classifier by retraining with adversarial samples generating new attack samples for the retrained classifier and rep", "they next observe that since fgsm is given by a simple perturbation of the sample point by the gradient of the loss that the fixed point of the above dynamics can be optimized for directly using gradient desc", "they call this approach sens fgsm and evaluate it empirically against the various iterates of the above approach", "they then generalize this approach to an arbitrary attacker strategy given by some parameter vector eg a neural net for generating adversarial sampl"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this case the attacker and defender are playing a minimax game and the authors propose finding the minimax or maximin parameters using an algorithm which alternates between maximization and minimization gradient step", "they conclude with empirical observations about the performance of this algorithm", "the paper is well-written and easy to follow", "however i found the empirical results to be a little underwhelm", "sens-fgsm outperforms the adversarial training defenses tuned for the ucwrongud iteration but it does not appear to perform particularly well with error rates well above %"], "labels": ["SMY", "SMY", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how does it stack up against other defense approaches eg https//arxivorg/pdf/pdf", "furthermore what is the significance of fgsm-curr fgsm- for sens-fgsm", "it is my understanding that sens-fgsm is not trained to a particular iteration of the uccat-and-mouseud gam", "why then does sens-fgsm provide a consistently better defense against fgsm-", "with regards to the second part of the paper using gradient methods to solve a minimax problem is not especially novel ie goodfellow et "], "labels": ["QSN", "QSN", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus i would liked to see more thorough experiments here as wel", "for example itus unlikely that the defender would ever know the attack network utilized by an attack", "how robust is the defense against samples generated by a different attack network", "the authors seem to address this in section  by stating that the minimax solution is not meaningful for other network classes however this is a bit unsatisfi", "any defense can be *evaluated* against samples generated by any attacker strategi"], "labels": ["SUG", "DIS", "QSN", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is it the case that the defenses fall flat against samples generated by different architectur", "minor commentssection  first line udfulgxyud appears to be a mistak", "in this paper the author propose a cnn based solution for somatic mutation calling at ultra low allele frequ", "the tackled problem is a hard task in computational biology and the proposed solution kittyhawk although designed with very standard ingredients several layers of cnn inspired to the vgg structure seems to be very effective on both the shown dataset", "the paper is well written up to a few misprints the introduction and the biological background very accurate although a bit technical for the broader audience and the bibliography reasonably complet"], "labels": ["QSN", "DFT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["maybe the manuscript part with the definition of the accuracy measures may be skip", "moreover the authors themselves suggest how to proceed along this line of research with further improv", "i would only suggest to expand the experimental section with further real examples to strengthen the claim", "overall i rate this manuscript in the top % of the accepted pap", "this paper presents a method for matrix factorization using dnn"], "labels": ["DIS", "DIS", "SUG", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the suggestion is to make the factorization machine eqn  deep by grouping the features meaningfully eqn  extracting nonlinear features from original inputs deep-in eqn  and adding additional nonlinearity after computing pairwise interactions deep-out eqn", "from the methodology point of view such extensions are relatively straightforward", "as an example from the experimental results it seems the grouping of features is done mostly with domain knowledge eg months of year and not learned automat", "the authors claim the proposed method can circumvent the cold-start problem and presented some experimental results on recommendation systems with text featur", "while the application problems look quite interesting in my opinion the paper needs to make the context and contribution clear"], "labels": ["SUG", "SMY", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the application problems look quite interesting in my opinion the paper needs to make the context and contribution clear", "in particular there is a huge literature in collaborative filtering and i believe there is by now sufficient work on collaborative filtering with input features and possibly dealing with the cold-start problem", "i think this paper does not connect very well with that literatur", "when reading it at times i felt the main purpose of this paper is to solve the application problems presented in experimental results instead of proposing a general framework", "i suggest the authors to demonstrate their method on some well-known datasets eg movielens netflix to give the readers an idea if the proposed method is indeed advantageous over more classical method"], "labels": ["APC", "SMY", "DFT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suggest the authors to demonstrate their method on some well-known datasets eg movielens netflix to give the readers an idea if the proposed method is indeed advantageous over more classical method", "or if the success of this paper is mostly due to clever processing of text features using dnn", "or if the success of this paper is mostly due to clever processing of text features using dnn", "some detailed comments eqn  does not indicate any rank-r factor", "some statements do not seem straightforward/justified to me      -- the paper uses the word inference several times without definit"], "labels": ["CRT", "SMY", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- if we were interested in interpreting the parameters we could constrain w to be non-neg", "is this easy to do and can the authors demonstrate this in their experiments and show interpretable exampl", "is this easy to do and can the authors demonstrate this in their experiments and show interpretable exampl", "-- note that if the dot product is replaced with a neural function fast inference for cold-start", "the experimental setup seems quite unusual to me since we only observe positive labels for such tasks in the test set we sample a labels according to the label frequ"], "labels": ["SMY", "SMY", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental setup seems quite unusual to me since we only observe positive labels for such tasks in the test set we sample a labels according to the label frequ", "this seems very problematic if most of the entries are not observ", "why cannot you use the typical evaluation procedure for collaborative filt", "where you hide some known entries during model training and evaluate on these entries during test", "the paper proposes a ucross view trainingu approach to semi-supervised learn"], "labels": ["DIS", "DFT", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the teacher-student framework for semi-supervised learning it introduces a new cross view consistency loss that includes auxiliary softmax layers linear layers followed by softmax on lower levels of the student model", "the auxiliary softmax layers take different views of the input for predict", "pros a simple approach to encourage better representations learned from unlabeled exampl", "experiments are comprehens", "cons the whole paper just presented strategies and empirical result"], "labels": ["SMY", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are no discussions of insights and why the proposed strategy work for what cases it will work and for what cases it will not work whi", "there are no discussions of insights and why the proposed strategy work for what cases it will work and for what cases it will not work whi", "the addition of auxiliary layers improves sequence tagging results margin", "the claim of cross-view for sequence tagging setting is problemat", "because the task is per-position tagging those added signals are essentially not part of the examples but the signals of its neighbor"], "labels": ["DFT", "QSN", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["adding n^ linear layers for image classification essentially makes the model much larg", "it is unfair to compare to the baseline models with much fewer paramet", "the cvt no noise should be compared to cvt random noise then to cvt adversarial noise the current results show that the improvements are mostly from vat instead of cvt", "the authors show that two types of singularities impede learning in deep neural networks elimination singularities where a unit is effectively shut off by a loss of input or output weights or by an overly-strong negative bias and overlap singularities where two or more units have very similar input or output weight", "they then demonstrate that skip connections can reduce the prevalence of these singularities and thus speed up learn"], "labels": ["CRT", "DFT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the analysis is thorough the authors explore alternative methods of reducing the singularities and explore the skip connection properties that more strongly reduce the singularities and make observations consistent with their overarching claim", "i have no major critic", "one suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficaci", "the authors could then use this procedure to make highly trainable networks and show that on test not training data the resultant network leads to high perform", "the authors could then use this procedure to make highly trainable networks and show that on test not training data the resultant network leads to high perform"], "labels": ["APC", "DIS", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents a method based on a bayesian classifier that improves classification of rare classes in datasets with long tail class distribut", "the method is based on balance the class-priors to generalize well for rare class", "by using a gaussian mixture model gmm authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models such as current deep learning classifi", "authors also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail", "they face class imbalance problems particular long tail distributions by fixing i the covariance matrices of all the classes to be the identity and ii the priors over each class to be uniform"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so all classes popular and rare have equal weight for bayesian classif", "to me this is not a fundamental way to solve the long-tail problem in the sense that by fixing isotropic likelihoods and flat priors authors are also ignoring information that can be relevant in some classification problems where a good prior can be useful to disambiguate confusing situ", "on other hand using a unimodal function to model each class is an over-simplification that ignores intra-class complex", "the datasets used by the authors are balanced so they artificially transform them into long-tail", "it will be good to test directly on real long-tailed dataset"], "labels": ["SMY", "CRT", "CRT", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimentation is only performed using small to medium datasets  k instances it will be good to show if the benefits of the proposed approach can also be present in the case of large dataset", "in this sense i agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy however the protocol proposed requires to train the model several times therefore it does not scale properly to large datasets that are the common rule in the deep learning world", "in this sense i agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy however the protocol proposed requires to train the model several times therefore it does not scale properly to large datasets that are the common rule in the deep learning world", "results respect to similar state-of-the-art techniques shows a reasonable improvement depends of the dataset approx -%", "overviewthis paper proposes an approach to curriculum learning where subsets of examples to train on are chosen during the training process"], "labels": ["DFT", "DFT", "CRT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is based on a submodular set function over the examples which is intended to capture diversity of the included examples and is added to the training objective eq", "the set is optimized to be as hard as possible maximize loss which results in a min-max problem", "this is in turn optimized approximately by alternating between gradient-based loss minimization and submodular maxim", "the theoretical analysis shows that if the loss is strongly convex then the algorithm returns a solution which is close to the optimal solut", "empirical results are presented for several benchmark"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is mostly clear and the idea seems nic", "on the downside there are some limitations to the theoretical analysis and optimization scheme see comments below", "comments- the theoretical result thm  studies the case of full optimization which is different than the proposed algorithm running a fixed number of weight upd", "it would be interesting to show results on sensitivity to the number of updates p", "- the algorithm requires tuning of quite a few hyperparameters sec"], "labels": ["APC", "CRT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- approximating a cluster with a single sample sec  seems rather crud", "there should be some theoretical and/or empirical study of its effect on quality of the solut", "minor/typos- what is gj|gj in eq", "- why cite anonymous  instead of appendix", "- define v in thm - in eq  it may be clearer to denote g_kw likewise in eq  hat{g}_hat{a}w and in eq  tilde{g}_{cal{a}}w"], "labels": ["DFT", "SUG", "QSN", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- figures readability can be improv", "this paper proposes a tensor factorization-type method for learning one hidden-layer neural network", "the most interesting part is the hermite polynomial expansion of the activation funct", "such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem", "they further redesign a new formulation for the tensor decomposition problem and show that the new formulation enjoys the nice strict saddle properties as shown in ge et "], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at last they also establish the sample complexity for recoveri", "the organization and presentation of the paper need some improv", "for example the authors defer many technical detail", "to make the paper accessible to the readers they could provide more intuitions in the first  pag", "there are also some typos for example the dimension of a is inconsist"], "labels": ["SMY", "SUG", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the abstract a is an m-dimensional vector and on page  a is a d-dimensional vector", "on page  pb should be a degree- polynomial of b", "the paper does not contains any experimental results on real data", "the paper studies methods for verifying neural nets through their piecewiselinear structur", "the authors survey different methods from the literaturepropose a novel one and evaluate them on a set of benchmark"], "labels": ["CRT", "SUG", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a major drawback of the evaluation of the different approaches is thateverything was used with its default paramet", "it is very unlikely that thesedefaults are optimal across the different benchmark", "to get a better impressionof what approaches perform well their parameters should be tuned to theparticular benchmark", "this may significantly change the conclusions drawn fromthe experi", "figures - are hard to interpret and do not convey a clear messag"], "labels": ["DFT", "DIS", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is noclear trend in many of them and a lot of nois", "it may be better to relate thestructure of the network to other measures of the hardness of a problem egthe phase transit", "again parameter tuning would potentially change all ofthese figures significantly as would eg a change in hardwar", "given the kindof general trend the authors seem to want to show here i feel that a moretheoretic measure of problem hardness would be more appropriate her", "the authors say of the proposed twinstream dataset that it may not berepresentative of real use-cases it seems odd to propose something that isentirely artifici"], "labels": ["CRT", "SUG", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the description of the empirical setup could be more detail", "are theproperties that are being verified different properties or the same property ondifferent network", "the tables look ugli", "it seems that the header data set should be approachor something similar", "in summary i feel that while there are some issues with the paper it presentsinteresting results and can be accept"], "labels": ["SUG", "QSN", "CRT", "SUG", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient desc", "i am not an expert in this area", "it is hard to judge the significance of this extens", "the paper largely follows the work of nachum et ", "the differences ie the claimed novelty from that work are the relative entropy and trust region method for train"], "labels": ["SMY", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the relative entropy term added seems like a marginal modif", "the paper first analyzes recent works in machine reading comprehension largely centered around squad and mentions their common trait that the attention is not fully-aware of all levels of abstraction eg word-level phrase-level etc", "in turn the paper proposes a model that performs attention at all levels of abstraction which achieves the state of the art in squad", "they also propose an attention mechanism that works better than others symmetric + relu", "strengths- the paper is well-written and clear"], "labels": ["CRT", "SMY", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i really liked table  and figure  it nicely summarizes recent work in the field", "- the multi-level attention is novel and indeed seems to work with convincing abl", "- nice engineering achievement reaching the top of the leaderboard in early octob", "weaknesses- the paper is long  pages but relatively lacks subst", "weaknesses- the paper is long  pages but relatively lacks subst"], "labels": ["APC", "APC", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ideally i would want to see the visualization of the attention at each level ie how they differ across the levels and also possibly this model tested on another dataset eg triviaqa", "ideally i would want to see the visualization of the attention at each level ie how they differ across the levels and also possibly this model tested on another dataset eg triviaqa", "- the authors claim that the symmetric + relu is novel but  i think this is basically equivalent to bilinear attention [] after fully connected layer with activation which seems quite standard", "still useful to know that this works better so would recommend to tone down a bit regarding the paper's contribut", "minor- probably figure  can be drawn bett"], "labels": ["DIS", "CRT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor- probably figure  can be drawn bett", "not easy to understand nor concret", "- section  gru citation should be cho et al []", "questions- contextualized embedding seems to give a lot of improvement in other works too", "could you perform ablation without contextualized embedding cov"], "labels": ["CRT", "CRT", "CRT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["reference[] luong et al effective approaches to attention-based neural machine transl", "emnlp [] cho et al learning phrase representations using rnn encoder-decoder for statistical machine translation emnlp", "the manuscript introduces the sensor transformation attention networks a generic neural architecture able to learn the attention that must be payed to different input channels sensors depending on the relative quality of each sensor with respect to the oth", "speech recognition experiments on synthetic noise on audio and video as well as real data are shown", "first of all i was surprised on the short length of the discussion on the state-of-the-art"], "labels": ["DIS", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["attention models are well known and methods to merge information from multiple sensors also very easily multiple kernel learning but many oth", "second from a purely methodological point of view stans boil down to learn the optimal linear combination of the input sensor", "there is nothing wrong about this but perhaps other more complex non-linear models to combine data could lead to more robust learn", "third the experiments with synthetic noise are significant to a reduced extend", "indeed adding gaussian noise to a replicated input is too artificial to be meaning"], "labels": ["DIS", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the network is basically learning to discard the sensor when the local standard deviation is high", "but this is not the kind of noise found in many applications and this is clearly shown in the performances on real data not always improving wrt state of the art", "the interesting part of these experiments is that the noise is not stationary and this is quite characteristic of real-world appl", "also to be fair when discussion the results the authors should say that simple concatenation outperforms the single sensor paradigm", "i am also surprised about the baseline choic"], "labels": ["CRT", "CRT", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a way to merge/discard sensors and there is no comparison with other ways of doing it apart from the trivial sensor concaten", "it is difficult to understand the benefit of this technique if no other baseline is benchmark", "this mitigates the impact of the manuscript", "i am not sure that the discussion in page corresponds to the actual number on table  i did not understand what the authors wrot", "the main concern of this submission is the novelti"], "labels": ["CRT", "CRT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["proposed method to visualize the loss function sounds too incremental from existing work", "one of the main distinctions is using filter-wise normalization but it is somehow trivi", "in experiments no comparisons against existing works is performed at least on toy/controlled environ", "in experiments no comparisons against existing works is performed at least on toy/controlled environ", "some findings in this submission indeed look interest"], "labels": ["APC", "SMY", "DFT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it is not clear if those results are something difficult to find with other existing standard way", "or even how reliable they are since the effectiveness has not been evalu", "or even how reliable they are since the effectiveness has not been evalu", "minor comments in introduction parameter with zero training error doesn't mean it's a global minimizerin section  it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima", "minor comments in introduction parameter with zero training error doesn't mean it's a global minimizerin section  it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima"], "labels": ["DFT", "DFT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in figure  why do we have solutions at  for small batch size and  for large batch size cas", "in figure  why do we have solutions at  for small batch size and  for large batch size cas", "why should they be differ", "the paper develops a technique to understand what nodes in a neural network are importantfor predict", "the approach they develop consists of using an indian buffet process to model a binary activation matrix with number of rows equal to the number of exampl"], "labels": ["DFT", "CRT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the binary variables are estimated by taking a relaxed version of the asymptotic map objective for this problem", "one question from the use of the indian buffet process how do the asymptotics of the feature allocation determine the number of hidden units select", "one question from the use of the indian buffet process how do the asymptotics of the feature allocation determine the number of hidden units select", "overall the results didn't warrant the complexity of the method", "the results are neat but i couldn't tell why this approach was better than oth"], "labels": ["SMY", "SMY", "QSN", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["lastly can you intuitively explain the additivity assumption in the distribution for py'", "i only got access to the paper after the review deadline and did not have a chance to read it until now hence the lateness and brev", "the paper tackles an important theoretical question and it offers results that are complementary to existing results eg soudry et ", "however the paper does not properly relate their results assumptions in the context of the existing literatur", "much explanation is needed in the author reply in order to clear these quest"], "labels": ["QSN", "DIS", "SMY", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work should not be evaluated from a practical perspective as it is of a theoretical natur", "i agree with most of the criticism raised by other review", "however i also believe the authors managed to clear essentially of the criticism in they repli", "the paper lacks in clarity as currently written", "the results are interesting but more explanation is needed for the main message to be conveyed more clearli"], "labels": ["DIS", "CRT", "FBK", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results are interesting but more explanation is needed for the main message to be conveyed more clearli", "i suggest  but the paper has a potential to become  in my eyes in a future resubmiss", "pros* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning model", "* the paper is clearly written and easy to understand", "cons* weak experiments performance of algorithms are not analyzed in terms of wall-clock and important baselines are not compared against making it difficult to judge the practical usefulness of the proposed algorithm"], "labels": ["APC", "FBK", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* weak theory although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice the proof is straightforward corollary of lin et ", "summary this paper proposes to update parameters of each layer in deep neural network asynchronously instead of updating all layers simultaneously and synchron", "authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between lay", "while asynchronous updates of parameters in stochastic gradient descent has been explored dating back to [] in  and authors should also be referring to [] to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studi", "since model-parallel training across machines has not been very successful and model-parallelism has been only exploited within machines asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machin"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately the practical usefulness of the algorithm has not been demonstr", "it remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures or in which situations this algorithm will be more useful than existing algorithm", "experiments are all reported in terms of the number of updates epochs but this is not useful in judging the practical advantage of the proposed algorithm", "what matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_ and i would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time as they can better exploit optimized tensor arithmetic on cpus and gpu", "also authors should compare against mini-batch gradient descent because this is the most popular way of training deep neural networks authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method"], "labels": ["DFT", "CRT", "CRT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm", "the algorithm operates in discrete time horizon and continuous time is not used anywher", "authors rely mostly on lin et al for the convergence proof which is also based on discrete time horizon", "authors argue in page  that continuous propagation is statistically superior to mini-batch gradient descent but i cannot find statistical superiority of the method", "also the upper bound of the time-delay t slows down the convergence rate proposition in the appendix so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient desc"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think which algorithm is faster depends on values of l t and m", "authors do not provide enough cit", "continuous-time characterization of gradient descent has a long history and authors should provide citation of it for example when  is introduc", "authors should provide more discussion of the history of model-parallel asynchronous sgd such as [] and [] and when mentioning alternatives like czarnecki et al  authors should discuss what advantages and disadvantages the proposed algorithm has against these altern", "[] distributed asynchronous deterministic and stochastic gradient optimization algorithms tsitsiklis bertsekas and athans [] hogwild! a lock-free approach to parallelizing stochastic gradient descent niu et "], "labels": ["DIS", "CRT", "CRT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overviewthe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous dataset", "overviewthe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous dataset", "the reinforcement learning part is based on a policy network which selects the data instance to be labeled next", "they use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional represent", "the authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myop"], "labels": ["SMY", "DIS", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments show the proposed approach is effective on  uci dataset", "strength* the paper is mostly clear and easy to follow", "* the overall idea is interesting and has many potenti", "* the experimental results are promising on multiple dataset", "* there are thorough discussion with related work"], "labels": ["APC", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weakness* the graph in p don't show the architecture of the network clearli", "* the motivation of using feature histograms as embedding is not clear", "* the description of the -d histogram on p is not clear", "the term posterior value sounds ambigu", "* the experiment sets a fixed budget of only  instances which seems to be rather few in some active learning scenarios especially for non-linear learn"], "labels": ["DFT", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the experiments takes a fixed k iterations for training and the convergence status eg whether the accumulated gradient has stabilized the policy is not clear", "* are there particular reasons in using policy learning instead of other reinforcement learning approach", "* the term az in the objective function can be more clearly describ", "* while many loosely-related works were surveyed it is not clear why literally none of them were compar", "there is thus no evidence on whether a myopic bandit learner say chu and lin's work is really worse than the rl polici"], "labels": ["DFT", "QSN", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is also no evidence on whether adaptive learning on the fly is needed or not", "* in equation  should there be a balancing parameter for the reconstruction loss", "* some typos    - page  some duplicate words in discriminative embedding session    - page  auxliary - auxiliary    - page  tescting - test", "* some typos    - page  some duplicate words in discriminative embedding session    - page  auxliary - auxiliary    - page  tescting - test", "this work proposed a reconfiguration of the existing state-of-the-art cnn model architectures including resnet and densnet"], "labels": ["SMY", "DFT", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by introducing new branching architecture coupled ensembles they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget", "additionally they also show that the proposed ensemble method results in better performance than other ensemble methods for example ensemble over independently trained models  not only in combined mode but also in individual branch", "paper strengths* the proposed coupled ensembles method truly show impressive results in classification benchmark densenet-bc l =  k =  e =", "* detailed analysis on different ensemble fusion methods on both training time and testing tim", "* simple but effective design to achieve a better result in testing time with same total parameter budget"], "labels": ["SMY", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["tpaper weakness* some detail about different fusing method should be mentioned in the main paper instead of in the supplementary materi", "* in practice how much more gpu memory is required to train the model with parallel branches with same parameter budgets because memory consumption is one of the main problems of networks with multiple branch", "* at least one experiment should be carried out on a larger dataset such as imagenet to further demonstrate the validity of the proposed method", "* at least one experiment should be carried out on a larger dataset such as imagenet to further demonstrate the validity of the proposed method", "* more analysis can be conducted on the training process of the model"], "labels": ["DFT", "QSN", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["will it converge fast", "what will be the total required training time to reach the same performance compared with single branch model with the same parameter budget", "the authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar", "the authors demonstrate on mnist and cifar that the added regularization leads to more robustness to various kinds of attack", "the authors further propose to enhance the network with cascaded adversarial training that is learning against iteratively generated adversarial inputs and showed improved performance against harder attack"], "labels": ["QSN", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea proposed is fairly straight-forward", "despite being a simple approach the experimental results are quite promis", "the analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insight", "as pointed out in section  increasing the regularization coefficient leads to degenerated embed", "have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings for example normalizing the inputs before sending it to the bidirectional or pivot loss or use cosine distance etc"], "labels": ["APC", "APC", "APC", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["table  and  seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set which is a bit counter-intuit", "do the authors have any insight on thi", "comments the writing of the paper could be improv", "for example transferability analysis in section  is barely understand", "arrow in figure  are not quite read"], "labels": ["CRT", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n the paper is over  pag", "the authors might want to consider shrink it down the recommended length", "in this paper the authors proposed an interesting algorithm for learning the l-svm and the fourier represented kernel togeth", "the model extends kernel alignment with random feature dual representation and incorporates it into l-svm optimization problem", "they proposed algorithms based on online learning in which the langevin dynamics is utilized to handle the nonconvex"], "labels": ["CRT", "SUG", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["under some conditions about the quality of the solution to the nonconvex optimization they provide the convergence and the sample complex", "empirically they show the performances are better than random feature and the lkrf", "i like the way they handle the nonconvexity component of the model", "however there are several issues need to be address", "in eq  although due to the convex-concave either min-max or max-min are equivalent such claim should be explained explicitli"], "labels": ["SMY", "APC", "APC", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in eq  although due to the convex-concave either min-max or max-min are equivalent such claim should be explained explicitli", "in the paper there is an assumption about the peak of random feature it is a natural assumption on realistic data that the largest peaks are close to the origin", "i was wondering where this assumption is us", "i was wondering where this assumption is us", "could you please provide more justification for such assumpt"], "labels": ["DFT", "SMY", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could you please provide more justification for such assumpt", "although the proof of the algorithm relies on the online learning regret bound the algorithm itself requires visit all the data in each update and thus it is not suitable for online learn", "although the proof of the algorithm relies on the online learning regret bound the algorithm itself requires visit all the data in each update and thus it is not suitable for online learn", "please clarify this in the paper explicitli", "please clarify this in the paper explicitli"], "labels": ["CRT", "DFT", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiment is weak", "the algorithm is closely related to boosting and mkl while there is no such comparison", "the algorithm is closely related to boosting and mkl while there is no such comparison", "meanwhile since the proposed algorithm requires extra optimization wrt random feature it is more convincing to include the empirical runtime comparison", "meanwhile since the proposed algorithm requires extra optimization wrt random feature it is more convincing to include the empirical runtime comparison"], "labels": ["CRT", "DFT", "CRT", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["suggestion it will be better if the author discusses some other model besides l-svm with such kernel learn", "this paper proposes an activation function called displaced relu drelu to improve the performance of cnns that use batch norm", "compared to relu drelu cut the identity function at a negative value rather than the zero", "as a result the activations outputted by drelu can have a mean closer to  and a variance closer to  than the standard relu", "the drelu is supposed to remedy the problem of covariate shift bett"], "labels": ["DFT", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the presentation of the paper is clear", "the proposed method shows encouraging results in a controlled setting ie all other units like dropout are remov", "statistical tests are performed for many of the experimental results which is solid", "however i have some concerns  as drelux = max{-delta x} what is the optimal strategy to determine delta", "if it is done by hyperparameter tuning with cross-validation the training cost may be too high"], "labels": ["APC", "APC", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe the control experiments are encourag", "but i do not agree that other techniques like dropouts are not us", "using drelu to improve the state-of-art neural network in an uncontrolled setting is import", "the arguments for skipping this experiments are respect", "but not convincing enough"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["batch normalization is popular especially for the convolutional neural network", "however its application is not universal which can limit the use of the proposed drelu it is a minor concern anyway", "after the rebuttali do not think i had a major misunderstanding of the pap", "i was aware that the features mostly refers to the inputs to softmax", "in my point  i was suggesting that in order to have clustering performance one might alternatively work on the softmax outputs instead of the input"], "labels": ["DIS", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my opinion on this paper remains and i think the contribution of this paper to machine learning is not very clearer at the current stag", "it might be the case that the considered scenarios indeed happen in computer vision related problems but i am not an expert in that regard", "========================================================================this paper proposes a regularization to the softmax layer which try to make the distribution of feature representation inputs fed to the softmax layer more meaningful according to the euclidean dist", "the proposed isotropic loss in equation  tries to equalize the squared distances from each point to the mean so the features are encouraged to lie close to a spher", "overall the proposed method is a relatively simple tweak to softmax"], "labels": ["CRT", "DIS", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors show that empirically features learned under softmax loss + isotropic regularization outperforms other features in euclidean metric-based task", "the authors show that empirically features learned under softmax loss + isotropic regularization outperforms other features in euclidean metric-based task", "my main concern with this paper is the motivation what are the practical scenarios in which one would want to used proposed method", "it is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  euclidean metric eg the problem depicted in figure   because they are not trained to do so their purpose is just to predict the correct label", "while the proposed regularization does lead to a nicer euclidean geometry there is not sufficient motivation and evidence showing this regularization improves classification accuraci"], "labels": ["SMY", "DIS", "QSN", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the proposed regularization does lead to a nicer euclidean geometry there is not sufficient motivation and evidence showing this regularization improves classification accuraci", "in table  the authors seem to indicate that not using the label information in the definition of isotropic loss is an advantag", "in table  the authors seem to indicate that not using the label information in the definition of isotropic loss is an advantag", "but this does not matter since you already use the labels in the softmax loss", "i can not easily think of scenarios in which we would like to perform knn in the feature space table  after training a softmax lay"], "labels": ["CRT", "SMY", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact table  shows knn is almost always worse than softmax in terms of classification accuraci", "running kmeans or agglomerative clustering in the feature space table  *using the euclidean metric* is again ill-posed because the softmax layer is not trained to do thi", "if one really wants good clustering performance one shall always try to learn a good metric or  why do not you perform clustering on the softmax output a probability vector", "", "the experiments on adversarial robustness and face verification seems more interesting to m"], "labels": ["CRT", "DIS", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the tasks were not carefully explained for someone not familiar with that literatur", "perhaps for these tasks multi-class classification is not the most correct objective and maybe the proposed regularization can help but the motivations are not given", "this paper introduces the concepts of counterfactual regret minimization in the field of deep rl", "specifically the authors introduce an algorithm called arm which can deal with partial observability bett", "the results is interesting and novel"], "labels": ["CRT", "CRT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper should be accept", "the presentation of the paper can be improved a bit", "much of the notation introduced in section  is not used later on", "there seems to be a bit of a disconnect before and after sect", "the algorithm in deep rl could be explained a bit bett"], "labels": ["APC", "CRT", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are some papers that could be connect", "notably the distributional rl work that was recently published could be very interesting to compare against in partially observed environ", "it could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the go", "the argument the authors made against recurrent value functions is that recurrent value could be hard to train", "an experiment illustrating this effect could be illumin"], "labels": ["DFT", "SUG", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can the proposed approach help when we have recurrent value funct", "since recurrence does not guarantee that all information needed is captur", "finally some miscellaneous pointsone interesting reference memory-based control with recurrent neuralnetworks by heess et ", "potential typos in the th bullet point in section  should it be rho^{pi}h s'", "there are many language issues rendering the text hard to understand eg-- in the abstract several convolution on graphs architectures-- in the definitions let data with n observation no verb no plural etc"], "labels": ["QSN", "CRT", "DIS", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- in the computational section training size is  and testing i", "so part of my negative impression may be pure mis-understanding of whatthe authors had to say", "still the authors clearly utilise basic concepts cf utilize eigenvector basis of the graph laplacian to do filtering in the fourier domain in waysthat do not seem to have any sensible interpretation whatsoever even allowingfor the mis-understanding due to grammar", "there are no clear insight no theorems and an empirical evaluation on an ill-defined problem in time-series forecast", "how does it relate to graph"], "labels": ["SMY", "DFT", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the graph in the time series or among the multiple time seri", "how do the authorsimplement the other graph-related approaches in this problem featuringtime seri", "how do the authorsimplement the other graph-related approaches in this problem featuringtime seri", "my impression is hence that the only possible outcome isreject", "the authors proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digit"], "labels": ["QSN", "SMY", "QSN", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method uses a learnable character embedding to transform the data but is an end-to-end approach", "the analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted featur", "here are my concerns as the price shows a high skewness in fig  it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model", "here are my concerns as the price shows a high skewness in fig  it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model", "that is making an error of $ for a plate that is priced $ has a huge difference in meaning to that for a plate priced as $"], "labels": ["SMY", "APC", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in sect", "if so the evaluation setting of dividing data into three *random* sets of training validation and test in  doesn't seem to be the right and most appropriate choic", "it should however be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the predict", "on the positive side the paper is well written and the problem is interest", "on the negative side there is very limited innovation in the techniques proposed that are indeed small variations of existing method"], "labels": ["APC", "CRT", "SUG", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the negative side there is very limited innovation in the techniques proposed that are indeed small variations of existing method", "this paper proposes a new character encoding scheme for use with character-convolutional language model", "this is a poor quality paper is unclear in the results what metric is even reported in table  and has little significance though this may highlight the opportunity to revisit the encoding scheme for charact", "the paper presents a novel representation of graphs as multi-channel image-like structur", "these structures are extrapolated  by  mapping the graph nodes into an embedding using an algorithm like nodevec compressing the embedding space using pca and extracting d slices from the compressed space and computing d histograms per slic"], "labels": ["CRT", "SMY", "CRT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these structures are extrapolated  by  mapping the graph nodes into an embedding using an algorithm like nodevec compressing the embedding space using pca and extracting d slices from the compressed space and computing d histograms per slic", "he resulting multi-channel image-like structures are then feed into vanilla d cnn", "he resulting multi-channel image-like structures are then feed into vanilla d cnn", "the papers is well written and clear and proposes an interesting idea of representing graphs as multi-channel image-like structur", "furthermore the authors perform experiments with real graph datasets from the social science domain and a comparison with the soa method both graph kernels and deep learning architectur"], "labels": ["DIS", "SMY", "DIS", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed algorithm in  out of  datasets two of theme with statistical signific", "this paper tackles the object counting problem in visual question answ", "it is based on the two-stage method that object proposals are generated from the first stage with attent", "it proposes many heuristics to use the object feature and attention weights to find the correct count", "in general it treats all object proposals as nodes on the graph"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with various agreement measures it removes or merges edges and count the final nod", "the method is evaluated on one synthetic toy dataset and one vqa v benchmark dataset", "the experimental results on counting are promis", "although counting is important in vqa the method is solving a very specific problem which cannot be generalized to other representation learning problem", "additionally this method is built on a series of heuristics without sound theoretically justification and these heuristics cannot be easily adapted to other machine learning appl"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i thus believe the overall contribution is not sufficient for iclr", "pros well written paper with clear presentation of the method", "useful for object counting problem", "experimental performance is convinc", "cons the application range of the method is very limit"], "labels": ["CRT", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the technique is built on a lot of heuristics without theoretical consider", "other comments and questions the determinantal point processes [] should be able to help with the correct counting the objects with proper construction of the similarity kernel", "it may also lead to simpler solut", "for example it can be used for deduplication using a eq  as the similarity matrix", "can the author provide analysis on scalability the proposed method"], "labels": ["DIS", "DIS", "DIS", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when the number of objects is very large the graph could be hug", "when the number of objects is very large the graph could be hug", "what are the memory requirements and computational complexity of the proposed method", "in the end of section  it mentioned that without normalization the method will not scale to an arbitrary number of object", "i think that it will only be a problem for extremely large numb"], "labels": ["SUG", "DIS", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wonder whether the proposed method scal", "could the authors provide more insights on why the structured attention etc did not significantly improve the result", "theoritically it solves the soft attention problem", "the definition of output confidence section  needs more motivation and theoretical justif", "[] kulesza alex and ben taskar determinantal point processes for machine learning foundations and trendsuae in machine learning u  -"], "labels": ["CRT", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing method", "in this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing method", "finally the authors propose a new method by using one unexplored combination of taxonomy featur", "the paper is well-written and easy to follow", "the proposed combination is straightforward"], "labels": ["SMY", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but lack of novelti", "from table  it seems that the only differences between the proposed method and depick is whether the method uses balanced assignment and pretrain", "i am not convinced that these changes will lead to a significant differ", "the performance of the proposed method and depick are also similar in t", "in addition the experiments section is not comprehensive enough as well the author only tested on two dataset"], "labels": ["CRT", "DFT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition the experiments section is not comprehensive enough as well the author only tested on two dataset", "more datasets should be tested for evalu", "more datasets should be tested for evalu", "in addition it seems that nearly all the experiments results from comparison methods are borrowed from the original publ", "the authors should finish the experiments on comparison methods and fill the entries in t"], "labels": ["DFT", "SUG", "DFT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary the proposed method is lack of novelty compare to existing method", "the survey part is nic", "however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenario", "therefore i think the paper cannot be accepted at this stag", "the paper proposes a way of generating adversarial examples that fool classification system"], "labels": ["CRT", "APC", "SUG", "FBK", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they formulate it for a blackbox and a semi-blackbox setting semi being needed for training their own network but not to generate new sampl", "they formulate it for a blackbox and a semi-blackbox setting semi being needed for training their own network but not to generate new sampl", "nthe model is a residual gan formulation where the generator generates an image mask m and input + m is the adversarial exampl", "nthe model is a residual gan formulation where the generator generates an image mask m and input + m is the adversarial exampl", "nthe paper is generally easy to understand and clear in their result"], "labels": ["DFT", "DIS", "DFT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not awfully familiar with the literature on adversarial examples to know if other gan variants exist", "i am not awfully familiar with the literature on adversarial examples to know if other gan variants exist", "from this paper's literature survey they dont exist", "so this paper is innovative in two parts- it applies gans to adversarial example gener", "n- the method is a simple feed-forward network so it is very fast to comput"], "labels": ["SMY", "FBK", "DFT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe experiments are pretty robust and they show that their method is better than the proposed baselin", "i am not sure if these are complete baselines or if the baselines need to cover other methods again not fully familiar with all literature her", "this paper proposed an end-to-end network that generates computer tokens from a single gui screenshot as input", "even though the introduced dataset in this paper is interesting there are some issues- on the model side this paper used the same architecture as the karpathy & fei-fei", "even though the introduced dataset in this paper is interesting there are some issues- on the model side this paper used the same architecture as the karpathy & fei-fei"], "labels": ["APC", "FBK", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a result in my view the paper has limited novelty and origin", "- experiments the experiments are not enough at all more specifically the paper didn't establish any baseline to show the difficulty of this problem and the dataset", "- experiments the experiments are not enough at all more specifically the paper didn't establish any baseline to show the difficulty of this problem and the dataset", "without a reasonable baseline it is hard to see how difficult is this dataset and this problem and can't say anything about the significance of this problem in this pap", "- the related works some recent papers in program synthesis are missing and should have been included in this paper such asrobustfill neural program learning under noisy i/o icml"], "labels": ["DFT", "SMY", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some other comments- in the last paragraph of section  and first paragraph section  the authors mentioned that cnn to perform unsupervised feature learn", "in my view it is not a correct statement to call the feature extraction from a cnn unsupervised feature learning as the referred cnns in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as wel", "in my view it is not a correct statement to call the feature extraction from a cnn unsupervised feature learning as the referred cnns in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as wel", "unfortunately at this point i do not see a sufficient contribution to warrant publication in iclr", "this paper introduces a neural network architecture for generating sketch draw"], "labels": ["SMY", "SMY", "APC", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts i agre", "the contribution of this paper of this paper is two-fold", "i firstly the paper introduces a large sketch dataset that future papers can rely on", "secondly the paper introduces the model for generating sketch draw", "ithe model is inspired by the variational autoencod"], "labels": ["SMY", "DIS", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i however the proposed method departs from the theory that justifies the variational autoencod", "i believe the following things would be interesting points to discuss / follow up", "- the paper preliminarily investigates the influence of the kl regularisation term on a validation data likelihood", "it seems to have a negative impact for the range of values that are discuss", "however i would expect there to be an optimum"], "labels": ["DIS", "SUG", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does the kl term help prevent overfitting at some stag", "answering this question may help understand what influence variational inference has on this model", "- the decoder model has randomness injected in it at every stage of the rnn", "because of this the latent state actually encodes a distribution over drawings rather than a single draw", "it seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high kl regularisation term"], "labels": ["QSN", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it help to rephrase the model to make the mapping from latent representation to drawing more determinist", "this definitely would bring it closer to the way the vae was originally introduc", "- the unconditional generative model *only* relies on the injected randomness for generating drawings as the initial state is initialised to", "this also is not in the spirit of the original vae where unconditional generation involves sampling from the prior over the latent spac", "i believe the design choices made by the authors to be valid in order to get things to work"], "labels": ["QSN", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well or whether it works bett", "but it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well or whether it works bett", "this would help interesting applications inform what is wrong with current theoretical view", "this would help interesting applications inform what is wrong with current theoretical view", "overall i would argue that this paper is a clear accept"], "labels": ["SUG", "DIS", "SUG", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems each feature map evaluates the fourier feature corresponding to the kernel at a set of randomly sampled quadrature point", "this gives an unbiased kernel estimator they prove a bound its variance and provide experiment evidence that for gaussian and arc-cos kernels their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression task", "the idea is straightforward", "the analysis seems correct", "and the experiments suggest the method has superior accuracy compared to prior rfms for shift-invariant kernel"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work is original but i would say incremental and the relevant literature is cit", "the method seems to give significantly lower kernel approximation error", "but the significance of the performance difference in downstream ml tasks is unclear", "--- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort", "since good performance on downstream tasks is the crucial feature that we want rfms to have it is not clear that this method represents a true improvement over the state-of-the-art"], "labels": ["APC", "APC", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the exposition of the quadrature method is difficult to follow and the connection between the quadrature rules and the random feature map is never explicitly stated eg equation  says how the kernel function is approximated as an integral but does not give the feature map that an ml practitioner should use to get that approximate integr", "it would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods which is more important in large-scale ml applications than the kernel approximation error", "it is not clear that the method is *not* more expensive in practice than previous methods table  gives superior asymptotic runtimes but i would like to see actual run times as evaluating the feature maps sound relatively complicated compared to other rfm", "on a related note i would also like to have seen this method applied to kernels where the probability density in the bochner integral was not the gaussian density eg the laplacian kernel the authors suggested that their method works there as well when one uses a gaussian approximation of the density which is not clear to m", "--- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-gaussian dens"], "labels": ["DFT", "SUG", "DFT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposed a novel regularizer that is to be applied to the rectifier discriminators in gan in order to encourage a better allocation of the model capacity of the discriminators over the potentially multi-modal generated / real data points which might in turn helps with learning a more faithful gener", "the paper is in general very well written with intuitions and technical details well explained and empirical studies carefully designed and execut", "some detailed comments / quest", "it seems the concept of binarized activation patterns which the proposed regularizer is designed upon is closely coupled with rectifier net", "i would therefore suggest the authors to highlight this assumption / constraint more clearly eg in the abstract"], "labels": ["SMY", "APC", "DIS", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in order for the paper to be more self-contained maybe list at least once the formula for rectifier net sth like a^t max wx + b + c", "this might also help the readers better understand where the polytopes in figure  come from", "in section  when presenting random variables u_  u_d i find the word bernourlli a bit misleading because typically people would expect u_i to take values from { } whereas here you assume {- +}", "this can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent deriv", "also k is already used to denote the mini-batch size so it's a slight abuse to reuse k to denote the kth margin"], "labels": ["SUG", "SUG", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  it may be clearer to explicitly point out the use of the -sigma rule for gaussian distributions her", "but i don't find it justified anywhere why leave % of i j pairs unpenalized is sth", "to be sought for her", "in section  when presenting corollary  of gavinsky & pudlak  n abruptly appears without proper introduction / context", "for the empirical study with d mog would an imbalanced mixture make it harder for the bre-regularized gan to escape from modal collaps"], "labels": ["SUG", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  is missing the sub-labels a b c d", "the paper proposes under the gan setting mapping real data points back to the latent space via the generator reversal procedure on a sample-by-sample basis hence without the need of a shared recognition network and then using this induced empirical distribution as the ideal prior targeting which yet another gan network might be trained to produce a better prior for the original gan", "i find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented as detailed below", "actually i find the entire notion of an ideal prior under the gan setting a bit strang", "to start with gan is already training the generator g to match the induced p_gx from pz with p_dx and hence by definition under the generator g there should be no better prior than pz itself because any change of pz would then induce a different p_gx and hence only move away from the learning target"], "labels": ["DFT", "SMY", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i get it that maybe under different pz the difficulty of learning a good generator g can be different and therefore one may wish to iterate between updating g under the current pz and updating pz under the current g and hopefully this process might converge to a better solut", "but i feel this sounds like a new angle and not the one that is adopted by the authors in this pap", "i think the discussions around eq  are not well ground", "just as you said right before presenting eq  typically the goal of learning a dgm is just to match q_x with the true data distrubution p_x it is **not** however to match qxz with pxz", "and btw don't you need to put e_z[  ] around the nd term on the rh"], "labels": ["SUG", "DIS", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i find the paper mingles notions from gan and vae sometimes and misrepresents some of the key differences between the two", "eg in the beginning of the nd paragraph in introduction the authors write generative models like gans vaes and others typically define a generative model via a deterministic generative mechanism or generator  while i think the use of a **deterministic** generator is probably one of the unique features of gan and that is certainly not the case with vae where typically people still need to specify an explicit probabilistic generative model", "and for this same reason i find the multiple references of a generative model px|z in this paper inaccurate and a bit mislead", "i'm not sure whether it makes good sense to apply an svd decomposition to the hat{z} vector", "it seems to me the variances u^_i shall be directly estimated from hat{z} as i"], "labels": ["CRT", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["otherwise the reference ideal distribution would be modeling a **rotated** version of the hat{z} samples which imo only introduces unnecessary discrep", "i don't quite agree with the asserted multi-modal structure in figur", "let's assume a d latent space where each quadrant represents one mnist digit eg  you may observe a similar structure in this latent space yet still learn a good generator under even a standard d gaussian prior i guess my point is a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it", "the generator reversal procedure needs to be carried out once for each data point separately and also when the generator has been updated which seems to be introducing a potentially significant bottleneck into the training process", "summarythe motivation for this work is to have an rl algorithm that can use imperfect demonstrations to accelerate learn"], "labels": ["CRT", "DIS", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes an actor-critic algorithm called normalized actor-critic nac based on the entropy-regularized formulation of rl which is defined by adding the entropy of the policy as an additional term in the reward funct", "entropy-regularized formulation leads to nice relationships between the value function and the policy and has been explored recently by many including [ziebart ] [schulman ] [nachum ] and [haarnoja ]", "the paper benefits from such a relationship and derives an actor-critic algorithm", "specifically the paper only parametrizes the q function and computes the policy gradient using the relation between the policy and q function appendix a", "through a set of experiments the paper shows the effectiveness of the method"], "labels": ["SMY", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["evaluationi think exploring and understanding entropy-regularized rl algorithm is import", "it is also important to be able to benefit from off-policy data", "i also find the empirical results encourag", "but i have some concerns about this paper- the derivations of the paper are unclear", "- the relation with other recent work in entropy-regularized rl should be expand"], "labels": ["DIS", "DIS", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the work is less about benefiting from demonstration data and more about using off-policy data", "- the algorithm that performs well is not the one that was actually deriv", "* unclear derivationsthe derivations of appendix a is unclear", "it makes it difficult to verify the deriv", "to begin with what is the loss function of which  and  are its gradi"], "labels": ["DIS", "CRT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to be more specific the choices of hat{q} in  and hat{v} in  are not clear", "for example just after  it is said that uchat{q} could be obtained through bootstrapping by r + gamma v_qud", "but if it is the case shouldnut we have a gradient of q in  too", "or show that it can be ignoredit appears that hat{q} and hat{v} are parameterized independently from q which is a function of theta", "later in the paper they are estimated using a target network but this is not specified in the deriv"], "labels": ["CRT", "CRT", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main problem boils down to the fact that the paper does not start from a loss function and compute all the gradients in a systematic way", "instead it starts from gradient terms each of which seems to be from different papers and then simplifies them", "for example the policy gradient in  which is further decomposed in appendix a as  and  and simplified appears to be eq  of [schulman et al ] https//arxivorg/abs/", "in that paper we have q_pi instead of hat{q} though", "i suggest that the authors start from a loss function and clearly derive all necessary step"], "labels": ["CRT", "CRT", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* unclear relation with other paperswhat part of the derivations of this work are novel", "currently the novelty is not obvi", "for example having the gradient of both q and v as in  has been stated by [haarnoja et al ] very similar formulation is developed in appendix b of https//arxivorg/abs/", "an algorithm that can work with off-policy data has also been developed by [nachum ] in the form of a bellman residual minimization algorithm as opposed to this work which essentially uses a fitted q-iteration algorithm as the crit", "i think the paper could do a better job differentiating from those other pap"], "labels": ["QSN", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the claim that this paper is about learning from demonstration is a bit question", "the paper essentially introduces a method to use off-policy data which is of course import", "but does not cover the important scenario where we only have access to stateaction pairs given by an expert", "here it appears from the description of algorithm  that the transitions in the demonstration data have the same semantic as the interaction data ie sarsu", "this makes it different from the work by [kim et al ] [piot et al ] and [chemali et al ] which do not require such a restriction on the demonstration data"], "labels": ["CRT", "APC", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the paper mentions that to formalize the method as a policy gradient one importance sampling should be used the paragraph after  but the performance of such a formulation is bad as depicted in figur", "as a result algorithm  does not use importance sampl", "this basically suggests that by ignoring the fact that the data is collected off-policy and treating it as an on-policy data the agent might perform bett", "this is an interesting phenomenon and deservers further study as currently doing the ucwrongud things is better than doing the ucrightud th", "i think a good paper should investigate this fact mor"], "labels": ["CRT", "CRT", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper presents results across a range of cooperative multi-agent tasks including a simple traffic simulation and starcraft micro-manag", "the architecture used is a fully centralized actor master which observes the central state in combination with agents that receive local observation ms-marl", "a gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each ag", "this contribution is added to the logits coming from each ag", "pros -the results on starcraft are encouraging and present state of the art performance if reproduc"], "labels": ["DIS", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons-the experimental evaluation is not very thorough", "no uncertainty of the mean is stated for any of the result", "evaluation runs is very low", "it is furthermore not clear whether training was carried out on multiple seeds or whether these are individual run", "-bicnet and commnet are both aiming to learn communication protocols which allow decentralized execut"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus they represent weak baselines for a fully centralized method such as ms-marl", "the only fully centralized baseline in the paper is gmezo however results stated are much lower than what is reported in the original paper eg % vs % for mv", "the paper is missing further centralized baselin", "-it is unclear to what extends the novelty of the paper specific architecture choices are requir", "for example the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings if at al"], "labels": ["CRT", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["detailed commentsfor all tasks the number of batch per training epoch is set to", "what does this mean", "figure  this figure is very helpful however the colour for m-s is wrong in the legend", "table gmezo win rates are low compared to the original publ", "what many independent seeds where used for train"], "labels": ["DIS", "QSN", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what are the confidence interv", "how many runs for evalu", "figure b what does it mean to feed two vectors into a tanh", "this figure currently very unclear", "what was the rational for choosing a vanilla rnn for the slave modul"], "labels": ["QSN", "QSN", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure a what was the rational for stopping training of commnet after  epoch", "the plot looks like commnet is still improv", "c this plot is disconcert", "training in this plot is very unst", "the final performance of the method 'ours' does not match what is stated in 'table '"], "labels": ["QSN", "DIS", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wonder if this is due to the very small batch size used a small batch size of", "it is well known that the original gan goodfellow et al suffers from instability and mode collaps", "it is well known that the original gan goodfellow et al suffers from instability and mode collaps", "indeed existing work has pointed out that the standard gan training process may not converge if we insist on obtaining pure strategies for the minmax gam", "indeed existing work has pointed out that the standard gan training process may not converge if we insist on obtaining pure strategies for the minmax gam"], "labels": ["DIS", "SMY", "DFT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the present paper proposes to obtain mixed strategy through an online learning approach", "online learning no regret algorithms have been used in finding an equilibrium for zero sum game however most theoretical convergence results are known for convex-concave loss", "one interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network and concave in m", "in particular the concave player plays the ftrl algorithm with standard l regularization term", "the regret of concave player can be bounded using existing result for ftrl"], "labels": ["SMY", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the regret for the other player is more interesting it uses the fact the adversary's strategy doesn't change too drast", "then a lemma by kalai and vempala can be us", "the theory part of the paper is reasonable and quite well written", "based on the theory developed the paper presents a practical algorithm", "compared to the standard gan training the new algorithm returns mixed strategy and examine several previous models instead of the latest in each iter"], "labels": ["SMY", "DIS", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper claims that this may help to prevent model collaps", "however the experimental part is less satisfi", "from figure  i don't see much advantage of checkhov gan", "in other experiments i don't see much improvement neither cifar and celeba", "the paper didn't really compare other popular gan models especially wgan and its improved vers"], "labels": ["SMY", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["which is already quite popular by now and should be compared with", "overall i think it is a borderline pap", "overall i think it is a borderline pap", "-------------------------i read the response and the new experimental results regarding wgan", "the experimental results make more sense now"], "labels": ["APC", "SUG", "FBK", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to see whether the idea can be applied to more recent gan models and still perform bett", "i raised my score to", "the authors proposed a novel rnn model where both the input and the state update of the recurrent cells are skipped adaptively for some time step", "the proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time step", "the experiments in the paper demonstrated skip rnns outperformed regular lstms and grus o thee addition pixel mnist and video action recognition task"], "labels": ["APC", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["strength- the experimental results on the simple skip rnns have shown a good improvement over the previous result", "weakness- although the paper shows that skip rnn worked well i found the appropriate baseline is lacking her", "comparable baselines i believe are regular lstm/gru whose inputs are randomly dropped out during train", "- most of the experiments in the main paper are on toy tasks with small lstm", "i thought the main selling point of the method is the computational gain"], "labels": ["APC", "APC", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it make more sense to show that on large rnns with thousands of hidden unit", "after going over the additional experiments in the appendix and i find the three results shown in the main paper seem cherry-picked and it will be good to include more nlp task", "after going over the additional experiments in the appendix and i find the three results shown in the main paper seem cherry-picked and it will be good to include more nlp task", "the paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regular", "the main idea is that in several datasets there are many instances of images for the same object/person and that taking this into account by learning a classifier that is invariant to the superficial changes or ucstyleud features eg hair color lighting rotation etc can improve the robustness and prediction accuraci"], "labels": ["QSN", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors show the benefit of this approach as opposed to the naive way of just using all images without any grouping in several toy experimental set", "although i really wanted to like the paper i have several concerns first and most importantly the paper is not citing several important related work", "especially i have the impression that the paper is focusing on a very similar setting causally to the one considered in  [gong et al ] http//proceedingsmlrpress/v/gonghtml as can be seen from fig  although not focusing on classification directly this paper also tries to a function tx such that py|tx is invariant to domain chang", "moreover in that paper the authors assume that even the distribution of the class can be changed in the different domains or interventions in this pap", "besides there are also other less related papers eg http//proceedingsmlrpress/v/zhangdpdf https//wwwaaaiorg/ocs/indexphp/aaai/aaai/paper/view// https//arxivorg/abs/ or potentially https//arxivorg/abs/ and https//arxivorg/abs/ that i think may be mentioned for a more complete pictur"], "labels": ["SMY", "CRT", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since there is some related work it may be also worth to compare with it or use the same dataset", "ium also not very happy with the term uccounterfactualud as the authors mention in footnote this is not the correct use of the term since counterfactual means ucagainst the factud", "for example a counterfactual query is ucwe gave the patient a drug and the patient died what would have happened if we didnut give the drug", "ud in this case these are just different interventions on possibly the same object", "ium not sure that in the practical applications one can assure that the noise variables stay the same which as the authors correctly mention would make it a bit closer to counterfactu"], "labels": ["SUG", "CRT", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it may sound pedantic but i donut understand why use the wrong and confusing terminology for no specific reason also because in practice the paper reduces to the simple idea of finding a classifier that doesnut vary too much in the different images of the single object", "**edit** i was satisfied with the clarifications from the authors and i appreciated the changes that they did with respect to the related work and terminology so i changed my evaluation from a  marginally below threshold to a  good paper accept", "summary the paper proposes a treetree architecture for nlp task", "both the encoder and decoder of this architecture make use of memory cells the encoder looks like a tree-lstm to encode a tree bottom-up the decoder generates a tree top-down by predicting the number of children first", "the objective function is a linear mixture of the cost of generating the tree structure and the target sent"], "labels": ["CRT", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees and outperforms an lstm seqseq on en-cn transl", "comment- the idea of treetree has been around recently but it is difficult to make it work", "i thus appreciate the authorsu effort", "however i wish the authors would have done it more properli", "- the computation of the encoder and decoder is not novel"], "labels": ["APC", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i was wondering how the encoder differs from tree-lstm", "the decoder predicts the number of children first but the authors donut explain why they do that nor compare this to existing tree gener", "- i donut understand the objective function eq  and", "both ls are not cross-entropy because label and childnum are not prob", "i also donut see why using adam is more convenient than using sgd"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i think eq  is incorrect because the decoder is not markovian", "to see this we can look at recurrent neural networks for language modeling generating the current word is conditioning on the whole history not only the previous word", "- i expect the authors would explain more about how difficult the tasks are eg some statistics about the datasets how to choose values for lambda what the contribution of the new objective i", "about writing- the paper has so many problems with wording eg articles plur", "- many terms are incorrect eg ucdependent parsing treeud should be ucdependency treeud ucconsistency parsingud should be ucconstituency parsingud"], "labels": ["CRT", "DIS", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in  socher et al do not use lstm", "- i suggest the authors to do some more literature review on tree gener", "this paper proposes a method for multitask and few-shot learning by completing a performance matrix which measures how well the classifier for task i performs on task j", "the matrix completion approach is based on robust pca", "when used for multitask learning mtl with n tasks the method has to first train one classifier for each task and so train a total of n classifiers and then evaluate the performance of each classifier on each and every task and so involves n^ testing round"], "labels": ["DIS", "SUG", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this can be computationally demand", "the key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster and if task classifier i does not perform well on task j then tasks i and j belong to different cluster the proposed algorithm then uses these performance values to perform task clust", "however in mtl we usually assume that there are not enough samples to learn each task and so this performance matrix may not be reli", "there have been a number of mtl methods based on task clust", "for example[] a convex formulation for learning task relationships in multi-task learning uai"], "labels": ["DIS", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] a dirty model for multi-task learning nip", "[] clustered multi-task learning a convex formulation nip", "[] convex multitask learning with flexible task clusters icml", "[] integrating low-rank and group-sparse structures for robust multi-task learning kdd[] learning incoherent sparse and low-rank patterns from multiple tasks kddin particular [] assumes that the combined weight matrix for all the tasks follows the robust pca model", "this is thus very similar to the proposed method which assumes that the performance matrix follows the robust pca model"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however a disadvantage of the proposed method is that it is a two-step approach first perform task clustering then re-learn the cluster weights while [] is not", "for few-shot learning the authors mentioned that the alpha's are adaptable parameters but did not mention how they are adapt", "experimental results are not convinc", "- comparison with existing clustered mtl methods mentioned above are miss", "- as mentioned above the proposed method can be computationally expensive when used for mtl but no timing results are report"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- as the authors mentioned in section  most of the tasks have a significant amount of training data and single-task baselines achieve good results and so this is not a good benchmark dataset for mtl", "this paper extends the framework of neural networks for finite-dimension to the case of infinite-dimension setting called deep function machin", "this theory seems to be interesting and might have further potential in appl", "this paper introduces bi-directional block self-attention model bi-biosan as a general-purpose encoder for sequence modeling tasks in nlp", "the experiments include tasks like natural language inference reading comprehension squad semantic relatedness and sentence classif"], "labels": ["CRT", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the new model shows decent performance when comparing with bi-lstm cnn and other baselines while running at a reasonably fast spe", "the advantage of this model is that we can use little memory as in rnns and enjoy the parallelizable computation as in sans and achieve similar or better perform", "the advantage of this model is that we can use little memory as in rnns and enjoy the parallelizable computation as in sans and achieve similar or better perform", "while i do appreciate the solid experiment section i don't think the model itself is sufficient contribution for a publication at iclr", "first there is not much innovation in the model architectur"], "labels": ["APC", "APC", "DIS", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of the bi-biosan model simply to split the sentence into blocks and compute self-attention for each of them and then using the same mechanisms as a pooling operation followed by a fusion level", "i think this more counts as careful engineering of the san model rather than a main innov", "second the model introduces much more parameters in the experiments it can easily use  times parameters than the commonly used encod", "second the model introduces much more parameters in the experiments it can easily use  times parameters than the commonly used encod", "what if we use the same amount of parameters for bi-lstm encoders will the gap between the new model and the commonly used ones be smal"], "labels": ["DIS", "DIS", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["====i appreciate the answers the authors added and i change the score to", "====i appreciate the answers the authors added and i change the score to", "the paper proposes a method to train deep multi-task networks using gradient norm", "the key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the train", "the authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset"], "labels": ["APC", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is an interesting paper with a novel approach to multi-task learn", "to improve the paper it would be helpful to evaluate the method under various set", "my detailed comments are below multi-task learning can have various settings for example we may have multiple groups of tasks where tasks are correlated within groups but tasks in different groups are not much correl", "also tasks may have hierarchical correlation structur", "these patterns often appear in biological dataset"], "labels": ["APC", "DIS", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am wondering how a variety of multi-task settings can be handled by the proposed approach", "it would be helpful to discuss the conditions where we can benefit from the proposed method", "one intuitive approach to task balancing would be to weight each task objective based on the variance of each task", "it would be helpful to add a few simple and intuitive baselines in the experi", "in section  it would be great to have more in-depth simulations eg multi-task learning in various set"], "labels": ["DFT", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also in the bottom right panel in figure  grandnorm and equal weighting decrease test errors effectively even after  steps but uncertainty weighting seems to reach a plateau discussions on this would be us", "it would be useful to discuss the implementation of the method as wel", "this authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural network", "the authors claimed to obtain efficiency improvement and better numerical st", "this is a short paper that contains five pag"], "labels": ["DIS", "SUG", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matric", "in terms of the overall time complexity the improvement seems quite limited considering that the normalization is not the bottleneck operations in the train", "in addition it is not very clear how the proposed approach benefits the mini-batch training of the network", "in terms of numerical stability though experimental results were reported there is no theoretical analysi", "in terms of numerical stability though experimental results were reported there is no theoretical analysi"], "labels": ["SMY", "DFT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are quite limit", "the experiments are quite limit", "the paper proposes to use a pretrained model-free rl agent to extract the developed state representation and further re-use it for learning forward model of the environment and plan", "the idea of re-using a pretrained agent has both pros and con", "on one hand it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environ"], "labels": ["DFT", "CRT", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the other hand the usefulness of the learned representation for planning is unclear", "a model-free agent can especially if trained with certain regularization exclude a lot of information which is potentially useful for planning but is it necessary for reactively taking act", "a reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned represent", "uin addition to that one could fine-tune the representation during forward model train", "it would be interesting to see if this can improve the result"], "labels": ["CRT", "DIS", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i personally miss a more technical and detailed exposition of the idea", "for example it is not described anywhere what loss is used for learning the model", "mcts is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contain", "again due to lack of equations i donut completely understand the last paragraph of  i suggest re-writing it as well as some other parts in a more explicit way", "again due to lack of equations i donut completely understand the last paragraph of  i suggest re-writing it as well as some other parts in a more explicit way"], "labels": ["DFT", "CRT", "SMY", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also could find the details on how figure  was produc", "as i understand mcts was not used in this experi", "if so how would one play with just a forward model", "it is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applic", "is mini-rts a deterministic environ"], "labels": ["QSN", "DIS", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it be possible to include a non-deterministic baseline in the experimental comparison", "experimentally the results are rather weak compared to pure model-free ag", "somewhat unsatisfying longer-term prediction results into weaker game play", "doesnut this support the argument about need in stochastic predict", "to me the paper in itus current form is not written well and does not contain strong enough empirical results so that i canut recommend accept"], "labels": ["QSN", "CRT", "CRT", "QSN", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor comments* matcha and predictpi models are not introduced under such nam", "* figure  that introduces them contains typo", "* formatting of figure  needs to be fix", "this figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening ther", "this paper introduces a comparison between several approaches for evaluating gan"], "labels": ["CRT", "CRT", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compar", "they compare the evaluation methods based on five criteria termed disciminability mode collapsing and mode dropping sample efficiencycomputation efficiency and robustness to transform", "this paper has some interesting insights and a few ideas of how to validate an evaluation method", "the topic is an important one and a very difficult on", "the topic is an important one and a very difficult on"], "labels": ["SMY", "SMY", "APC", "SUG", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the work has some problems in rigor and justification and the conclusions are overstated in my view", "pros-several interesting ideas for evaluating evaluation metrics are propos", "-the authors tackle a very challenging subject", "cons-it is not clear why gans are the only generative model consid", "cons-it is not clear why gans are the only generative model consid"], "labels": ["DFT", "APC", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-unprecedented visual quality as compared to other generative models has brought the gan to prominence and yet this is not really a big factor in this pap", "-the evaluations rely on using a pre-trained imagenet model as a represent", "the authors point out that different architectures yield similar results for their analysis however it is not clear how the biases of the learned representations affect the result", "the use of learned representations needs more rigorous justif", "-the evaluation for discriminative metric increased score when mix of real and unreal increases is interesting but it is not convincing as the sole evaluation for ucdiscriminativenessud and seems like something that can be gam"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors implicitly contradict the argument of theis et al against monolithic evaluation metrics for generative models but this is not strongly support", "several references i suggesthttps//arxivorg/abs/ fid scorehttps//arxivorg/abs/ mmd as evalu", "qualitythe theoretical results presented in the paper appear to be correct", "however the experimental evaluation is globally limited  hyperparameter tuning on test which is not fair", "however the experimental evaluation is globally limited  hyperparameter tuning on test which is not fair"], "labels": ["SMY", "SUG", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["claritythe paper is mostly clear", "even though some parts deserve more discussion/clarification algorithm experimental evalu", "originalitythe theoretical results are original and the sgd approach is a priori original as wel", "significancethe relaxed dual formulation and ot/monge maps convergence results are interesting and can of of interest for researchers in the area", "the other aspects of the paper are limit"], "labels": ["APC", "DFT", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros-theoretical results on the convergence of ot/monge maps-regularized formulation compatible with sgd", "cons-experimental evaluation limit", "cons-experimental evaluation limit", "-the large scale aspect lacks of thorough analysi", "-the large scale aspect lacks of thorough analysi"], "labels": ["APC", "DFT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-the paper presents  contributions but at then end of the day the development of each of them appears limit", "-the paper presents  contributions but at then end of the day the development of each of them appears limit", "comments-the weak convergence results are interest", "however the fact that no convergence rate is given makes the result weak", "however the fact that no convergence rate is given makes the result weak"], "labels": ["DFT", "CRT", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular it is possible that the number of examples needed for achieving a given approximation is at least exponenti", "this can be coherent with the problem of domain adaptation that can be np-hard even under the co-variate shift assumption ben-david&urner alt", "this can be coherent with the problem of domain adaptation that can be np-hard even under the co-variate shift assumption ben-david&urner alt", "then i think that the claim of page  saying that domain adaptation can be performed nearly optimally has then to be rephras", "i think that results show that the approach is theoretically justified but optimality is not here yet"], "labels": ["DIS", "DFT", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theorem  is only valid for entropy-based regularizations what is the difficulty for having a similar result with l regular", "-the experimental evaluation on the running time is limited to one particular problem", "if this subject is important it would have been interesting to compare the approaches on other large scale problems and possibly with other implement", "it is also surprising that the efficiency the l-regularized version is not evalu", "for a paper interesting in large scale aspects the experimental evaluation is rather weak"], "labels": ["QSN", "DFT", "DIS", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the  methods compared in fig  reach the same objective values at convergence but is there any particular difference in the solutions found", "-algorithm  is presented without any discussion about complexity rate of converg", "could the authors discuss this aspect", "the presentation of this algo is a bit short and could deserve more space in the supplementari", "-for the da application the considered datasets are classic but not really large scale anyway this is a minor remark"], "labels": ["QSN", "CRT", "QSN", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the setup is not completely clear", "since the approach is interesting for out of sample data", "so i would expect the map to be computed on a small sample of source data and then all source instances to be projected on target with the learned map", "this point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point", "this point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point"], "labels": ["CRT", "APC", "DIS", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while this is an interesting aspect of the paper this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classif", "hyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup it seems that the parameters are tuned on test for all methods which is not fair since target label information will not be available from a practical standpoint", "the authors claim that they did not want to compete with state of the art da but the approach of perrot et al  seems to a have a similar objective and could be used as a baselin", "experiments on generative optimal transport are interesting and probably generate more discussion/perspect", "--after rebuttal--authors have answered to many of my comments i think this is an interesting paper i increase my scor"], "labels": ["APC", "CRT", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper mainly focuses on the square loss function of linear network", "it provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear network", "based on this characterization the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear network", "based on this characterization the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear network", "as an extension the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep relu networks although only a subset of non-global-optimal critical points are discuss"], "labels": ["SMY", "APC", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in general this manuscript is well written", "pros this manuscript provides the sufficient and necessary characterization of critical points for deep network", "compared to previous work the current analysis for one-hidden-layer linear networks doesnut require assumptions on parameter dimensions and data matric", "the novel analyses especially the technique to characterize critical points and the proof of item  in proposition  will probably be interesting to the commun", "it provides an example when a local minimum is not global for a one-hidden-layer neural network with relu activ"], "labels": ["APC", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons i'm concerned that the contribution of this manuscript is a little increment", "the equivalence of global minima and local minima for linear networks is not surprising based on existing works eg hardt & ma  and kawaguchi", "unlike one-hidden-layer linear networks the characterizations of critical points for deep linear networks and deep relu networks seem to be hard to be interpret", "this manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum which actually has been shown by existing works like kawaguchi  with some assumpt", "the behaviors of linear networks and practical deep and nonlinear networks are very differ"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["under such circumstance the results about one-hidden-layer linear networks are less interesting to the deep learning commun", "minorsthere are some mixed-up notations tilde{a_i} = a_i  and ranka_ = ranka_ in proposit", "summarythe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural network", "in particular- in the case of deep linear networks they characterize whether a critical point is a global optimum or a saddle point by a simple criterion", "this improves over recent work by kawaguchi who showed that each critical point is either a global minimum or a saddle point ie none is a local minimum by relaxing some hypotheses and adding a simple criterion to know in which case we ar"], "labels": ["CRT", "CRT", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in the case of nonlinear network they provide a sufficient condition for a solution to be a global optimum using a function space approach", "qualitythe quality is very good", "the paper is technically correct and nontrivi", "all proofs are provided and easy to follow", "claritythe paper is very clear"], "labels": ["DIS", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["related work is clearly cited and the novelty of the paper well explain", "the technical proofs of the paper are in appendices making the main text very smooth", "originalitythe originality is weak", "it extends a series of recent papers correctly cit", "there is some originality in the proof which differs from recent related pap"], "labels": ["APC", "APC", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["significancethe result is not completely surpris", "but it is significant given the lack of theory and understanding of deep learn", "although the model is not really relevant for deep networks used in practice the main result closes a question about characterization of critical points in simplified models if neural network which is certainly interesting for many peopl", "the principal problem that the paper addresses is how to integrate error-backpropagation learning in a network of spiking neurons that use a form of sigma-delta cod", "the main observation is that static sigma-delta coding as proposed in oconnor and welling b is not correct when the weights change during training as past activations are taken into account with the old rather than the new weight"], "labels": ["CRT", "DIS", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the solution proposed in this work is to have past activations decay exponentially to reduce this problem", "the coding scheme then mimics the proporitional-integral-derivative idea from control-theori", "the result spikes having an exponentially decaying effect on the postsynaptic neuron is similar to that observed in biological spiking neuron", "the authors show how spike-based learning can be implemented with spiking neurons using such coding and demonstrate the results on an mlp with one hidden layer applied to the temporal mnist dataset and to the youtube-bb dataset", "this approach is original and signific"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though the presented results are a bit on the thin sid", "as presented the spiking networks are not exactly deep i am puzzled by the statement that in the youtube-bb dataset only the top  layers are spik", "the network for the mnist dataset is similarly only  layers deep input hidden output", "is there a particular reason for thi", "the presentation right now suggests that the scheme does in practise not work for deep network"], "labels": ["CRT", "CRT", "SMY", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with regard to the learning rule while the rule is formulated in terms of spikes it should be noted that for neuron with many inputs and outputs this update will have to be computed very very often even for networks with low average firing r", "the paper is clear in most points with some parts that could use further elucid", "in particular in sec  the feedback pass for weight updating is comput", "it is unclear from the text that this is an ongoing process in parallel to the feedforward pass", "in sec  e_t is termed the postsynaptic pre-nonlinearity activation which is confusing as the computation is going the other way post-to-pr"], "labels": ["DIS", "APC", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these two sections would benefit from a more careful layout of the process what is going on in a forward pass a backward pass how does this interact", "section  tries to relate the spike-based learning rule to the biologically observed stdp phenomenon", "while the formulation in terms of pre-post spike-times is interesting the result is clearly different from stdp and ignores the fact that e_t refers to the backpropagating error which presumably would be conveyed by a feedback network applying the plotted pre-post spike-time rule in the same setting as where stdp is observed will not achieve error-backpropag", "the shorthand notation in the paper is hard to follow in the first place btw perhaps this could be elaborated/remedied in an appendix there is also some rather colloquial writing in placesobscene wast of energy abstract there's aren't  p", "the correspondence of spiking neurons to sigma-delta modulation is incorrectly attributed to zambrano and bohte  but is rather presented in yoon / check original date of publication!"], "labels": ["DIS", "DIS", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes a method for few-shot learning using a new image representation called visual concept embed", "visual concepts were introduced in wang et al  which are clustering centers of feature vectors in a lattice of a cnn", "for a given image its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concept", "using the visual concept embedding two simple methods are used for few-shot learning a nearest neighbor method and a probabilistic model with bernoulli distribut", "experiments are conducted on the mini-imagenet dataset and the pascald+ dataset for few-shot learn"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["positives- the three properties of visual concepts described in the paper are interest", "negatives- the novelty of the paper is limit", "the idea of visual concept has been proposed in wang et ", "using a embedding representation based on visual concepts is straightforward", "the two baseline methods for few-shot learning provide limited insights in solving the few-shot learning problem"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the paper uses a hard thresholding  in the visual concept embed", "it would be interesting to see the performance of other strategies in computing the embedding such as directly using the distances without threshold", "this paper formulates gan as a lagrangian of a primal convex constrained optimization problem", "they then suggest to modify the updates used in the standard gan training to be similar to the primal-dual updates typically used by primal-dual subgradient method", "technically the paper is sound"], "labels": ["CRT", "SUG", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it mostly leverages the existing literature on primal-dual subgradient methods to modify the gan training procedur", "i think this is a nice contribution that does yield to some interesting insight", "however i do have some concerns about the way the paper is currently written and i find some claims mislead", "prior convergence proofs i think the way the paper is currently written is mislead", "the authors quote the paper from ian goodfellow ucfor gans there is no theoretical prediction as towhether simultaneous gradient descent should converge or not"], "labels": ["SMY", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud however the f-gan paper gave a proof of convergence see theorem  here https//arxivorg/pdf/pdf", "a recent nips paper by nagarajan and kolter  also study the convergence properties of simultaneous gradient desc", "another problem is of course the assumptions required for the proof that typically donut hold in practice see comment below", "convex-concave assumption in practice the gan objective is optimized over the parameters of the neural network rather than the generative distribut", "this typically yields a non-convex non-concave optimization problem"], "labels": ["DIS", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be mentioned in the paper and i would like to see a discussion concerning the gap between the theory and the practical algorithm", "relation to existing regularization techniques combining equations  and  the second terms acts as a regularizer that minimizes [lapha f_dx_i]^", "this looks rather similar to some of the recent regularization techniques such asimproved training of wasserstein gans https//arxivorg/pdf/pdf", "stabilizing training of generative adversarial networks through regularization https//arxivorg/pdf/pdfcan the authors comment on thi", "i think this would also shed some light as to why this approach alleviates the problem of mode collaps"], "labels": ["SMY", "SMY", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["curse of dimensionality nonparametric density estimators such as the kde technique used in this paper suffer from the well-known curse of dimension", "for the synthetic data the empirical evidence seem to indicate that the technique proposed by the authors does work", "but ium not sure the empirical evidence provided for the mnist and cifar- datasets is sufficient to judge whether or not the method does help with mode collaps", "the inception score fails to capture this properti", "could the authors explore other quantitative measur"], "labels": ["QSN", "APC", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["have you considered trying your approach on the augmented version of the mnist dataset used in metz et al  and che et ", "experimentstypo should say ucthe data distribution is p_dx = {x=}ud", "* paper summarythis paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks dnn", "it makes use of the cp or rank- tensor decomposition to define the meaning of rank for a tensor", "when a tensor is decomposed as a sum of rank- tensors outer products the number of operations in a dnn forward pass decreases leading to a faster testing runtim"], "labels": ["QSN", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this form of network compression has been worked on befor", "the contribution of this paper seems to be the specific way the decomposition is used in training the dnn", "it seems that this training process follows a projected gradient descent procedure where the filter weights of the network are iteratively updated using regular stochastic gradient descent and then they are projected onto the set of rank-r tensor", "the authors devise a heuristic way based on an innovated measure that combines computational complexity with performance to select the tensor rank to be us", "experiments are conducted on the task of image classification for a couple well-known dnn architectures vgg and resnet to show a speedup of runtime in testing significant compression of the network and minimal degradation in perform"], "labels": ["DIS", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* related workthe authors do a good job describing and listing the papers most related to the current submiss", "* technical noveltyone main limitation of the paper is the lack of technical contribut", "the idea of using rank- tensor decomposition for training low-rank filtering operations in dnns has already been proposed and used in several other work", "from what i understood from the paper the only technical contribution is the use of a so called -pass decomposition which is simply an implementation of projected gradient descent on the set of rank-r tensor", "in particular if we seek to minimize fw such that w belongs to asset that can be easily projected on then projected gradient descent would apply traditional gradient descent on the current iterate followed by a projection step onto this set"], "labels": ["APC", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f", "this iterative projection tends to perform better than iteratively optimizing fw and then applying the projection step only once at the very end of the optimization assumedly the cp-als method that is used for comparison", "put in this light the proposed paper does not contribute much", "* paper presentationin the reviewerus opinion the primary limitation of the paper is how it is written and organ", "the paper is badly written"], "labels": ["DIS", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is riddled with grammar choice of word and spelling mistak", "the paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work", "these issues make the paper hard to read", "for example the authors spend quite a bit of space focusing on the rank- cp decomposition which is well known as opposed to focusing on the merits of their technical contribut", "also the experiments are not clearly explain"], "labels": ["CRT", "SUG", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is hard to understand the experimental setup of each experiment and what the conclusions ar", "for example it is unclear whether the baseline in table  also uses the two-pass decomposition or not", "also the authors should provide a clear and standard description of the experimental setup for each experiment eg which network which dataset which task/loss which measure etc", "* experimental resultsfrom what i understood from the experiments it seems that using the uctwo-pass decompositionud ie projected gradient descent is better than cp-als gradient descent ended with a single projection step", "this conclusion seems to be intuitive and expect"], "labels": ["CRT", "CRT", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however as mentioned earlier the paper writing and organization makes it hard to understand what exactly is being shown", "for example table  shows that the baseline method uses less filters than the proposed method that selects the number of filters through an innovated heuristic measur", "then in table  we see that the baseline is less stable ie its performance decreases across the different iterations of projected gradient desc", "isnut this expected since the baseline uses less filt", "it is unclear from the text if this is the cas"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should do a better job explaining and comparing the overall experimental result", "for example it seems that the proposed projected gradient descent method leads to better speedup results in vgg as opposed to resnet with very similar reduction in accuraci", "the authors do not comment on thi", "it would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases eg cases when the low-rank decomposition leads to a significant decrease in perform", "all of this analysis provided more insight into the method and helps the reader understand its ext"], "labels": ["SUG", "APC", "DFT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithm", "since introduced model has to split data into subsets it leads to non-differentiable paths in the graph which authors propose to tackle with rl and policy gradi", "the whole model can be seen as an rl agent trained to do splitting action on a set of instances in such a way that jointly trained predictor t quality is maximised and thus its current log prob log py|px becomes a reward for an rl ag", "authors claim that model like this strengthened with pointer networks/graph nets etc depending on the application leads to empirical improvement on three tasks - convex hull finding k-means clustering and on tsp", "however while results on convex hull task are good"], "labels": ["SMY", "SMY", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["k-means ones use a single artificial problem and do not test dcn but rather a part of it and on tsp dcn performs significantly worse than baselines in-distribution and is better when tested on bigger problems than it is trained on", "however the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success stori", "i will be happy to revisit the rating if the experimental section is enrich", "pros- very easy to follow idea and model", "- simple merge or rl and sl in an end-to-end trainable model- improvements over previous solut"], "labels": ["CRT", "CRT", "FBK", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- k-means experiments should not be run on artificial dataset there are plenty of benchmarking datasets out ther", "in current form it is just a proof of concept experiment rather than evaluation + if is only for splitting not for the entire architecture propos", "it would be also beneficial to see the score normalised by the cost found by k-means itself say using lloyd's method as otherwise numbers are impossible to interpret", "with normalisation claiming that it finds % worse solution than k-means is indeed meaning", "- tsp experiments show that in distribution dcn perform worse than baselines and when generalising to bigger problems they fail more gracefully however the accuracies on higher problem are pretty bad thus it is not clear if they are significant enough to claim success"], "labels": ["CRT", "CRT", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["maybe tsp is not the best application of this kind of approach as authors state in the paper - it is not clear how merging would be applied in the first plac", "- in general - experimental section should be extended as currently the only convincing success story lies in convex hull experi", "side notes- dcn is already quite commonly used abbreviation for deep classifier network as well as dynamic capacity network thus might be a good idea to find different nam", "- please fix cite calls to citep when authors name is not used as part of the sentence for examplegraph neural network nowak et al  should begraph neural network nowak et ", "# after the updateevaluation section has been updated threefold- tsp experiments are now in the appendix rather than main part of the pap"], "labels": ["CRT", "APC", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- k-means experiments are lloyd-score normalised and involve one cifar clust", "- knapsack problem has been ad", "paper significantly benefited from these changes however experimental section is still based purely on toy datasets clustering cifar patches is the least toy problem but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that and in both cases simple problem-specific baseline lloyd for k-means greedy knapsack solver beats proposed method", "i can see the benefit of trainable approach her", "the fact that one could in principle move towards other objectives where deriving lloyd alternative might be hard however current version of the paper still does not show that"], "labels": ["DIS", "DIS", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i increased rating for the pap", "however in order to put the clear accept mark i would expect to see at least one problem where proposed method beats all basic baselines thus it has to either be the problem where we do not have simple algorithms for it and then beating ml baseline is fine or a problem where one can beat the typical heuristic approach", "i am not sure how to interpret this pap", "the paper seems to be very thin technically unless i missed some important detail", "two proposals in the paper are using a learning rate decay scheme that is fixed relative to the number of epochs used in train"], "labels": ["FBK", "FBK", "DIS", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and  extract the penultimate layer output as features to train a conventional classifier such as svm", "i don't understand why  differs from other approaches in the sense that one cannot simply reduce the number of epochs without hurting perform", "and for  it is a relatively standard approach in utilizing cnn featur", "essentially if i understand correctly this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier which is not that away from the softmax classifier that cnns usually us", "i fail to see how this would lead to superior performance compared to conventional cnn"], "labels": ["SMY", "DIS", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["let me first note that i am not very familiar with the literature on program generation molecule design or compiler theory which this paper draws heavily from so my review is an educated guess", "this paper proposes to include additional constraints into a vae which generates discrete sequences namely constraints enforcing both semantic and syntactic valid", "this is an extension to the grammar vae of kusner et al which includes syntactic constraints but not semantic on", "this is an extension to the grammar vae of kusner et al which includes syntactic constraints but not semantic on", "these semantic constraints are formalized in the form of an attribute grammar which is provided in addition to the context-free grammar"], "labels": ["DIS", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors evaluate their methods on two tasks program generation and molecule gener", "their method makes use of additional prior knowledge of semantics which seems task-specific and limits the generality of their model", "they report that their method outperforms the character vae cvae and grammar vae gvae of kusner et ", "however it isn't clear whether the comparison is appropriate the authors report in the appendix that they use the kekulised version of the zinc dataset of kusner et al whereas kusner et al do not make any mention of thi", "the baselines they compare against for cvae and gvae in table  are taken directly from kusner et al though"], "labels": ["SMY", "CRT", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can the authors clarify whether the different methods they compare in table  are all run on the same dataset format", "typos- page  while in sampling procedure - while in the sampling procedur", "- page  a deep convolution neural networks - a deep convolutional neural network", "- page  kl-divergence that proposed in - kl-divergence that was proposed in", "- page  since in training time - since at training tim"], "labels": ["QSN", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- page  can effectively computed - can effectively be comput", "- page  reset for training - rest for train", "the paper is a pain to read", "most of the citation styles are off ie without parenthes", "most of the sentences are not grammatically correct"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["most if not all of the determiners are miss", "it is ironic that the paper is proposing a model to generate grammatically correct sentences while most of the sentences in the paper are not grammatically correct", "the experimental numbers look skept", "for example / of the training results are worse than the test results in t", "it also happens a few times in t"], "labels": ["DFT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["either the models are not properly trained or the models are heavily tuned on the test set", "the running times in table  are also skept", "why are the concorde models faster than unigrams and bigram", "maybe this can be attributed to the difference in the size of the vocabulary but why is the unigram model slower than the bigram model", "this paper combines a simple pac-bayes argument with a simple perturbation analysis lemma  to get a margin based generalization error bound for relu neural networks theorem  which depends on the product of the spectral norms of the layer parameters as well as their frobenius norm"], "labels": ["CRT", "CRT", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main contribution of the paper is the simple proof technique to derive theorem  much simpler than the one use in the very interesting work [bartlett et al ] appearing at nips  which got an analogous bound but with a dependence on the l-norm of the layers instead of the frobenius norm", "the authors make a useful comparison between these bounds in section  showing that none is dominating the others but still analyzing their properties in terms of structural properties of the weight matric", "i enjoyed reading this pap", "one could think that it makes a somewhat incremental contribution with respect to the more complete work both theory and practice from [bartlett et al ]", "nevertheless the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of nn"], "labels": ["APC", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written", "though i make some suggestions for the camera ready version below to improve clar", "i verified most of the math", "== detailed suggestions == the authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with relu activation functions* so that the current context of the result is more transpar", "it is quite unclear how one could generalize the theorem  to arbitrary activation functions phi given the crucial use of the homogeneity of the relu at the beginning of p"], "labels": ["APC", "SUG", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though the proof of lemma  only appears to be using the -lipschitzness property of phi as well as phi = unless they can generalize further i also suggest that they explicitly state in the interesting lemma  that it is for the relu activations like they did in theorem", "a footnote or citation could be useful to give a hint on how the inequality /e beta^d- = tilde{beta}^d- = e beta^d- is proven from the property |beta-tilde{beta}|= /d beta middle of p", "equation  -- put the missing  subscript for the l norm of |f_w+ux - f_wx|_ on the lhs for clar", "one extra line of derivation would be helpful for the reader to rederive the bound|w|^/sigma^  = o just above equ", "ie first doing the expansion keeping the beta terms and frobenius norm sum and then going directly to the current o term"], "labels": ["SUG", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["bottom of p use hat{l}_gamma =  instead of l_gamma = for more clar", "top of p the sentence since we need tilde{beta} to satisfy  is currently awkwardly st", "i suggest instead to say that |tilde{beta}- beta| = /d gamma/b^/d is a sufficient condition to have the needed condition |tilde{beta}-beta| = /d beta over this range thus we can use a cover of size dm^/d", "typo below  citetbarlett last paragraph p recalling that w_i is *at most* a hxh matrix as your result do not require constant size layers and covers the rectangular cas", "the original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further train"], "labels": ["SUG", "SUG", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper extends that work by- training from reinforcement signals only rather than near-expert trajectories- making the transition model more state-depdendent- scaling to larger problem domains by propagating reward values for navigational goals in a special way", "the paper is fairly clear and these extensions are reason", "however i just don't think the focus on d grid-based navigation has sufficient interest and impact", "it's true that the original vin paper worked in a grid-navigation domain but they also had a domain with a fairly different structure  i believe they used the gridworld because it was a convenient initial test case but not because of its inherent valu", "so making improvements to help solve grid-worlds better is not so motiv"], "labels": ["SMY", "SMY", "DFT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it may be possible to motivate and demonstrate the methods of this paper in other domains howev", "the work on dynamic environments was an interesting step  it would have been interesting to see how the models learned for the dynamic environments differed from those for static environ", "this paper introduces a parameter server architecture to improve distributed training of cnns in the presence of straggl", "specifically the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the paramet", "this technique is combined with existing methods such as partial pushing pan et al  for a partial synchronous sgd method"], "labels": ["DIS", "APC", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is evaluated with resnet - using synthetic delay", "comments for the authorthe paper is well-written and easy to follow", "comments for the authorthe paper is well-written and easy to follow", "the problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large block", "n the partial pushing method pan et al  section  shows a clear evidence for the problem using a real workload with a large number of work"], "labels": ["DIS", "APC", "FBK", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately in your figure  this is not as obvious and not real since it is using simulated delay", "more specifically it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them  did you modify your code to support block-wise sending of gradients some description of how the framework was modified will be help", "the idea is to send partial parameter blocks and when 'b' blocks are received compute the gradi", "i feel that with such a design you may actually end up hurting the performance by sending a large number of small packets in the no failure cas", "for real large data centers this may cause a packet storm and subsequent throughput collapse eg the incast problem"], "labels": ["DFT", "QSN", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["you need to show the evidence that you do not hurt the failure-free case for a large number of work", "you need to show the evidence that you do not hurt the failure-free case for a large number of work", "the evaluation is on fairly small workloads cifar-", "again evaluating over imagenet and demonstrating a clear speedup over existing sync methods will be help", "furthermore a clear description of your ucpullud configuration such as in figure  i"], "labels": ["DIS", "FBK", "APC", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how many actual bytes or blocks are sent and what is the threshold will be helpful beyond a vague %", "another concern with partial synchronization methods that i have is that how do you pick these configurations pull  etc", "these appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baselin", "these appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baselin", "these appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baselin"], "labels": ["QSN", "QSN", "SUG", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown", "to propose a solution for stragglers evaluation should be done in a datacenter environment with the presence of stragglers and not small workloads with synthetic delay", "furthermore the proposed technique despite the simplicity appears as a rather incremental contribut", "the paper proposes replacing each layer in a standard residual convnet with a set of convolutional modules which are run in parallel", "the input to each model is a sparse sum of the outputs of modules in the previous set"], "labels": ["DFT", "DIS", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper shows marginal improvements on image classification datasets % on cifar % on imagenet over the resnext architecture that they build on", "pros- the connectivity is constrained to be sparse between modules and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weight", "furthermore this learning extends to large-scale image dataset", "- there is indeed a boost in classification performance and the approach shows promise for automatically reducing the number of parameters in the network", "cons- overall the approach seems to be an incremental improvement over the previous work resnext"], "labels": ["APC", "APC", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the datasets used are not very interesting cifar is too small and imagenet is essentially solv", "from the standpoint of the computer vision community increasing performance on these datasets is no longer a meaningful object", "- the modifications add complex", "the paper is well written and conceptually simpl", "however i feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate accept"], "labels": ["CRT", "CRT", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper tried to analyze the subspaces of the adversarial examples neighborhood", "more specifically the authors used local intrinsic dimensionality to analyze the intrinsic dimensional property of the subspac", "the characteristics and theoretical analysis of the proposed method are discussed and explain", "this paper helps others to better understand the vulnerabilities of dnn", "the authors propose a method for reducing the computational burden when performing inference in deep neural network"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is based a previously-developed approach called incomplete dot products which works by pruning some of the inputs in the dot products via the introduction of pre-specified coeffici", "the authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product", "unfortunately this paper was hard to follow for someone who does not actively work in this field making it hard to judge if the contribution is significant or not", "while the description of the problem itself is adequate when it comes to describing the tesla procedure and the alternative training procedure the relevant passages are in my opinion too vague to allow other researchers to implement this procedur", "positive points- the application seems relevant and the task-wise procedure seems like an improvement over the original idp proposal- application to two well-known benchmarking dataset"], "labels": ["SMY", "SMY", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["negative points- the method is not described in sufficient detail to allow reproducibility the algorithms are no more than sketch", "- it is not clear to me what the advantage of this approach is as opposed to alternative ways of compressing the network eg via group lasso regularization or training an emulator on the full model for each task", "minor point- figure  is unclear and requires a better capt", "the authors propose a method for performing transfer learning and domain adaptation via a clustering approach", "the primary contribution is the introduction of a learnable clustering objective lco that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class"], "labels": ["CRT", "DFT", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["once the lco is trained it is applied to the unlabeled target data and effectively serves to provide soft labels for whether or not pairs of target data belong to the same class", "a separate model can then be trained to assign target data to clusters while satisfying these soft labels thereby ensuring that clusters are made up of similar data point", "the proposed lco is novel and seems sound serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest", "the authors also demonstrate a variety of extensions such as how to handle the case when the number of target categories is unknown as well as how the model can make use of labeled source data in the setting where the source and target share the same task", "the way the method is presented is quite confusing and required many more reads than normal to understand exactly what is going on"], "labels": ["SMY", "SUG", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to point out one such problem point section  introduces f a network that classifies each data instance into one of k clust", "however f seems to be mentioned only in a few times by name despite seeming like a crucial part of the method", "explaining how f is used to construct the ccn could help in clarifying exactly what role f plays in the final model", "likewise the introduction of g during the explanation of the lco is rather abrupt and the intuition of what purpose g serves and why it must be learned from data is unclear", "additionally because g is introduced alongside the lco i was initially misled into understanding was that g was optimized to minimize the lco"], "labels": ["SMY", "DIS", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further text explaining intuitively what g accomplishes soft labels transferred from the auxiliary dataset to the target dataset and perhaps a general diagram of what portions of the model are trained on what datasets g is trained on a ccn is trained on t and optionally s' would serve the method section greatly and provide a better overview of how the model work", "the experimental evaluation is very thorough spanning a variety of tasks and set", "strong results in multiple settings indicate that the proposed method is effective and generaliz", "further details are provided in a very comprehensive appendix which provides a mix of discussion and analysis of the provided result", "it would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model work"], "labels": ["SMY", "APC", "APC", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm also curious how well the model works if you do not make use of the labeled source data in the cross-domain setting thereby mimicking the cross-task setup", "at times the experimental details are a little unclear", "consistent use of the a t and s' dataset abbreviations would help", "also the results section seems to switch off between calling the method ccn and lco interchang", "finally a few of the experimental settings differ from their baselines in nontrivial way"], "labels": ["DIS", "CRT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the office experiment the lco appears to be trained on imagenet data", "while this seems similar in nature to initializing from a network pre-trained on imagenet it's worth noting that this requires one to have the entire imagenet dataset on hand when training such a model as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the office data", "similarly the evaluation on svhn-mnist makes use of auxiliary omniglot data which makes the results hard to compare to the existing literature since they generally do not use additional training data in this set", "in addition to the existing comparison perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literatur", "overall the paper seems to have both a novel contribution and strong technical merit"], "labels": ["SMY", "SMY", "CRT", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the presentation of the method is lacking and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is train", "i think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submiss", "=========================update after author rebuttal=========================i have read the author's response and have looked at the changes to the manuscript", "i am satisfied with the improvements to the paper and have changed my review to 'accept'", "the authors perform a comprehensive validation of lstm-based word and character language models establishing that recent claims that other structures can consistently outperform the older stacked lstm architecture result from failure to fully explore the hyperparameter spac"], "labels": ["CRT", "SUG", "SMY", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead with more thorough hyperparameter search lstms are found to achieve state-of-the-art results on many of these language modeling task", "this is a significant result in language modeling and a milestone in deep learning reproducibility research", "the paper is clearly motivated and authoritative in its conclus", "but it's somewhat lacking in detailed model or experiment descript", "some further points- there are several hyperparameters set to the standard or default value like adam's beta parameter and the batch size/bptt length"], "labels": ["DFT", "APC", "APC", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even if it would be prohibitive to include them in the overall hyperparameter search the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justifi", "- the description of the model is ambiguous on at least two points first it wasn't completely clear to me what the down-projection is if it's simply projecting down from the lstm hidden size to the embedding size it wouldn't represent a hyperparameter the tuner can set so i'm assuming it's separate and prior to the conventional output project", "second the phrase additive skip connections combining outputs of all layers has a couple possible interpretations eg skip connections that jump from each layer to the last layer or my assumption skip connections between every pair of lay", "- fully evaluating the claims of collins et al  that capacities of various cells are very similar and their apparentdifferences result from trainability and regularisation would likely involve adding a fourth cell to the hyperparameter sweep one whose design is more arbitrary and is neither the result of human nor machine optim", "- the reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model perform"], "labels": ["DIS", "DFT", "DFT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the plot in figure  is clear and persuasive but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a t", "- the plot in figure  is clear and persuasive but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a t", "the history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of rnn hyperparameter spac", "for instance it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparamet", "- the choice between tied and clipped sak et al  lstm gates and their comparison to standard untied lstm gates is discussed only minimally although it represents a significant difference between this paper and the most standard or conventional lstm implementation eg as provided in optimized gpu librari"], "labels": ["SUG", "APC", "DIS", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the choice between tied and clipped sak et al  lstm gates and their comparison to standard untied lstm gates is discussed only minimally although it represents a significant difference between this paper and the most standard or conventional lstm implementation eg as provided in optimized gpu librari", "in addition to further discussion on this point this result also suggests evaluating other recently proposed minor changes to the lstm architecture such as multiplicative lstm krause et ", "- it would also have been nice to see a comparison between the variational/recurrent dropout parameterization in which there is further sharing of masks between gates and the one with independent noise for the gates as described in the footnot", "there has been some confusion in the literature as to which of these parameterizations is better or more standard simply justifying the choice of parameterization a little more would also help", "there has been some confusion in the literature as to which of these parameterizations is better or more standard simply justifying the choice of parameterization a little more would also help"], "labels": ["DIS", "DIS", "SUG", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors of this paper propose some extensions to the dynamic coattention networks models presented last year at iclr", "first they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descript", "the other main modification is to train their dcn+ model using both cross entropy loss and f score using rl supervision in order to  reward the system for making partial matching predict", "empirical evaluations conducted on the squad dataset indicates that this architecture achieves an improvement of at least % both on f and exact match accuracy over other comparable system", "an ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model perform"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written ideas are presented clearly and the experiments section provide interesting insights such as the impact of rl on system training or the capability of the model to handle long questions and/or answ", "it seems to me that this paper is a significant contribution to the field of question answering system", "in this paper the authors proposed to learn word embedding for the target domain in the lifelong learning mann", "the basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpu", "overall the descriptions of the proposed model section  - section  are hard to follow"], "labels": ["APC", "FBK", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is not because the proposed model is technically difficult to understand", "on the contrary the model is heuristic and simple but the descriptions are unclear", "section  is supposed to give an overview and high-level introduction of the whole model using the figure  and figure  not figure  mentioned in text", "however after reading section  i do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is us", "section  and  section  are supposed to give details of different components of the proposed model and explain the motiv"], "labels": ["SMY", "CRT", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however descriptions in these two sections are very confusing eg many symbols in algorithm  are presented with any descript", "moreover the motivations behind the proposed methods for different components are miss", "also a lot of types make the descriptions more difficult to follow eg may not helpful or even harmful 'figure  we show this section  large size a vocabulary etc", "another major concern is that the technical contributions of the proposed model is quite limit", "the only technical contributions are  and the way to construct the co-occurrence information a"], "labels": ["CRT", "CRT", "CRT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however such contributions are quite minor and technically heurist", "moreover regarding the aggregation layer in the pairwise network it is similar to feature engin", "in this case why not just train a flat classifier like logistic regression with rich feature engineering in stead of using a neural network", "regarding experiments one straight-forward baseline is miss", "as n domains are supposed to be given in advance before the n+ domain target domain comes one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain"], "labels": ["SMY", "SMY", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance one can learn n pairwise  out of n sources + the target cross-domain word embedding and combine them using the similarity between each source and the target as the weight", "in a previous work [] an auto-placement better model partition on multi gpus method was proposed to accelerate a tensorflow modelus runtim", "however this method requires the rule-based co-locating step in order to resolve this problem the authors of this paper purposed a fully connect network fcn to replace the co-location step", "in particular hand-crafted features are fed to the fcn and the output is the prediction of group id of this oper", "then all the embeddings in each group are averaged to serve as the input of a seqseq encod"], "labels": ["SUG", "SMY", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then all the embeddings in each group are averaged to serve as the input of a seqseq encod", "overall speaking this work is quite interest", "however it also has several limitations as explained below", "nfirst the computational cost of the proposed method seems very high", "it may take more than one day on - gpus for training i did not find enough details in this paper but the training complexity will be no less than the in []"], "labels": ["DIS", "APC", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this makes it very hard to reproduce the experimental results in order to verify it and its practical value becomes quite restrictive very few organizations can afford such a cost", "this makes it very hard to reproduce the experimental results in order to verify it and its practical value becomes quite restrictive very few organizations can afford such a cost", "second as the author mentioned itus hard to compare the experimental results in this paper wit those in [] because different hardware devices and software versions were us", "however this is not a very sound excus", "i would encourage the authors to implement colocrl [] on their own hardware and software systems and make direct comparison"], "labels": ["SUG", "DFT", "DFT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would encourage the authors to implement colocrl [] on their own hardware and software systems and make direct comparison", "otherwise it is very hard to tell whether there is improvement and how significant the improvement i", "otherwise it is very hard to tell whether there is improvement and how significant the improvement i", "in addition it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the plac", "[] mirhoseini a pham h le q v et al device placement optimization with reinforcement learning[j] arxiv preprint arxiv  https//arxivorg/pdf/pdf"], "labels": ["DFT", "SUG", "DIS", "DFT", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper considers the problem of reinforcement learning in time-limited domain", "it begins by observing that in time-limited domains an agent unaware of the remaining time can experience state-alias", "to combat this problem the authors suggest modifying the state representation of the policy to include an indicator of the amount of remaining tim", "the time-aware agent shows improved performance in a time-limited gridworld and several control domain", "next the authors consider the problem of learning a time-unlimited policy from time-limited episod"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they show that by bootstrapping from the final state of the time-limited domain they are able to learn better policies for the time-unlimited cas", "prosthe paper is well-written and clear if a bit verbos", "the paper has extensive experiments in a variety of domain", "consin my opinion the substance of the contribution is not enough to warrant a full paper and the problem of time-limited learning is not well motiv", "it's not clear how frequently rl agents will encounter time-limited domains of interest"], "labels": ["APC", "APC", "APC", "FBK", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["currently most domains are terminated by failure/success conditions rather than tim", "the author's choice of tasks seem somewhat artificial in that they impose time limits on otherwise unlimited domains in order to demonstrate experimental improv", "is there good reason to think rl agents will need to contend with time-limited domains in the futur", "the inclusion of remaining-time as a part of the agent's observations and resulting improvement in time-limited domains is somewhat obvi", "it's well accepted that in any partially observed domain inclusion of the latent variables as a part of the agent's observation will result in a fully observed domain less state-aliasing more accurate value estimates and better perform"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the author's inclusion of the latent time variable as a part of the agent's observations reconfirms this well-known fact but doesn't tell us anything new", "i have the same questions about partial episode bootstrapping is there a task in which we find our rl agents learning in time-limited settings and then evaluated in unlimited on", "the experiments in this direction again feel somewhat contrived by imposing time limits and then removing them", "the proposed solution of bootstrapping from the value of the terminal state vs_t clearly works and i suspect that any rl-practitioner faced with training time-limited policies that are evaluated in time-unlimited settings might come up with the same solut", "while the experiments are well don"], "labels": ["DIS", "QSN", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i don't think the substance of the algorithmic improvement is enough", "i think this paper would improve by demonstrating how time-aware policies can help in domains of interest which are usually not time-limit", "i could imagine a line of experiments that investigate the idea of selectively stopping episodes when the agent is no longer experiencing useful transitions and then showing that the partial episode bootstrapping can save on overall sample complexity compared to an agent that must experience the entirety of every episod", "this paper focuses on imitation learning with intentions sampled from a multi-modal distribut", "the papers encode the mode as a hidden variable in a stochastic neural network and suggest stepping around posterior inference over this hidden variable which is generally required to do efficient maximum likelihood with a biased importance sampling estim"], "labels": ["CRT", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["lastly they incorporate attention for large visual input", "the unimodal claim for distribution without randomness is weak", "the distribution could be replaced with a normalizing flow", "the distribution could be replaced with a normalizing flow", "the use of a latent variable in this setting makes intuitive sense but i don't think multimodality motivates it"], "labels": ["SMY", "DFT", "SUG", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover it really felt like the biased importance sampling approach should be compared to a formal inference schem", "i can see how it adds value over sampling from the prior but it's unclear if it has value over a modern approximate inference scheme like a black box variational inference algorithm or stochastic gradient mcmc", "i can see how it adds value over sampling from the prior but it's unclear if it has value over a modern approximate inference scheme like a black box variational inference algorithm or stochastic gradient mcmc", "how important is using the pretrained weights from the deterministic rnn", "finally i'd also be curious about how much added value you get from having access to extra rollout"], "labels": ["DFT", "DFT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary of paperthis work proposes an extension to an existing method franceschi  to optimize regularization hyperparamet", "their method claims increased stability in contrast to the existing on", "summary of reviewthis is an incremental change of an existing method", "this is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical argu", "i did not find either to be the cas"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theoretical arguments are interesting but lacking in rigor", "the proposed method introduces hyper-hyperparameters which may be hard to tun", "the experiments are small scale and it is unclear how much the method improves random grid search", "for these reasons i cannot recommend this paper for accept", "comments paper should cite domke  in related work sect"], "labels": ["CRT", "CRT", "CRT", "FBK", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["should state and verify conditions for application of implicit function theorem on pag", "fix notation on pag", "dot is used on the right hand side to indicate an argument but not left hand side for equation after with respect to lambda", "i would like to see more explanation for the figure in appendix a", "what specific optimization is being depict"], "labels": ["SUG", "SUG", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this figure could be moved into the paper's main body with some additional clarif", "i did not understand the paragraph beginning with this poor estim", "is this just a restatement of the previous paragraph which concluded convergence will be slow if eta is too smal", "i do understand the notation used in equation  on page  are   meant to denote less than/greater than or something els", "discussion of weight decay on page  seems tangential to main point of the paper could be reduced to a sentence or two"], "labels": ["SUG", "CRT", "QSN", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see some experimental verification that the proposed method significantly reduces the dropout gradient variance page  if the authors claim that tuning dropout probabilities is an area they succeed where others don't", "experiments are unconvincing first only one hyperparameter is being optimized and random search/grid search are sufficient for thi", "second it is unclear how close the proposed method is to finding the optimal regularization parameter lambda", "all one can conclude is that it performs slightly better than grid search with a small number of run", "i would have preferred to see an extensive grid search done to find the best possible lambda then seen how well the proposed method does compared to thi"], "labels": ["SUG", "CRT", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would have liked to see a plot of how the value of lambda changes throughout optim", "if one can initialize lambda arbitrarily and have this method find the optimal lambda that is more impressive than a method that works simply because of a fortunate initi", "typos optimization - optimize bottom of pag", "should be a period after sentence starting several algorithms on pag", "in algorithm box on page  enable_projection is never us"], "labels": ["SUG", "DIS", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["seems like warmup_time should also be an input to the algorithm", "this paper proposes a new method to train dnns with quantized weights by including the quantization as a constraint in a proximal quasi-newton algorithm which simultaneously learns a scaling for the quantized values possibly different for positive and negative weight", "the paper is very clearly written and the proposal is very well placed in the context of previous methods for the same purpos", "the experiments are very clearly presented and solidly design", "in fact the paper is a somewhat simple extension of the method proposed by hou yao and kwok  which is where the novelty resid"], "labels": ["SUG", "SMY", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consequently there is not a great degree of novelty in terms of the proposed method and the results are only slightly better than those of previous method", "nfinally in terms of analysis of the algorithm the authors simply invoke a theorem from hou yao and kwok  which claims convergence of the proposed algorithm", "nfinally in terms of analysis of the algorithm the authors simply invoke a theorem from hou yao and kwok  which claims convergence of the proposed algorithm", "however what is shown in that paper is that the sequence of loss function values converges which does not imply that the sequence of weight estimates also converges because of the presence of a non-convex constraint $b_j^t in q^{n_l}$", "this may not be relevant for the practical results but to be accurate it can't be simply stated that the algorithm converges without a more careful analysi"], "labels": ["CRT", "SMY", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper considers distribution to distribution regression with mlp", "the authors use an energy function based approach", "they test on a few problems showing similar performance to other distribution to distribution alternatives but requiring fewer paramet", "this seems to be a nice treatment of distribution to distribution regression with neural network", "the approach is methodological similar to using expected likelihood kernel"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while similar performance is achieved with fewer parameters it would be more enlightening to consider accuracy vs runtime instead of accuracy vs paramet", "thatus what we really care about  in a sense because this problem has been considered several times in slightly different model classes there really ought to be a pretty strong empirical investigation in the discussion it say", "ucfor future work a possible study is to investigate what classes of problems drn can solv", "ud  it feels like in the present work there should have been an investigation about what classes of problems the drn can solv", "its practical utility is questionable  itus not clear how much value there is adding yet another distribution to distribution regression approach this time with neural networks without some pretty strong motivation which seems to be lacking as well as experi"], "labels": ["SUG", "DIS", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the introduction it would also improve the paper to outline clear points of methodological novelti", "the authors present a study that aims at inferring the emotional tags provided by thumblr users starting from images and texts in the capt", "for text processing the authors use a standard lstm taking as input glove vectors of words in a sent", "for visual information authors use a pretrained cnn with fine tun", "a fully connected layer is used to fuse the multimodal inform"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental results are reported in a self generated data set", "the contribution from the rl perspective is limited in the sense that the authors simply applied standard models to predict a bunch of labels in this case emotion label", "the contribution from the rl perspective is limited in the sense that the authors simply applied standard models to predict a bunch of labels in this case emotion label", "it is interesting the psychological analysis that the authors present in sect", "still i think the contribution in that part is a sentiment-psychologically inspired analysis of the thumbrl data set"], "labels": ["SMY", "DFT", "CRT", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the author's statement on that this study leads to a more plausible psychological model of emotion is not well founded they also mention to learn to recognize the latent emotional st", "whereas it is true that psychological studies rely on self - filled questionnaires comparing a questionnaire produced by expert psychologist to the tags provided by users in a social network is to ambitious in some parts the authors make explicit this is an approximation this should be stressed in every part of the pap", "summary of paper the paper proposes an rnn-based neural network architecture for embedding programs focusing on the semantics of the program rather than the syntax", "the application is to predict errors made by students on programming task", "this is achieved by creating training data based on program traces obtained by instrumenting the program by adding print stat"], "labels": ["CRT", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the neural network is trained using this program traces with an objective for classifying the student error pattern eg list indexing branching conditions looping bounds---quality the experiments compare the three proposed neural network architectures with two syntax-based architectur", "it would be good to see a comparison with some techniques from reed & de freitas  as this work also focuses on semantics-based embed", "clarity the paper is clearly written", "originality this work doesn't seem that original from an algorithmic point of view since reed & de freitas  and cai et al  among others have considered using execution trac", "however the application to program repair is novel as far as i know"], "labels": ["SMY", "SUG", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["significance this work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand", "---some questions/comments- do we need to add the print statements for any new programs that the students submit", "what if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be autom", "---references cai j shin r & song d", "making neural programming architectures generalize via recurs"], "labels": ["APC", "QSN", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in international conference on learning representations iclr", "quick summarythis paper shows how to train a gan in the case where the dataset is corrupted by some measurement noise process", "they propose to introduce the noise process into the generation pipeline such that the gan generates a clean image corrupts its own output and feeds that into the discrimin", "the discriminator then needs to decide whether this is a real corrupted measurement or a generated on", "the method is demonstrated to the generate better results than the baseline on a variety of datasets and noise process"], "labels": ["DIS", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["qualityi found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simpl", "originalityi'm pretty sure this is the first paper to tackle this problem directly in gener", "significancethis is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumst", "pros* important problem", "* elegant and simple solut"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* nice results and decent experiments but see below", "cons* the assumption that the measurement process *and* parameters are known is quite a strong on", "though it is quite common in the literature to assume this it would have been interesting to see if there's a way to handle the case where it is unknown either the process parameters or both", "though it is quite common in the literature to assume this it would have been interesting to see if there's a way to handle the case where it is unknown either the process parameters or both", "* the baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the fixed version which is fed into them"], "labels": ["APC", "DIS", "SUG", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the fixed version which is fed into them", "i can't however think of other baselines other than ignore so i guess that is accept", "* i wish the authors would show that they get a *useful* model eventually - for example can this be used to denoise other images from the dataset", "summarythis is a nice paper which deals with an important problem has some nice results and while not groundbreaking certainly merits a publ", "summarythis is a nice paper which deals with an important problem has some nice results and while not groundbreaking certainly merits a publ"], "labels": ["CRT", "DIS", "QSN", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activ", "empirically the authors demonstrate on two different task domains that one can trade off some accuracy for a little robustness -- qualitatively speak", "on one hand the approach is simple to implement and has minimal impact computationally on pre-trained network", "on the other hand i find it lacking in terms of theoretical support other than the fact that the added stochasticity induces a certain amount of robust", "on the other hand i find it lacking in terms of theoretical support other than the fact that the added stochasticity induces a certain amount of robust"], "labels": ["SMY", "SMY", "SMY", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example how does this compare to random perturbation say zero-mean of the weight", "this adds stochasticity as well so why and why not this work", "the authors do not give any insight in this regard", "overall i still recommend acceptance weakly since the empirical results may be valuable to a general practition", "the paper could be strengthened by addressing the issues above as well as including more empirical results if nothing els"], "labels": ["QSN", "QSN", "DFT", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer it is not able to capture the full complexity of languag", "as a result they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components her", "this leads to improvements in the word-level perplexities of the ptb and wikitext data sets and switchboard bleu scor", "this leads to improvements in the word-level perplexities of the ptb and wikitext data sets and switchboard bleu scor", "the question of the expressiveness of the softmax layer as well as its suitability for word-level prediction is indeed an important one which has received too little attent"], "labels": ["SMY", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the question of the expressiveness of the softmax layer as well as its suitability for word-level prediction is indeed an important one which has received too little attent", "this makes a lot of the questions asked in this paper extremely relevant to the field", "this makes a lot of the questions asked in this paper extremely relevant to the field", "however it is unclear that the rank of the logit matrix is the right quantity to consid", "for example it is easy to describe a rank d nxm matrix where up to ^d lines have max values at different indic"], "labels": ["DIS", "SMY", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further the first two observations in section  would be more accurately described as intuitions of the author", "as they write themselves there is no evidence showing that semantic meanings are fully linearly correl", "why then try to link meanings to basis vectors for the rows of a", "to be clear the proposed model is undoubtedly more expressive than a regular softmax and although it does come at a substantial computational cost a back-of-the envelope calculation tells us that computing  components of d mos takes the same number of operations as one with dimension  = sqrt ** it apparently manages not to drastically increase overfitting which is signific", "unfortunately this is only tested on relatively small data sets up to m tokens and a vocabulary of size k for language model"], "labels": ["CRT", "CRT", "QSN", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they do constitute a good starting place to test a model", "but given the importance of regularization on those specific tasks it is difficult to predict how the mos would behave if more training data were available and if one could eg simply try a  dimension embedding for the softmax without having to worry about overfit", "another important missing experiment would consist in varying the number of mixture components this could very well be done on wikitext", "this could help validate the hypothesis how does the estimated rank vary with the number of compon", "how about the performance and pairwise kl diverg"], "labels": ["APC", "CRT", "DFT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper offers a promising direction for language modeling research", "but would require more justification or at least a more developed experimental sect", "pros- important starting quest", "- thought-provoking approach", "- experimental gains on small data set"], "labels": ["APC", "DFT", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- the link between the intuition and reality of the gains is not obvi", "- experiments limited to small data sets some obvious questions remain", "the authors consider a method which they trace back to  but may have a longer history of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized using a stochastic multiplicative upd", "the basic observation for sgd is that if theta_{t+} = theta_t - alpha abla ftheta_t then partial/partialalpha ftheta_{t+} = -abla ftheta_t abla ftheta_{t+} ie that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update wrt the learning rate alpha", "i have seen this before for sgd the authors do not claim that the basic idea is novel but i believe that the application to other algorithms the authors explicitly consider nesterov momentum and adam are novel as is the use of the multiplicative and normalized update of equation  particularly the norm"], "labels": ["CRT", "DFT", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are well-presented and appear to convincingly show a benefit", "figure  which explores the robustness of the algorithms to the choice of alpha_ and beta is particularly nicely-done and addresses the most natural criticism of this approach that it replaces one hyperparameter with two", "the authors highlight theoretical convergence guarantees as an important future work item and the lack of them here aside from theorem  which just shows asymptotic convergence if the learning rates become sufficiently small is a weakness but not i think a critical on", "this appears to be a promising approach and bringing it back to the attention of the machine learning community is valu", "learning to solve combinatorial optimization problems using recurrent networks is a very interesting research top"], "labels": ["APC", "APC", "DIS", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["learning to solve combinatorial optimization problems using recurrent networks is a very interesting research top", "however i had a very hard time understanding the pap", "it certainly doesnut help that ium not familiar with the architectures the model is based on nor with state-of-the-art integer programming solv", "the architecture was described but not really motiv", "the authors chose to study only random instances which are known to be bad representatives of real-world problmes instead of picking a standard benchmark problem"], "labels": ["APC", "DIS", "DIS", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors chose to study only random instances which are known to be bad representatives of real-world problmes instead of picking a standard benchmark problem", "furthermore the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal if ani", "furthermore the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal if ani", "the experimental issues especially regarding the baseline raised by the anonymous comments below were rather troubling itus a pity they were left unansw", "hopefully other expert reviewers will be able to provide constructive feedback"], "labels": ["CRT", "DFT", "CRT", "CRT", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper satisfies the following necessary conditions foraccept", "the writing is clear and i was able to understand thepresented method and its motivation despite not being too familiarwith the relevant literatur", "explicitly writing the auto-encodersas pseudo-code algorithms was particular help", "i found no technicalerror", "the problem addressed is one worth solving - building agenerative model of observed data"], "labels": ["SMY", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is some empirical testingwhich show the presented method in a good light", "there is some empirical testingwhich show the presented method in a good light", "the authors are careful to relate the presented method with existingones most notably vae and aa", "i suppose one could argue that theclose connection to existing methods means that this paper is notinnovative enough", "i think that would be unfair - most new methodshave close relations with existing ones - it is just that sometimesthe authors do not flag this up as they should"], "labels": ["SUG", "DFT", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["wae is a bit oversold", "the authors state that wae generates samplesof better quality than vae without any condition being put on whenit does thi", "there is no proof that it is always better and i can'tsee how there could b", "any method of inferring a generative modelfrom data must make some 'inductive' assumpt", "surely one coulddevise situations where vae outperforms wa"], "labels": ["CRT", "SMY", "DFT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think this issue shouldhave been examined in more depth", "i think this issue shouldhave been examined in more depth", "i found no typo or grammatical errors which is unusual - good carefuljob", "the authors test a cnn on images with color channels modified such that the values of the three channels after modification are invariant to permut", "the main positive point is that the performance does not degrade too much"], "labels": ["SUG", "DFT", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are several important negative points which should prevent this work as it is from being publish", "why is this type of color channel modification relevant for real life vis", "the invariance introduced here does not seem to be related to any real world phenomenon", "the nets in principle could learn to recognize objects based on shape only and the shape remains stable when the color channels are chang", "why is the crash car dataset used in this scenario"], "labels": ["CRT", "QSN", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear to me why this types of theoretical invariance is tested on such as specific dataset", "is there a real reason for that", "the writing could be significantly improved both at the grammatical level and the level of high level organization and present", "i think the authors should spend time on better motivating the choice of invariance used as well as on testing with different potentially new architectures color change cases and dataset", "there is no theoretical novelty and the empirical one seems to be very limited with less convincing result"], "labels": ["QSN", "QSN", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["strengths-tthere is an interesting analysis on how cnnus perform better spatial-relation problems in contrast to same-different problems and how spatial-relation problems are less sensitive to hyper paramet", "-tthe authors bring a good point on the limitations of the svrt dataset u mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations which make it difficult to quantify the effect of image variability on the task", "and propose a challenge that addresses these issues and allows controlling different aspects of image vari", "-tthe paper shows how state of the art relational networks performing well on multiple relational tasks fail to generalize to same-ness relationship", "weaknesses-twhile the proposed psvrt dataset addresses the  noted problems in svrt using only  relations in the study is very limit"], "labels": ["SMY", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tthe paper describes two sets of relationships but it soon suggests that current approaches actually struggle in same-different relationship", "however they only explore this relationship under identical object", "it would have been interesting to study more kinds of such relationships such as equality up to translation or rotation to understand the limitation of such network", "would that allow improving generalization to varying item or image s", "comments-tin page  authors suggest that from that gufclueehre bengio  that for visual relations ucfailure of feed-forward networks [u] reflects a poor choice of hyper paramet"], "labels": ["CRT", "CRT", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this seems to contradict the later discussion where they suggest that probably current architectures cannot handle such visual relationship", "-tthe point brought about cnnus failing to generalize on same-ness relationships on sort-of-clevr is interesting but it would be good to know why psvrt provides better gener", "-tthe point brought about cnnus failing to generalize on same-ness relationships on sort-of-clevr is interesting but it would be good to know why psvrt provides better gener", "what would happen if shapes different than random squared patterns were used at test tim", "-tauthors reason about biological inspired approaches using attention and memory based on existing literatur"], "labels": ["CRT", "APC", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while they provide some good references to support this statement it would have been interesting to show whether they actually improve tta under image parameter vari", "this is an interesting idea and written clearli", "the experiments with baird's and cartpole were both convincing as preliminary evidence that this could be effect", "however it is very hard to generalize from these toy problem", "first we really need a more thorough analysis of what this does to the learning dynamics itself"], "labels": ["APC", "APC", "APC", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["baring theoretical results you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directli", "i think ideally i would want to see this on atari or some of the continuous control domains often us", "if this allows the removing of the target network for instance in those more difficult tasks then this would be a huge d", "additionally i do not think the current gridworld task adds anything to the experiments i would rather actually see this on a more interesting linear function approximation on some other simple task like mountain car than a neural network on gridworld", "the reason this might be interesting is that when the parameter space is lower dimensional not an issue for neural nets but could be problematic for linear fa the constraint might be too much leading to significantly poorer perform"], "labels": ["SUG", "SUG", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suspect this is the actual cause for it not converging to zero for baird's although please correct me if i'm wrong on that", "as is i cannot recommend acceptance given the current experiments and lack of theoretical result", "but i do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it", "but i do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it", "prossimple interesting ideaworks well on toy problems and able to prevent divergence in baird's counter-exampl"], "labels": ["DIS", "FBK", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["conslacking in theoretical analysis or significant experimental result", "the main idea is to use the accuracy of a classifier trained on synthetic training examples produced by a generative model to define an evaluation metric for the generative model", "specifically compare the accuracy of a classifier trained on a noise-perturbed version of the real dataset to that of a classifier trained on a mix of real data and synthetic data generated by the model being evalu", "results are shown on mnist and fashion mnist", "the paper should discuss the assumptions needed for classifier accuracy to be a good proxy for the quality of a generative model that generated the classifier's training data"], "labels": ["CRT", "SMY", "DIS", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it may be the case that even a bad generative model according to some other metric can still result in a classifier that produces reasonable test accuraci", "since a classifier can be a highly nonlinear function it can potentially ignore many aspects of its input distribution such that even poor approximations as measured by say kl lead to similar test accuracy as good approxim", "the sensitivity of the evaluation metric defined in equation  to the choice of hyperparameters of the classifier and the metric itself eg alpha is not evalu", "is it possible that a different choice of hyperparameters can change the model rank", "should the hyperparameters be tuned separately for each generative model being evalu"], "labels": ["DIS", "DIS", "DFT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the intuition behind comparing against a classifier trained on a noise-perturbed version of the data is not explained clearli", "why not compare a classifier trained on only unperturbed real data to a classifier trained on both real and synthetic data", "evaluation on two datasets is not sufficient to provide insight into whether the proposed metric is us", "evaluation on two datasets is not sufficient to provide insight into whether the proposed metric is us", "other datasets such as imagenet cifar/ celeb a etc should also be includ"], "labels": ["DFT", "DFT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper describes some interesting work", "but for a combination of reasons i think it's more like a workshop-track pap", "there is not much that's technically new in the paper-- at least not much that's really understand", "there is some text about a variant of ctc but it does not explain very clearly what was done or what the motivation wa", "there are also quite a few misspel"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since the system is presented without any comparisons to alternatives for any of the individual components it doesn't really shed any light on the significance of the various modeling decisions that were mad", "that limits the valu", "if rejected from here it could perhaps be submitted as an icassp or interspeech pap", "# summary and assessmentthe paper addresses an important issueuthat of making learning of recurrent networks tractable for sequence lengths well beyond us of time step", "a key problem here is that processing such sequences with ordinary rnns requires a reduce operation where the output of the net at time step t depends on the outputs of *all* its predecessor"], "labels": ["CRT", "CRT", "SUG", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors now make a crucial observation namely that a certain class of rnns allows evaluation in a non-linear fashion through a so-called scan oper", "here if certain conditions are satisfied the calculation of the output   can be parallelised mass", "in the following the authors explore the landscape of rnns satisfying the necessary condit", "the performance is investigated in terms of wall clock tim", "further experimental results of problems with previously untacked sequence lengths are report"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is certainly relevant as it can pave the way towards the application of recurrent architectures to problems that have extremely long term depend", "to me the execution seems sound", "the experiments back up the claim", "## minor- i challenge the claim that thousands and millions of time steps are a common issue in ucrobotics remote sensing control systems speech recognition medicine and financeud as claimed in the first paragraph of the introduct", "imho most problems in these domains get away with a few hundred time steps nevertheless iud appreciate a few examples where this is a case to better justify the method"], "labels": ["SMY", "APC", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["imho most problems in these domains get away with a few hundred time steps nevertheless iud appreciate a few examples where this is a case to better justify the method", "this paper focuses on solving query completion problem with error correction which is a very practical and important problem", "the idea is character bas", "and in order to achieve three important targets which are auto completion auto error correction and real time the authors first adopt the character-level rnn-based modeling which can be easily combined with error correction and then carefully optimize the inference part to make it real tim", "pros the paper is very well organized and easy to read"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is nicely designed to solve the specific real problem for example the edit distance is modified to be more consistent with the task", "detailed information are provided about the experiments such as data model and infer", "cons no direct comparisons with other methods are provid", "i am not familiar with the state-of-the-art methods in this field", "if the performance hit rate or coverage of this paper is near stoa methods then such experimental results will make this paper much more solid"], "labels": ["APC", "APC", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss", "this allows the authors to turn the standard min max problem of adversarial training into a single minimization problem which is easier to", "the method is demonstrated on a toy example and on the task of unsupervised domain adapt", "strengths- the derivation of the dual formulation is novel", "- the dual formulation simplifies adversarial train"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the experiments show the better behavior of the method compared to adversarial training for domain adapt", "weaknesses- it is unclear that this idea would generalize beyond a logistic regression classifier which might limit its applicability in practic", "- it would have been nice to see results on other tasks than domain adaptation such as synthetic image generation for which gans are often us", "- it would be interesting to see if the da results with a kernel classifier are better comparable to the state of the art", "- the mathematical derivations have some error"], "labels": ["APC", "CRT", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["detailed comments- the upper bound used to derive the formulation applies to a logistic regression classifi", "while effective such a classifier might not be as powerful as multi-layer architectures that are used as discrimin", "i would be interested to know if they authors see ways to generalize to better classifi", "- the second weakness listed above might be related to the first on", "did the authors tried their approach to non-da tasks such as generating images as often done with gan"], "labels": ["DIS", "CRT", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["showing such results would be more convinc", "however i wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other task", "- the da results are shown with a linear classifier for the comparison to the baselines to be fair which i appreci", "however to evaluate the effectiveness of the method it would be interesting to also report results with a kernel-based classifier so as to see how it compares to the state of the art", "- there are some errors and unclear things in the mathematical derivations* in the equation above eq  alpha should in fact be alpha_i and it is not a vector no need to transpose it"], "labels": ["SUG", "APC", "APC", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* in eq  it should be alpha_i alpha_j instead of alpha^talpha*", "in eq  it is unclear to me where the constraint  leq alpha leq  comes from", "the origin of the last equality constraints on the sums of alpha_a and alpha_b is also unclear to m", "* in eq  it is also not clear to me why the third term has a different constant weight than the first two", "this would have an impact on the relationship to the mmd"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the idea of sample reweighting within the mmd was in fact already used for da eg huang et al nips  gong et al icml", "what is done here is quite different but i think it would be worth discussing these relationships in the pap", "- the paper is reasonably clear", "but could be improved with some more details on the mathematical derivations eg explaining where the constraints on alpha come from and on the experiments it is not entirely clear how the distributions of accuracies were obtain", "but could be improved with some more details on the mathematical derivations eg explaining where the constraints on alpha come from and on the experiments it is not entirely clear how the distributions of accuracies were obtain"], "labels": ["DIS", "SUG", "APC", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper develops an interesting approach for solving multi-class classification with softmax loss", "the key idea is to reformulate the problem as a convex minimization of a double-sum structure via a simple conjugation trick", "sgd is applied to the reformulation in each step samples a subset of the training samples and labels which appear both in the double sum", "the main contributions of this paper are u-max idea for numerical stability reasons and an proposing an implicit sgd ideaunlike the first review i see what the term exact in the title is supposed to mean i believe this was explained in the pap", "i agree with the second reviewer that the approach is interest"], "labels": ["SMY", "SMY", "SMY", "SMY", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i also agree with the criticism double sum formulations exist in the literature comments about experiments and will not repeat it her", "i will stress though that the statement about newton in the paper is not justified newton method does not converge globally with linear rate cubic regularisation is needed for global convergence local rate is quadrat", "i believe the paper could warrant acceptance if all criticism raised by reviewer  is address", "i apologise for short and late review i got access to the paper only after the original review deadlin", "this work presents a cnn training setup that uses half precision implementation that can get x speedup for train"], "labels": ["CRT", "CRT", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work is clearly presented and the evaluations seem convinc", "the presented implementations are competitive in terms of accuracy when compared to the fp represent", "i'm not an expert in this area but the contribution seems relevant to me and enough for being publish", "the authors present a scalable model for questioning answering that is able to train on long docu", "on the triviaqa dataset the proposed model achieves state of the art results on both domains wikipedia and web"], "labels": ["APC", "SMY", "FBK", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the formulation of the model is straight-forward", "however i am skeptical about whether the results prove the premise of the paper eg multi-mention reasoning is necessari", "furthermore i am slightly unconvinced about the authors' claim of effici", "nevertheless i think this work is important given its performance on the task", "why is this model success"], "labels": ["APC", "CRT", "CRT", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["multi-mention reasoning or more document context", "i am not convinced of the necessity of multi-mention reasoning which the authors use as motivation as shown in the examples in the pap", "for example in figure  the answer is solely obtained using the second last passag", "the other mentions provide signal but does not provide conclusive evid", "the other mentions provide signal but does not provide conclusive evid"], "labels": ["QSN", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["perhaps i am mistaken but it seems to me that the proposed model cannot seem to handle negation can the authors confirm/deny thi", "perhaps i am mistaken but it seems to me that the proposed model cannot seem to handle negation can the authors confirm/deny thi", "i am also skeptical about the computation efficiency of a model that scores all spans in a document which is on^ where n is the document length", "can you show some analysis of your model results that confirm/deny this hypothesi", "why is the computational complexity not a function of the number of span"], "labels": ["QSN", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why is the computational complexity not a function of the number of span", "it seems like the derivations presents several equations that score a given span", "perhaps i am mistaken but there seems to be n^ spans in the document that one has to scor", "shouldn't the computational complexity then be at least on^ which makes it actually much slower than say squad models that do greedy decoding on + nm", "some minor notes-  seems like an attention computation in which the attention context over the question and span is computed using the quest"], "labels": ["CRT", "DIS", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["explicitly mentioning this may help the reading grasp the formul", "- same for  which seems like the biattention seo  or coattention xiong  from previous squad work", "- the sentence we define  to be the embeddings of the l words of the sentence that contains s is not very clear", "do you mean that the sentence contains l word", "it could be interpreted that the span has l word"], "labels": ["SUG", "DIS", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- there is a typo in your  level  complexity there is an extra o inside the big o not", "the paper presents a method for finding related images analogies from different domains based on matching-by-synthesi", "the general idea is interesting and the results show improvements over previous approaches such as cyclegan with different initializations pre-learned or not", "the algorithm is tested on three dataset", "while the approach has some strong positive points such as good experiments and theoretical insights the idea to match by synthesis and the proposed loss which is novel and combines the proposed concept"], "labels": ["CRT", "SMY", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper lacks clarity and sufficient detail", "instead of the longer intro and related work discuss", "i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experi", "also the matching part which is discussed at the theoretical level could be better explained and presented at a more visual level", "it is hard to understand sufficiently well what the formalism means without more insigh"], "labels": ["CRT", "SUG", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["talso the experiments need more details for example it is not clear what the numbers in table  mean", "this paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable funct", "it builds on previous works montufar et al  and raghu et al  and presents improved bounds on the maximum number of linear region", "it also evaluates the number of regions of small networks during train", "the improved upper bound given in theorem  appeared in sampta  - mathematics of deep learning ``notes on the number of linear regions of deep neural networks'' by montufar"], "labels": ["DFT", "SMY", "APC", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the improved lower bound given in theorem  is very modest but neat", "theorem  follows easily from thi", "the improved upper bound for maxout networks follows a similar intuition but appears to be novel", "the paper also discusses the exact computation of the number of linear regions in small trained network", "it presents experiments during training and with varying network s"], "labels": ["APC", "DIS", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these give an interesting picture consistent with the theoretical bounds and showing the behaviour during train", "here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypothes", "this paper proposes to train a classifier neural network not just to classifier but also to reconstruct a representation of its input in order to factorize the class information from the appearance or style as used in this pap", "this is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial nll loss and a reconstruction loss at the last hidden lay", "experiments on mnist are provided to analyse what this approach learn"], "labels": ["APC", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately i fail to see a significantly valuable contribution from this work", "first the paper could do a better job at motivating the problem being address", "why is it important to separate class from styl", "should it allow better classification perform", "if so it's never measured in this work"], "labels": ["CRT", "SUG", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if that's not the motivation then what is it", "second all experiments were conducted on the mnist dataset", "in  most would expect experiments on at least one other more complex dataset to trust any claims on a method", "finally the results are not particularly impress", "i don't find the reconstructions demonstrated particularly compelling they are generally pretty different from the original input"], "labels": ["QSN", "SMY", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also that the style representation contain less and i'd say slightly less in figure  b and d we still see a lot of same class nearest neighbors is not exactly a surprising result", "and the results of figure  showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style success", "the interpolation results of figure  are also underwhelming though possibly mostly because the reconstructions are in general not great", "but most importantly none of these results are measured in a quantitative way they are all qualitative and thus subject", "but most importantly none of these results are measured in a quantitative way they are all qualitative and thus subject"], "labels": ["CRT", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for all these reasons i'm afraid i must recommend this paper be reject", "the paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for rnn", "the activation function keeps the activation roughly zero-cent", "in general this is an interesting direction to explore the idea is interest", "however i would like to see more experi"], "labels": ["FBK", "SMY", "SMY", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors tested out this new activation function on rnn", "it would be interesting to see the results of the new activation function on lstm", "the experimental results are fairly weak compared to the other methods that also uses many lay", "for ptb and text the results are comparable to recurrent batchnorm with similar number of parameters however the recurrent batchnorm model has only  layer whereas the proposed architecture has  lay", "it would also be nice to show results on tasks that involve long term dependencies such as speech model"], "labels": ["SMY", "SUG", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the authors could test out the new activation function on lstms it would be interesting to perform a comparison between lstm baseline lstm + new activation function lstm + recurrent batch norm", "it would be nice to see the gradient flow with the new activation function compared to the ones without", "the theorems and proofs are rather preliminary they may not necessarily have to be presented as theorem", "this paper investigates identity space learning with well-controlled variations using an artistic portraits dataset", "especially the authors propose a visual turing test to evaluate the synthesize quality of three generative models wgan-gp dfc-vae and pixel va"], "labels": ["SUG", "SUG", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the submission has following pros+ the proposed visual turing test provides a novel solution to evaluate the generation qu", "the test not only distinguishes real from synthesized faces but also evaluates the observer ability by determining whether the observer is a human", "this is a merit compared with existing protocols used in generation evalu", "+ the generated face images are very impressive especially the improved x-pixel output", "+ the paper presents a promising application in police composite sketching which can significantly improve human-in-the-loop search in face model"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the submission also suffers from multiple cons- the novelty of this paper is limit", "the only novelty i can pinpoint is the proposed visual turing test", "the only novelty i can pinpoint is the proposed visual turing test", "the dataset as well as all investigated models/approaches are existing work", "the visual turing test is interesting but not concrete enough to support an iclr publ"], "labels": ["CRT", "DFT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- a very small dataset  subjects and  images is used in the whole investig", "- a very small dataset  subjects and  images is used in the whole investig", "it is doubtful that the conclusion or results obtained in this small dataset could be scaled up to real-world applications or datasets millions of subjects and imag", "it is doubtful that the conclusion or results obtained in this small dataset could be scaled up to real-world applications or datasets millions of subjects and imag", "it would be favorable to empirically prove this by designing additional experi"], "labels": ["DFT", "CRT", "DFT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- missing detail", "ain section  how to use formal method ledig et al  to enlarge the portrait from x to x is unclear", "b lacking details of the model setups and training strategi", "the generation models are usually highly sensitive to details set", "the readers can hardly reproduce the results or evaluate possible performance by reading the pap"], "labels": ["DFT", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["c if the paper length is limited a supplementary material about those details would be pref", "- typosa page  figure  shows the seeds and example images for  round", "---- figure  should be figure b page  yet is is unclear how many pixels are required---- yet is i", "this paper presents several theoretical results regarding the expressiveness and learnability of relu-activated deep neural network", "i summarize the main results as below any piece-wise linear function can be represented by a relu-acteivated dnn"], "labels": ["SUG", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["any smooth function can be approximated by such network", "the expressiveness of -layer dnn is stronger than any -layer dnn", "using a polynomial number of neurons the relu-acteivated dnn can represent a piece-wise linear function with exponentially many piec", "the relu-activated dnn can be learnt to global optimum with an exponential-time algorithm", "among these results    are sort of known in the literatur"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper extends the existing results in some subtle way", "for  the authors show that the dnn has a tighter bound on the depth", "for  the hard functions has a better parameterization and the gap between -layer and -layer is proved bigg", "for  although the algorithm is exponential-time it guarantees to compute the global optimum", "the stronger results of    all rely on the specific piece-wise linear nature of relu"], "labels": ["DIS", "SMY", "DIS", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["other than that i don't get much more insight from the theoretical result", "when the input dimension is n the representability result of  fails to show that a polynomial number of neurons is suffici", "perhaps an exponential number of neurons is necessary in the worst case but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough", "result  is more interesting as it is a new result", "the authors present a constructive proof to show that relu-activated dnn can represent many linear piec"], "labels": ["DFT", "CRT", "SUG", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the construction seems artificial and these functions don't seem to be visually very complex", "overall this is an incremental work in the direction of studying the representation power of neural network", "the results might be of theoretical interest", "but i doubt if a pragmatic relu network user will learn anything by reading this pap", "- paper summarythe paper proposes a label-conditional gan generator architecture and a gan training objective for the image modeling task"], "labels": ["CRT", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed gan generator consists of two components where one focuses on generating foreground while the other focuses on generating background", "the gan training objective function utilizing  conditional classifi", "it is shown that through combining the generator architecture and the gan training objective function one can learn a foreground--background decomposed generative model in an unsupervised mann", "it is shown that through combining the generator architecture and the gan training objective function one can learn a foreground--background decomposed generative model in an unsupervised mann", "the paper shows results on the mnist svhn and celebrity faces dataset"], "labels": ["SMY", "DIS", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- poor experimental validationwhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised mann", "- poor experimental validationwhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised mann", "it is clear how this capability can help practical applications especially no such examples are shown in the pap", "the paper also fails to provide any quantitative evaluation of the proposed method", "for example the paper will be more interesting if inception scores were shown for various challenging dataset"], "labels": ["SMY", "DFT", "APC", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the paper will be more interesting if inception scores were shown for various challenging dataset", "in additional there is no ablation study analyzing impacts of each design choic", "as a result the paper carries very little scientific valu", "i liked this paper mostly because it surprised me and because it might spur the development of novel variants of difference target-propagation dtp", "the paper does a good job of highlighting the relevant background and issues and introduces a slight variation to dtp which actually works as well while being more biologically plaus"], "labels": ["DFT", "DFT", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there was a concern or assumption in the original dtp paper about the target for the penultimate layer before the output layer which seems to have been excessive ie the dtp propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it at least according to these experi", "in call cases the variant using the dtp target update everywhere works about as well as using the true gradient for the output lay", "another quirk that the proposed variant sdtp removes from the orignal dtp paper is the way noise is handled and i agree that denoising makes a lot of sense than noise preservation while being more biologically plaus", "finally the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop", "the paper is very clear and i have just outlined the original contributions and significance dtp may have been a bit forgotten and is worth another look appar"], "labels": ["DIS", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the negatives the paper should mention in the discussion and intro that all the tp variants ignore the issue of dynam", "we know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward which seems not very plaus", "in the experimental results section it would be good to report the cnn results as well with shared weights same architectur", "also training errors should be shown since i suspect that underfitting may be happening especially in the case of imagenet", "if that was the case future work should first explore higher capacity which may require larger-memory gpu"], "labels": ["SUG", "CRT", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally in the description of architectures please define the structure notation eg  x    sam", "the authors present a testing framework for deep rl methods in which difficulty can be controlled along a number of dimensions including reward delay reward sparsity episode length with terminating rewards binary vs real rewards and perceptual complex", "the authors then experiment with a variety of td and mc based deep learners to explore which methods are most robust to increases in difficulty along these dimens", "the key finding is that mc appears to be more robust than td in a number of ways and in particular the authors link this to domains with greater perceptual challeng", "this is a well motivated and explained paper in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuit"], "labels": ["SUG", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors discover some interesting characteristics of mc based deep-rl which may influence future work in this area and dig down a little to uncover the principles a littl", "the testing framework will be made public too which adds to the value of this pap", "i recommend the paper for acceptance and expect it will garner interest from the commun", "detailed comments  u [p basic health gathering task] the goal is to survive and maintain as much healthas possible by collecting health kit", "the reward is + when the agent collects a health kit and  otherwis"], "labels": ["APC", "APC", "FBK", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the reward suggests that the goal is to collect as many health kits as possible for which surviving and maintaining health are secondari", "u [p delayed rewards] it might be interesting to have a delay sampled from a distribution with some known mean", "otherwise the structure of the environment might support learning even when the reward delay would otherwise not", "u [p sparse rewards] i am not sure it is fair to say that the general difficulty is kept fix", "rather the average achievable reward for an oracle that knows whether health packs are is fix"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u [p] dosovitskiy & koltun  have not tested dfp on atari gam", "probably fairer/safer to say did not report results on atari gam", "the authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming languag", "in this setting they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable nam", "conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketch"], "labels": ["DIS", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["uapositivesuatutnovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposedtutvery good abstract", "it explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive result", "tutexcellently structured and presented paperuatut", "motivation given in form of relevant applications and mention that it is relatively unstudiedtut", "the hypothesis/ the papers goal is clearly st"], "labels": ["APC", "SMY", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is introduced with 'we ask' followed by two well formulated lines that make up the hypothesi", "it is repeated multiple times throughout the pap", "every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners clearly stating discrep", "tutexplanations are exceptionally well done terms that might not be familiar to the reader are explain", "this is true for mathematical aspects as well as program generating specific term"], "labels": ["APC", "CRT", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["examples are given where appropriate in a clear and coherent mannertut", "problem statement well defined mathematically and understandable for a broad audiencetut", "mentioning of failures and limitations demonstrates a realistic  view on the projecttut complexity and time analysis providedtut", "paper written so that it's easy for a reader to implement the methodstut", "detailed descriptions of all instantiations even parameters and comparison methodstutsystem specifiedtutvalidation method specifiedtutdata and repositori"], "labels": ["APC", "APC", "SMY", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as well as cleaning process providedtutevery figure and plot is well explained and interpretedtut", "large successful evaluation section providedtut", "many different evaluation measures defined to measure different properties of the projecttut", "different observability modestutevaluation against most compatible methods from other sources tut", "results are in line with hypothesistutthorough appendix clearing any open questions ua"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would have been good to have a summary/conclusion/future work sectionuasummary accept", "the authors present a very intriguing novel approach that  in a clear and coherent way", "the approach is thoroughly explained for a large audi", "the task itself is interesting and novel", "the large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promis"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even though no conclusive section is provid", "the paper is not missing any inform", "this paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition which is similar to yogatama et ", "differently from yogatama et al  this paper doesnut use reinforcement learning to induce a hard structure but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way", "overall i think it is really an interesting direction and the proposed method sounds reason"], "labels": ["CRT", "SMY", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am concerned about the following points  - the improvements are really limited on both the snli and the reverse dictionary task", "yogatama et al  demonstrate results on  tasks and i think itud be helpful to present results on a diverse set of tasks and see if conclusions can generally hold", "also it would be much better to have a direct comparison to yogatama et al  including the performance and also the induced tree structur", "- the computational complexity of this model shouldnut be neglect", "if i understand it correctly the model needs to compute on^ lstm composit"], "labels": ["CRT", "APC", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be at least discussed in the pap", "this should be at least discussed in the pap", "and i am not also sure how hard this model is being converged in all experiments compared to lstm or supervised tree-lstm", "n- i am wondering about the effects of the temperature parameter t is that important for train", "minor- what is the difference between lstm and left-branching lstm"], "labels": ["SUG", "DIS", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i am not sure if the attention overt chart is a highlight of the paper or not", "if so better move that part to the models section instead of mention it briefly in the experiments sect", "also if any visualization over the chart can be provided thatud be helpful to understand what is going on", "the paper addresses an interesting problem of dnn model compress", "the main idea is to combine the approaches in han et al  and ullrich et al  to get a loss value constrained k-means encoding method for network compress"], "labels": ["DIS", "SUG", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["an iterative algorithm is developed for model optim", "experimental results on mnist cifar- and svhn are reported to show the compression perform", "the reviewer would expect papers submitted for review to be of publishable qu", "however this manuscript is not polished enough for publication it has too many language errors and imprecisions which make the paper hard to follow", "in particular there is no clear definition of problem formulation and the algorithms are poorly presented and elaborated in the context"], "labels": ["SMY", "SMY", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros - the network compression problem is of general interest to iclr audi", "cons- the proposed approach follows largely the existing work and thus its technical novelty is weak", "- paper presentation quality is clearly below the standard", "- empirical results do not clearly show the advantage of the proposed method over state-of-the-art", "summary and significance the authors prove that for expressing simple multivariate monomials over n variables networks of depth  require expn many neurons whereas networks of depth n can represent these monomials using only on neuron"], "labels": ["APC", "CRT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks and quantifying the improvement provided by depth", "+vesexplaining the power of depth in nns is fundamental to an understanding of deep learn", "the paper is very easy to follow", "and the proofs are clearly written", "the theorems provide exponential gaps for very simple polynomial funct"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-ves my main concern with the paper is the novelty of the contribution to the techniqu", "the results in the paper are more general than that of lin et al but the proofs are basically the same and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new idea", "the second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives same requirements as for the results of lin et ", "finally in prop  reducing from uniform approximations to taylor approximations the inequality |eubx| = ub^d+ |nx - px| does not follow from the definition of a taylor approxim", "despite these criticisms i contend that the significance of the problem and the clean and understandable results in the paper make it a decent paper for iclr"], "labels": ["CRT", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification lay", "numerical experiments show that such sparse networks can have similar performance to fully connected on", "they introduce a concept of ucscatterud that correlates with network perform", "although  i found the results useful and potentially promis", "i did not find much insight in this pap"], "labels": ["SMY", "DIS", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it was not clear to me why scatter the way it is defined in the paper would be a useful performance proxy anywhere but the first classification lay", "once the signals from different windows are intermixed how do you even define the window", "minorsecond line of section  uclesserud - less or few", "the paper proposes improving performance of large rnns by combing techniques of model pruning and persistent kernel", "the authors further propose model-pruning optimizations which are aware of the persistent implement"], "labels": ["CRT", "QSN", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's not clear if the paper is relevant to the iclr audience due to its emphasize on low-level optimization which has little insight in learning represent", "the exposition in the paper is also not well-suited for people without a systems background although i'll admit i'm mostly using myself as a proxy for the average machine learning researcher her", "for instance the authors could do more to explain lamport timestamps than a  cit", "modulo problems of relevance and expected audience the paper is well-written and presents useful improvements in performance of large rnns and the work has potential for impact in industrial applications of rnn", "the work is clearly novel and the contributions are clear and well-justified using experiments and abl"], "labels": ["FBK", "CRT", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a dnn called subspace network for nonlinear multi-task censored regression problem", "the topic is important experiments on real data show improvements compared to several traditional approach", "my major concerns are as follow", "the paper is not self-contain", "the authors claim that they establish both asymptotic and non-asymptotic convergence properties for algorithm"], "labels": ["SMY", "APC", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however for some key steps in the proof they refer to other refer", "if this is due to space limitation in the main text they may want to provide a complete proof in the appendix", "the experiments are unconvinc", "they compare the proposed sn with other traditional approaches on a very small data  set with  samples and  featur", "they compare the proposed sn with other traditional approaches on a very small data  set with  samples and  featur"], "labels": ["CRT", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a major merit of dnn is that it can automatically extract useful featur", "however in this experiment the features are handcrafted before they are fed into the model", "thus i would like to see a comparison between sn with vanilla dnn", "this is a good paper makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detect", "it demonstrates clear performance improvement via comprehensive comparison with state-of-the-art method"], "labels": ["CRT", "CRT", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is the number of gaussian mixtures 'k' a hyper-parameter in the training process", "can it be a trainable paramet", "also it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework such as what kind of data points normal and anomalous are moving apart due to the joint learn", "learning sparse neural networks through l regularis", "nsummary the authors introduce a gradient-based approach to minimise an objective function with an l sparse penalti"], "labels": ["QSN", "QSN", "SUG", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables representing whether a variable is present or not to an expectation over continuous variables inspired by earlier work from maddison et al iclr  where a similar transformation was used to learn over discrete variable prediction tasks with neural network", "the problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables representing whether a variable is present or not to an expectation over continuous variables inspired by earlier work from maddison et al iclr  where a similar transformation was used to learn over discrete variable prediction tasks with neural network", "here the application is to learn sparse feedforward networks in standard classification tasks although the framework described is quite general and could be used to impose l sparsity to any objective function in princip", "the method provides equivalent accuracy and sparsity to published state-of-the-art results on these dataset", "but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark standard training with dropout rather than through empirical testing against other implement"], "labels": ["SMY", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["prosthe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theori", "optimisation under l regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse paramet", "the work is put in context and related to some previous relaxation approaches to spars", "the method allows for sparsity to be learned during training rather than after training as in standard dropout approaches and this allows the algorithm to obtain significant per-iteration speed-ups which improves through train", "consthe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art method"], "labels": ["APC", "SMY", "DIS", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore the main advance is in terms of learning speed to obtain this similar perform", "however the learning speed-up is presented against a theoretical flops estimate per iteration for a similar network with dropout", "it would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures consideredeg does the proposed sparse learning method converge at the same rate as the oth", "i felt a more thorough experimental section would have greatly improved the work focussing on this learning speed aspect", "it was unclear how much tuning of the lambda hyper-parameter which tunes the sparsity would be required in a practical application since tuning this parameter would increase computation tim"], "labels": ["APC", "APC", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it might be useful to provide a full bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learn", "minor point it wasnut completely clear to me why the fact  is a variational approximation to a spike-and-slab is important appendix", "i donut see why the spike-and-slab is any more fundamental than the l norm prior in", "it is just more convenient in bayesian inference because it is an iid prior and potentially allows an informative prior over each paramet", "in the context here this didnut seem a particularly relevant addition to the pap"], "labels": ["APC", "APC", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an interesting paper focusing on building discrete reprentations of sequence by autoencod", "however the experiments are too weak to demonstrate the effectiveness of using discrete represent", "the design of the experiments on language model is problemat", "there are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax but the lack of comparisons to benchmarks is a critical defect of this pap", "generally continuous vector representations are more powerful than discrete ones but discreteness corresponds to some inductive biases that might help the learning of deep neural networks which is the appealing part of discrete representations especially the stochastic discrete represent"], "labels": ["SMY", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart", "the proposal of dsae might help evaluate the usage of the 'autoencoding function' cs but it is certainly not enough to convince peopl", "how is the performance if cs is replaced with the representations achieved from autoencoder variational autoencoder or simply the sentence vectors produced by language model", "the qualitative evaluation on 'deciperhing the latent code' is not enough eith", "in addition the language model part doesn't sound correct because the model cheated on seeing the further before predicting the words autoregress"], "labels": ["DFT", "DFT", "QSN", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one suggestion is to change the framework to variational auto-encoder otherwise anything related to perplexity is not correct in this cas", "overall this paper is more suitable for the workshop track", "it also needs a lot of more studies on related work", "the authors show empirically that formulating multitask rl itself as an active learning and ultimately as an rl problem can be very fruit", "they design and explore several approaches  to the active learning or active sampling problem from a basic change to the distribution to ucb to feature-based neural-network based rl"], "labels": ["SUG", "FBK", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the domain is video gam", "all proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks one multitask  problem had  task", "pros- very promising results with an interesting active learning approach to multitask rl", "- a number of approaches developed for the basic idea", "- a variety of experiments on challenging multiple task problems up to  tasks/gam"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- paper is overall well written/clear", "cons- comparison only to a very basic baseline ie uniform sampl", "couldn't comparisons be made in some way to other multitask work", "additional  comments- the assumption of the availability of a target score goes againstthe motivation that one need not learn individual network", "authorssay instead one can use 'published' scores but that only assumessomeone else has done the work and furthermore published it!"], "labels": ["APC", "CRT", "QSN", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors do have a section on eliminating the need by doubling anestimate for each task which makes this work more acceptable shownfor  tasks or mt compared to baseline uniform sampl", "clearly there is more to be done here for a future direction could bementioned in future work sect", "- the averaging metrics geometric harmonic vs arithmetic whether  or not to clip max score achieved are somewhat interesting but in  the main paper i think they are only used in section  seems like  a waste of spac", "consider moving some of the results on showing  drawbacks of arithmetic mean with no clipping table  in appendix e from the appendix to  the main pap", "- the can be several benefits to multitask learning in particular  time and/or space savings in learning new tasks via learning more  general featur"], "labels": ["APC", "CRT", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sections  and  on specificity/generality of  features were interest", "-- can the authors show that a trained network via their multitask    approached learns significantly faster on a brand new game    that's similar to games already trained on compared to learning from    scratch", "-- how does the performance improve/degrade or the variance on the    same set of tasks if the different multitask instances mt_i    formed a supersets hierarchy ie if mt_ contained all the    tasks/games in mt_ could training on mt_ help average    performance on the games in mt_", "could go either way since the network   has to allocate resources to learn other games too", "but is there a pattern"], "labels": ["APC", "QSN", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- 'figure ' in section  refers to figur", "- can you motivate/discuss better why not providing the identity of a  game as an input is an advantag", "why not explore both  poss", "what are the pros/cons sect", "the paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it"], "labels": ["DIS", "QSN", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energi", "the authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problem", "however since the complexity of such a problem is exponential the authors propose a collection of heuristics/approximations to solve the problem", "these include a heuristic for setting the targets at each layer using a soft hinge loss mini-batch training and such", "using these modifications the authors propose an algorithm algorithm  in appendix to train such models effici"], "labels": ["APC", "APC", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator sste on a couple of datasets namely cifar- and imagenet", "they show superiority of their algorithm over sst", "i thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard threshold", "the authors formulation of the problem as one of combinatorial optimization and proposing algorithm  is also quite interest", "the results are moderately convincing in favor of the proposed approach"], "labels": ["DIS", "DIS", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though a disclaimer here is that i'm not % sure that sste is the state of the art for this problem", "overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research commun", "there are a few flaws/weaknesses in the paper though making it somewhat los", "- the authors start of by posing the problem as a clean combinatorial optimization problem and propose algorithm", "realizing the limitations of the proposed algorithm given the assumptions under which it was conceived in the authors relax those assumptions in the couple of paragraphs before section  and pretty much throw away all the nice guarantees such as checks for feasibility discussed earli"], "labels": ["CRT", "APC", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the result of this is another algorithm i guess the main result of the paper which is strangely presented in the appendix as opposed to the main text which has no such guarante", "- there is no theoretical proof that the heuristic for setting the target is a good one other than a rough intuit", "- the authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach", "the entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit", "this paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output exampl"], "labels": ["CRT", "CRT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured output", "in addition the paper assembles a template-based synthetic dataset of task descriptions and program", "results show that a seqtree model outperforms a seqseq model that adding search to seqtree improves result", "and that search without any training performs worse although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a techniqu", "strengths- reasonable approach quality is good"], "labels": ["SMY", "SMY", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the dsl is richer than that of previous related work like balog et ", "- results show a reasonable improvement in using a seqtree model over a seqseq model which is interest", "weaknesses- there are now several papers on using a trained neural network to guide search and this approach doesn't add too much on top of previous work", "using beam search on tree outputs is a bit of a minor contribut", "- the baselines are just minor variants of the proposed method"], "labels": ["APC", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be stronger to compare against a range of different approaches to the problem particularly given that the paper is working with a new dataset", "- data is synthetic and it's hard to get a sense for how difficult the presented problem is as there are just four example problems given", "questions- why not compare against seqseq + search", "- how about comparing wall time against a traditional program synthesis technique ie no machine learning ignoring the descript", "i would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth  which makes me skeptical that figure  is a fair representation of how well a non neural network-based search could do"], "labels": ["SUG", "CRT", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth  which makes me skeptical that figure  is a fair representation of how well a non neural network-based search could do", "- are there plans to release the dataset", "could you provide a large sample of the data at an anonymized link", "i'd re-evaluate my rating after looking at the data in more detail", "this paper examines the nature of convolutional filters in the encoder and a decoder of a vae and a generator and a discriminator of a gan"], "labels": ["DIS", "QSN", "QSN", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors treat the inputs x and outputs y of each filter throughout each step of the convolving process as a time series which allows them to do a discrete time fourier transform analysis of the resulting sequ", "by comparing the power spectral density of the input and the output they get a spectral dependency ratio sdr ratio that characterises a filter as spectrally independent neutral correlating amplifies certain frequencies or anti-correlating dampens frequ", "this analysis is performed in the context of the independence of cause and mechanism icm framework", "the authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in vae and gan whereby the former are anti-causal and the latter are causal in line with the icm framework", "they also claim that this analysis can be used to improve the performance of the model"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros-- sdr characterisation of the convolutional filters is interest", "-- the authors show that filters with different characteristics are responsible for different aspects of image model", "-- the authors show that filters with different characteristics are responsible for different aspects of image model", "cons-- the authors do not actually demonstrate how their analysis can be used to improve vaes or gan", "-- their proposed sdr analysis does not actually find much difference between the generator and the discriminator of the gan"], "labels": ["APC", "SMY", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- the clarity of the writing could be improved eg the discussion in section  seems inaccurate in the current form", "grammatical and spelling mistake are frequ", "more background information could be helpful in sect", "all figures but in particular figure  need more informative capt", "all figures but in particular figure  need more informative capt"], "labels": ["SUG", "CRT", "SUG", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- the authors talk a lot about disentangling in the introduction but this does not seem to be followed up in the rest of the text", "furthermore they are missing a reference to beta-vae higgins et al  when discussing vae-based approaches to disentangled factor learn", "in summary the paper is not ready for publication in its current form", "the authors are advised to use the insights from their proposed sdr analysis to demonstrate quantifiable improvements the vaes/gan", "summarythe paper presents an interesting view on the recently proposed maml formulation of meta-learning finn et "], "labels": ["CRT", "CRT", "FBK", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main contribution is a insight into the connection between the maml procedure and map estimation in an equivalent linear hierarchical bayes model with explicit prior", "b insight into the connection between maml and map estimation in non-linear hb models with implicit prior", "c based on these insights the paper proposes a variant of malm using a laplace approximation with additional approximations for the covariance matrix", "the paper finally provides an evaluation on the mini imagenet problem without significantly improving on the maml results on the same task", "pro-            the topic is timely and of relevance to the iclr community continuing a current trend in building meta-learning system for few-shot learn"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-            provides valuable insight into the maml objective and its relation to probabilistic model", "con-            the paper is generally well-written", "but i find as a non-meta-learner expert that certain fundamental aspects could have been explained better or in more detail see below for detail", "-            the toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method compared to eg maml", "-            i do not think the empirical results provide enough evidence that it is a useful/robust method"], "labels": ["APC", "APC", "DIS", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-            i do not think the empirical results provide enough evidence that it is a useful/robust method", "especially it does not provide insight into which types of problems small/large linear/ non-linear the method is applicable to", "detailed comments/questions-            the use of laplace approximation is in the paper motivated from a probabilistic/bayes and uncertainty point-of-view", "it would however seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mod", "could the authors perhaps comment ona whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization given the psd approximation to the hessian or does it only make sense after converg"], "labels": ["CRT", "CRT", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b the consequence of the approximation errors on the general convergence of the proposed method consistency and rate-", "sec  p last equation perhaps useful to explain the term $logphi_j^* | theta$ and why it is not in subroutin", "should $phi^*$  be $hat phi$", "-            sec  uca straightforwarduud i think it would improve readability to refer back to the to the previous equation ie h such that it is clear what is meant by ucstraightforwardud-", "sec  several ideas are being discussed in sec  and it is not entirely clear to me what has actually been adopted here perhaps consider formalizing the actual computations in subroutine  u and provide a clearer argument preferably proof that this leads to consistent and robust estimator of theta"], "labels": ["DIS", "DIS", "QSN", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-            it is not clear from the text or experiment how the learning parameters are set", "-            sec  it took some effort to understand exactly what was going on in the example and particular figure  eg in the model definition in the body text there is no mention of the nn mentioned/used in figure  the blue points are not defined in the caption the terminology eg  ucpre-update densityud is new at this point", "i think it would benefit the readability to provide the reader with a bit more guid", "-            sec  while the qualitative example is useful with a bit more text i believe it would have been more convincing with a quantitative example to demonstrate eg the convergence of the proposal compared to std maml and possibly compare to a std bayesian inference method from the hb formulation of the problem in the linear cas", "-            sec  the abstract clams increased performance over maml but the empirical results do not seem to be significantly better than maml"], "labels": ["CRT", "CRT", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the signific", "-            sec  the authors have left out ucmishral et alud from the comparison due to the model being significantly larger than oth", "could the authors provide insight into why they did not use the resnet structure from the  tcml paper in their l-mlma schem", "-            sec + the paper clearly states that it is not the aim to generally formulate the maml as a hb", "given the advancement in gradient based inference for hb the last couple of years eg variational nested laplace  expectation propagation etc for explicit models could the authors perhaps indicate why they believe their approach of looking directly to the maml objective is more scalable/useful than trying to formulate the same or similar objective in an explicit hb model and using established inference methods from that area"], "labels": ["QSN", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor-            sec  ucueach integral in the sum in uud eq  is a product", "this paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable perform", "however i have the following concerns on novelti", "although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model it does not give new formulation or algorithm to handle domain adapt", "although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model it does not give new formulation or algorithm to handle domain adapt"], "labels": ["CRT", "SMY", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at this point the novelty is weaken", "in the introduction the authors mentioned uclimitations of msda is that it needs to explicitly form the covariance matrix of input features and then solves a linear system which can be computationally expensive in high dimensional set", "ud however msda cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input featur", "it is not clear why msda cannot handle time-series data but dauto can", "dauto does not consider the sequence/ordering of data eith"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if my understanding is not wrong the proposed dauto is just a simple combination of three losses ie prediction loss reconstruction loss domain difference loss", "as far as i know this kind of loss is commonly used in most existing method", "the paper adds to the discussion on the question whether generative adversarial nets gans learn the target distribut", "recent theoretical analysis of gans by arora et al show that of the discriminator capacity of is bounded then there is a solution the closely meets the objective but the output distribution has a small support", "the paper attempts to estimate the size of the support for solutions produced by typical gans experiment"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main idea used to estimate the support is the birthday theorem that says that with probability at least / a uniform sample with replacement of size s from a set of  n elements will have a duplicate given s  sqrt{n}", "the suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists then estimate the size of the support to be s^", "one should note that the birthday theorem assumes uniform sampl", "in the revised versions it has been clarified that the tested distribution is not assumed to be uniform but the distribution has effectively small support size using an indistinguishability not", "given this method to estimate the size of the support the paper also tries to study the behaviour of estimated support size with the discriminator capac"], "labels": ["SMY", "DIS", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["arora et al showed that the output support size has nearly linear dependence on the discriminator capac", "experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described abov", "a result similar to that of arora et al is also given for the special case of encoder-decoder gan", "evaluation significance the question whether gans learn the target distribution is important and any  significant contribution to this discussion is of valu", "clarity the paper is written well and the issues raised are well motivated and proper background is given"], "labels": ["SMY", "DIS", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality the main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new", "quality the main idea of this work is to give a estimation technique for the support size for the output distribution of gan", "summarythis paper proposes a framework for private deep learning model inference using fhe schemes that support fast bootstrap", "the main idea of this paper is that in the two-party computation setting in which the client's input is encrypted while the server's deep learning model is plain", "this hybrid argument enables to reduce the number of necessary bootstrapping and thus can reduce the computation tim"], "labels": ["APC", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper gives an implementation of adder and multiplier circuits and uses them to implement private model infer", "comments i recommend the authors to tone down their claim", "for example the authors mentioned that there has been no complete implementation of established deep learning approaches in the abstract however the authors did not define what is complet", "actually the secureml paper in s&p' should be able to privately evaluate any neural networks although at the cost of multi-round information exchanges between the client and serv", "also the claim that we show efficient designs is very thin to me since there are no experimental comparisons between the proposed method and existing work"], "labels": ["SMY", "SUG", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["actually the level fhe can be very efficient with a proper use of message packing technique such as [a] and [c]", "for a relatively shallow model as this paper has used level fhe might be faster than the binary fh", "i recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is bett", "i think the hybrid argument ie when one input wire is plain is a very common trick that used in the circuit design field such as garbled circuit [b] to reduce the depth of the circuit", "i appreciate that optimizations such as low-precision and point-wise convolution are discussed in this pap"], "labels": ["DIS", "SUG", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["such optimizations are very common in deep learning field while less known in the field of secur", "[a] dowlin et al cryptonets applying neural networks to encrypted data with high throughput and accuraci", "[b] v kolesnikov et al improved garbled circuit free xor gates and appl", "[c] liu et al oblivious neural network predictions via minionn transform", "this paper presents a model-based approach to variance reduction in policy gradient method"], "labels": ["DIS", "SUG", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the basic idea is to use a multi-step dynamics model as a baseline more properly a control variate as the terminology in the paper uses but i think baselines are more familiar to the rl community to reduce the variance of a policy gradient estimator while remaining unbias", "the authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem essentially using off-policy data via importance weighting and they demonstrate the effectiveness of the approach on four continuous control task", "this paper presents a nice idea and i'm sure that with some polish it will become a very nice conference submiss", "this paper presents a nice idea and i'm sure that with some polish it will become a very nice conference submiss", "but right now at least as of the version i'm reviewing the paper reads as being half-finish"], "labels": ["DFT", "SMY", "DFT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["several terms are introduced without being properly defined and one of the key formalisms presented in the paper the idea of embedding an imaginary trajectory remains completely opaque to m", "further the paper seems to simply leave out some portions the introduction claims that one of the contributions is we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates but i see literally no section that hints at anything like this no mention of dynamic unfolding or latent space trajectory embedding ever occurs later in the pap", "further the paper seems to simply leave out some portions the introduction claims that one of the contributions is we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates but i see literally no section that hints at anything like this no mention of dynamic unfolding or latent space trajectory embedding ever occurs later in the pap", "in a bit more detail the key idea of the paper at least to the extent that i understood it was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term", "but because unlike traditional baselines introducing it alone would affect the actual estimate they actually just add and subtract this term and separate out the two terms in the policy gradient the new policy gradient like term will be much smaller and the other term can be computed with less variance using model-based methods and the reparameterization trick"], "labels": ["DFT", "DFT", "CRT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but because unlike traditional baselines introducing it alone would affect the actual estimate they actually just add and subtract this term and separate out the two terms in the policy gradient the new policy gradient like term will be much smaller and the other term can be computed with less variance using model-based methods and the reparameterization trick", "but beyond this and despite fairly reasonable familiarity with the subject i simply don't understand other elements that the paper is talking about", "the paper frequently refers to embedding imaginary trajectories into the dynamics model and i still have no idea what this is actually referring to the definition at the start of section  is completely opaque to m", "i also don't really understand why something like this would be needed given the understanding above but it's likely i'm just missing something her", "but i also feel that in this case it borders on being an issue with the paper itself as i think this idea needs to be described much more clearly if it is central to the underlying pap"], "labels": ["CRT", "CRT", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i also feel that in this case it borders on being an issue with the paper itself as i think this idea needs to be described much more clearly if it is central to the underlying pap", "finally although i do think the extent of the algorithm that i could follow is interesting the second issue with the paper is that the results are fairly weak as they stand curr", "the improvement over trpo is quite minor in most of the evaluated domains other than possibly in the swimmer task even with substantial added complexity to the approach", "and the experiments are described with very little detail or discussion about the experimental setup", "nor are either of these issues simply due to space constraints the paper is  pages under the soft iclr limit with no appendix"], "labels": ["CRT", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not that there is anything wrong with short papers but in this case both the clarity of presentation and details are lack", "my honest impression is simply that this is still work in progress and that the write up was done rather hastili", "i think it will eventually become a good paper but it is not ready yet", "this paper proposes an interesting variational posterior approximation for the weights of an rnn", "the paper also proposes a scheme for assessing the uncertainty of the predictions of an rnn"], "labels": ["DFT", "DFT", "SUG", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros--i liked the posterior sharpening idea", "it was well motivated from a computational cost perspective hence the use of a hierarchical prior", "--i liked the uncertainty analysi", "there are many works on bayesian neural networks but they never present an analysis of the uncertainty introduced in the weight", "these works can benefit from the uncertainty analysis scheme introduced in this pap"], "labels": ["APC", "APC", "APC", "CNT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--the experiments were well carried through", "cons--change the title! the title is too vagu", "bayesian recurrent neural networks already exist and is rather vague for what is being described in this pap", "bayesian recurrent neural networks already exist and is rather vague for what is being described in this pap", "--there were a lot of unanswered questions  how does sharpening lead to lower vari"], "labels": ["APC", "DFT", "DFT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--there were a lot of unanswered questions  how does sharpening lead to lower vari", "this was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment sect", "how is the level of uncertainty related to perform", "it would have been insightful to see effect of sigma_ on the performance rather than report the best result", "what was the actual computational cost for the bbb rnn and the baselin"], "labels": ["QSN", "DFT", "QSN", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what was the actual computational cost for the bbb rnn and the baselin", "--there were very minor typos and some unclear connot", "for example there is no such thing as a variational bayes model", "i am willing to adjust my rating when the questions and remarks above get address", "the description of the proposed method is very unclear"], "labels": ["QSN", "CRT", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from the paper it is very difficult to make out exactly what architecture is propos", "i understand that the prior on the z_i in each layer is a pixel-cnn but what is the posterior", "equations  and  would suggest it is of the same form pixel-cnn but this would be much too slow to sample during train", "i'm guessing it is just a factorized gaussian with a separate factorized gaussian pseudo-prior", "that is in figure  all solid lines are factorized gaussians and all dashed lines are pixel-cnn"], "labels": ["CRT", "DFT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the word layers is sometimes used to refer to latent variables z and sometimes to parameterized neural network layers in the encoder and decod", "eg the top stochastic layer z_l in fame is a fully-connected dense lay", "no z_l is a vector of latent vari", "are you saying the encoder produces it using a fully-connected lay", "* section  starts talking about deterministic layers h"], "labels": ["DIS", "DIS", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are these part of the encoder or decod", "what is meant by number of layers connecting the stochastic latent vari", "* section  what is meant by reconstruction data", "if my understanding of the method is correct the novelty is limit", "autoregressive priors were used previously in eg the lossy vae by chen et al and iaf-vae by kingma et "], "labels": ["QSN", "QSN", "QSN", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the reported likelihood results are very impressive though and would be reason for acceptance if correct", "however the quality of the sampled images shown for cifar- doesn't match the reported likelihood", "there are multiple possible reasons for this but after skimming the code i believe it might be due to a faulty implementation of the variational lower bound", "instead of calculating all quantities in the log domain the code takes explicit logs and exponents and stabilizes them by adding small quantities eps this is not guaranteed to give the right result", "please fix this and re-run your experiments ie in _losspy don't use x/expy+eps but instead use x*exp-i"], "labels": ["FBK", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["don't use logvar+eps with var=softplusx but instead use var=softplusx+eps or parameterize the variance directly in the log domain", "this paper presents a pixel-matching based approach to synthesizing rgb images from input edge or normal map", "the approach is compared to isola et alus conditional adversarial networks and unlike the conditional gan is able to produce a diverse set of output", "overall the paper describes a computer visions system based on synthesizing images and not necessarily a new theoretical framework to compete with gan", "with the current focus of the paper being the proposed system it is interesting to the computer vision commun"], "labels": ["SUG", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however if one views the paper in a different light namely showing some ucblind-spotsud of current conditional gan approaches like lack of diversity then it can be of much more interest to the broader iclr commun", "pros overall the paper is well-written", "makes a strong case that random noise injection inside conditional gans does not produce enough divers", "shows a number of qualitative and quantitative result", "concerns about the paper it is not clear how well the proposed approach works with cnn architectures other than pixelnet"], "labels": ["APC", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since the paper used ucthe pre-trained pixelnet to extract surface normal and edge mapsud for ground-truth generation it is not clear whether the approach will work as well when the input is a ground-truth semantic segmentation map", "since the paper describes a computer-vision image synthesis system and not a new theoretical result i believe reporting the actual run-time of the system will make the paper strong", "can pixelnn run in real-tim", "how does the timing compare to isola et alus conditional gan", "minor comments the paper mentions making predictions from ucincompleteud input several times but in all experiments the input is an edge map normal map or low-resolution imag"], "labels": ["CRT", "SUG", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when reading the manuscript the first time i was expecting experiments on images that have regions that are visible and regions that are masked out", "however i am not sure if the confusion is solely mine or shared with other read", "equation  contains the norm operator twice and the first norm has no subscript while the second one has an l_ subscript", "equation  contains the norm operator twice and the first norm has no subscript while the second one has an l_ subscript", "i would expect the notation style to be consistent within a single equation ie use ||w||_^ ||w||^ or ||w||_{l_}^"], "labels": ["DIS", "DIS", "DFT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["table  has two sub-tables left and right the sub-tables have the ap column in different plac", "table  has two sub-tables left and right the sub-tables have the ap column in different plac", "ucdense pixel-level correspondencesud are discussed but not evalu", "the authors propose an extension to the neural statistician which can model contexts with multiple partially overlapping featur", "this model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between dataset"], "labels": ["SMY", "DIS", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a which leads to a disentangled represent", "this is also not dissimilar to ideas used in 'bayesian representation learning with oracle constraints' karaletsos et al  where similar contextual features c are learned to disentangle representations over observations and implicit supervis", "the authors provide a clean variational inference algorithm to learn their model", "however a key problem is the following the nature of the discrete variables being used makes them hard to be inferred with variational infer", "the authors mention categorical reparametrization as their trick of choice but do not go into empirical details int heir experiments regarding the success of this approach"], "labels": ["APC", "DIS", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact it would be interesting to study which level of these variables could be analytically collapsed such as done in the semi-supervised learning work by kingma et al  and which ones can be sampled effectively using a form of reparametr", "in fact it would be interesting to study which level of these variables could be analytically collapsed such as done in the semi-supervised learning work by kingma et al  and which ones can be sampled effectively using a form of reparametr", "this also touches on the main criticism of the paper while the model technically makes sense and is cleanly described and deriv", "the empirical evaluation is on the weak side and the rich properties of the model are not really shown off", "the empirical evaluation is on the weak side and the rich properties of the model are not really shown off"], "labels": ["SUG", "DIS", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy set", "can the model recover richer structure that was imposed during data gener", "how limiting is the learning of a", "how does the likelihood of the model behave under the circumst", "the experiments do not really convey how well this all will work in practic"], "labels": ["SUG", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper introduces a non-volume-preserving generalization of hmc whose transitions are determined by a set of neural network funct", "these functions are trained to maximize expected squared jump dist", "this works because each variable of the state space is modified in turn so that the resulting update is invertible with a tractable transformation inspired by dinh et ", "overall i believe this paper is of good quality clearly and carefully written and potentially accelerates mixing in a state-of-the-art mcmc method hmc in many practical cas", "a few downsides are commented on below"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental section proves the usefulness of the method on a range of relevant test cases in addition an application to a latent variable model is provided sec", "fig a presents results in terms of numbers of gradient evaluations but i couldn't find much in the way of computational cost of lhmc in the pap", "i can't see where the number x in sec  stems from", "as a user i would be interested in the typical computational cost of both mcmc sampler training and mcmc sampler usage inference compared to competing method", "this is admittedly hard to quantify objectively but just an order of magnitude would be helpful for orient"], "labels": ["APC", "DFT", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it be relevant in sec to compare to other methods than just hmc eg lahmc", "i am missing an intuition for several things eq the time encoding defined in appendix cappendix fig i cannot quite see how the caption claim is supported by the figure just hardly for vae but not for hmc", "the number x ess in sec seems at odds with the number in the abstract x", "# minor errors- sec the sampler is trained to minimize a variation should be maximizeas well as on a the real-world", "- sec and / v^t v the kinetic energy miss"], "labels": ["QSN", "CRT", "CRT", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- sec the acronym lhmc is not expanded anywhere in the pap", "the sentence we will denote the complete augmentedpd might be moved to after from a uniform distribution in the same paragraph", "in paragraph starting we now update x    - specify for clarity the first update which yields x' / the second update which yields x''", "- only affects $x_{bar{m}^t}$ should be $x'_{bar{m}^t}$  prime miss", "- the syntax using subscript m^t is confusing to read wouldn't it be clearer to write this as a function eg maskx'm^t"], "labels": ["CRT", "SUG", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the syntax using subscript m^t is confusing to read wouldn't it be clearer to write this as a function eg maskx'm^t", "- inside zeta_ and zeta_ do you not mean $m^t and $bar{m}^t$", "- inside zeta_ and zeta_ do you not mean $m^t and $bar{m}^t$", "- sec add reference for first mention of a nice mc", "- appendix a     - let's - let"], "labels": ["QSN", "DFT", "QSN", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- eq should be x''=-", "appendix c space missing after sect", "- appendix d in this section is presented  sounds odd", "n- appendix d presumably this should consist of the figure   maybe specifi", "this paper presents a reading comprehension model using convolutions and attent"], "labels": ["SUG", "DFT", "DFT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this model does not use any recurrent operation but it is not per se simpler than a recurrent model", "furthermore the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine transl", "on squad dataset their results show some small improvements using the proposed augmentation techniqu", "their best results however do not outperform the best results reported on the leader board", "overall this is an interesting study on squad dataset"], "labels": ["DIS", "APC", "DIS", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see results on more datasets and more discussion on the data augmentation techniqu", "at the moment the description in section  is fuzzy in my opinion", "interesting information could be- how is the performance of the nmt system", "- how many new data points are finally added into the training data set", "- what do udata augu x  or x  exactly mean"], "labels": ["DIS", "CRT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary of the paper this paper presents a method called alpha-dm the authors used this name because they are using alpha-divergence to measure the distance between two distribut", "that addresses three important problems simultaneously a objective score discrepancy ie in ml we minimize a cost function but we measure performance using something else eg minimizing cross entropy and then measuring performance using bleu score in machine translation mt", "b sampling distribution discrepancy the model is trained using samples from true distribution but evaluated using samples from the learned distribut", "c sample inefficiency the rl model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective functionus optim", "then the authors present the results for machine translation task and also analysis of their proposed method"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my comments / feedback the paper is well written and the problem addressed by the paper is an important on", "my main concerns about this work are have two aspects atnoveltytthe idea is a good one and is great incremental research building on the top of previous idea", "i do not agree with statements like ucwe demonstrate that the proposed objective function generalizes ml and rl objective functions uud that authors have made in the abstract there is not enough evidence in the paper to validate this stat", "btexperimental resultstthe performance of the proposed method is not significantly better than other models in mt task", "i am also wondering why authors have not tried their method on at least one more task"], "labels": ["APC", "APC", "CRT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["eg in cnn+lstm based image captioning the perplexity is minimized as cost function but the performance is measured by bleu etc", "some minor comments tin page  th line after eq  ucu these two problemsud -- ucu these three problemsud", "tin page  the line before the last line ucu resolbing problemud -- ucu resolving problemud", "this paper discusses several gradient based attribution methods which have been popular for the fast computation of saliency maps for interpreting deep neural network", "the paper provides several advances- epsilon-lrp and deeplift are formulated in a way that can be calculated using the same back-propagation as train"], "labels": ["DFT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- this gives a more unified way of understanding and implementing the method", "- the paper points out situations when the methods are equival", "- the paper analyses the methods' sensitivity to identifying single and joint regions of sensit", "- the paper proposes a new objective function to measure joint sensit", "overall i believe this paper to be a useful contribution to the literatur"], "labels": ["APC", "APC", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing method", "especially the latter will be appreci", "the authors discuss the regularized objective function minimized by standard sgd in the context of neural nets and provide a variational inference perspective using the fokker-planck equ", "they note that the objective can be very different from the desired loss function if the sgd noise matrix is low rank as evidenced in their experi", "overall the paper is written quite well and the authors do a good job of explaining their thesi"], "labels": ["APC", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i was unable to identify any real novelty in the theory the fokker-planck equation has been widely used in analysis of stochastic noise in mcmc samplers in recent years and this paper mostly rephrases those result", "also the fact that sgd theory only works for isotropic noise is well known and that there is divergence from the true loss function in case of low rank noise is obvi", "thus i found most of section  to be a reformulation of known results including theorem  and its proofsame goes for section  the symmetric- anti symmetric split is a common technique used in the stochastic mcmc literature over the last few years and i did not find any new insight into those manipulations of the fokker-planck equation from this pap", "thus i think that although this paper is written wel", "the theory is mostly recycled and the empirical results in section  are known thus it is below acceptance threshold due to lack of novelti"], "labels": ["CRT", "DIS", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theory is mostly recycled and the empirical results in section  are known thus it is below acceptance threshold due to lack of novelti", "the paper considers a problem of predicting hidden information in a pomdp with an application to starcraft", "authors propose a number of baseline models as well as metrics to assess the quality of ucdefoggingud", "i find the problem of defogging quite interesting even though it is a bit too starcraft-specific some findings could perhaps be translated to other partially observed environ", "authors use the dataset provided for starcraft brood war by lin et "], "labels": ["FBK", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my impression about the paper is that even though it touches a very interesting problem", "it neither is written well nor it contains much of a novelty in terms of algorithms methods or network architectur", "detailed comments* authors should at very least cite vinyals et al  and explain why the environment and the dataset released for starcraft  is less suited than the one provided by lin et ", "detailed comments* authors should at very least cite vinyals et al  and explain why the environment and the dataset released for starcraft  is less suited than the one provided by lin et ", "* problem statement in section  should certainly be improv"], "labels": ["APC", "CRT", "SUG", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors introduce rather heavy notation which is then used in a confusing way", "for example what is the top index in $s_t^{-p}$ supposed to mean", "the notation is not much used after sec  for example figure  does not use it", "* a related issue is that the definition of metrics is very informal and again does not use the already defined not", "including explicit formulas would be very helpful because for example it looks like when reported in table  the metrics are spatially averaged yet i could not find an explicit notion of that"], "labels": ["CRT", "QSN", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* authors seem to only consider deterministic defogging model", "however to me it seems that even in  game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it", "at least the concept of stochastic predictions should be discussed* the rule-based baselines are not described in detail", "what does ucusing game rules to infer the existence of unit typesud mean", "* another detail which i found missing is whether authors use just a screen a mini-map or both"], "labels": ["DIS", "CRT", "SUG", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the game of starcraft only screen contains information about unit-types but itus field of view is limit", "hence itus unclear to me whether a model should infer hidden information based on just a single screen + minimap observation or a history of them or due to how the dataset is constructed all units are observed without spatial limitations of the screen", "- this paper shows an equivalence between proto value functions and successor represent", "it then derives the idea of eigen options from the successor representation as a mechanism for option discoveri", "the paper shows that even under a random policy the eigen options can lead to purposeful opt"], "labels": ["DFT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i think this is an important conceptual pap", "automatic option discovery from raw sensors is perhaps one of the biggest open problems in rl research", "this paper offers a new conceptual setup to look at the problem and consolidates different views successor repr proto values eigen decomposition in a principled mann", "- i would be keen to see eigen options being used inside of the ag", "have authors performed any experi"], "labels": ["APC", "DIS", "APC", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- how robust are the eigen options for the atari experi", "basically how hand picked were the opt", "- is it possible to compute eigenoptions onlin", "this seems crucial for scaling up this approach", "the paper proposed a subgraph image representation and validate it in image classification and transfer learning problem"], "labels": ["QSN", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix", "the experimental results supports the claim", "it is very positive that the figures are very helpful for delivering the inform", "the work seems to be a little bit increment", "the proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix"], "labels": ["SMY", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a novelty of this work seems to be transforming a graph into an imag", "by the proposed representation the authors are able to apply image classification methods supervised or unsupervised to subgraph classif", "it will be better if the authors could provide more details in the methodology or framework sect", "the experiments on  networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based method", "it seem to be promising when using transfer learn"], "labels": ["APC", "APC", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the last two process figures in  can be improv", "no caption or figure number is provid", "it will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearbi", "for examplethe test example is correctly classified if and only if its ground truth matches cp", "we carry out this exercise  times and set n to    and  respect"], "labels": ["SUG", "CRT", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["psome minor issueszhu et al discuss heterogeneous transfer learning where in they usepeach label vector a tuple of label label-probability pairs incomplete sentencep", "-----update------the authors addressed my concerns satisfactorili", "given this and the other reviews i have bumped up my score from a  to a", "----------------------this paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples i adding a high temperature to the softmax and ii adding adversarial perturbations to the input", "this is a novel use of existing method"], "labels": ["CRT", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some roughly chronological comments followin the abstract you don't mention that the result given is when cifar- is mixed with tinyimagenet", "the paper is quite well written aside from some grammatical issu", "in particular articles are frequently missing from noun", "some sentences need rewriting eg in  which is as well used by hendrycks in  performance becomes unchang", "it is perhaps slightly unnecessary to give a name to your approach odin but in a world where there are hundreds of different kinds of gans you could be forgiven"], "labels": ["SMY", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm not convinced that the performance of the network for in-distribution images is unchanged as this would require you to be able to isolate % of the in-distribution imag", "i'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution eg by simply counting them as incorrect classif", "i'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution eg by simply counting them as incorrect classif", "would there be a correlation between difficult-to-classify images and those that don't appear to be in distribut", "would there be a correlation between difficult-to-classify images and those that don't appear to be in distribut"], "labels": ["DFT", "DIS", "QSN", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again", "in terms of experimentation it would be interesting to see the reciprocal of the results between two dataset", "for instance how would a network trained on tinyimagenet cope with out-of-distribution images from cifar section  felt out of place as to me the discussion section flowed more naturally from the experimental result", "this may just be a matter of tast", "ni did like the observations in  about class deviation although then what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution on"], "labels": ["DFT", "APC", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is in part addressed in the cifar  experiments in the appendic", "this appears to be a borderline paper as i am concerned that the method isn't sufficiently novel although it is a novel use of existing method", "pros- baseline performance is exceeded by a large margin- novel use of adversarial perturbation and temperature- interesting analysi", "cons- doesn't introduce and novel methods of its own- could do with additional experiments as mentioned above- minor grammatical error", "this paper introduces a new model to perform image classification with limited computational resources at test tim"], "labels": ["SMY", "DFT", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the model is based on a multi-scale convolutional neural network similar to the neural fabric saxena and verbeek", "but with dense connections huang et al  and with a classifier at each lay", "the multiple classifiers allow for a finer selection of the amount of computation needed for a given input imag", "the multi-scale representation allows for better performance at early stages of the network", "the multi-scale representation allows for better performance at early stages of the network"], "labels": ["SMY", "SMY", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following lay", "a thorough evaluation on imagenet and cifar shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of comput", "a thorough evaluation on imagenet and cifar shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of comput", "pros- the presentation is clear and easy to follow", "- the structure of the network is clearly justified in sect"], "labels": ["DIS", "APC", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interest", "- the evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenario", "- results are very promising with x speed-ups and same or better accuracy that previous model", "- the extensive experimentation shows that the proposed network is better than previous approaches under different regim", "cons- results about the more efficient densenet* could be shown in the main pap"], "labels": ["APC", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["additional comments- why in training you used logistic loss instead of the more common cross-entropy loss", "has this any connection with the final performance of the network", "- in fig  left for completeness i would like to see also results for densenet^mt and resnet^mt", "- in fig  left i cannot find the % and % higher accuracy with x^ to x^ flops as mentioned in section  anytime prediction result", "- how the budget in terms of mul-adds is actually estim"], "labels": ["QSN", "QSN", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ni think that this paper present a very powerful approach to speed-up the computational cost of a cnn at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them", "the experimental evaluation is complete and accur", "the authors proposed a graph neural network based architecture for learning generative models of graph", "compared with traditional learners such as lstm the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data", "the representation is clear with detailed empirical studi"], "labels": ["APC", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i support its accept", "the draft does need some improvements and here is my suggest", "figure  could be improved using a concrete example like in figur", "if space allowed an example of different ordering leads to the same graph will also help", "more details on how node embedding vectors are initi"], "labels": ["FBK", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how does different initializations affect result", "why is nodes at different stages with the same initialization problemat", "more details of how conditioning information is used especially for the attention mechanism used later in parse tree gener", "the sequence ordering is import", "while the draft avoids the issue theoretically it does has interesting results in molecule generation experi"], "labels": ["QSN", "QSN", "SUG", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suggest the authors at least discuss the empirical over-fitting problem with respect to ord", "in section  the choice of er random graph as a baseline is too simplist", "it does not provide a meaningful comparison", "a better generative model for cycles and trees could help", "when comparing training curves with lstm it might be helpful to also include the complexity comparison of each iter"], "labels": ["APC", "CRT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a new architecture for end to end neural machine transl", "inspired by the phrase based approach the translation process is decomposed as follows  source words are embedded and then reordered a bilstm then encodes the reordered source a sleep wake network finally generates the target sequence as a phrase sequence built from left to right", "this kind of approach is more related to ngram based machine translation than conventional phrase based on", "the idea is nic", "the proposed approach does not rely on attention based model"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this opens nice perpectives for better and faster infer", "my first concern is about the architecture descript", "for instance the swan part is not really stand alon", "for reader who does not already know this net i'm not sure this is really clear", "moreover there is no link between notations used for the swan part and the ones used in the reordering part"], "labels": ["APC", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then one question arises why don't you consider the reordering of the whole source sent", "maybe you could motivate your choice at this point", "this is the main contribution of the paper since swan already exist", "finally the experimental part shows nice improv", "but / you must provide baseline results with a well tuned phrase based mt system"], "labels": ["QSN", "DIS", "CRT", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["/ the datasets are small ones as well as the vocabularies you should try with larger datasets and bpe for sake of comparison", "/ the datasets are small ones as well as the vocabularies you should try with larger datasets and bpe for sake of comparison", "/ the datasets are small ones as well as the vocabularies you should try with larger datasets and bpe for sake of comparison", "the authors disclosed their identity and violated the terms of double blind reviewspage  in our previous work aly & dugan also the page  is full of typos and hard to read", "the authors disclosed their identity and violated the terms of double blind reviewspage  in our previous work aly & dugan also the page  is full of typos and hard to read"], "labels": ["SUG", "DFT", "CRT", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper proposes an adversarial learning framework for machine comprehension task", "specifically authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task", "authors report results in  different reading comprehension datasets and the proposed learning framework results in improving the performance of gmemnn", "my commentsthis paper is a direct application of adversarial learning to the task of reading comprehens", "it is a reasonable idea and authors indeed show that it work"], "labels": ["SMY", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper needs a lot of edit", "please check the minor com", "why is the adversary called narrator network", "it is bit confusing because the job of that network is to obfuscate the passag", "why do you motivate the learning method using self-play"], "labels": ["CRT", "CRT", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is just using the idea of adversarial learning like gan and it is not related to self-play", "in section  first paragraph authors mention that the narrator prevents catastrophic forget", "how is this happen", "can you elaborate mor", "the learning framework is not explained in a precise way"], "labels": ["CRT", "DIS", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what do you mean by re-initializing and retraining the narrator isnut it costly to reinitialize the network and retrain it for every turn", "how many such epochs are don", "you say that test set also contains obfuscated docu", "is it only for the validation set", "can you please explain if you use obfuscation when you report the final test performance too"], "labels": ["QSN", "QSN", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be more clear if you can provide a complete pseudo-code of the learning procedur", "how does the narrator choose which word to obfusc", "do you run the narrator model with all possible obfuscations and pick the best choic", "why donut you treat number of hops as a hyper-parameter and choose it based on validation set", "i would like to see the results in table  where you choose number of hops for each of the three models based on validation set"], "labels": ["DIS", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in figure  how are rounds construct", "does the model sees the same document again and again for  times or each time it sees a random document and you sample documents with replac", "this will be clear if you provide the pseudo-code for learn", "i do not understand author'su justification for figure-", "is it the case that the model learns to attend to last sentences for all the quest"], "labels": ["QSN", "QSN", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or where it attends varies across examples are you willing to release the code for reproducing the resultsminor commentspage  ucexploit his own decisionud should be ucexploit its own decis", "udin page  section  sentence starting with ucindeed a too low percentage uud needs to be fix", "page  ucforgetting is compensateud should be ucforgetting is compensatedud", "page  ucfor one sentencesud needs to be fix", "page  ucunknowud should be ucunknownudpage  uc"], "labels": ["QSN", "CRT", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud needs to be fix", "page  ucfor the two first datasetsud needs to be fix", "table  ucgmennnud should be ucgmemnnud", "in caption is it mean accuracy or maximum accuraci", "page  ucdataset was achievesud needs to be fix"], "labels": ["CRT", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["page  ucdocument by obfuscated this wordud needs to be fix", "npage  ucoverall aspect of the two first readersud needs to be fix", "page  last para references needs to be fix", "page  first sentence please check grammar", "section  last sentence is irrelev"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper develops models which attempt to explain the existence of universal perturbations which fool neural networks u ie the existence of a single perturbation which causes a network to misclassify most input", "the paper develops two models for the decision boundarya a locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspac", "b a locally positively curved model in which there is a positively curved outer bound for the collection of points which are assigned a given label", "the paper works out a probabilistic analysis arguing that when either of these conditions obtains there exists a fooling perturbation which affects most of the data", "the theoretical analysis in the paper is straightforward in some sense following from the definit"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations argue experimentally that they occur in some neural networks of practical interest", "one challenge in assessing the experimental claims is that practical neural networks are nonsmooth the quadratic model developed from the hessian is only valid very loc", "this can be seen in some of the illustrative examples in figure  there *is* a coarse-scale positive curvature but this would not necessarily come through in a quadratic model fit using the hessian", "the best experimental evidence for the authorsu perspective seems to be the fact that random perturbations from s_c misclassify more points than random perturbations constructed with the previous method", "i find the topic of universal perturbations interesting because it potentially tells us something structural class-independent about the decision boundaries constructed by artificial neural network"], "labels": ["SMY", "DIS", "DIS", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to my knowledge the explanation of universal perturbations in terms of positive curvature is novel", "the paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations or even what it means geometrically that positive curvature obtains at every data point", "visually these perturbations seem to have strong oriented local high-frequency content u perhaps they cause very large responses in specific filters in the lower layers of a network and conventional architectures are not robust to thi", "it would also be nice to see some visual representations of images perturbed with the new perturbations to confirm that they remain visually similar to the original imag", "this paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optim"], "labels": ["APC", "SUG", "QSN", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the argument is that due to the central limit theorem clt the distribution on the neuron pre-activations is approximately gaussian with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribut", "as a result the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick", "the authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the clt or to prevent underfit", "the method is evaluated on multiple experi", "this paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practic"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my main issue is that while the authors argue about novelty the first application of clt for sampling neuron pre-activations at neural networks with discrete rvs is performed at []", "while [] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution the extension was very straightforward", "i would thus suggest that the authors update the paper accordingli", "other than that i have some other com", "- the l regularization on the distribution parameters for the ternary weights is a bit ad-hoc why not penalise according to the entropy of the distribution which is exactly what you are trying to achiev"], "labels": ["CRT", "CRT", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- for the binary setting you mentioned that you had to reduce the entropy thus added a ucbeta density regulariserud did you add rp or log rp to the objective funct", "also with alpha beta =  the beta density is unimodal with a peak at p= essentially this will force the probabilities to be close to  ie exactly what you are trying to avoid", "to force the probability near the endpoints you have to use alpha beta   which results into a ucbowlud shaped beta distribut", "i thus wonder whether any gains you observed from this regulariser are just an artifact of optim", "- i think that a baseline at least for the binary case where you learn the weights with a continuous relaxation such as the concrete distribution and not via clt would be help"], "labels": ["QSN", "DIS", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["maybe for the network to properly converge the entropy for some of the weights needs to become small hence break the clt", "[] wang & manning fast dropout train", "edit after the authors rebuttal i have increased the rating of the pap", "- i still believe that the connection to [] is stronger than what the authors allude to eg the first two paragraphs of sec  could easily be attributed to []", "- the argument for the entropy was to include a term - lambda * hp in the objective function with hp being the entropy of the distribution p"], "labels": ["SUG", "SUG", "APC", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the lambda term would then serve as an indicator to how much entropy is necessari", "- there indeed was a misunderstanding with the usage of the rp regularizer at the objective function which is now resolv", "- the authors showed benefits compared to a continuous relaxation baselin", "this is a good application paper can be quite interesting in a workshop related to deep learning applications to physical sciences and engin", "lacks in sufficient machine learning related novelty required to be relevant in the main confer"], "labels": ["SMY", "DIS", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["design solving inverse problem using deep learning are not quite novel seestoecklein et al deep learning for flow sculpting insights into efficient learning using scientific simulation data scientific reports  article numb", "however this paper introduces two different types of networks for parametrization and physical behavior mapping which is interesting can be very useful as surrogate models for cfd simul", "it will be interesting to see the impacts of physics based knowledge on choice of network architecture hyper-parameters and other training consider", "just claiming the generalization capability of deep networks is not enough need to show how much the model can interpolate or extrapol", "just claiming the generalization capability of deep networks is not enough need to show how much the model can interpolate or extrapol"], "labels": ["CRT", "APC", "SUG", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what are the effects of regulariazations in this regard", "this paper studies the issue of truncated backpropagation for meta-optim", "backpropagation through an optimization process requires unrolling the optimization which due to computational and memory constraints is typically restricted or truncated to a smaller number of unrolled steps than we would lik", "this paper highlights this problem as a fundamental issue limiting meta-optimization approach", "the authors perform a number of experiments on a toy problem stochastic quadratics which is amenable to some theoretical analysis as well as a small fully connected network trained on mnist"], "labels": ["QSN", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["side note i was assigned this paper quite late in the review process and have not carefully gone through the derivations--specifically theorems  and", "the paper is generally clear and well written", "major comments-------------------------i was a bit confused why  sgd+mom steps pre-training steps were needed as far as i can tell pre-training is not typically done in the other meta-optimization literatur", "the authors suggest this is needed because the dynamics of training are different at the very start compared to later stages which is a bit vague perhaps the authors can expand upon  this point", "the conclusion suggests that the difference in greedy vs fully optimized schedule is due to the curvature poor scaling of the object"], "labels": ["DIS", "APC", "DIS", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--but fig  and earlier discussion talked about the noise in the objective as introducing the bias eg from earlier in the paper the noise in the problem adds uncertainty to the objective resulting in failures of greedy schedul", "which is the real issue noise or curvatur", "would running the problem on quadratics with different condition numbers be insight", "minor comments-------------------------the stochastic gradient equation in sec  is missing a subscript h_i instead of h", "nit would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in figure  just to get a sense of how that compares with the greedy and optimized curv"], "labels": ["DIS", "QSN", "QSN", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nit looks like there was an upper bound constraint placed on the optimized learning rate in figure --is that correct", "i couldn't find a mention of the constraint in the paper the optimized learning rate remains at  for the first ~ step", "figure  and elsewhere i would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation 'optimized' makes it more clear that the curve was obtained using an optimization process", "figure  can you change the line style or thickness so that we can see both the red and blue curves for the deterministic cas", "i assume the red curve is hiding beneath the blue one--but it would be good to see this explicitli"], "labels": ["QSN", "QSN", "SUG", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  is fantastic--it succinctly and clearly demonstrates the problem of truncated unrol", "i would add a note in the caption to make it clear that the smd trajectories are the red curves eg smd trajectories red during meta-optimization of initial effect", "i would also change the caption to use meta-training losses instead of training losses i believe those numbers are for the meta-loss correct", "i would also change the caption to use meta-training losses instead of training losses i believe those numbers are for the meta-loss correct", "finally i would add a colorbar to indicate numerical values for the different grayscale valu"], "labels": ["APC", "SUG", "SUG", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some recent references that warrant a mention in the text- both of these learn optimizers using longer numbers of unrolled stepslearning gradient descent better generalization and longer horizons lv et al icml learned optimizers that scale and generalize wichrowska et al icml - another application of unrolled optimizationunrolled generative adversarial networks metz et al iclr", "in the text discussing figure  middle of pg   which is obtained by using should be which are obtained by us", "in the conclusion optimal for deterministic objective should be deterministic object", "the paper proposes a new neural network based method for recommend", "the main finding of the paper is that a relatively simple method works for recommendation compared to other methods based on neural networks that have been recently propos"], "labels": ["SUG", "SUG", "SUG", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this contribution is not bad for an empirical pap", "there's certainly not that much here that's groundbreaking methodologically though it's certainly nice to know that a simple and scalable method work", "there's not much detail about the data it is after all an industrial pap", "it would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets compared to the same baselines in order to get a sense as to whether the improvement is actually due to having a better model versus being due to some unique attributes of this particular industrial dataset under consider", "as it is i am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewher"], "labels": ["APC", "APC", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["other than that it's nice to see an evaluation on real production data and it's nice that the authors have provided enough info that the method should be more or less reproduc", "there's some slight concern that maybe this paper would be better for the industry track of some conference given that it's focused on an empirical evaluation rather than really making much of a methodological contribut", "again this could be somewhat alleviated by evaluating on some standard and reproducible benchmark", "this paper proposes a method dual-ac for optimizing the actorpolicy and criticvalue function simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor", "in order to achieve that they take the linear programming approach of solving the bellman optimality equations outline the deficiencies of this approach and propose solutions to mitigate those problem"], "labels": ["APC", "DIS", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the discussion on the deficiencies of the naive lp approach is mostly well don", "their main contribution is extending the single step lp formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regular", "they perform an empirical study in the inverted double pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improv", "lastly there are empirical experiments done to conclude the superior performance of dual-ac in contrast to other actor-critic algorithm", "overall this paper could be a significant algorithmic contribution with the caveat for some clarifications on the theory and experi"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given these clarifications in an author response i would be willing to increase the scor", "for the theory there are a few steps that need clarification and further clarification on novelti", "for novelty it is unclear if theorem  and theorem  are both being stated as novel result", "it looks like theorem  has already been shown in randomized linear programming solves the discounted markov decision problem in nearly-linear running timeud", "there is a statement that ucchen & wang  wang  apply stochastic first-order algorithms nemirovski et al  for the one-step lagrangian of the lp problem in reinforcement learning set"], "labels": ["FBK", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however as we discussed in section  their algorithm is restricted to tabular parametrizationud", "is you theorem  somehow an extension is theorem  completely new", "this is particularly called into question due to the lack of assumptions about the function class for value funct", "it seems like the value function is required to be able to represent the true value function which can be almost as restrictive as requiring tabular parameterizations which can represent the true value funct", "this assumption seems to be used right at the bottom of page  where u^{pi*} = v^*"], "labels": ["DIS", "QSN", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further eta_v must be chosen to ensure that it does not affect constrain the optimal solution which implies it might need to be very smal", "more about conditions on eta_v would be illumin", "there is also one step in the theorem that i cannot verifi", "on page  how is the squared removed for difference between u and upi", "the transition from the second line of the proof to the third line is not clear"], "labels": ["SUG", "SUG", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would also be good to more clearly state on page  how you get the first inequality for || v^* ||_{mu}^", "for the experiments the following should be address", "it would have been better to also show the performance graphs with and without the improvements for multiple domain", "the central contribution is extending the single step lp to a multi-step formul", "it would be beneficial to empirically demonstrate how increasing k the multi-step parameter affects the performance gain"], "labels": ["SUG", "DIS", "SUG", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["increasing k also comes at a computational cost", "i would like to see some discussions on this and how long dual-ac takes to converge in comparison to the other algorithms tested ppo and trpo", "the authors concluded the presence of local convexity based on hessian inspection due to the use of path regular", "it was also mentioned that increasing the regularization parameter size increases the convergence r", "empirically how does changing the regularization parameter affect the performance in terms of reward maxim"], "labels": ["SMY", "SUG", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the experimental section of the appendix it is mentioned that multiple regularization settings were tried but their performance is not ment", "also for the regularization parameters that were tried based on hessian inspection did they all result in local convex", "a bit more discussion on these choices would be help", "minor comments page  in equation  there should not be a 'ds' in the dual variable constraint", "this paper made some efforts in smoothing the top-k losses proposed in lapin et "], "labels": ["CRT", "QSN", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a family of smooth surrogate loss es was proposed with the help of which the top-k error may be minimized directli", "the properties of the smooth surrogate losses were studied and the computational algorithms for svm with these losses function were also propos", "pros the paper is well presented and is easy to follow", "the contribution made in this paper is sound and the mathematical analysis seems to be correct", "the experimental results look convinc"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["conssome statements in this paper are not clear to m", "conssome statements in this paper are not clear to m", "for example the authors mentioned sparse or non-sparse loss funct", "this statement in my view could be misleading without further explanation the non-sparse loss was mentioned in the abstract", "this statement in my view could be misleading without further explanation the non-sparse loss was mentioned in the abstract"], "labels": ["DFT", "CRT", "SMY", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper addresses the problem of learning mappings between different domains without any supervis", "it belongs to the recent family of papers based on gan", "the paper states three conjectures predictions in the pap", "gan are sufficient to learn uabuasemantic mappingsuaubb in an unsupervised way if the considered networks are small enough controlling the complexity of the network ie the number of the layers is crucial to come up with what is called uabuasemanticuaubb mappings when learning in an unsupervised way", "more precisely there is tradeoff to achieve between the complexity of the model and its simpl"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a rich model is required in order to minimize the discrepancy between the distributions of the domains while a  not too complex model is necessary to avoid mappings that are not uabuameaningfuluaubb to this aim the authors  introduce a new notion of function complexity which can be seen as a proxy of kolmogorov complex", "the introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered funct", "based on this definition and assuming identifiability ie uniqueness up to invariants and for networks with leaky relu activations  the authors prove that if the number of mappings which preserve a degree of discrepancy density preserving in the text is small then the  set of uabuaminimaluaubb mappings  of complexity c   that achieve the same degree of  discrepancy is also smal", "this result is related to the third conjecture of the paper that is  the number of the number of mappings which preserve a degree of discrepancy  is smal", "the authors also prove a byproduct result stating that identifiability holds for leaky relu networks with one hidden lay"], "labels": ["SMY", "SMY", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper  comes with a series of experiments to empirically uabuademonstrateuaubb the conjectur", "the paper is well written", "the different ideas are clearly stated and discussed and hence open interesting questions and deb", "some of these questions that need to be addressed imho- a critical general question if the addressed problem is the alignment between eg images and not image generation why not formalizing the problem as a similarity search one using eg emd or any other transport metr", "the alignment task  hence reduces to computing a ranking from this similar"], "labels": ["SMY", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have the impression that we use a jackhammer to break a small brick here no offence but maybe that ium missing something here- several works consider the size and the depth of the network as hyper-parameters to optimize and this is not new", "what is the actual contribution of the paper wrt to this body of work", "- it is considered that the gan are trained without any problem and therefore work in an optimal regim", "but the training of the gan is in itself a problem", "how does this affect the paper statements and results-"], "labels": ["CRT", "QSN", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are the results still valid for another measure of discrepancy based for instance on another measure eg wasserstein", "some minor remarks - p the following sentence is not clear  uabua our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic funct", "uaubb- p $c^{epsilon_}_{ab}$ is used after def  before being defined - p build-builtsection ii a diagram explaining  the different mappings h_a h_b h_ab etc and their spaces d_a d_b d_z would greatly help the understand", "papers 's pros - clarity- technical result", "cons- doubts about the interest and origin"], "labels": ["QSN", "CRT", "QSN", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe authors provided detailed and convincing answers to my quest", "i thank them for that", "my scores were changed accrodingli", "the paper entitled 'siamese survival analysis' reports an application of a deep learning to three cases of competing risk survival analysi", "the author follow the reasoning that ' these ideas were not explored in the context of survival analysis' thereby disregarding the significant published literature based on the concordance index ci"], "labels": ["DIS", "DIS", "FBK", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["besides this deficit the paper does not present a proper statistical setup eg 'is censoring assumed to be at random   and numerical results are only referring to some standard implementations thereby again neglecting the state-of-the-art solut", "besides this deficit the paper does not present a proper statistical setup eg 'is censoring assumed to be at random   and numerical results are only referring to some standard implementations thereby again neglecting the state-of-the-art solut", "that being said this particular use of deep learning in this context might be novel", "the paper proposes to generate embedding of named-entities on the fly during dialogue sess", "if the text is from the user a named entity recognizer is us"], "labels": ["DFT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if it is from the bot response then it is known which words are named entities therefore embedding can be constructed directli", "the idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way", "one thing i hope the author could provide more clarification is the use of n", "for example the experimental result on structured qa task section  where it states that the performance different between models of with-ne-table and w/o-ne-table is positioned on the oov nes not present in the training subset", "to my understanding because of the presence of the ner in the with-ne-table model you could directly do update to the ne embeddings and query from the db using a combination of embedding and the ne words as the paper does whereas the w/o-ne-table model cannot because of lack of the n"], "labels": ["SMY", "APC", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this seems to prove that an ner is useful for tasks where db queries are needed rather than that the dynamic ne-table construction is us", "you could use an ner for w/o-ne-table and update the ne embeddings and it should be as good as with-ne-table model and fairer to compare with too", "that said overall the paper is a nice contribution to dialogue and qa system research by pointing out a simple way of handling named entities by dynamically updating their embed", "it would be better if the paper could point out the importance of ner for user utterances and the fact that using the knowledge of which words are nes in dialogue models could help in tasks where db queries are necessari", "this paper presents a novel application of machine learning using graph nn's on asts to identify incorrect variable usage and predict variable names in context"], "labels": ["DIS", "SUG", "APC", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is evaluated on a corpus of m sloc which is a substantial strength of the pap", "the paper is to be commended for the following aspects detailed description of ggnns and their comparison to lstm", "the inclusion of ablation studies to strengthen the analysis of the proposed techniqu", "validation on real-world software data", "the performance of the technique is reasonable enough to actually be us"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in reviewing the paper the following questions come to mind is the false positive rate too high to be pract", "how should this be tuned so developers would want to use the tool", "how does the approach generalize to other languages presumably well but something to consider for future work", "despite these questions though this paper is a nice addition to deep learning applications on software data and i believe it should be accept", "this paper proposes spectral normalization -- constraining the spectral norm of the weights of each layer -- as a way to stabilize gan training by in effect bounding the lipschitz constant of the discriminator funct"], "labels": ["QSN", "QSN", "QSN", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper derives efficient approximations for the spectral norm as well as an analysis of its gradi", "experimental results on cifar- and stl- show improved inception scores and fid scores using this method compared to other baselines and other weight normalization method", "overall this is a well-written paper that tackles an important open problem in training gans using a well-motivated and relatively simple approach", "the experimental results seem solid and seem to support the authors' claim", "i agree with the anonymous reviewer that connections and differences to related work should be made clear"], "labels": ["APC", "SUG", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["like the anonymous commenter i also initially thought that the proposed spectral normalization  is basically the same as spectral norm regularization but given the authors' feedback on this i think the differences should be made more explicit in the pap", "overall this seems to represent a strong step forward in improving the training of gans and i strongly recommend this paper for publ", "small nits section  in order to evaluate the efficacy of our experiment i think you mean approach", "there are a few colloquial english usages which made me smile eg  * sec  as we prophesied  and in the paragraph below  *  is a tad slow", "the authors introduce the polar transformer a special case of the spatial transformer jaderberg et al  that achieves rotation and scale equivariance by using a log-polar sampling grid"], "labels": ["SUG", "APC", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is very well written easy to follow and substantiates its claims convincingly on variants of mnist", "a weakness of the paper is that it does not attempt to solve a real-world problem", "however i think because it is a conceptually novel and potentially very influential idea it is a valuable contribution as it stand", "issues- the clutter in simmnist is so small that predicting the polar origin is essentially trivially solved by a low-pass filt", "although this criticism also applies to most previous work using uclutteredu variants of mnist i still think it needs to be consid"], "labels": ["APC", "CRT", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what happens if predicting the polar origin is not trivial and prone to error", "these presumably lead to catastrophic failure of the post-transformer network which is likely to be a problem in any real-world scenario", "- ium not sure if section  strengthens the pap", "unlike the rest of the paper it feels very uquick & dirtyu and not very principl", "it doesnut live up to the promise of rotation and scale equivariance in d"], "labels": ["QSN", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if i understand it correctly itus simply a polar transformer in xy with z maintained as a linear axis and assumed to be parallel to the axis of rot", "this means that the promise of rotation and scale equivariance holds up only along xi", "this means that the promise of rotation and scale equivariance holds up only along xi", "i guess itus not possible to build full d rotation/scale equivariance with the authorsu approach spherical coordinates probably donut do the job but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in d", "so instead of showing a quick uhacku i would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provid"], "labels": ["CRT", "DIS", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so instead of showing a quick uhacku i would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provid", "this paper try to analyze the intrinsic structure of vgg and give a new insight of deep neural network", "the authors propose to use svd tools to estimate the dimension of the deep manifolds and conduct experiments on three categories of imagenet", "the papers are written well and easy to follow", "the analysis of manifold structure of dnn is important direction but i am afraid novelty and insight of this work is not enough for accept"], "labels": ["DIS", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the analysis of manifold structure of dnn is important direction but i am afraid novelty and insight of this work is not enough for accept", "pros  the paper is well written and easy to follow", "manifold analysis of the intrinsic structure of dnn is a important direction for further studi", "cons  svd is a standard tool for subspace and manifold analysis for decades of year", "i do not think using it in dnn is a big contribut"], "labels": ["FBK", "APC", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should explain why choosing vgg for analysi", "do other deep neural networks such as resnet googlenet can  have the same phenomenon", "why the authors choose persian cat container ship and volcano in the experi", "do other categories have the similar result", "the authors can indicate the application scenario of this work"], "labels": ["QSN", "QSN", "QSN", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example this work may guide to design better cnn structure for higher accuracy and lower computation cost", "it may help the readers better understand the values of this work", "in this paper the authors studied the problem of semi-supervised few-shot classification by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor class", "the studied problem is interesting and the paper is well-written", "extensive experiments are performed to demonstrate the effectiveness of the proposed method"], "labels": ["DIS", "DIS", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the proposed method is a natural extension of the existing works ie soft k-means and meta-learn", "on top of that it seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outli", "overall i would like to vote for a weakly acceptance regarding this pap", "overall i would like to vote for a weakly acceptance regarding this pap", "mars is suggested to combine multiple adversaries with different rol"], "labels": ["DIS", "CRT", "CRT", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wear", "experiments a are satisfying and show good performance when compared to other method", "it could be made clearer how significance is tested given the frequent usage of the term", "the idea is slightly novel and the framework otherwise state-of-the-art", "the paper is well written but can use some proof-read"], "labels": ["SMY", "APC", "SUG", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written but can use some proof-read", "referencing is okay", "my main concern is the relevance of this paper to iclr", "this paper is much related not to representation learning but to user-interfac", "the paper is not well organized and so the technical novelty of the method is unclear"], "labels": ["APC", "SMY", "CNT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the existing method and proposed method seems to be mixed in sect", "you should clearly divide the existing study and your work", "you should clearly divide the existing study and your work", "the experimental setting is also unclear", "kss seems to need the user studi"], "labels": ["DFT", "SUG", "CRT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["kss seems to need the user studi", "but i do not catch the details of the user study eg the number of us", "but i do not catch the details of the user study eg the number of us", "the paper describes a manifold learning method that adapts the old ideas of multidimensional scaling with geodesic distances in particular to neural network", "the goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extens"], "labels": ["CRT", "DFT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper has several major shortcomings* any paper dealing with mds and geodesic distances should test the proposed method on the swiss roll which has been the most emblematic benchmark since the isomap paper in", "not showing the swiss roll would possibly let the reader think that the method does not perform well on that exampl", "in particular dr is one of the last fields where deep learning cannot outperform older methods like t-sn", "please add the swiss roll exampl", "* distance preservation appears more and more like a dated dr paradigm"], "labels": ["CRT", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["simple example from d to d are easily handled but beyond the curse of dimensionality makes things more complicated in particular due to norm comput", "computation accuracy of the geodesic distances in high-dimensional spaces can be poor", "this could be discussed and some experiments on very hd data should be report", "* some key historical references are overlooked like the sammann", "there is also an over-emphasis on spectral methods with the necessity to compute large matrices and to factorize them probably owing to the popularity of spectral dr metods a decade ago"], "labels": ["CRT", "QSN", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["other methods might be computationally less expensive like those relying on space-partitioning trees and fast multipole methods subquadratic complex", "finally auto-encoders could be mentioned as well they have the advantage of providing the parametric inverse of the mapping too", "* as a tool for unsupervised learning or exploratory data visualization dr can hardly benefit from a parametric approach", "the motivation in the end of page  seems to be computational onli", "* section  should be further detailed step  in particular"], "labels": ["CRT", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the experiments are rather limited with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress", "the running times are not in favor of the proposed method", "the data sets sizes are however quite limited with n for point cloud data and n for the image manifold", "* the conclusion sounds a bit vague and pompous 'by allowing a limited infusion of axiomatic computation'", "what is the take-home message of the pap"], "labels": ["CRT", "CRT", "DFT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors train an rnn to perform deduced reckoning ded reckoning for spatial navigation and then study the responses of the model neurons in the rnn", "they find many properties reminiscent of neurons in the mammalian entorhinal cortex ec grid cells border cells etc", "when regularization of the network is not used during training the trained rnns no longer resemble the ec", "this suggests that those constraints lower overall connectivity strengths and lower metabolic costs might play a role in the ec's navigation funct", "the paper is overall quite interesting and the study is pretty thorough no major cons come to mind"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some suggestions / criticisms are given below the findings seem conceptually similar to the older sparse coding ideas from the visual cortex", "that connection might be worth discussing because removing the regularizing ie metabolic cost constraint from your rnns makes them learn representations that differ from the ones seen in ec", "the sparse coding models see something similar without sparsity constraints the image representations do not resemble those seen in v but with sparsity the learned representations match v quite wel", "that the same observation is made in such disparate brain areas v ec suggests that sparsity / efficiency might be quite universal constraints on the neural cod", "the finding that regularizing the rnn makes it more closely match the neural code is also foreshadowed somewhat by the  nature neuro paper by susillo et "], "labels": ["DIS", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that could be worthy of some brief discuss", "sussillo d churchland m m kaufman m t & shenoy k v", "a neural network that finds a naturalistic solution for the production of muscle act", "nature neuroscience  -", "why the different initializations for the recurrent weights for the hexagonal vs other environ"], "labels": ["DIS", "DIS", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm guessing it's because the rnns don't work in all environments with the same initialization ie they either don't look like ec or they don't obtain small errors in the navigation task", "that seems important to explain more thoroughly than is done in the current text", "what happens with ongoing train", "animals presumably continue to learn throughout their l", "with on-going continous training do the rnn neurons' spatial tuning remain stable or do they continue to drift so that border cells turn into grid cells turn into irregular cells or some such"], "labels": ["DIS", "DIS", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that result could make some predictions for experiment that would be testable with chronic methods like ca+ imaging that can record from the same neurons over multiple experimental sess", "it would be nice to more quantitatively map out the relation between speed tuning direction tuning and spatial tuning illustrated in fig", "specifically i would quantify the cells' direction tuning using the circular variance methods that people use for studying retinal direction selective neuron", "and i would quantify speed tuning via something like the slope of the firing rate vs speed curv", "and quantify spatial tuning somehow a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific loc"], "labels": ["DIS", "QSN", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then make scatter plots of these quantities against each oth", "basically i'd love to see the trends for how these types of tuning relate to each other over the whole populations those trends could then be tested against experimental data possibly in a future studi", "the article proposes to use dense skip-connections on the vertical between-layers connections of recurrent network", "moreover the article proposes to use separate attention-heads that run on the outputs of each encoder's layer with each attention selecting other regions in the input to attend to", "the experiments demonstrate that the changes yield small bleu score improvements on translation and summarization task"], "labels": ["QSN", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not convinced by the presented results for the following reasons the paper introduces two concepts - the dense skip-connections and the multi-head attent", "experiments only show their joint impact yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attent", "experiments only show their joint impact yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attent", "the results suggest that deeper model are better with the densely connected networks being up to twice deeper than the baselin", "what happens for deeper and narrower baselines that have a similar number of paramet"], "labels": ["DFT", "DFT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what happens for deeper and narrower baselines that have a similar number of paramet", "looking at the training curves thanks for including them the densely connected model seems to converge faster by annealing the learning faster i treat the jumps in the training curves as signs of learning rate ann", "maybe this is what help", "i know the authors use an automaton to anneal the learning rate but maybe the impact of learning rates should be evalu", "qualitygoodclaritythe paper is clearly written"], "labels": ["QSN", "SMY", "QSN", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originalitythe addition of dense connections to recurrent networks is trivi", "pros&cons+ the proposed additions dense skip connections and multi-head attentions yield performance improvements- the impact of the two contributions is not disentangled in the paper- the two contributions are fairly obvi", "the main contribution of this work is just a combination of lsh schemes and sgd upd", "since hashing schemes essentially reduce the dimension lsh brings computational benefits to the sgd oper", "the targeted issue is fundamentally important and the proposed approach exploiting lsh schemes seems to be sound"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically lsh schemes fit into the sgd schemes since they hash two vectors to the same bucket with probability in proportional to their distance here inner product or cosine similar", "strengths  a sound approach a simple and straightforward idea that is shown to work well in evalu", "weaknesses  the phrase of computational chicken-and-egg loop in the title and also in the main body is misleading and not accur", "the so-called chicken-and-eggud issue concerns the causality dilemma two causally related things which comes the first", "in the paper the authors concerned more accurate gradients and faster convergence their causality is very clear the first leads to the second and there is no causality dilemma"], "labels": ["APC", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even from a computational perspective sdg schemes aim for computational efficiency and stochastic makes the convergence slow down are not a causality dilemma", "the reason behind is that the latter is the cost of the first one just the old saying that there is no such thing as a free lunch", "therefore this disordered logic makes the title very misleading and all the corresponding descriptions in the main body are obscured by twisted and unnatural log", "the depth is so limit", "besides a good observation that lsh fits well into sdg there are no more in-depth results provid"], "labels": ["DIS", "DIS", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["besides a good observation that lsh fits well into sdg there are no more in-depth results provid", "the theorems theorems ~ are trivial with loose relations with lsh", "t  the lsh schemes are not correctly referred to", "since the similarity metric is inner-product the authors are expected to refer to cosine similarity and inner-product based lshs which were published recently in nip", "it is not in depth to assume any known lsh scheme in alg"], "labels": ["CRT", "CRT", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["accordingly again theorems ~ are unrelated with this specific kind of similarity metric cosine similar", "as the authors tried hard to stick to the unnecessary a bit bragging phrase computational chicken-and-egg loop the organization and presentation of the whole manuscript are poor", "occasionally there are typos and it is not good to use words in formulas please proof-read car", "the paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm which is mixed with maximum likelihood object", "the seqseq architecture incorporates both intra-temporal and intra-decoder attention and a pointer copying mechan"], "labels": ["CRT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a hard constraint is imposed during decoding to avoid trigram repetit", "most of the modelling ideas already exists but this paper show how they can be applied as a strong summarization model", "the approach obtains strong results on the cnn/daily mail and nyt dataset", "results show that intra-attention improves performance for only one of the dataset", "results show that intra-attention improves performance for only one of the dataset"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rl results are reported with only the best-performing attention setup for each dataset", "my concern with that is that the authors might be using the test set for model selection it is not a priori clear that the setup that works better for ml should also be better for rl especially as it is not the same across dataset", "so i suggest that results for rl should be reported with and without intra-attention on both datasets at least on the validation set", "so i suggest that results for rl should be reported with and without intra-attention on both datasets at least on the validation set", "it is shown that intra-decoder attention decoder improves performance on longer sent"], "labels": ["SMY", "DFT", "SMY", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to see more analysis on this especially analyzing what the mechanism is attending to as it is less clear what its interpretation should be than for intra-temporal attent", "further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the perform", "for the mixed decoding objective how is the mixing weight chosen and what is its effect on perform", "for the mixed decoding objective how is the mixing weight chosen and what is its effect on perform", "if it is purely a scaling factor how is the scale quantifi"], "labels": ["DFT", "SMY", "SMY", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is claimed that readability correlates with perplexity so it would be interesting to see perplexity results for the model", "the lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work", "this is a strong paper that presents a significant improvement in document summar", "the paper is well-written but is lacking detailed information in some areas see list of quest", "the paper is well-written but is lacking detailed information in some areas see list of quest"], "labels": ["APC", "SMY", "APC", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward", "the evaluation part of this paper is hard to assess due to the unavailability of the  datasets and appropriate baselin", "therefore i am currently leaning towards rejecting this pap", "p what parts are pre-train", "is e_o fixed for the non-structured knowledg"], "labels": ["APC", "DFT", "FBK", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or is it joint learning and you learn all lstms and cnns yourself besides the reuse of vgg i could not find this information explicitly stated within the pap", "p the word embeddings for the cnn are pre-trained wordvec/glove/xyz embed", "how do you deal with words or even the whole string for which you have no word embed", "p do you have one model for all the relations or does every relation has its own lstm cnn feed-forward network", "ie  or  feed-forward networks for age zip code and release d"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["p how does ucratings onlyud work as distmult gets no information of the specific ent", "is it just choosing the most common class", "p what does ucfind the mid-point of the binud mean and should it not be  instead of  bin", "+ insights on how different modalities affect the prediction result", "+ the approach is capable of theoretically handling all linked information to an entity as additional information to the link structur"], "labels": ["QSN", "QSN", "QSN", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- as the evaluation data is not available it is really hard to assess the quality of the model", "no simple baseline like the unstructured [] + simple concatenation of an image vector is provid", "- training of cnns lstms and so on is not clearsee question regarding whether the models are pre-trained or whether the models are also directly learned from the data", "further comments* in figure  the feed-forward network looks like an encoder-decoder network and it does not show the projection from r to r^d which is mentioned in the text", "* the found hyperparameter of the grid-search would also be interesting to know"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] bordes a glorot x weston j and bengio y  a semantic matching energy function for learning with multi- relational data machine learning u", "this paper presents a seqtree model to translate a problem statement in natural language to the corresponding functional program in a dsl", "the model usesan rnn encoder to encode the problem statement and uses an attention-baseddoubly recurrent network for generating tree-structured output", "the learnt model is then used to perform tree-beam search using a search algorithm that searches for different completion of trees based on node typ", "the evaluation is performedon a synthetic dataset and shows improvements over seqseq baseline approach"], "labels": ["SUG", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this paper tackles an important problem of learning programs from natural language and input-output example specif", "unlike previousneural program synthesis approaches that consider only one of the specification mechanisms examples or natural language this paper considers both of them simultan", "however there are several issues both in the approach and the current preliminary evaluation which unfortunately leads me to a reject scor", "but the general idea of combining different specifications is quite promis", "first the paper does not compare against a very similar approach of parisotto et alneuro-symbolic program synthesis iclr  that uses a similar rnn networkfor generating the program tree incrementally by decoding one node at a tim"], "labels": ["APC", "APC", "FBK", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can the authors comment on the similarity/differences between the approach", "would it be possible to empirically evaluate how the rnn performs on this dataset", "second it seems that the current model does not use the input-output examples at all for training the model", "the examples are only used during the search algorithmseveral previous neural program synthesis approaches deepcoder iclr  robustfill icml  have shown that encoding the examples can help guide the decoder to perform efficient search", "it would be good to possibly add another encoder network to see if encoding the examples as well help improve the accuraci"], "labels": ["QSN", "QSN", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["similar to the previous point it would also be good to evaluate the usefulness ofencoding the problem statement by comparing the final model against a model in whichthe encoder only encodes the input-output exampl", "finally there is also an issue with the synthetic evaluation dataset", "since the problem descriptions are generated syntactically using a template based approach the improvements in accuracy might come directly from learning the training templatesinstead of learning the desired semant", "the paper mentions that it is prohibitively expensive to obtain human-annotated set but can it be possible to at least obtain a handful of real tasks to evaluate the learnt model", "there are also some recent datasets such as wikisql https//githubcom/salesforce/wikisql that the authorsmight consider in futur"], "labels": ["SUG", "CRT", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["questions for the authorswhy was max_visited only limited to", "what happens when it is set to ^ or ^", "the search algorithm only shows an accuracy of % with max_visited=", "what wouldthe performance be for a simple brute-force algorithm with a timeout of say  minstable  reports an accuracy of % whereas the text mentions that the best resultis % page what all function names are allowed in the dsl figur", "can you clarify the contributions of the paper in comparison to the rnn"], "labels": ["QSN", "QSN", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor typospage  allows to add constrains -- allows to add constraint", "page  over max_visited programs has been -- over max_visited programs have been", "this work exploits the causality principle to quantify how the weights of successive layers adapt to each oth", "some interesting results are obtained such as enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectur", "generally the result is interesting and the presentation is easy to follow"], "labels": ["CRT", "CRT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the proposed approach and the experiments are not convincible enough", "for example  it is hard to obtain the conclusion more independence lead to better performance from the experimental result", "maybe more justifications are need", "maybe more justifications are need", "the paper reformulates the model-agnostic meta-learning algorithm maml in terms of inference for parameters of a prior distribution in a hierarchical bayesian model"], "labels": ["CRT", "CRT", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this provides an interesting and as far as i can tell novel view on maml", "the paper uses this view to improve the maml algorithm", "the writing of the paper is excel", "experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method", "except for tcml which has been exluded using a not so convincing argu"], "labels": ["APC", "DIS", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the overview of the literature is also very well don", "the topic is interesting however the description in the paper is lacking clar", "the paper is written in a procedural fashion - i first did that then i did that and after that i did third", "having proper mathematical description and good diagrams of what you doing would have immensely help", "having proper mathematical description and good diagrams of what you doing would have immensely help"], "labels": ["APC", "SMY", "SMY", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another big issue is the lack of proper validation in sect", "even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations such as listening tests in speech synthesi", "given that i see only one example i can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having non", "given that i see only one example i can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having non", "this paper presents a continuous surrogate for the ell_ norm and focuses on its applications in regularized empirical regularized minim"], "labels": ["DFT", "DIS", "SUG", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones with additional stretching and thresholding oper", "under a compound construction of sparsity the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables or be combined with other types of regularizations on the magnitude of non-zero compon", "the efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline method", "pros - the paper is clearly written self-contained and a pleasure to read", "- based on the evidence provided the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regular"], "labels": ["DIS", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons - it would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_ and other ell_p choic", "- it is unclear what properties does the proposed hard-concrete distribution have eg closed-form density convexity etc", "- if the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice this paper would be more signific", "the paper present online algorithms for learning multiple sequential problem", "the main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm"], "labels": ["DIS", "DFT", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental results are given on different multi-task inst", "the contributions are interesting and experimental results seem promis", "but the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the appendix ten sections in the appendix and  pag", "also most of the paper is devoted to the study of algorithms for which the expected target scores are known", "this is a very strong assumpt"], "labels": ["SMY", "APC", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in my opinion the authors should have put the focus on the duac algorithm which get rids of this assumpt", "in my opinion the authors should have put the focus on the duac algorithm which get rids of this assumpt", "therefore i am not convinced that the paper is ready for publication at iclr'", "* differences between bac and other algorithms are said to be a consequence of the probability distribution over task", "the gap is so large that i am not convinced on the fairness of the comparison"], "labels": ["SUG", "CRT", "FBK", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance bac algorithm  in appendix c does not have the knowledge of the target scores while others heavily rely on this knowledg", "* i do not see how the single output layer is defin", "* as said in the general comments in my opinion section  should be developped and more experiments should be done with the duac algorithm", "* section  it is not clear why degradation does not happen", "it seems to be only an experimental fact"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper makes some bold claim", "in particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks bn adam do not actually address the problems they set out to allevi", "this is either a very important paper or the analysis is incorrect but it's not my area of expertis", "actually understanding it at depth and validating the proofs and validity of the experiments will require some digest", "it's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on eg i have mostly seen resnets in the context of cnns but they analyze on fc topologies the form of the loss etc but that's a guess and there are some further analysis in the supp material for these networks which i haven't looked at in detail"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regardless - an important note to the authors is that it's a particularly long and verbose paper coming in at  pages of the main paper! with nearly  ! pages of supplementary material where the heart and meat of the proofs and experiments resid", "as such it's not even clear if this is proper for a confer", "the authors have already provided several pages worth of additional comments on the website on further related work", "i view this as an issue in and of itself", "being succinct and applying rigour in editing is part of doing science and reporting findings and a wise guideline to follow"], "labels": ["CRT", "FBK", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the authors may claim it's necessary to use that much space to make their point i will argue that this length is uncalibrated to standard", "i've seen many papers that need to go through much more complicated derivations and theory and remain within a - page limit by being precise and strictly to the point", "perhaps godel could be a good inspiration here with a  page phd thesis that fundamentally changed mathemat", "in addition to being quite bold in claims it is also somewhat confrontational in styl", "i understand the authors are trying to make a very serious claim about much of the common wisdom but again having reviewed papers for many years this is highly unusual and it is questionable whether it is necessari"], "labels": ["CRT", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so while i cannot vouch for the correctness i think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field", "so while i cannot vouch for the correctness i think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field", "this work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseas", "differently from previous applications of machine learning in neurodegeneration modeling the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scor", "the framework is represented by a feed-forward deep architecture analogous to a residual network"], "labels": ["SUG", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at each layer a low-rank constraint is enforced on the linear transformation while the cost function is specified in order to differentially account for the bounds of the predicted vari", "the idea of explicitly accounting for the boundedness of clinical scores is interest", "although the assumption of the proposed model is still incorrect clinical scores are defined on discrete scal", "for this reason the gaussian assumption for the cost function used in the method is still not appropriate for the proposed appl", "furthermore while being the main methodological drive of this work the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression target"], "labels": ["SMY", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed algorithm is also generally compared with respect to linear methods and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches eg random forests nn gp u", "overall the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disord", "moreover the proposed experimental paradigm appears flaw", "what is the interest of predicting baseline or  months at best cognitive scores relatively low-cost and part of any routine clinical assessment from brain imaging data high-cost and not routin", "other remarks - in section  and  there is some confusion between iteration indices and samples indices uciud"], "labels": ["CRT", "DIS", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- contrarily to what is stated in the introduction the loss functions proposed in page  first two formulas only accounts for the lower bound of the predicted vari", "-  figure  synthetic data", "the scale of the improvement of the subspace difference is quite tiny in the order of e- when compared to u and of e- across iter", "the loss function of figur", "b also does not show a strong improvement across iterations while indicating a rather large instability of the optimisation procedur"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these aspects may be a sign of convergence issu", "- the dimensionality of the subspace representation importantly depends on the choice of the rank r of u and v", "this is a crucial parameters that is however not discussed nor analysed in the pap", "this is a crucial parameters that is however not discussed nor analysed in the pap", "- the synthetic example of page  is quite misleading and potentially biased towards the proposed model"], "labels": ["CRT", "DIS", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors are generating the synthetic data according to the model and it is thus not surprising that they managed to obtain the best perform", "in particular due to the nonlinear nature of  all the competing linear models are expected to perform poorly in this kind of set", "- the computation time for the linear model shown in table  is quite surprising ~ minutes for linear regression of k observ", "is there anything that i am miss", "the paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optim"], "labels": ["CRT", "CRT", "APC", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is rigorous and ideas are clearly st", "the idea to constraint the dimension reduction to fit a certain model here a gmm is relevant and the paper provides a thorough comparison with recent state-of-the-art method", "my main concern is that the method is called unsupervised but it uses the class information in the training and also evalu", "i'm also not convinced of how well the gaussian model fits the low-dimensional representation and how well can a neural network compute the gmm mixture membership", "the framework uses the class information ie uconly data samples from the normal class are used for trainingud but it is still considered unsupervis"], "labels": ["APC", "APC", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies ie a priori inform", "i would like to see a plot of the sample energy as a function of the number of data point", "is there an elbow that indicates the threshold cut", "better yet it would be to use methods like local outlier factor lof breunig et al  u lofidentifying density-based local outliers to detect the outliers these methods also have parameters to tune sure but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data", "is there a theoretical justification for computing the mixture memberships for the gmm using a neural network"], "labels": ["DIS", "SUG", "QSN", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how do the regularization parameters lambda_ and lambda_ influence the result", "the idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets eg yang et al  -  unsupervised dimensionality reduction for gaussian mixture model", "those approaches should at least be discussed in the related work if not compared against", "those approaches should at least be discussed in the related work if not compared against", "the authors state that estimating the mixture memberships with a neural network for gmm in the estimation network instead of the standard em algorithm works bett"], "labels": ["QSN", "DIS", "SUG", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could you provide a comparison with em", "in the newly constructed space that consists of both the extracted features and the representation error is a gaussian model truly relev", "does it well describe the new spac", "do you normalize the features the output of the dimension reduction and the representation error are quite differ", "fig a doesn't seem to show that the output is a clear mixture of gaussian"], "labels": ["QSN", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the setup of the kddcup seems a little bit weird where the normal samples and anomalies are reversed because of percentage where the model is trained only on anomalies and it detects normal samples as anomali", "i'm not convinced that it is the best example especially that is it the one having significantly better results ie scores ~  vs scores ~/ score for the other dataset", "the authors mention that ucwe can clearly see from fig a that dagmm is able to well separ", "ud - it is not clear to me it does look better than the other ones but not clear", "if there is a clear separation from a different view show that one instead"], "labels": ["CRT", "CRT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we don't need the same view for all method", "in the experiments the reduced dimension used is equal to  for two of the experiments and  for one of them  this seems very drastic!", "minor comments fig what dimension reduction did you use add axis label", "ucdagmm preserves the key information of an input sampleud - what does key information mean", "in fig  when plotting the results for kddcup i would have liked to see results for the best  methods from table  oc-svm performs better than pa"], "labels": ["CRT", "DIS", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also dsebm-e and dsebm-r seems to perform very well when looking at the three measures combin", "they are the best in terms of precis", "is the error in table  averaged over multiple runs if yes how mani", "quality u the paper is thoroughly written and the ideas are clearly pres", "it can be further improved as mentioned in the com"], "labels": ["APC", "APC", "QSN", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarity u the paper is very well written with clear statements a pleasure to read", "originality u fairly original but it still needs some work to justify it bett", "significance u constraining the dimension reduction to fit a certain model is a relevant topic but i'm not convinced of how well the gaussian model fits the low-dimensional representation and how well can a neural network compute the gmm mixture membership", "this paper examines sparse connection patterns in upper layers of convolutional image classification network", "networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection mask"], "labels": ["APC", "SUG", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["heuristics for distributing connections among windows/groups and a measure called scatter are introduced to construct the connectivity masks and evaluated experimentally on cifar- and - mnist and morse code symbol", "while it seems clear in general that many of the connections are not needed and can be made sparse figures  and  i found many parts of this paper fairly confusing both in how it achieves its objectives as well as much of the notation and method descript", "i've described many of the points i was confused by in more detailed comments below", "detailed comments and questionsthe distribution of connections in windows are first described to correspond to a sort of semi-random spatial downsampling to get different views distributed over the full imag", "but in the upper layers the spatial extent can be very small compared to the image size sometimes even x depending on the network downsampling structur"], "labels": ["SMY", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so are do the windows correspond to spatial windows and if so how", "or are they different maybe arbitrary groupings over the feature map", "also a bit confusing is the notation conv conv etc", "these names usually indicate the name of a single layer within the network conv for the second convolutional layer or series of layers in the second spatial size after downsampling for exampl", "but here it seems just to indicate the number of cl layers   and p says that the cl layers are those often referred to as fc layers not conv though they may be convolutionally applied with spatial x kernel"], "labels": ["QSN", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense but i'm not convinced this will work well in all situations and may even be sub-optimal for the examined dataset", "for example to distinguish mnist  vs  vs  it is most important to see the top-left  whether it is empty has a horizontal line or a loop", "so some regions are more important than others and the top half may be more important than an equally spaced global view", "so the description of how to space connections between windows makes some intuitive sense but i'm unclear on whether other more general connections might be even better including some that might not be as easily analyzed with the scatter metric describ", "another broader question i have is in the distinction between lower and upper layers those referred to as feature extracting and classification in this pap"], "labels": ["CRT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's not clear to me that there is a crisply defined difference here though some layers may tend to do more of one or the other function such as we might interpret", "so it seems that expanding the investigation to include all layers or at least more layers would be good  it might be that more of the classification function is pushed down to lower layers as the upper layers are reduced in s", "how would they respond to similar reduct", "i'm also unsure why on p mnist uses d windows while cifar uses d --- the paper mentions the extra dimension is for features but mnist would have a features dimension as well at this stage i think", "i'm also unsure whether the windows are over spatial extent only or over featur"], "labels": ["DIS", "SUG", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary the paper proposes to use the cyk chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive nn", "the key idea is to maintain a chart to take into account all possible span", "the paper also introduces an attention method over chart cel", "the experimental results show that the propped model outperforms tree-lstm using external pars", "comment i kinda like the idea of using chart and the attention over chart cel"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is very well written", "- my only concern about the novelty of the paper is that the idea of using cyk chart-based mechanism is already explored in le and zuidema", "- le and zudema use pooling and this paper uses weighted sum", "any differences in terms of theory and experi", "- i like the new attention over chart cel"], "labels": ["APC", "CRT", "DIS", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i was surprised that the authors didnut use it in the second experiment reverse dictionari", "- in table  it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm % is ucgood enoughud", "in which cases the former did correctly but the latter didnut", "- in table  what if we use the right-branching tree-lstm with attent", "- in table  why do hill et al lstm and bow perform much better than the oth"], "labels": ["CRT", "CRT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically chang", "high level paper- i believe the writing is a bit sloppi", "for instance equation  takes the minimum over all m in c but c is defined to be a set of c_  c_k and other examples see section  below", "this is unfortunate because i believe this method which takes as input a large complex network and compresses it so the loss in accuracy is small would be really appealing to companies who are resource constrained but want to use neural network model", "high level technical- i'm confused at the first and second lines of equ"], "labels": ["SMY", "CRT", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the first line shouldn't the first term not contain delta w", "in the second line shouldn't the first term be tilde{mathcal{l}}w_ + delta w", "- for cifar- and svhn you're using binarized neural networks and the two nice things about this method are a that the memory usage of the network is very small and b network operations can be specialized to be fast on binary data", "my worry is if you're compressing these networks with your method are the weights not treated as binary anymor", "now i know in binarized neural networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright"], "labels": ["QSN", "QSN", "SMY", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and a and b above no longer appli", "- your compression ratio is much higher for mnist but your accuracy loss is somewhat dramatic especially for mnist an increase of  in error nearly doubles your error and makes the network worse than many other competing methods http//rodrigobgithubio/are_we_there_yet/build/classification_datasets_resultshtml#d", "what is your compression ratio for  accuracy loss", "i think this is a key experiment that should be run as this result would be much easier to compare with the other method", "- previous compression work uses a lot of tricks to compress convolutional weights does your method work for convolutional lay"], "labels": ["DIS", "DIS", "QSN", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the first paper to propose weight sharing was not han et al  it was actuallychen w wilson j t tyree s weinberger k q chen y compressing neural networks with the hashing trick icml although they did not learn the weight sharing function but use random hash funct", "low level technical- the end of section  has an extra 'p' charact", "- section  here x and y define a set of samples and ideal output distributions we use for training this sentence is a bit confus", "here y isn't a distribution but also samples drawn from some distribution actually i don't think it makes sense to talk about distributions at all in sect", "- section  w is the learnt modelhat{w} is the final trained model this is unclear w and hat{w} seem to describe the same th"], "labels": ["CRT", "DFT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would just remove is the learnt model and", "review summarywhile the trust-region-like optimization of the method is nice and i believe this method could be useful for practitioners i found the paper somewhat confusing to read", "this combined with some key experimental questions i have make me think this paper still needs work before being accepted to iclr", "the paper proposes to use -d image representation techniques as a means of learning representations of graphs via their adjacency matric", "the adjacency matrix or a subgraph of it is first re-ordered to produce some canonical ordering which can then be fed into an image representation method"], "labels": ["SUG", "CRT", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this can then be fed into a classifi", "this is a little too unprincipled for my tast", "in particular the paper uses a caffe reference model on top of the adjacency matrix rather than learning a method specifically for graph", "perhaps this is due to a lack of available graph training data but it doesn't seem to make a lot of sens", "maybe i missed or overlooked some detail but i didn't spot exactly what the classification task wa"], "labels": ["SMY", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the goal is to identify which of the graphs a subgraph belongs to", "i'm not sure how relevant this graph classification task i", "the method does prove that the caffe reference model maintains some information that can be used for classification but this doesn't really suggest a generalizable method that we could confidently use for a variety of task", "it's surprising that it works at all but ultimately doesn't reveal a big scientific finding that could be re-us", "the authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data"], "labels": ["QSN", "QSN", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not afford", "the authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework", "the authors evaluate their proposed methods on one toy problem and two real-world problem", "the paper is well written easy to follow and have good experimental studi", "my main problem with the paper is the lack of enough motivation and justification for the proposed method the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work"], "labels": ["APC", "SMY", "SMY", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["here are some questions that comes to my mind   why first building a student model only using the weak data and why not all the data together to train the student model", "to me it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data", "what are the sensitivity of the procedure to how weakly the weak data are annotated this could be studied using both toy example and real-world exampl", "the authors explicitly suggest using an unsupervised method check baseline no to annotate data weakli", "why not learning the representation using an unsupervised learning method unsupervised pre train"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be at least one of the baselin", "the idea of using surrogate labels to learn representation is also not new", "one example work is discriminative unsupervised feature learning with exemplar convolutional neural networks the authors didn't compare their method with this on", "this paper extends the previous results on differentially private sgd to user-level differentially private recurrent language model", "it experimentally shows that the proposed differentially private lstm achieves comparable utility compared to the non-private model"], "labels": ["QSN", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy commun", "this work makes a pretty significant contribution to such top", "it adapts techniques from some previous work to address the difficulties in training language model and providing user-level privaci", "it adapts techniques from some previous work to address the difficulties in training language model and providing user-level privaci", "the experiment shows good privacy and util"], "labels": ["APC", "APC", "SMY", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the presentation of the paper can be improved a bit", "for example it might be better to have a preliminary section before section introducing the original differentially private sgd algorithm with clipping the original fedavg and fedsgd and moments accountant as well as privacy amplification otherwise it can be pretty difficult for readers who are not familiar with those concepts to fully understand the pap", "such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work", "the authors introduce the task of defogging by which they mean attempting to infer the contents of areas in the game starcraft hidden by the fog of war", "the authors train a neural network to solve the defogging task define several evaluation metrics and argue that the neural network beats several naive baseline model"], "labels": ["SUG", "SUG", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the positive side the task is a nice example of reasoning about a complex hidden state space which is an important problem moving forwards in deep learn", "on the negative side from what i can tell the authors don't seem to have introduced any fundamentally new architectural choices in their neural network so the contribution seems fairly specific to mastering starcraft but at the same time the authors don't evaluate how much their defogger actually contributes to being able to win starcraft gam", "on the negative side from what i can tell the authors don't seem to have introduced any fundamentally new architectural choices in their neural network so the contribution seems fairly specific to mastering starcraft but at the same time the authors don't evaluate how much their defogger actually contributes to being able to win starcraft gam", "all of their evaluation is based on the accuracy of defog", "granted being able to infer hidden states is of course an important problem"], "labels": ["APC", "DFT", "CRT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win starcraft competitions meaning that at least as the paper is currently framed the critical evaluation metric would be showing that a defogger helps to win gam", "two ways i could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build defoggers for other domains and spelling out more explicitly what domains the authors expect their insights to generalize to or doubling down on the starcraft application specifically and showing that the defogger helps to win gam", "a minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to on", "all that said as a paper on an application of deep learning the paper appears to be solid and if the area chairs are looking for that sort of contribution then the work seems accept", "minor points- is there a benefit to having a model that jointly predicts unit presence and count rather than having two separate models eg one that feeds into the next"], "labels": ["CRT", "SUG", "DIS", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could predicting presence or absence separately be a way to encourage sparsity since absence of a unit is already representable as a count of zero", "the choice to have one model seems especially peculiar given the authors say they couldn't get one set of weights that works for both their classification and regression task", "- notation i believe the space u is never described in the main text", "what components precisely does an element of u hav", "- the authors say they use gameplay from no later than  minutes in the game to avoid the difficulties of increasing vari"], "labels": ["QSN", "CRT", "SUG", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how long is a typical gam", "is this a substantial fraction of the time of the games studi", "if it is not then perhaps the defogger would not help so much at win", "- the f performance increases are somewhat smal", "the l performance gains are bigger but the authors only compare l on true posit"], "labels": ["QSN", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this means they might have very bad error on false posit", "the authors state they are favoring the baseline in this comparison but it would be nice to have those numbers- i don't understand when the authors say the deep model has better memory than baselines which includes a perfect memory baselin", "the paper extends the idea of eigenoptions recently proposed by machado et al to domains with stochastic transitions and where state features are learn", "an eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation sr which is an occupancy measure induced here by a uniform polici", "in high-dimensional state space the authors propose to approximate that matrix with a convolutional neural network cnn"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach is evaluated in a tabular domain ie rooms and atari gam", "overall the paper is well-written and quite clear", "the proposed ideas for the extension seem natural ie use of sr and cnn", "the theorem stated in the paper seems to provide an interesting link between sr and the laplacian", "however a few points are not clear to me- is the result new or not"], "labels": ["SMY", "APC", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if i understand correctly stachenfeld et al discussed this result but didn't prove it", "is that correct", "so the provided proof is new", "- besides how are d and w exactly defin", "- finally as the matrix is not symmetric do real eigenvalues always exist"], "labels": ["DIS", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the execution of the proposed ideas in the experiments was a bit disappointing to m", "the approximated eigenoption was simply computed as a one-step greedy polici", "besides the eigenoptions seem to help for exploration as a uniform policy was used as indicated by plot d but could they help for other tasks eg learn to play atari games faster or bett", "i think that would be a more useful measure for the learned eigenopt", "during learning sr and the features what would be the impact if the gradient for sr estimation were also propag"], "labels": ["CRT", "DIS", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in figure  the trajectories generated by the different eigenoptions are barely vis", "some typos- section in the definition of g_t the expectation is taken over p as welli_w and t_w should be a subset of ", "- in  the hat is missing over psiin the definition of v_pis r only depends on s'", "this seems inconsistent with the previous definition of psi- p", "in the definition of l_{sr}s s' why psi takes phis as argument- in conclusionthat that"], "labels": ["CRT", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper takes a closer look at the analysis of sgd as variational inference first proposed by duvenaud et al and mandt et ", "in particular the authors point out that in general sgd behaves quite differently from langevin diffusion due to the multivariate nature of the gaussian nois", "as the authors show based on the fokker-planck equation of the underlying stochastic process there exists a conservative current a gradient of an underlying potential and a non-conservative current which might induce stationary persistent currents at long tim", "the non-conservative part leads to the fact that the dynamics of sgdtmay show oscillations and these oscillations may even prevent the algorithm from converging to the 'right' local optima", "the theoretical analysis is carried-out very nicely and the theory is supported by experiments on two-dimensional toy examples and fourier-spectra of the iterates of sgd"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a nice paper which i would like to see accept", "in particular i appreciate that the authors stress the importanceof 'non-equilibrium physics' for understanding the sgd process", "also the presentation is quite clear and the paper well written", "there are a few minor points which i would like to ask the authors to address why cite kingma and welling as a source for variational inference intsect", "vi is a much oldertfield and kingma and welling proposed a very special form of vi namely amortized vi with inference network"], "labels": ["APC", "APC", "APC", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a better citation would be jordan ett", "i'm not sure how much to trust the fourier-spectra", "in particular perhaps the deviations from brownian motion could also be due to the discretetnature of sgd ie that the continuous-time formalism is only an approximation of a discrete process", "could you elaborate on thi", "could you give the reader more details on how the uncertainty estimates on the fourier transformations were obtainedthank"], "labels": ["APC", "DIS", "QSN", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input spac", "the main contributions are the introduction of a unified framework that expresses  common attribution techniques gradient * input integrated gradient eps-lrp and deeplift in a similar way as modified gradient functions and the definition of a new evaluation measure 'sensitivity n' that generalizes the earlier defined properties of 'completeness' and 'summation to delta'", "the unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-lrp and deeplift substantially more easy on modern framework", "however as correctly stated by the authors some of the unification eg relation between lrp and gradient*input has been already mentioned in prior work", "sensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combin"], "labels": ["SMY", "SMY", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the measure shows interesting trends towards a linear behaviour for simpler method", "it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different method", "furthermore the authors could comment on the relation between sensitivity-n and region perturbation techniques samek et al ieee tnnl", "sensitivtiy-n seems to be an extension of the region perturbation idea to m", "it would be interesting to see the relation between the unified gradient-based explanation methods and approaches eg saliency maps alpha-beta lrp deep taylor deconvolution networks grad-cam guided backprop  which do not fit into the unification framework"], "labels": ["APC", "CRT", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's good that the author mention these works still it would be great to see more discussion on the advantages/disadvantages because these methods may have some nice theoretically properties see eg the discussion on gradient vs decompositiion techniques in montavon et al digital signal processing  which can not be incorporated into the unified framework", "it's good that the author mention these works still it would be great to see more discussion on the advantages/disadvantages because these methods may have some nice theoretically properties see eg the discussion on gradient vs decompositiion techniques in montavon et al digital signal processing  which can not be incorporated into the unified framework", "the authors propose the use of a gamma prior as the distribution over the latent representation space in gan", "the authors propose the use of a gamma prior as the distribution over the latent representation space in gan", "the motivation behind it is that in gans interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass region"], "labels": ["SUG", "DIS", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the motivation behind it is that in gans interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass region", "the use of the proposed gamma distribution as a simple alternative overcomes this problem", "in general the proposed work is very interesting and the idea is neat", "the paper is well presented and i want to underline the importance of thi", "the authors did a very good job presenting the problem motivation and solution in a coherent fashion and easy to follow"], "labels": ["DIS", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work itself is interesting and can provide useful alternatives for the distribution over the latent spac", "this paper introduces a conditional variant of the model defined in the neural statistician https//arxivorg/abs/", "the generative model defines the process that produces the dataset", "this model is first a mixture over contexts followed by iid generation of the dataset with possibly some unobserved random vari", "this corresponds to a mixture of neural statisician"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors suggest that such a model could help with disentangling factors of variation in data", "in the experiments they only consider training the model with the context selection variable and the data variables observ", "unfortunately there is minimal quantitative evaluation visualizing  mnist samples is not enough", "the only quantitative evaluation is in table  and it seems the model is not able to generalize reliably to all rotations and all digit", "clearly we can't expect perfect performance but there are some troubling results  accuracy on non-rotated s  accuracy on non-rotated "], "labels": ["SMY", "SMY", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["every digit has at least one rotation that is not well classified so this section could use more discussion and analysi", "for example how would this metric classify vae samples with contexts corresponding only to digit type no rotations how would this metric classify vanilla vae samples that are hand label", "moreover the context selection variable a should be considered part of the dataset and as such the paper should report how a was select", "moreover the context selection variable a should be considered part of the dataset and as such the paper should report how a was select", "this model is a relatively simple extension of the neural statistician so the novelty of the idea is not enough to counterbalance the lack of quantitative evalu"], "labels": ["CRT", "QSN", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this model is a relatively simple extension of the neural statistician so the novelty of the idea is not enough to counterbalance the lack of quantitative evalu", "i do think the idea is well-motivated and represents a promising way to incorporate prior knowledge of concepts into our training of va", "still the paper as it stands is not complete and i encourage the authors to followup with more thorough quantitative empirical evalu", "still the paper as it stands is not complete and i encourage the authors to followup with more thorough quantitative empirical evalu", "summarythis paper proposes a new approach to tackle the problem of prediction underthe shift in design which consists of the shift in policy conditionaldistribution of treatment given features and the shift in domain marginal distribution of featur"], "labels": ["CRT", "APC", "DFT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given labeled samples from a source domain and unlabeled samples from a targetdomain this paper proposes to minimize the risk on the target domain by jointly learning the shift-invariant representation and the re-weighting function for the induced represent", "according to lemma  and its finitesample version in theorem  the risk on the target domain can be upper boundedby the combination of  the re-weighted empirical risk on the source domain", "and  the distributional discrepancy between the re-weighted source domain andthe target domain", "these theoretical results justify the objective functionshown in equ", "experiments on the ihdp dataset demonstrates the advantage of the proposedapproach compared to its competing altern"], "labels": ["SMY", "DIS", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments this paper is well motiv", "for the task of prediction under the shift indesign shift-invariant representation learning shalit  is biased even inthe inifite data limit", "on the other hand although re-weighting methods areunbiased they suffer from the drawbacks of high variance and unknown optimalweight", "the proposed approach aims to overcome these drawback", "the theoretical results justify the optimization procedures presented insect"], "labels": ["APC", "APC", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental results on the ihdp dataset confirm the advantage ofthe proposed approach", "i have some questions on the detail", "in order to make sure the second equality in equation  holds p_mu y|xt = p_pi y|xt should hold as wel", "is this a standard assumption in the literatur", "two drawbacks of previous methods motivate this work including the bias ofrepresentation learning and the high variance of re-weight"], "labels": ["APC", "DIS", "DIS", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["according tolemma  the proposed method is unbiased for the optimal weights in the largedata limit", "however is there any theoretical guarantee or empirical evidenceto show the proposed method does not suffer from the drawback of high vari", "experiments on synthetic datasets where both the shift in policy and theshift in domain are simulated and therefore can be controlled would better demonstrate how the performance of the proposed approach and thsoe baseline methods changes as the degree of design shift vari", "besides ihdp did the authors run experiments on other real-world datasets such as jobs twins etc", "this paper proposes a hybrid homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm"], "labels": ["DIS", "QSN", "SUG", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it", "it is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear", "the results obtained as compared to fhew seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the pap", "for example reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext whenever the number of s in the plain bit-string is greater than the number of s /pag", "result interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets us"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall fine work well organized decomposed and its rationale clearly explain", "the good results obtained support the design decisions mad", "our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claim", "nota      - in figure /page  and table a/b shouldn't  a and b b", "- unlike figure /page  in figure /page  shouldn't  operations' precedence prevail no brackets therefore +*="], "labels": ["APC", "APC", "SUG", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposed a procedure for assessing the performance of gans by re-considering the key of observ", "and using the procedure to test and improve current version of gans it demonstrated some interesting stuff", "it is not easy to follow the main idea of the pap", "the paper just told difference stories section by sect", "based on my understanding the claims are  the new formalization of the goal of gan training and  using this test to evaluate the success of gan algorithms empir"], "labels": ["SMY", "SMY", "CRT", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suggested that the author should reform the structure ignore some unrelated content and make the clear claims about the contributions on the introduction part", "regarding the experimental part it can not make strong support for all the claim", "figure  showed almost similar plots for all the varieti", "meanwhile the results are performed on some specific model configurations like resnet and settings it is difficult to justify whether it can generalize to other cas", "some of the figures do not have the notations of curvey making people hard to compar"], "labels": ["SUG", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore i think the current version is not ready to be published the author can make it stronger and consider next venu", "overall i like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image super-resolution etc", "unlike the generative distribution sampling of gans the method provides an interesting compositional scheme where the low frequencies are regressed and the high frequencies are obtained by copying patches from the training set", "in some cases the results are similar to pix-to-pix also in the numerical evaluation but the method allows for one-to-many image generation which is a important contribut", "another positive aspect of the paper is that the synthesis results can be analyzed providing insights for the generation process"], "labels": ["FBK", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while most of the paper is well written", "some parts are difficult to pars", "for example the introduction has some parts that look more like related work that is mostly a personal preference in writting also in section  the paragraph for distance functions do not provide any insight about what is used but it is included in the next paragraph i would suggest either merging or not highlighting the paragraph", "q the spatial grouping that is happening in the compositional stage is it solely due to the multi-scale hypercolumn", "would the result be more inconsistent if the hypercolumns had smaller receptive field"], "labels": ["APC", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["q for the multiple outputs the k neighbor is selected at random", "the paper combines several recent advances on generative modelling including a ladder variational posterior and a pixelcnn decoder together with the proposed convolutional stochastic layers to boost the nll results of the current va", "the numbers in the tables are good but i have several comments on the motivation originality and experi", "the numbers in the tables are good but i have several comments on the motivation originality and experi", "most parts of the paper provide a detailed review of the literatur"], "labels": ["QSN", "SMY", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the resulting model is quite like a combination of the existing advances and the main contribution of the paper ie the convolution stochastic layer is not well discuss", "why should we introduce the convolution stochastic lay", "could the layers encode the spatial information better than a deterministic convolutional layer with the same architectur", "what's the exact challenge of training vaes addressed by the convolution stochastic lay", "please strengthen the motivation and originality of the pap"], "labels": ["CRT", "QSN", "QSN", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["please strengthen the motivation and originality of the pap", "though the results are good", "i still wonder what is the exact contribution of the convolutional stochastic layers to the nll result", "can the authors provide some results without the ladder variational posterior and the pixelcnn decoder on both the gray-scaled and the natural imag", "according to the experimental setting in the section  page  paragraph  in case of gray-scaled images the stochastic latent layers are dense with sizes      equivalent to sufnderby et al  and for the natural images they are spatial cf t"], "labels": ["DIS", "APC", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there was no significant difference when using feature maps as compared to dense layers for modelling gray-scaled images there is no stochastic convolutional lay", "then is there anything new in fame on the gray imag", "furthermore how could fame advance the previous state-of-the-art", "it seems because of other factors instead of the stochastic convolutional lay", "the results on the natural images are not complet"], "labels": ["CRT", "QSN", "QSN", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["please present the generation results on the imagenet dataset and the reconstruction results on both the cifar and imagenet dataset", "please present the generation results on the imagenet dataset and the reconstruction results on both the cifar and imagenet dataset", "the quality of the samples on the cifar dataset seems not competitive to the baseline papers listed in the t", "though the visual quality does not necessarily agree with the nll results but such large gap is still strang", "besides why fame can obtain both good nll and generation results on the mnist and omniglot datasets when there is no stochastic convolutional lay"], "labels": ["DIS", "DFT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["meanwhile why fame cannot obtain good generation results on the cifar dataset", "is it because there is a lot randomness in the stochastic convolutional lay", "it is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the nnl result", "minor thingsplease rewrite the sentence when performing reconstructions during training  while also using the stochastic latent variables z = z    z l", "*summary*the paper applies variational inference vi with the 'reparameterisation' trick for bayesian recurrent neural networks brnn"], "labels": ["QSN", "QSN", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper first considers the bayes by backprop approach of blundell et al  and then modifies the brnn model with a hierarchical prior over the network parameters which then requires a hierarchical variational approximation with a simple linear recognition model", "several experiments demonstrate the quality of the prediction and the uncertainty over dropout", "several experiments demonstrate the quality of the prediction and the uncertainty over dropout", "*originality + significance*to my knowledge there is no other previous work on vi with the reparameterisation trick for brnn", "however one could say that this paper is on careful examination an application of reparameterisation gradient vi for a specific appl"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nevertheless the parameterisation of the conditional variational distribution qtheta | phi x y using recognition model is interesting and could be useful in other model", "however this has not been tested or concretely shown in this pap", "the idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new see hierarchical variational model ranganath et al  for exampl", "*clarity*the paper is in general well-written however the presentation in  is hard to follow", "*clarity*the paper is in general well-written however the presentation in  is hard to follow"], "labels": ["APC", "DFT", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would prefer if appendix a was moved up front -- in this case it would make it clear that the model is modified to contain phi a variational approximation over both theta and phi is needed and a q that couples theta phi and and the gradient of the log likelihood term wrt phi is chosen", "additional commentswhy is the variational approximation called sharpenedat test time normal vi just uses the fixed qtheta after train", "it's not clear to me how prediction is done when using 'posterior sharpening' -- how is qtheta | phi x in eqs - parameteris", "the first paragraph of page  uses qtheta | phi x y but y is not known at test tim", "what is c in eq"], "labels": ["CRT", "QSN", "CRT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this comment variational typically underestimate the uncertainty in the posteriorwhereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty is not precis", "this comment variational typically underestimate the uncertainty in the posteriorwhereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty is not precis", "ep can do mode averaging as well as mode seeking depending on the underlying and approximate factor graph", "in the bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood ep is just as mode-seeking as vari", "on the other hand variational methods can avoid modes too see the mixture of gaussians example in the two problems with variational em  paper by turner and sahani"], "labels": ["DFT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energi", "was there any kl reweighting scheduling as done in the original bbb pap", "what is the significance of the difference between bbb and bbb with sharpening in the language modelling task", "was sharpening used in the image caption generation task", "what is the computational complexity of bbb with posterior sharpen"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["twice that bbb if this is the case would bbb get to the same performance if we optimise it for long", "would be interesting to see the time/accuracy fronti", "the paper studies convolutional neural networks where the stride is smaller than the convolutional filter s", "the so called overlapping convolutional architectur", "the main object of study is to quantify the benefits of overlap in convolutional architectur"], "labels": ["QSN", "FBK", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main claim of the paper is theorem  which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures i", "the main claim of the paper is theorem  which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures i", "there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architectur", "whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectur", "the main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of cohen and shashua which capture the relationship between the inputs and the outputs the function implemented by the neural network"], "labels": ["SMY", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results of the paper hold only for product pooling and linear activation function except for the representation layer which allows general funct", "it is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special cas", "that this is the case should be made clear in the title and abstract", "the paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general cas", "the paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general cas"], "labels": ["DFT", "DFT", "QSN", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but since it is left as future work the paper should make it clear throughout", "the experiment is minimal and even the given experiment is not described wel", "what data augmentation was used for the cifar- dataset", "it is only mentioned that the data is augmented with translations and horizontal flip", "what is the factor of augment"], "labels": ["DFT", "APC", "QSN", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how much transl", "these are important because there maybe a much simpler explanation to the benefit of overlap it is able to detect these translated patterns easili", "indeed this simple intuition seems to be why the authors chose to make the problem by introducing translations and flip", "indeed this simple intuition seems to be why the authors chose to make the problem by introducing translations and flip", "it is unclear if the paper resolves the mystery that they set out to solve which is a reconciliation of the following two observ"], "labels": ["QSN", "SMY", "DFT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is unclear if the paper resolves the mystery that they set out to solve which is a reconciliation of the following two observ", "a why are non-overlapping architectures so common", "b why only slight overlap is used in practic", "the paper seems to claim that since overlapping architectures have higher expressivity that answers a", "it appears that the paper does not answer b well it points out that since there is exponential increase there is no reason to increase it beyond a particular point"], "labels": ["QSN", "QSN", "QSN", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it appears that the paper does not answer b well it points out that since there is exponential increase there is no reason to increase it beyond a particular point", "it seems the right resolution will be to show that after the overlap is set to a certain small valu", "it seems the right resolution will be to show that after the overlap is set to a certain small valu", "there will be *only* linear increase with increasing overlap ie the paper should show that small overlap networks are efficient with respect to *large* overlap networks a comparison that does not seem to be made in the pap", "there will be *only* linear increase with increasing overlap ie the paper should show that small overlap networks are efficient with respect to *large* overlap networks a comparison that does not seem to be made in the pap"], "labels": ["DIS", "SMY", "DIS", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["small typo the dimensions seem to be wrong in the line below the equation in pag", "the paper makes important progress on a highly relevant problem using a new methodology borrowed from a previous pap", "however the writing is hurried and the high-level conclusions are not fully supported by theory and experi", "this is a very well-written paper that shows how to successfully use generative autoencoders together with the discriminative domain adversarial neural network dann of ganin et ", "the construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem"], "labels": ["DFT", "APC", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the only criticism that i have towards this analysis is that the concept of shared parameter between the discriminative and predictive model denoted by zeta in the paper disappear when it comes to designing the learning model", "the authors perform numerous empirical experiments on several types of problem", "they successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation task", "", "on the downside all these experiments concern predictive discriminative problem"], "labels": ["CRT", "SMY", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the paper title i would have expected some experiments in a generative context", "also a comparison with the generative adversarial networks of goodfellow et al  would be a plu", "i would also like to see the results obtained using dann stacked on msda representations as it is done in ganin et ", "minor comments- paragraph below equation   the meaning of $phipsi$ is unclear", "- equation  phi and psi seems invert"], "labels": ["DIS", "SUG", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- section  the acronym mlp is used but never defin", "=== update ===i lowered my score and confidence see my new post below", "this paper proposes a clever new test based on the birthday paradox for measuring diversity in generated sampl", "the main goal is to quantify mode collapse in state-of-the-art generative model", "the authors also provide a specific theoretical construction that shows bidirectional gans cannot escape specific cases of mode collaps"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["using the birthday paradox test the experiments show that gans can learn and consistently reproduce the same examples which are not necessarily exactly the same as training data eg the triplets in figur", "the results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative model", "bidirectional models ali bigans however demonstrate significantly higher diversity that dcgans and mix+dcgan", "finally the authors verify empirically the hypothesis that diversity grows linearly with the size of the discrimin", "this is a very interesting area and exciting work"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main idea behind the proposed test is very insight", "the main theoretical contribution stimulates and motivates much needed further research in the area", "in my opinion both contributions suffer from some significant limit", "however given how little we know about the behavior of modern generative models it is a good step in the right direct", "the biggest issue with the proposed test is that it conflates mode collapse with non-uniform"], "labels": ["APC", "APC", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors do mention this issue but do not put much effort into evaluating its implications in practice or parsing theorems  and", "my current understanding is that in practice when the birthday paradox test gives a collision i have no way of knowing whether it happened because my data distribution is modal or because my generative model has bad divers", "anecdotally real-life distributions are far from uniform so this should be a common issu", "i would still use the test as a part of a suite of measurements but i would not solely rely on it", "i feel that the authors should give a more prominent disclaimer to potential users of the test"], "labels": ["CRT", "DIS", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also given how mode collapse is the main concern it seems to me that a discussion on coverage is miss", "the proposed test is a measure of diversity not coverage so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribut", "as long as they yield collisions at the same rate these two generative models are uequally diverseu isnut coverage of equal import", "the other main contribution of the paper is theorem  which showsuvia a very particular construction on the generator and encoderuthat bidirectional gans can also suffer from serious mode collaps", "i welcome and are grateful for any theory in the area"], "labels": ["DFT", "CRT", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this theorem might very well capture the underlying behavior of bidirectional gans however being constructive it guarantees nothing in practic", "in light of this the statement in the introduction that ucencoder-decoder training objectives cannot avoid mode collapseud might need to be qualifi", "in particular the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of theorem", "the paper attempts to extend the predictive coding model to a multilayer network", "the math is developed for a learning rule and some demonstrations are shown for reconstructions of cifar- imag"], "labels": ["CRT", "CRT", "CRT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the overall idea and approach being pursued here is a good on", "but the model needs further develop", "the paper presents an interesting spectral algorithm for multiscale hmm", "the derivation and analysis seems correct", "however it is well-known that spectral algorithm is not robust to model mis-specif"], "labels": ["APC", "CRT", "SMY", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear whether the proposed algorithm will be useful in practic", "how will the method compare to em algorithms and neural network based approach", "authors describe a procedure of building their production recommender system from scratch begining with formulating the recommendation problem label data formation model construction and learn", "they use several different evaluation techniques to show how successful their model is offline metrics a/b test results etc", "most of the originality comes from integrating time decay of purchases into the learning framework"], "labels": ["DFT", "QSN", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rest of presented work is more or less standard", "paper may be useful to practitioners who are looking to implement something like this in product", "qualitythe paper appears to be correctclaritythe paper is clear although more formalization would help sometimesoriginalitythe paper presents an analysis for unsupervised learning of mapping between  domains that is totally new as far as i know", "significancethe points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domain", "pros/conspros-adresses an important problem in representation learn"], "labels": ["APC", "APC", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["]n-the paper proposes interesting assumptions and results for measuring the complexity of semantic map", "-a new cross domain mapping is proposed-large set of experimentscons-some parts deserve more formalization/justification-too many materials for a conference paper-the cost of the algorithm seem", "high summarythis paper studies the problem of unsupervised learning of semantic map", "it proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain map", "a theoretical result shows that the number of low-discrepancy between cross-domains mappings of low complexity is rather smal"], "labels": ["DIS", "DIS", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a large set of experiments are provided to support the claims of the pap", "comments-the work is interesting for an important problemin representation learning while in machine learning in general with the unsupervised aspect", "-in a sense i find that the approach suggested by algorithm  has some connections with structural risk minimization by increasing k and k - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss", "the approach seems costly anyway and i wonder if the authors could think of a smoother version of the algorithm to make it more effici", "n-for counting the minimal complexity mappings i wonder if one can make a connection with algorithm robustness of xu&mannorcolt where instead of comparing losses you work with discrepanciestyposection  is build of - is built of"], "labels": ["DIS", "APC", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is clear and well written", "the proposed approach seems to be of interest and to produce interesting result", "as datasets in various domain get more and more precise the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem and this paper is a promising contribution to handle those issues bett", "as datasets in various domain get more and more precise the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem and this paper is a promising contribution to handle those issues bett", "the paper proposes to use a top-k loss such as what has been explored with svms in the past but with deep model"], "labels": ["APC", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as the loss is not smooth and has sparse gradients the paper suggests to use a smoothed version where maximums are replaced by log-sum-exp", "i have two main concerns with the present", "a/ in addition to the main contribution the paper devotes a significant amount of space to explaining how to compute the smoothed loss", "this can be done by evaluating elementary symmetric polynomials at well-chosen valu", "the paper argues that classical methods for such evaluations eg using the usual recurrence relation or more advanced methods that compensate for numerical errors are not enough when using single precision floating point arithmet"], "labels": ["SUG", "SMY", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper also advances that gpu parallelization must be used to be able to efficiently train the network", "those claims are not substantiated however and the method proposed by the paper seems to add substantial complexity without really proving that it is us", "those claims are not substantiated however and the method proposed by the paper seems to add substantial complexity without really proving that it is us", "the paper proposes a divide-and-conquer approach where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial valu", "i am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch"], "labels": ["SMY", "DFT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe the paper could justify this approach better by providing a bit more insights as to why it is requir", "i believe the paper could justify this approach better by providing a bit more insights as to why it is requir", "for instance- what accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomi", "- how do those compare with ce and l_{ } with the proposed method", "- are numerical instabilities making this completely unfeas"], "labels": ["SUG", "APC", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this would be especially interesting to understand if this explodes in practice or if evaluations are just a slightly inaccurate without much accuracy loss", "b/ no mention is made of the object detection problem although multiple of the motivating examples in figure  consider cases that would fall naturally into the object detection framework", "although top-k classification considers in principle an easier problem no localization a discussion as well as a comparison of top-k classification vs eg discarding localization information out of object detection methods could be interest", "additional comments- figure b this visualization is confus", "this is presented in the same figure and paragraph as the cifar result"], "labels": ["SMY", "DFT", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but instead uses a single synthetic data point in dimension  and k=", "this is not convinc", "an actual experiment using full dataset or minibatch gradients on cifar and the same k value would be more interest", "an actual experiment using full dataset or minibatch gradients on cifar and the same k value would be more interest", "summary of the paperthe paper suggests to use stochastic parameters in combination with the local reparametrisation trick previously introduced by kingma et al  to train neural networks with binary or ternary wight"], "labels": ["SMY", "DFT", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results on mnist cifar- and imagenet are very competit", "pros- the proposed method leads to state of the art result", "- the paper is easy to follow and clearly describes the implementation details needed to reach the result", "cons- the local reprarametrisation trick it self is not new and applying it to a multinomial distribution with one repetition instead of a gaussian is straight forward", "but its application for learning discrete networks is to my best knowledge novel and interest"], "labels": ["SMY", "SMY", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it could be nice to include the results of zuh et al  in the results table and to indicate the variance for different samples of weights resulting from your methods in bracket", "minor comments- some citations have a strange format eg ucin hubara et al  restegari et al uc would be better readable as   ucby hubara et al  and restegari et al uc", "-  to improve notation it could be directly written that w is the set of all w^l_{ij} and mathcal{w} is the joint distribution resulting from independently sampling from  mathcal{w}^l_{ij}", "- page  ucon the last full precision networkud should probably be ucon the last full precision layerud", "uc distributions hasud -  uc distributions haveud"], "labels": ["SUG", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent which is called narrator by the authors that obfuscates the document ie changing words in the docu", "the authors mention that word dropout can be considered as its special case which randomly drops words without any prior", "then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent which in turn would improve the performance of the reader as wel", "hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer which predicts the word that would make the reader fail if the word is obfusc", "i think the idea is interesting and novel"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while there have been numerous gan-like approaches for language understanding very few if any have shown worthy result", "so if this works it could be an impactful achiev", "however i am concerned with the experimental result", "first cbt ne and cn numbers are too low", "even a pure lstm achieves no attention no memory % and % respectively yu et "], "labels": ["DIS", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these are % and % higher than the reported numbers for adversarial gmemnn", "so it is very difficult to determine if the model is appropriate for the dataset in the first place and whether the gain from the non-adversarial setting is due to the adversarial setup or not", "second cambridge dialogs the dataset's metric is not accuracy-based while the paper reports accuracy so i assume some preprocessing and altering have been done on the dataset", "so there is no baseline to compar", "though i understand that the point of the paper is the improvement via the adversarial setting it is hard to gauge how good the numbers ar"], "labels": ["DIS", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["third tripadvisor the dataset paper by wang et al  is not evaluated on accuracy rather on ranking etc", "did you also make changes to the dataset", "again this makes the paper less strong because there is no baseline to compar", "in short the only comparable dataset is cbt which has too low accuracy compared to a very simple baselin", "in order to improve the paper i recommend the authors to evaluate on more common datasets and/or use more appropriate reading model"], "labels": ["CRT", "QSN", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["---typospage  first para one the first hand - on the first handpage  first para minimize to probability - minimize the probabilitypage  first para compensate - compensatedpage  last para softmaxis - softmax ispage  sec  similar to the reader - similarly to the readerpage  sec  unknow - unknown", "page  sec  first para missing reference at a given dialogpage  first para concretly - concretelytable  gmennn - gmemnntable  what is difference between mean and averagepage  last para missing reference at iterative attentive readerpage  sec  last para several citations missing eg which paper is by tesauro", "[yu et al ] adams wei yu hongrae kim and quoc v le learning to skim text acl", "the paper is about hyperparameter optimization which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithm", "the paper is about hyperparameter optimization which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithm"], "labels": ["CRT", "CRT", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at a high-level hyperparameter optimization for the challenging case of discrete variables can be seen as a black-box optimization problem where we have only access to a function evaluation oracle but no gradients etc", "in the entirely unstructured case there are strong lower bounds with an exponential dependence on the number of hyperparamet", "in order to sidestep these impossibility results the current paper assumes structure in the unknown function mapping hyperparameters to classification accuraci", "in particular the authors assume that the function admits a representation as a sparse and low-degree polynomi", "while the authors do not empirically validate whether this is a good model of the unknown function it appears to be a reasonable assumption the authors *do* empirically validate their overall approach"], "labels": ["DFT", "SMY", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["based on the sparse and low-degree assumption the paper introduces a new algorithm called harmonica for hyperparameter optim", "the main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements ie function evalu", "the authors derive relevant sample complexity results for their approach", "moreover the method also yields new algorithms for learning decision tre", "in addition to the theoretical results  the authors conduct a detailed study of their algorithm on cifar"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they compare to relevant recent work in hyperparameter optimization bayesian optimization random search bandit algorithms and find that their method significantly improves over prior work", "the best parameters found by harmonica improve over the hand-tuned results for their base architecture resnet", "noverall i find the main idea of the paper very interesting and well executed both on the theoretical and empirical sid", "hence i strongly recommend accepting this pap", "hence i strongly recommend accepting this pap"], "labels": ["APC", "APC", "APC", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["small comments and quest", "it would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial eg mse of the best fit", "a comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem", "the current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques eg zoph & le iclr", "the current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques eg zoph & le iclr"], "labels": ["DFT", "APC", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while it might be hard to compare to this approach directly in experiments it would still be good to mention this work and discuss how it relates to the current pap", "did the authors tune the hyperparameters directly using the cifar test accuraci", "would it make sense to use a slightly smaller training set and to hold out say k images for hyperparameter evaluation before making the final accuracy evaluation on the test set", "would it make sense to use a slightly smaller training set and to hold out say k images for hyperparameter evaluation before making the final accuracy evaluation on the test set", "the current approach could be prone to overfit"], "labels": ["APC", "QSN", "DIS", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while random search does not explicitly exploit any structure in the unknown function it can still implicitly utilize smoothness or other benign properties of the hyperparameter spac", "while random search does not explicitly exploit any structure in the unknown function it can still implicitly utilize smoothness or other benign properties of the hyperparameter spac", "it might be worth adding this in the discussion of the related work", "n algorithm  why is the argmin for g_i  what does the index i refer to", "why does psr truncate the indices in alpha"], "labels": ["SMY", "DIS", "DFT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at least in standard compressed sensing the lasso also has recovery guarantees without truncation and empirically works sometimes better without", "at least in standard compressed sensing the lasso also has recovery guarantees without truncation and empirically works sometimes better without", "n definition  should c be a class of functions mapping {- }^n to r  note the superscript", "on page  we assume that k =  but theorem  still maintains a dependence on k", "it might be cleaner to either treat the general k case throughout or state the theorem for k ="], "labels": ["SMY", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on cifar the best hyperparameters do not improve over the state of the art with other models eg a wide resnet", "on cifar the best hyperparameters do not improve over the state of the art with other models eg a wide resnet", "it could be interesting to run harmonica in the regime where it might improve over the best known models for cifar", "similarly it would be interesting to see whether the hyperparameters identified by harmonica carry over to give better performance on imagenet", "the authors claim in c that the hyperparameters identified by harmonica generalize from small networks to large network"], "labels": ["SMY", "DIS", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as wel", "testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as wel", "this paper proposes a model using hidden neurons with self-organising activation function whose outputs feed to classifier with softmax output funct", "it is trained with supervised learning by minimising the cross-entropy error between labels and the softmax outputthe paper's claim of combining unsupervised self-organising with supervised training is misleading and confus", "in this model self-organising is a property of the hidden neurons' activation eq - and the training procedure is entirely supervis"], "labels": ["SMY", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is misleading to claim any unsupervised or semi-supervised learning based on the *self-organising part* of for example eq  which is merely a result of applying chain rule through the hidden neurons' activ", "while this model is proposed as an extension of kohonen's self-organising map som the paper fails to mention or compare with several historically important extension of som which should perhaps at least include the generative topographic mapping gtm bishop et al  an important probabilistic generalisation of som", "while this model is proposed as an extension of kohonen's self-organising map som the paper fails to mention or compare with several historically important extension of som which should perhaps at least include the generative topographic mapping gtm bishop et al  an important probabilistic generalisation of som", "finally the evaluation of the model in comparison with other models is question", "for example while the configuration the paper's baseline models are not given the baseline accuracy of mnist classification using mlp is %"], "labels": ["CRT", "DFT", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is much worse than the baseline of % in lecun et al  using simple linear classifier without any preprocess", "the % accuracy from the proposed model is not in the range of modern deep learning models the state-of-art accuracy is %", "similar problem also exist in results from other dataset", "they are therefore unable to support the paper's claim on robust perform", "prosthe question of internal representation is interest"], "labels": ["DFT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["combining self-organising with classif", "comparing learned representations from different model", "consnot clearly written", "mixing the concept of unsupervised/semi-supervised learning is confus", "model evaluation is question"], "labels": ["DIS", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does not compare existing extensions of som", "the paper proposes a new method for detecting out of distribution sampl", "the core idea is two fold when passing a new image through the already trained classifier first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second add a temperature to the softmax", "then a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribut", "this paper is well written easy to understand and presents a simple and apparently effective method of detecting out of distribution sampl"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors evaluate on cifar-/ and several out of distribution datasets and this method outperforms the baseline by significant margin", "they also examine the effects of the temperature and step size of the perturb", "my only concern is that the parameter delta threshold used to determine in/out distribution is not discussed much", "they seem to optimize over this parameter but this requires access to the out of distribution set prior to the final evalu", "could the authors comment on how sensitive the method is to this paramet"], "labels": ["SMY", "SMY", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how much of the out of distribution dataset is used to determine this value and what are the effects of this size during tun", "what happens if you set the threshold using one out of distribution dataset and then evaluate on a different on", "this seems to be the central part missing to this pap", "and if the authors are able to address it satisfactorily i will increase my scor", "and if the authors are able to address it satisfactorily i will increase my scor"], "labels": ["QSN", "QSN", "DFT", "SUG", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper introduces a generative model for graph", "the three main decision functions in the sequential process are computed with neural net", "the neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step t", "the paper is well written", "but in my opinion a description of the learning framework should be given in the pap"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also a summary of the hyperparameters used in the proposed system should be given", "it is claimed that all possible types of graphs can be learned which seems rather optimist", "for instance when learning trees the system is tweaked for generating tre", "also it is not clear whether models for large graphs can be learn", "the paper contain many interesting contribut"], "labels": ["CRT", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but in my opinion the model is too general and the focus should be given on some retricted classes of graph", "therefore i am not convinced that the paper is ready for publication at iclr'", "* introduction i am not convinced by the discussion on graph grammars in the second paragraph", "it is known that there does not exist a definition of regular grammars in graph see courcelle and engelfriet graph structure and monadic second-order log", "moreover many problems are known to be undecid"], "labels": ["APC", "FBK", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for weighted automata the reference droste and gastin considers weighted word automata and weighted logic for word", "therefore i does not seem pertinent her", "a more complete reference is handbook of weighted automata by drost", "also many decision problems for wighted automata are known to be undecid", "i am not sure that the paragraph is useful for the pap"], "labels": ["DIS", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a discussion on learning as in footnote  shoud me more interest", "* related work", "i am not expert in the field but i think that there are recent references which could be cited for probablistic models of graph", "* section  constraints can be introduced to impose structural properties of the generated graph", "this leads to the question of cheating in the learning process"], "labels": ["SUG", "DIS", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* section  the functions f_m and g_m for defining graph embedding are left undefin", "as the graph embedding is used in the generating process and for learning the functions must be defined and their choice explained and justifi", "* section  as said before a general description of the learning framework should be given", "also it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process", "therefore it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learn"], "labels": ["CRT", "DIS", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consequently it is difficult to see the influence of t", "also it should be said whether the node embeddings and graph embeddings for the output graph can be us", "* section  a summary of all the hyperparameters should be given", "* section  the number of steps is not given", "do you present the same graph multiple tim"], "labels": ["CRT", "CRT", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why t= and not  or", "* section  from table  it seems that all permutations are used for training which is rather large for molecules of s", "do you use tweaks in the generation process", "* section  the generation process is adapted for generating trees which seems to be ch", "again the choice of t seems ad hoc and based on computational burden"], "labels": ["QSN", "DIS", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* section  should contain a discussion on complexity issues because it is not clear how the model can learn large graph", "* section  the discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparamet", "* acronyms should be expansed at their first us", "in this paper the authors look to improve neural architecture search nas which has been successfully applied to discovering successful neural network architectures albeit requiring many computational resourc", "the authors propose a new approach they call efficient neural architecture search enas whose key insight is parameter shar"], "labels": ["CRT", "DIS", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in nas the practitioners have to retrain for every new architecture in the search process but in enas this problem is avoided by sharing parameters and using discrete mask", "in both approaches reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metr", "since we can encode a neural network as a sequence the policy can be parameterized as an rnn where every step of the sequence corresponds to an architectural choic", "in their experiments enas achieves test set metrics that are almost as good as nas yet require significantly less computational resources and tim", "the authors present two enas models one for cnns and another for rnn"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["initially it seems like the controller can choose any of b operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connect", "however in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large so the authors use only one restriction to reduce the size of the state spac", "this is a limitation as the model space is not as flexible as one would desire in a discovery task", "moreover their best results and those they choose to report in the abstract are due to fixing  parallel branches at every layer combined with a  x  convolution and using enas to learn the skip connect", "thus they are essentially learning the skip connections while using a human-selected model"], "labels": ["SMY", "SMY", "DFT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["enas for rnns is similar while nas searches for a new architecture the authors use a recurrent highway network for each cell and use enas to find the skip connect", "thus it seems like the term efficient neural architecture search promises too much since in both tasks they are essentially only using the controller to find skip connect", "although finding an appropriate architecture for skip connections is an important task finding an efficient method to structure rnn cells seems like a significantly more important go", "overall the paper is well-written and it brings up an important idea that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process", "moreover using binary masks to control network path essentially corresponding to training different models is a neat idea"], "labels": ["SMY", "CRT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is also impressive how much faster their model performs on tasks without sacrificing much perform", "the main limitation is that the best architectures as currently described are less about discovery and more about human input", "-- finding a more efficient search path would be an important next step", "this work proposes to densely connected layers to rnns by concatenating previously constructed layers together as an input to the current lay", "in addition attention context is computed for each layer then combined together as a single context"], "labels": ["APC", "DFT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental results on english-french and english-german translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of paramet", "motivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reason", "motivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reason", "however i have some concerns to this pap", "- it is a combination of two techniques dense connections and multiple attention and it is not clear where the actual gain come from"], "labels": ["SMY", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it is a combination of two techniques dense connections and multiple attention and it is not clear where the actual gain come from", "i'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechan", "- it is not clear why the experiments for dense sticked to a particular hidden size eg  for machine translation and varies only the number of lay", "do you have experiments by fixing the number of layers and varying the hidden s", "other comment- section  sequence-to=sequence - sequence-to-sequence- it is not clear why the concatenation of all layers is not experimented which is mentioned in section  memory problem"], "labels": ["CRT", "CNT", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["congratulations on a very interesting and clear pap", "while iclr is not focused on neuroscientific studies this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navig", "interestingly these include representations that have been observed in mammals and that have attracted considerable attention even honored with a nobel pr", "i found it is very interesting that the emergence of these representations was contingent on some regularization constraint", "this seems similar to the visual domain where edge detectors emerge easily when trained on natural images with sparseness constraints as in olshausen&field and later reproduced with many other models that incorporate sparseness constraint"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do have some questions about the training itself", "the paper mentions a metabolic cost that is not specified in the pap", "this should be ad", "my biggest concern is about figure a", "i am puzzled why is the error is coming down before the boundary interact"], "labels": ["DIS", "DFT", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even more puzzling why does this error go up again for the blue curve no interaction shouldnut at least this curve be smooth", "the authors argue that the spectral dimensionality reduction techniques are too slow due to the complexity of computing the eigenvalue decomposition and that they are not suitable for out-of-sample extens", "they also note the limitation of neural networks which require huge amounts of data to properly learn the data structur", "the authors therefore propose to first sub-sample the data and afterwards to learn an mds-like cost function directly with a neural network resulting in a parametric framework", "the paper should be checked for grammatical errors such as eg consistent use of no hyphen in low-dimensional or low dimension"], "labels": ["QSN", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the abbreviations should be written out on the first use eg mlp mds lle etc", "in the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points or that moving to parametric techniques would reduce memory and computational complex", "this is in general not tru", "even if the number of parameters is small learning them might require complex computations on the whole data set", "on the other hand even if the number of parameters is equal to the number of data points the computations could be trivial thus resulting in a complexity of on"], "labels": ["CRT", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  the authors claim spectral techniques are non-parametric in nature this is wrong again", "eg pca can be formulated as mds thus spectral but can be seen as a parametric mapping which can be used to project new word", "in section  it says observation that the double cent", "can you provide a citation for thi", "in section  the authors propose they technique which should be faster and require less data than the previous methods but to support their claim they do not perform an analysis of computational complex"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not quite clear from the text what the resulting complexity would b", "with n as number of data points and m as number of landmarks from the description on page  it seems the complexity would be on + m^ but the steps  and  on page  suggest it would be on^ + m^", "unfortunately it is also not clear what the complexity of previous techniques eg drlim i", "figure  contrary to text does not provide a visualisation to the sampling mechan", "in the experiments section can you provide a citation for adam and explain how the parameters were select"], "labels": ["CRT", "DIS", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also it is not meaningful to measure the quality of a visualisation via the mds fit", "there are more useful approaches to this task such as the quality framework [*]", "in figure a x-axis should be number of landmark", "it is not clear why the equation  hold", "citat"], "labels": ["CRT", "CRT", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is also not clear how exactly the equation  is evalu", "it says by varying the number of layers and the number of nodes but the nodes and layer are not a part of the equ", "the notation for equation  is not explain", "figure a shows visualisations by different techniques and is evaluated by looking at it", "again use [*][*] lee john aldo  verleysen michel scale-independent quality criteria for dimensionality reduction in pattern recognition letters vol  no  p -  doi/jpatrec"], "labels": ["CRT", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a very clearly written paper and a pleasure to read", "it combines some mechanisms known from previous work for summarization intra-temporal attention pointing mechanism with a switch with novel architecture design components intra-decoder attention as well as a new training objective drawn from work from reinforcement learning which directly optimizes rouge-l", "the model is trained by a policy gradient algorithm", "while the new mechanisms are simple variants of what is taken from existing work the entire combination is well tested in the experi", "rouge results are reported for the full hybrid rl+ml model as well as various versions that drop each of the new components rl training intra-attent"], "labels": ["APC", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the best method finally outperforms the lea-d baseline for summar", "what makes this paper more compelling is that they compared against a recent extractive method durret et al  and the fact that they also performed human readability and relevance assessments to demonstrate that their ml+rl model doesn't merely over-optimize on roug", "it was a nice result that only optimizing rouge directly leads to lower human evaluation scores despite the fact that that model achieves the best rouge- and rouge-l performance on cnn/daily mail", "some minor points that i wonder about - the heuristic against repeating trigrams seems quite crud", "some minor points that i wonder about - the heuristic against repeating trigrams seems quite crud"], "labels": ["SMY", "SMY", "APC", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some minor points that i wonder about - the heuristic against repeating trigrams seems quite crud", "is there a more sophisticated method that can avoid redundancy without this heurist", "is there a more sophisticated method that can avoid redundancy without this heurist", "- what about a reward based on a general language model rather than one that relies on l_{ml} in equ", "- what about a reward based on a general language model rather than one that relies on l_{ml} in equ"], "labels": ["CRT", "SMY", "QSN", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the lm part really is to model grammaticality and coherence a general lm might be suitable as wel", "if the lm part really is to model grammaticality and coherence a general lm might be suitable as wel", "- why does rouge-l seem to work better than rouge- or rouge- as the reward", "do you have any insights are speculations regarding thi", "instead of either optimization-based variational em or an amortized inference scheme implemented via a neural network as in standard vae models this paper proposes a hybrid approach that essentially combines the two"], "labels": ["SMY", "APC", "APC", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the vae inference step ie estimation of qz|x is conducted via application of a recent learning-to-learn paradigm andrychowicz et al  whereby direct gradient ascent on the elbo criteria with respect to moments of qz|x is replaced with a neural network that iteratively outputs new parameter estimates using these gradi", "the resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estim", "although probably difficult for someone to understand that is not already familiar with vae models i felt that this paper was nonetheless clear and well-presented with a fair amount of useful background information and context", "from a novelty standpoint though the paper is not especially strong given that it represents a fairly straightforward application of andrychowicz et ", "indeed the paper perhaps anticipates this perspective and preemptively offers that variational inference is a qualitatively different optimization problem than that considered in andrychowicz et al  and also that non-recurrent optimization models are being used for the inference task unlike prior work"], "labels": ["DIS", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but to me these are rather minor differentiating factors since learning-to-learn is a quite general concept already and the exact model structure is not the key novel ingredi", "that being said the present use for variational inference nonetheless seems like a nice application and the paper presents some useful insights such as section  about approximating posterior gradi", "beyond background and model development the paper presents a few experiments comparing the proposed iterative inference scheme against both variational em and pure amortized inference as in the original standard va", "while these results are enlighten", "most of the conclusions are not entirely unexpect"], "labels": ["CRT", "APC", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example given that the model is directly trained with the iterative inference criteria in place the reconstructions from fig  seem like exactly what we would anticipate with the last iteration producing the best result", "it would certainly seem strange if this were not the cas", "and there is no demonstration of reconstruction quality relative to existing models which could be helpful for evaluating relative perform", "and there is no demonstration of reconstruction quality relative to existing models which could be helpful for evaluating relative perform", "likewise for fig  where faster convergence over traditional first-order methods is demonstrated but again these results are entirely expected as this phenomena has already been well-documented in andrychowicz et "], "labels": ["CRT", "DIS", "DFT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in terms of fig b and table  the proposed approach does produce significantly better values of the elbo critera however is this really an apples-to-apples comparison", "in terms of fig b and table  the proposed approach does produce significantly better values of the elbo critera however is this really an apples-to-apples comparison", "for example does the standard vae have the same number of parameters/degrees-of-freedom as the iterative inference model or might eq  involve fewer parameters than eq  since there are fewer inputs  overall i wonder whether iterative inference is better than standard inference with eq  or whether the recurrent structure from eq  just happens to implicitly create a better neural network architecture for the few examples under consider", "in other words if one plays around with the standard inference architecture a bit perhaps similar results could be obtain", "other minor comment* in fig a it seems like the performance of the standard inference model is still improv"], "labels": ["APC", "QSN", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the iterative inference model has mostly satur", "* a downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time whereas the standard vae model would not", "this paper deals with improving language models on mobile equipmentsbased on small portion of text that the user has ever input", "for thispurpose authors employed a linearly interpolated objectives between userspecific text and general english and investigated which method learningwithout forgetting and random reheasal and which interepolation works bett", "moreover authors also look into privacy analysis to guarantee some level ofdifferential privacy is preserv"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["basically the motivation and method is good the drawback of this paper isits narrow scope and lack of necessary explan", "basically the motivation and method is good the drawback of this paper isits narrow scope and lack of necessary explan", "reading the papermany questions arise in mind- the paper implicitly assumes that the statistics from all the users must  be collected to improve general english", "why is this necessari", "why not  just using better enough basic english and the text of the target us"], "labels": ["DFT", "CRT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- to achieve the goal above huge data not the portion of the general english should be communicated over the network", "is this really worth do", "if only  the portion of general english must be communicated why is it valid", "- for measuring performance authors employ keystroke saving r", "for the  purpose of mobile input this is ok but the use of language models will  cover much different situation where keystrokes are not necessarily   available such as speech recognition or machine transl"], "labels": ["DFT", "QSN", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since this   paper is concerned with a general methodology of language modeling   perplexity improvement or other criteria generally applicable is also  import", "- there are huge number of previous work on context dependent language models  let alone a mixture of general english and specific model", "are there any  comparison with these previous effort", "finally this research only relates to iclr in that the language model employedis lstm in other aspects it easily and better fit to ordinary nlp conferences such as emnlp naacl or so", "i would like to advise the authors to submitthis work to such conferences where it will be reviewed by more nlp expert"], "labels": ["SMY", "SMY", "QSN", "FBK", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor- t of $g_t$ in page  is not defined so far- what is gr in sect", "minor- t of $g_t$ in page  is not defined so far- what is gr in sect", "minor- t of $g_t$ in page  is not defined so far- what is gr in sect", "summary this paper studies a series of reinforcement learning rl techniques in combination with recurrent neural networks rnns to model and synthesise molecul", "summary this paper studies a series of reinforcement learning rl techniques in combination with recurrent neural networks rnns to model and synthesise molecul"], "labels": ["DFT", "QSN", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments seem extensive using many recently proposed rl method", "the experiments seem extensive using many recently proposed rl method", "and show that most sophisticated rl methods are less effective than the simple hill-climbing technique with ppo is perhaps the only except", "originality and signific", "the conclusion from the experiments could be valuable to the broader sequence generation/synthesis field showing that many current rl techniques can fail dramat"], "labels": ["SMY", "DFT", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniqu", "clarity the paper is generally well-written however i'm not an expert in molecule design so might not have caught any trivial errors in the experimental set-up", "clarity the paper is generally well-written however i'm not an expert in molecule design so might not have caught any trivial errors in the experimental set-up", "this paper extends the idea of forming an unsupervised representation of sentences used in the skipthought approach by using a broader set of evidence for forming the representation of a sent", "rather than simply encoding the preceding sentence and then generating the next sentence the model suggests that a whole bunch of related sentences could be encoded including document title section title footnotes hyperlinked sentencesthis is a valid good idea and indeed improves result"], "labels": ["DFT", "DIS", "FBK", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the other main new and potentially useful idea is a new idea for handling oovs in this context where they are represented by positional placeholder variables this also seems help", "the paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolut", "on the negative side the model of the paper isn't very excitingly different it's a fairly straightforward extension of the earlier skipthought model to a situation where you have multiple generators of related text", "there isn't a clear evaluation that shows the utility of the added oov handler since the results with and without that handling aren't compar", "the oov handler is also related to positional encoding ideas that have been used in nmt but aren't refer"], "labels": ["APC", "APC", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and the coreference experiment isn't that clearly described nor necessarily that meaning", "finally the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion which is okay and works but not super neural and excit", "other comments - p another related sentence you could possibly use is first sentence of paragraph related to all other sentences works if people write paragraphs with a topic sentence at the begin", "other comments - p another related sentence you could possibly use is first sentence of paragraph related to all other sentences works if people write paragraphs with a topic sentence at the begin", "- p notation seemed a bit non-standard i thought most people use sigma for a sigmoid makes sense right whereas you use it for a softmax and use calligraphic s for a sigmoid"], "labels": ["DFT", "DFT", "DIS", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- p notation seemed a bit non-standard i thought most people use sigma for a sigmoid makes sense right whereas you use it for a softmax and use calligraphic s for a sigmoid", "- p section  suggests the standard way to do oovs is to average all word vectors that's one well-know way but hardly the only way a trained unk encoding and use of things like character-level encoders is also quite common", "- p section  suggests the standard way to do oovs is to average all word vectors that's one well-know way but hardly the only way a trained unk encoding and use of things like character-level encoders is also quite common", "- p the basic idea of the oov encoder seems a good one in domain specific contexts you want to be able to refer to and re-use words that appear in related sentences since they are likely to appear again and you want to be able to generate them", "a weakness of this section however is that it makes no reference to related work whatsoever it seems like there's quite a bit of related work"], "labels": ["DIS", "DFT", "DIS", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of using a positional encoding so that you can generate rare words by position has previously been used in nmt eg luong et al google brain acl", "more generally a now quite common way to handle this problem is to use pointing or copying which appears in a number of papers eg vinyals et al  and might also have been used here and might be expected to work too", "- p why such an old wikipedia dump most people use a more recent one!", "- p why such an old wikipedia dump most people use a more recent one!", "n - p the paraphrase results seem good and prove the idea works it's a shame they don't let you see the usefulness of the oov model"], "labels": ["DIS", "DIS", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- p for various reasons the coreference results seem less useful than they could have been", "but they do show some value for the technique in the area of domain-specific corefer", "this paper is an extension of the ucprototypical networkud which will be published in nip", "the classical few-shot learning has been limited to using the unlabeled data while this paper considers employing the unlabeled examples available to help train each episod", "the paper solves a new semi-supervised situation which is more close to the setting of the real world with an extension of the prototype network"], "labels": ["DFT", "DIS", "CNT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sufficient implementation detail and analysis on result", "however this is definitely not the first work on semi-supervised formed few-shot learn", "there are plenty of works on this topic [r r r]", "the authors are advised to do a thorough survey of the relevant works in multimedia and computer vision commun", "the authors are advised to do a thorough survey of the relevant works in multimedia and computer vision commun"], "labels": ["SMY", "DFT", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another concern is that the novelti", "another concern is that the novelti", "this work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data", "the experiments are also not enough", "the experiments are also not enough"], "labels": ["SMY", "FBK", "SMY", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not only some other works such as [r r r] but also the other nauefve baselines should also be compared such as directly nearest neighbor classifier logistic regression and neural network in traditional supervised learn", "additionally in the -shot non-distractor setting on tiered imagenet only the soft kmeans method gets a little bit advantage against the semi-supervised baseline does it mean that these methods are not always powerful under different dataset", "[r] ucvideostory a new multimedia embedding for few-example recognition and translation of eventsud in acm mm [r] uctransductive multi-view zero-shot learningud ieee tpami [r] ucvideovec embeddings recognize events when examples are scarceud ieee tpami", "this paper studies a toy problem a random binary image is generated and treated as a maze =wall =freely moveable spac", "a random starting point is gener"], "labels": ["DFT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the task is to learn whether the center pixel is reachable from the starting point", "a deep architechture is proposed to solve the problem see fig", "a conv net on the image is combined with that on a state image the state being interpreted as rechable pixel", "this can work if each layer expands the reachable region the state by one pixel if the pixel is not block", "two local minima are observed  the network ignores stucture and guesses if the task is solvable by aggregate statist"], "labels": ["SMY", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it works as described above but propagates the rechable region on a checkerboard onli", "the paper is chiefly concerned with analysing these local minima by expanding the cost function about them", "this analysis is hard to follow for non experts graph theori", "this is partly because many non-trivial results are mentioned with little or no explan", "the paper is hard to evalu"], "labels": ["DIS", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the actual setup seems somewhat arbitrari", "but the method of analysing the failure modes is interest", "it may inspire more useful research in the futur", "if we trust the authors then the paper seems good because it is fairly unusu", "but it is hard to determine whether the analysis is correct"], "labels": ["DIS", "APC", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper borrows the classic idea of spectral regularization recently applied to deep learning by yoshida and miyato  and use it to normalize gan object", "the ensuing gan coined sn-gan essentially ensures the lipschitz property of the discrimin", "this lipschitz property has already been proposed by recent methods and has showed some success", "however  the authors here argue that spectral normalization is more powerful it allows for models of higher rank more non-zero singular values which implies a more powerful discriminator and eventually more accurate gener", "this is demonstrated in comparison to weight normalization in figur"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental results are very good and give strong support for the proposed norm", "while the main idea is not new to machine learning or deep learning to the best of my knowledge it has not been applied on gan", "the paper is overall well written though check comment  below it covers the related work well and it includes an insightful discussion about the importance of high rank model", "i am recommending accept", "though i anticipate to see a more rounded evaluation of the exact mechanism under which sn improves over the state of the art"], "labels": ["APC", "CRT", "APC", "FBK", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more details in the comments below", "comments one concern about this paper is that it doesnut fully answer the reasons why this normalization works bett", "comments one concern about this paper is that it doesnut fully answer the reasons why this normalization works bett", "i found the discussion about rank to be very intuit", "however this intuition is not fully test"], "labels": ["DIS", "DFT", "CRT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  reports layer spectra for sn and wn", "the authors claim that other methods like arjovsky et al  also suffer from the same rank defici", "i would like to see the same spectra includ", "continuing on the previous point maybe there is another mechanism at play beyond just rank that give sn its apparent edg", "one way to test the rank hypothesis and better explain this method is to run a couple of truncated-sn experi"], "labels": ["DIS", "DIS", "SMY", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what happens if you run your sn but truncate its spectrum after every iteration in order to make it comparable to the rank of wn do you get comparable inception scores or does sn still win", "section  needs some careful editing for language and grammar", "this paper answers recent critiques about ``standard gan'' that were recently formulated to motivate variants based on other losses in particular using ideas from optimal transport", "it makes main points ``standard gan'' is an ill-defined term that may refer to two different learning criteria with different properti", "though the non-saturating variant see eq  of ``standard gan'' may converge towards a minimum of the jensen-shannon divergence it does not mean that the minimization process follows gradients of the jensen-shannon divergence and conversely following gradient paths of the jensen-shannon divergence may not converge towards a minimum but this was rather the point of the previous critiques about ``standard gan''"], "labels": ["QSN", "SUG", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the penalization strategies introduced for ``non-standard gan'' with specific motivations may also apply successfully to the ``standard gan'' improving robustness thereby helping to set hyperparamet", "note that item  is relevant in many other setups in the deep learning framework and is often overlook", "overall i believe that the paper provides enough material to substantiate these claims even if the message could be better deliv", "overall i believe that the paper provides enough material to substantiate these claims even if the message could be better deliv", "in particular the writing is sometimes ambiguous eg in section  the reader who did not follow the recent developments on the subject on arxiv will have difficulties to rebuild the cross-references between authors acronyms and formula"], "labels": ["SMY", "SMY", "SUG", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the answers to the critiques referenced in the  paper are convincing though i must admit that i don't know how crucial it is to answer these critics since it is difficult to assess wether they reached or will reach a large audi", "details- p  please do not qualify kl as a distance metr", "- section  every gan variant was trained for  iterations and  discriminator updates were done for each generator update is ambiguous what is exactly meant by iteration and sometimes step elsewher", "- section  the performance measure is not relevant regarding distributions the l distance is somewhat ok for means but it makes little sense for covariance matric", "the main result specifies a trigger strategy ccc and corresponding algorithm that leads to an efficient outcome in social dilemmas the theoretical basis of which is provided by theorem"], "labels": ["DIS", "SUG", "QSN", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour", "the paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other's act", "the paper is also clearly written and the theoretical result is accompanied by some supporting experi", "the numerical experiments show that using ccc strategy leads to an increase in the proportion of efficient equilibrium outcom", "however in order to solidify the experimental validation the authors could consider a broader range of experimental evalu"], "labels": ["DIS", "SMY", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are also a number of items that could be added that i believe would strengthen the contribution and novelty in particularsome highly relevant references on prosocial reward shaping in social dilemmas are missing such as babes munoz de cote and littman  and for the iterated prisoner's dilemma vassiliades and christodoulou  which all provide important background material on the subject", "in addition it would be useful to see how the method put forward in the paper compares with other reward-shaping techniques within marl especially in the perfect information case in the pong players' dilemma ppd experiment such as those already ment", "the authors could therefore provide more detail in relating the contribution to these papers and other relevant past work and existing algorithm", "the paper also omits any formal discussion on the equilibrium concepts being used in the markov game setting eg markov perfect equilibrium or markov-nash equilibrium which leaves a notable gap in the theoretical analysi", "there are also some questions that to me remain unaddressed namelyi] the model of the experiments particularly a description of the structure of the pong players' dilemma in terms of the elements of the partially observed markov game described in definit"], "labels": ["SUG", "SUG", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular what are the state space and transit", "ii the equilibrium concepts being considered ie does the paper consider markov perfect equilibria", "some analysis on the conditions that under which the continuation equilibria eg cooperation in the social dilemma is expected to arise would also be benefici", "iii although the formal discussion is concerned with markov games ie repeated games with stochastic transitions with multiple states the experiments particularly the ppd appear to apply to repeated games this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being us", "iv in part  of the proof of the main theorem it seems unclear why the sign of the main inequality has changed after application of cauchy convergence in probability equation at the top of the pag"], "labels": ["QSN", "QSN", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as this is an important component of the proof of the main result the paper would benefit from an explanation of this step", "the paper addresses the task of dealing with named entities in goal oriented dialog system", "named entities and rare words in general are indeed troublesome since adding them to the dictionary is expensive replacing them with coarse labels ne_loc unk looses information and so on", "the proposed solution is to extend neural dialog models by introducing a named entity table instantiated on the fly where the keys are distributed representations of the dialog context and the values are the named entities themselv", "the approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is propos"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the resulting model is illustrated on a few goal-oriented dialog task", "i found the paper difficult to read", "the concrete mappings used to create the ne keys and attention keys are miss", "providing more structure to the text would also be useful vs long wordy paragraph", "here are some specific questions how are the keys gener"], "labels": ["SMY", "CRT", "CRT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that are the functions us", "does the knowledge of the current user utterance include the word itself", "the authors should include the exact model specification including for the hred model", "according to the description referring to an existing named entity must be done by generating a key to match the keys in the ne table and then retrieve the corresponding value and use it", "is there a guarantee that a same named entity appearing later in the dialog will be given the same key"], "labels": ["QSN", "QSN", "SUG", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or are the keys for already found entities retrieved directly by valu", "in the decoding phase how does the system decide whether to query the db", "how is the model train", "in its current form it's not clear how the proposed approach tackles the shortcomings mentioned in the introduct", "furthermore while the highlighted contribution is the named entity table it is always used in conjunction to the database approach"], "labels": ["QSN", "QSN", "QSN", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this raises the question whether the named entity table can only work in this context", "for the structured qa task there are  training examples and  named entities this means that the number of training examples per named entity is very smal", "is that correct", "if yes then it's not very surprising that adding the named entities to the vocabulary leads to overfit", "have you compared with using random embeddings for the named ent"], "labels": ["QSN", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["typos page  second-to-last paragraph firs - first page  second to last paragraph and and - and", "this paper describes the use of latent context-free derivations usinga crf-style neural model as a latent level of representation in neuralattention models that consider pairs of sent", "the model implicitlylearns a distribution over derivations and uses marginals under thisdistribution to bias attention distributions over spans in one sentencegiven a span in another sent", "this is an intriguing idea", "i had a couple of reservations however* the empirical improvements from the method seem pretty marginal to thepoint that it's difficult to know what is really helping the model"], "labels": ["DFT", "SMY", "SMY", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wouldliked to have seen more explanation of what the model has learned andmore comparisons to other baselines that make use of attention over span", "for example what happens if every span is considered as an independent randomvariable with no use of a tree structure or the cky chart", "* the use of the alpha^ vs alpha^ variables is not entirely clear", "once theyhave been calculated in algorithm  how are they us", "do the rho valuessomewhere treat these two quantities differ"], "labels": ["DFT", "QSN", "DFT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* i'm skeptical of the type of qualitative analysis in section  unfortun", "i think something much more extensive would be interesting here as oneexample the pp attachment example with at a large venue is highly suspectthere's a / chance that any attachment like this will be correct there'sabsolutely no way of knowing if the model is doing something interesting/corrector performing at a chance level given a single exampl", "this paper reviews the existing literature on attribute-based collaborative filt", "the author categories the existing works int four categori", "while the categorization is reason"], "labels": ["DFT", "DFT", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no proposed new work beyond the existing approach", "no new insight is being discuss", "no new insight is being discuss", "such survey style paper is not appropriate to for iclr", "the quality of this paper is good"], "labels": ["CRT", "DFT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the presentation is clear", "but i find lack of description of a key top", "the proposed model is not very innovative but works fine for the dqa task", "for the te task the proposed method does not perform better than the state-of-the-art system", "- as esim is one of the key components in the experiments you should briefly introduce esim and explain how you incorporated with your vector representations into esim"], "labels": ["APC", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- as esim is one of the key components in the experiments you should briefly introduce esim and explain how you incorporated with your vector representations into esim", "- the reference of esim is not correct", "- figure  is hard to understand", "what do you indicate with the box and arrow", "arrows seem to have some different mean"], "labels": ["DFT", "CRT", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what corpus did you use to pre-train word vector", "- as the proposed method was successful for the qa task", "you need to explain qa data sets and how the questions are solv", "- i also expect performance and  error analysis of the task result", "- i also expect performance and  error analysis of the task result"], "labels": ["QSN", "APC", "APC", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- to claim task-agnostic you need to try to apply your method to other nlp tasks as wel", "- to claim task-agnostic you need to try to apply your method to other nlp tasks as wel", "- page  sigma is not defin", "this paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bas", "a closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations without making discrete approximations as in standard cnn"], "labels": ["SUG", "DIS", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed continuous convolutional neural networks ccnns project input data into a rkhs with a gaussian kernel function evaluated at a set of inducing points the parameters defining the inducing points are optimized via backprop", "filters in convolutional layers are represented in a similar manner yielding a closed-form expression for convolution between input and filt", "experiments train ccnns on several standard small-scale image classification datasets mnist cifar- stl- and svhn", "while the idea is interesting and might be a good alternative to standard cnn", "the paper falls short in terms of providing experimental validation that would demonstrate the latter point"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it unfortunately only experiments with ccnn architectures with a small number eg  lay", "they do well on mnist but mnist performance is hardly informative as many supervised techniques achieve near perfect result", "the cifar- stl- and svhn results are disappoint", "ccnns do not outperform the prior cnn results listed in t", "moreover these tables do not even cite more recent higher-performing cnn"], "labels": ["CRT", "APC", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["see results table in * for cifar- and svhn results on recent resnet and densenet cnn designs which far outperform the methods listed in this pap", "the problem appears to be that ccnns are not tested in a regime competitive with the state-of-the-art cnns on the datasets usedwhy not", "to be competitive deeper ccnns would likely need to be train", "i would like to see results for ccnns with many layers eg + layers rather than just  lay", "do such ccnns achieve performance compatible with resnet/densenet on cifar or svhn"], "labels": ["CRT", "QSN", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given that cifar and svhn are relatively small datasets training and testing larger networks on them should not be computationally prohibit", "in addition for such experiments a clear report of parameters and flops for each network should be included in the results t", "in addition for such experiments a clear report of parameters and flops for each network should be included in the results t", "this would assist in understanding tradeoffs in the design spac", "additional questionswhat is the receptive field of the ccnns vs those of the standard cnns to which they are compar"], "labels": ["CRT", "DFT", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the ccnns have effectively larger receptive field does this create a cost in flops compared to standard cnn", "for ccnns why does the ccae initialization appear to be essential to achieving high performance on cifar- and svhn", "standard cnns trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-train", "such dependence for ccnns appears to be a weakness in comparison", "this paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called divide and conquer networks dcn to solve such problem"], "labels": ["QSN", "QSN", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the network has two components one component learns to split the problem and the other learns to combine solutions to sub-problem", "using this setup the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach", "in particular the authors test their approach on computing convex hulls computing a minimum cost k-means clustering and the euclidean traveling salesman problem tsp problem", "in all three cases the proposed solution outperforms the baselines on larger problem inst", "this paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector"], "labels": ["SMY", "APC", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with dot product proportionality properti", "it also proposes generalized binarization transformation for the first layer of a neural network", "in general i think the paper is written clearly and in detail", "some typos and minor issues are listed in the cons part below", "prosthe authors lead a very nice exploration into the binary nets in the paper from the most basic analysis on the converging angle between original and binarized weight vectors to how this convergence could affect the weight-activation dot product to pointing out that binarization affects differently on the first lay"], "labels": ["SMY", "SMY", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["many empirical and theoretical proofs are given as well as some practical tricks that could be useful for diagnosing binary nets in the futur", "cons* it seems that there are quite some typos in the paper for example     section  in the second contribution there are two then", "section  the citation format of bengio et al  should be bengio et ", "section  the citation format of bengio et al  should be bengio et ", "* section  there is an ordering mistake in introducing han et al's work deepcomporession actually comes before the dsd"], "labels": ["APC", "CRT", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* section  there is an ordering mistake in introducing han et al's work deepcomporession actually comes before the dsd", "* fig c the correlation between the theoretical expectation and angle distribution from b seems not very clear", "* in appendix section  lemma  could you include some of the steps in getting grow to make it clear", "i think the length of the proof won't matter a lot since it is already in the appendix but it makes the reader a lot easier to understand it", "the paper proposes a novel workflow for acceleration and compression of cnn"], "labels": ["CRT", "CRT", "QSN", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed workflow consists of the novel two-pass decomposition of a group of layers and the fine-tuning of the remaining network", "this process is applied iteratively to different groups of lay", "the authors also propose a way to determine the target rank of each layer given the target overall acceler", "the authors report the highest measured acceleration of vgg using low-rank approximation techniques x vs x previously with a similar accuracy drop % vs % previ", "the paper is well-structured and the proposed method is clearly describ"], "labels": ["APC", "SMY", "SMY", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however it would be nice to see the difference to other related methods more clearli", "some results are counterintuitive if the reader is not familiar with related works eg the zhang et al  achieves a lower acceleration with much lower rank", "the main concern is the motivation of the two-pass decomposit", "it is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor", "there are no theoretical results regarding this question in the paper and the empirical justification is also lack"], "labels": ["SUG", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be necessary to see the tensor reconstruction error during the following  scenarioswe apply the cp decomposition to a pretrained network", "we apply the cp decomposition to a pretrained network then restore it back into the dense format optimize it and then apply the cp decomposition again", "what is the reconstruction error in cas", "what is the reconstruction error during the second cp decomposition in", "what is the accuracy drop after fine-tuning in both scenario"], "labels": ["DIS", "DIS", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  could have answered this question however it is not clear from the paper whether the cp-als procedure was followed by fine-tuning or not", "if it wasnut then the comparison is unfair as the results for cp-als are drastically underestim", "it would also be nice to see the full learning curves for all experiments where different stages decompose-optimize-decompose-finetune- are explicitly mark", "the reported tables seem to ignore a lot of the relevant inform", "also astrid and lee  do not seem to report the instabilities during fine-tuning of the decomposed layers and argue that these layers should not be freez"], "labels": ["QSN", "CRT", "SUG", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as they use a very similar iterative fine-tuning workflow it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freez", "these two methods seem to be closely related and should be thoroughly compar", "the improvement wrt other methods seems margin", "the previous sota result on vgg was x acceleration with % accuracy drop and here the reported result is x acceleration with % accuracy drop", "the authors claim that the previous sota result was carefully fine-tuned with a low learning rate and that in this paper they used only default fine-tuning with a high learning r"], "labels": ["DIS", "SUG", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is it possible to further improve the accuracy by a more careful fine-tun", "right now the results are not very convinc", "i would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with astrid and lee  are answ", "other comments and remarksthe meaning of the following sentence is not clear it probably should be rephrased ucwe observed that if the network is trained in the restored dense form the training result can be more stable because of its smoother convex", "ud what does ucsmoother convexud mean"], "labels": ["QSN", "CRT", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it should be stated more clearly how the results from figure  were obtain", "it would be interesting to see the accuracy of the fitness approximation during the rank selection procedur", "is it possible to perform the cp decomposition by minimizing the activation reconstruction loss like proposed by zhang et al  and not the tensor reconstruction loss as usu", "it seems as a more natural way to do it", "the convergence constraint procedure from table  is not clear"], "labels": ["DIS", "SUG", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ucour experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than %", "ud - what does ucthe accuracy improvement is smaller than %ud mean", "this paper presents an extension of binary networks and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net and achieve better performance speed / memori", "the paper addresses a real problem which is meaningful and provides interesting insights but it is more of an extens", "the description of the heterogeneous bitwidth binarization algorithm is interesting and simple and potentially can be practical however it also adds more complication to real world implementations and might not be an elegant enough approach for practical usag"], "labels": ["DIS", "QSN", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the description of the heterogeneous bitwidth binarization algorithm is interesting and simple and potentially can be practical however it also adds more complication to real world implementations and might not be an elegant enough approach for practical usag", "experiments wise the paper has done solid experiments comparing with existing approaches and showed the gain", "results are promis", "overall i am leaning towards a rejection mostly due to limited novelti", "this paper proposed a framework to connect the solving of gan with finding the saddle point of a minimax problem"], "labels": ["CRT", "SMY", "APC", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a result the primal-dual subgradient methods can be directly introduced to calculate the saddle point", "additionally this idea not only fill the relatviely lacking of theoretical results for gan or wgan but also provide a new perspective to modify the gan-type model", "but this saddle point model reformulation  in section  is quite standard with limited theoretical analysis in theorem", "as follows the resulting algorithm  is also standard primal-dual method for a saddle point problem", "most important i think the advantage of considering gan-type model as a saddle point model is that first--order methods can be designed to solve it"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the numerical experiments part seems to be a bit weak because the minst or cifar- dataset is not large enough to test the extensibility for large-scale cas", "this paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data", "this approach extends the method presented on arxiv on sigma delta quantized networks peter ouconnor and max welling sigma delta quantized networks arxiv preprint arxiv b", "overall the paper is interesting and promis", "only a few works tackle the problem of learning with spikes showing the potential advantages of such form of comput"], "labels": ["APC", "APC", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper however is not flawless", "the authors demonstrate the method on just two datasets and effectively they show results of training only for feed-forward neural nets the authors claim that ucthe entire spiking network end-to-end worksud referring to their pre-trained vgg but this paper presents only training for the three top lay", "furthermore even if suitable datasets are not available the authors could have chosen to train different architectur", "the first dataset is the well-known benchmark mnist also presented in a customized temporal-mnist", "although it is a common base-line some choices are not clear why using a ffnn instead that a cnn which performs better on this dataset how data is presented in terms of temporal series u this applies to the temporal mnist too why performances for temporal mnist u which should be a more suitable dataset u are worse than for the standard mnist what is the meaning of the right column of figure  since itus just a linear combination of the gops result"], "labels": ["CRT", "DFT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the second dataset some points are not clear too why the labels and the pictures seem not to match in appendix e why there are more training iterations with spikes wrt the not-spiking cas", "overall the paper is mathematically sound", "except for the ucfuture updatesud meaning which probably deserves a clearer explan", "moreover i donut see why the learning rule equations - are described in the appendix while they are referred constantly in the main text", "the final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in appendix d perhaps this problem affects ccns more than ffn"], "labels": ["CRT", "APC", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally there are some minor issues here and there the authors show quite some lack of attention for just  pag", "-ttwo times ucgetud in ucwe get get a decoding schemeud in the introduct", "-ttwo times ucupdateud in ucour true update update asud in sec", "-tpag correct the capital s in", "n-tpag figure  increase font size also for figure close bracket after equation  n number of spikes is not defin"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tpag ucone-hotud or uconehotud", "-tin the inline equation the sum goes from n= to s while in eq it goes from n= to n", "-teq and some lines have a typo a cdot just before some of the w", "-tpag k_{beta} is not defined in the main text", "-tpag there are two ucso thatud in  capital letter ucit used x^ud beside here why do not report the difference in computation wrt not-spiking net"], "labels": ["DIS", "CRT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tpag in  ucdiscussed in ud is sect", "-tpag appendix e why the labels donut match the pictur", "-tpag appendix f explain better the architecture used for this experi", "the paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural network", "the paper is an extension of kawaguchi'"], "labels": ["QSN", "QSN", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it also provides some sufficient conditions for the non-linear cas", "i think the main technical concerns with the paper is that the technique only applies to a linear model and it doesn't sound the techniques are much beyond kawaguchi'", "i think the main technical concerns with the paper is that the technique only applies to a linear model and it doesn't sound the techniques are much beyond kawaguchi'", "i am happy to see more papers on linear models but i would expect there are more conceptual or technical ingredients in it", "i am happy to see more papers on linear models but i would expect there are more conceptual or technical ingredients in it"], "labels": ["SMY", "DFT", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i can see the same technique here will fail for non-linear models for the same reason as kawaguchi's techniqu", "also i think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum", "this won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle point", "this paper presents a neural architecture for converting natural language queries to sql stat", "the model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined sql vocabulari"], "labels": ["CRT", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives this result could be useful beyond just seqsql models eg for summar", "experiments on the wikisql dataset demonstrate state-of-the-art results and detailed ablations measure the impact of each component of the model", "overall even though the architecture is not very novel", "the paper is well-written and the results are strong", "as such i'd recommend the paper for accept"], "labels": ["APC", "APC", "CRT", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some questions- how can the proposed approach scale to more complex queries ie those not found in wikisql", "could the output grammar be extended to support joins for inst", "as the grammar grows more complex the typed decoder may start to lose its effectivenesssome discussion of these issues would be help", "- how does the additional preprocessing done by the authors affect the performance of the original baseline system of zhong et ", "in general some discussion of the differences in preprocessing between this work and zhong et al would be good do they also use column annot"], "labels": ["QSN", "QSN", "DIS", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proves the weak convergence of the regularised ot problem to kantorovich / monge optimal transport problem", "i like the weak convergence results but this is just weak converg", "it appears to be an overstatement to claim that the approach nearly-optimally transports one distribution to the other cf eg conclus", "there is a penalty to pay for choosing a small epsilon -- it seems to be visible from figur", "also near-optimality would refer to some parameters being chosen in the best possible way"], "labels": ["DIS", "APC", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do not see that from the pap", "however the weak convergence results are good", "a better result hinting on how optimal this can be would have been to guarantee that the solution to regularised ot is within fepsilon from the optimal one or from within fepsilon from the one with a smaller epsilon more possibilities exist", "this is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum", "i also like the choice of the two regularisers and wonder whether the authors have tried to make this more general considering other regularis"], "labels": ["DFT", "APC", "APC", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after all the l one is just an approximation of the entropic on", "typoes- kanthorovich - kantorovich intro- cal c - c eq", "the paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction ie adjusting updates based on their size in the output space instead of the input spac", "this goal is to achieve a similar effect to that of natural gradient but with lighter comput", "the authors claim that their regularization is related to wasserstein metric but the connection is not clear to me read below"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments on mnist with show improved generalization but the baseline is chosen poorly read below", "the paper is easy to read and organized very well and has adequate literature review", "however the contribution of the paper itself needs to be strengthened in both the theory and empirical sid", "on the theory side the authors claim that their regularization is based on wasserstein metric in the title of the paper as well as sect", "however this connection is not very clear to me [if there is a rigorous connection please elaborate]"], "labels": ["APC", "APC", "SUG", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from what i understand the authors argue that their proposed loss+regularization is equivalent to the kantorovich-rubinstein form", "however in the latter the optimization objective is the f itself sup e[f_]-e[f_] but in your scheme you propose adding the regularization term which can be added to any objective function and then the whole form loses its connection to wasserstrin metr", "on the practical side the chosen baseline is very poor", "the authors only experiment with mnist dataset", "the baseline model lacks both batch normalization and dropout which i guess is because otherwise the proposed method would under-perform against the baselin"], "labels": ["DIS", "CRT", "CRT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baselin", "this paper proposes a computationally fast method to train neural networks with normalized weight", "experiments demonstrate that their method is promising compared to the competitor ucnormpropud which explicitly normalizes the weights of neural network", "pros  the paper is easy to follow", "authors use figures that are easy to understand to explain their core idea ie maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weight"], "labels": ["CRT", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons if we count the matrix multiplication operation in fc layer along with normalization in common cases normalization should follow a weighted layer the whole computation complexity becomes omn rather than on+m so i doubt how fast it could be in the common cas", "authors did a mnist experiment with a -fc layer neural network for comparing their fastnorm to normprop", "it is a bit strange that they do not show the difference of speed but show that fastnorm can outperform normprop in terms of classification accuracy with a higher learning r", "since the efficiency is one of the main contributions i suggest authors add this comparison", "the proposed fastnorm improves the stability by observing the standard deviation of validation accuracies in training phas"], "labels": ["DIS", "SMY", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors attribute this to the reduction of accumulated rounding error in training process which is somewhat against the communityus consensus ie float precision is not that important so we can use float or even float to train/do inference for neural network", "ium curious if this phenomenon still holds if authors use float in the experi", "some typosfirst line in page  ucbriningud should be ucbringingud", "noverall i think the current version of the paper is not ready for iclr confer", "authors need more experiments to show their approachus effect"], "labels": ["CRT", "DIS", "CNT", "FBK", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example batching and convolution as mentioned by authors would be more signific", "the paper seems to be significant since it integrates pgm inference with deep model", "specifically the idea is to use the structure of the pgm to perform efficient infer", "specifically the idea is to use the structure of the pgm to perform efficient infer", "a variational message passing approach is developed which performs natural-gradient updates for the pgm part and stochastic gradient updates for the deep model part"], "labels": ["SUG", "APC", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["performance comparison is performed with an existing approach that does not utilize the pgm structure for infer", "the paper does a good job of explaining the challenges of inference and provides a systematic approach to integrating pgms with deep model upd", "as compared to the existing approach where the pgm parameters must converge before updating the dnn parameters the proposed architecture does not require this due to the re-parameterization which is an important contribut", "the motivation of the paper and the description of its contribution as compared to existing methods can be improv", "one of the main aspects it seems is generality but the encodings are specific to  types pgm"], "labels": ["SMY", "APC", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can this be generalized to arbitrary pgm structur", "how about cases when computing z is intract", "could the proposed approach be adapted to such cas", "i was not very sure as to why the proposed method is more general than existing approach", "regarding the experiments as mentioned in the paper the evaluation is performed on two fairly small scale dataset"], "labels": ["QSN", "QSN", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach shows that the proposed methods converge faster than existing method", "however i think there is value in the approach and the connection between variational methods with dnns is interest", "summarythis paper studies learning forward models on latent representations of the environment and use these for model-based planning eg via mcts in partial-information real-time-strategy gam", "the testbed used is minirts a simulation environemnt for v rt", "forecasting the future suffers from buildup / propagation of prediction errors hence the paper uses multi-step errors to stabilize learn"], "labels": ["DIS", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper describes how to train strong agents that might have learned an informative latent representation of the observed state-spac", "evaluates how informative the latent states are via state reconstruct", "trains variatns of a forward model f on the hidden states of the various learned ag", "evaluates different f within mcts for minirt", "pro- this is a neat idea and addresses the important question of how to learn accurate models of the environment from data and how to integrate them with model-free method"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the experimental setting is very non-trivial and novel", "con- the manuscript is unclear in many parts -- this should be greatly improv", "the different forward models are not explained well what is matchpi matcha predn", "which forward model is trained from which model-free ag", "how is the forward model / value function used in mct"], "labels": ["APC", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i assume it's similar to what alphago does but right now it's not clear at all how everything is put togeth", "- the paper devotes a lot of space sect  on details of learning and behavior of the model-free agents x", "yet it is unclear how this informs us about the quality of the learned forward models f", "it would be more informative to focus in the main text on the aspects that inform us about f and put the training details in an appendix", "- as there are many details on how the model-free agents are trained and the system has many moving parts it is not clear what is important and what is not wrt to the eventual winrate comparisons of the mcts model"], "labels": ["CRT", "DIS", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["right now it is not clear to me why matcha / predn differ so much in fig", "- the conclusion seems quite negative the model-based methods fare *much* worse than the model-free ag", "is this because of the mcts approach", "because f is not good because the latent h is not informative enough this requires a much more thorough evaluation overalli think this is an interesting direction of research but the current manuscript does provide a complete and clear analysi", "detailed- what are the right prediction tasks that ensure the latent space captures enough of the forward model"], "labels": ["CRT", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what is the error of the raw h-predict", "only the state-reconstruction error is shown now", "- figure  / sect  which model-free agent is us", "also fig  misses capt", "- figure  scrambled capt"], "labels": ["QSN", "DIS", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- does scheduled sampling / dagger ross et al improve the long-term stability in this cas", "the main idea of this paper is to replace the feedforward summationy = fw*x + bwhere xyb are vectors w is a matrixby an integraly = fint w x + bwhere xyb are functions and w is a kernel", "a deep neural network with this integral feedforward is called a deep function machin", "the motivation is along the lines of functional pca if the vector x was obtained by discretization of some function x then one encounters the curse of dimensionality as one obtains finer and finer discret", "the idea of functional pca is to view x as a function is some appropriate hilbert space and expands it in some appropriate basi"], "labels": ["QSN", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this way finer discretization does not increase the dimension of x nor its approximation but rather improves the resolut", "this paper takes this idea and applies it to deep neural network", "unfortunately beyond rather obvious approximation results the paper does not get major mileage out of this idea", "this approach amounts to a change of basis - and therefore the resolution invariance is not surpris", "in the experiments results of this method should be compared not against nns trained on the data directly but against nns trained on dimension reduced version of the data eg first fixed number of pca compon"], "labels": ["DIS", "SMY", "DFT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately this was not done i suspect that in this case the results would be very similar", "paper summaryexisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different task", "this work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different task", "experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural network", "paper strengths- the proposed technique seems simple yet effective for multi-task learn"], "labels": ["DFT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- experiments on two different network architectures showcasing the generality of the proposed method", "major weaknesses- the main weakness of this work is the unclear exposition of the proposed techniqu", "entire technique is explained in a short section- with many important details miss", "there is no clear basis for the main equations  and", "how does equation- follow from equation- where is the expectation coming from what exactly does ufu refer to there is dependency of ufu on only one of sides in equations  and  more importantly how does the gradient normalization relate to loss weight upd"], "labels": ["APC", "DFT", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is very difficult to decipher these details from the short descriptions given in the pap", "- also several details are missing in toy experi", "what is the task here what are input and output distributions and what is the relation between input and output are they just random noises if so is the network learning to overfit to the data as there is no relationship between input and output", "minor weaknesses- there are no training time comparisons between the proposed technique and the standard fixed loss learn", "- authors claim that they operate directly on the gradients inside the network"], "labels": ["DFT", "DFT", "QSN", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but as far as i understood the authors only update loss weights in this pap", "did authors also experiment with gradient normalization in the intermediate cnn lay", "- no comparison with state-of-the-art techniques on the experimented tasks and dataset", "clarifications- see the above mentioned issues with the exposition of the techniqu", "- in the experiments why are the input images downsampled to x"], "labels": ["DFT", "QSN", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what does it mean by uunofficial datasetu page-", "any references her", "- why is 'task normalized' test-time loss as good measure for comparison between models in the toy example sect", "the loss ratios depend on initial loss which is not important for the final performance of the system", "suggestions- i strongly suggest the authors to clearly explain the proposed technique to get this into a publishable st"], "labels": ["QSN", "DFT", "QSN", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the term ugradnormu seem to be not defined anywhere in the pap", "review summarydespite promising results the proposed technique is quite unclear from the pap", "with its poor exposition of the technique it is difficult to recommend this paper for publ", "in this paper the authors propose a method of compressing network by means of weight ternar", "the network weights ternatization is formulated in the form of loss-aware quantization which originally proposed by hou et "], "labels": ["SUG", "CRT", "FBK", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to this reviewerus understanding the proposed method can be regarded as the extension of the previous work of lab and twn which can be the main contribution of the work", "while the proposed method achieved promising results compared to the competing methods it is still necessary to compare their computational complexity which is one of the main concerns in network compress", "it would be appreciated to have discussion on the results in table  which tells that the performance of quantized networks is better than the full-precision network", "it would be appreciated to have discussion on the results in table  which tells that the performance of quantized networks is better than the full-precision network", "this paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular set"], "labels": ["SMY", "SUG", "SUG", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it tries to provide an explanation for the phenomenon and a procedure to test when it happen", "however i don't find the paper of high significance or the proposed method solid for publication at iclr", "the paper is based on the cyclical learning rates proposed by smith   i don't understand what is offered beyond the original pap", "the super-convergence occurs under special settings of hyper-parameters for resnet only and therefore i am concerned if it is of general interest for deep learning model", "also the authors do not give a conclusive analysis under what condition it may happen"], "labels": ["SMY", "CRT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the explanation of the cause of super-convergence from the perspective of  transversing the loss function topology in section  is rather illustrative at the best without convincing support of argu", "i feel most content of this paper section    is observational results and there is lack of solid analysis or discussion behind these observ", "i feel most content of this paper section    is observational results and there is lack of solid analysis or discussion behind these observ", "this paper wants to probe the non-linear invariances learnt by cnns this is attempted by selecting a particular layer and modelling the space of filters that result in activations that are indistinguishable from activations generated by the real filters using a gan", "for a gan noise vector a plausible filter set is created and for a data sample a set of plausible activations are comput"], "labels": ["DFT", "DFT", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the noise vector is perturbed and a new plausible filter set is created the input data can be optimised to find the input that produces the same set of activ", "the claim is that the found input represents the non-linear transformations that the layer is invariant to", "this is a really interesting perspective on probing invariances and should be explored mor", "i am not convinced that this particular method is showing much information or highlighting anything particularly interesting but could be refined in the future to do so", "it seems that the generated images are not actually plausible images at all and so not many conclusions can be drawn from this method"], "labels": ["SMY", "SMY", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead of performing the optimisation to find x' have you tried visualising the real data sample that gives the closest activ", "i think you may want to consider minimising ||ax'|z - ax|z_k|| instead to show that moving from x - x' is the same as is invariant under the transformation z - z_k  and thus the corresponding movement in filter spac", "this the space between x and x' i think is more interpretable as the invariance corresponding to the space between z and z_k have you tried that", "this the space between x and x' i think is more interpretable as the invariance corresponding to the space between z and z_k have you tried that", "there is no notion of class invariance so the gan can find the space of filters that transform layer inputs into other classes which may not be desirable have you tried conditioning the gan on class"], "labels": ["QSN", "SUG", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no notion of class invariance so the gan can find the space of filters that transform layer inputs into other classes which may not be desirable have you tried conditioning the gan on class", "overall i think this method is inventive and shows promise for probing invari", "i'm not convinced the current incarnation is showing anything insightful or us", "it also should be shown on more than a single dataset and for a single network at the moment this is more of a workshop level paper in terms of breadth and depth of result", "the paper seems clear enough and original enough"], "labels": ["QSN", "APC", "DFT", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit", "where the paper falls short is motivating the problem set", "traditionally for determining optimal execution plans one may resort to cost-based optimization eg database management system", "this paper's introduction provides precisely  statement to suggest that may not work for deep learn", "here's the relevant phrase the cost function is typically non-stationary due to the interactions between multiple devic"], "labels": ["SMY", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately this statement raises more questions than it answ", "unfortunately this statement raises more questions than it answ", "why are the cost functions non-stationari", "what exactly makes them dynam", "are we talking about a multi-tenancy setting where multiple processes execute on the same devic"], "labels": ["DFT", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are we talking about a multi-tenancy setting where multiple processes execute on the same devic", "unlikely because gpus are involv", "without a proper motivation its difficult to appreciate the methods devis", "without a proper motivation its difficult to appreciate the methods devis", "pros- jointly optimizing forming of groups and placing these seems to have merit"], "labels": ["QSN", "DFT", "SUG", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n- experiments show improvements over placement by human expert", "- targets an important problemcons- related work seems inadequately referenc", "- targets an important problemcons- related work seems inadequately referenc", "- targets an important problemcons- related work seems inadequately referenc", "there exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed set"], "labels": ["APC", "SMY", "SUG", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper should at least cite those papers and qualitatively compare against those approach", "here's one reference others should be easy to find systemml's optimizer plan generation for large-scale machine learning programs by boehm et al ieee data engineering bulletin", "- the methods are not well motiv", "there are many approaches to devising optimal execution plans eg rule-based cost-based learning-bas", "in particular what makes cost-based optimization inapplic"], "labels": ["SMY", "SMY", "APC", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic optimally forming groups and placing them is learn-", "- the template seems off", "i don't see the usual two lines under the title anonymous authors paper under double-blind review", "- the title seems mislead", "device placement seems to suggest that one is placing devices when in fact the operators are being plac"], "labels": ["DFT", "DFT", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this was an interesting read", "i feel that there is a mismatch between intuition of what a model could do based on the structure of the architecture versus what a model do", "just because the transition function is shared and the model could learn to construct a tree when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour", "more to a point i think the search tree perspective is interest", "but isnut this just a deeper model with shared weights and a max oper"], "labels": ["APC", "CRT", "DFT", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state right", "it seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state right", "is there any specific attempt to visualize or understand the embeddings inside the tre", "the same regarding the rewards if there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards why would the model use those free latent variables to encode reward", "the same regarding the rewards if there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards why would the model use those free latent variables to encode reward"], "labels": ["DIS", "QSN", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think this is a pitfall that many deep network papers fall where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution where the latent have prescribed semantics i would argue that is rarely the cas", "i think this is a pitfall that many deep network papers fall where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution where the latent have prescribed semantics i would argue that is rarely the cas", "when the system is learned end-to-end the structure does not impose the behaviour of the model and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree and this is not by showing final performance on a gam", "if indeed the model does anything similar to search than all intermediary representations should correspond to what semantically they should", "ignoring my verbose comment another view is that the baseline are disadvantaged to the treeqn because they have less parameters and are less deep which has a huge impact on the learnability and expressivity of the deep network"], "labels": ["DFT", "DIS", "CRT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper considers distributed synchronous sgd and proposes to use partial pulling to alleviate the problem with slow serv", "this paper considers distributed synchronous sgd and proposes to use partial pulling to alleviate the problem with slow serv", "the motivation is that the server may be a straggl", "the authors suggested one possibility namely that the server and some workers are located on the same machine and the workers take most of the computational resourc", "the authors suggested one possibility namely that the server and some workers are located on the same machine and the workers take most of the computational resourc"], "labels": ["SMY", "DFT", "DFT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however if this is the case a simple solution would be to move the server to a different nod", "a more convincing argument for a slow server should be provid", "though the authors claimed that they used  techniques to accelerate synchronous sgd only partial pulling is proposed by them the other  are borrowed straightforwardly from existing pap", "though the authors claimed that they used  techniques to accelerate synchronous sgd only partial pulling is proposed by them the other  are borrowed straightforwardly from existing pap", "though the authors claimed that they used  techniques to accelerate synchronous sgd only partial pulling is proposed by them the other  are borrowed straightforwardly from existing pap"], "labels": ["APC", "DFT", "SMY", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the mechanism of partial pulling is very simple just let sgd proceed after pulling a partial parameter block instead of the whole block", "as mentioned by the authors in section  any relaxation in synchrony brings more noise and higher variance to the updates and also may cause slow convergence or convergence to a poor solut", "however the authors provided no theoretical study on any of these aspect", "experimental results are not convinc", "only one relatively small dataset cifar is used moreover the slow server problem is only simulated by artificially adding delays to the serv"], "labels": ["DIS", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[after author feedback]i think the approach is interesting and warrants publ", "however i think some of the counter-intuitive claims on the proposal learning are overly strong and not supported well enough by the experi", "in the paper the authors also need to describe the differences between their work and the concurrent work of maddison et al and naesseth et ", "[original review]the authors propose auto-encoding sequential monte carlo smc extending the vae framework to a new monte carlo objective based on smc", "the authors show that this can be interpreted as standard variational inference on an extended space and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the smc procedur"], "labels": ["FBK", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be benefici", "the approach is interesting and the paper is well-written", "however i have some comments and questions- it seems clear that the aesmc bound does not in general optimize for qx|y to be close to px|y except in the iwae special cas", "this seems to mean that we should not expect for q - p when k increas", "- figure  seems inconclusive and it is a bit difficult to ascertain the claim that is mad"], "labels": ["SMY", "APC", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if i'm not mistaken k= is regular elbo and not iwae/aesmc", "have you estimated the probability for positive vs negative gradient values for  k=", "to me it looks like the probability of it being larger than zero is something like /", "k is difficult to see from this plot alon", "- is there a typo in the bound given by eq"], "labels": ["QSN", "QSN", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["seems like there are two identical term", "also i'm not sure about the first equality in this equatiion is i^ =  or is there a typo", "- the discussion in section  and results in the experimental section  seem a bit counter-intuitive especially learning the proposals for smc using i", "have you tried this for high-dimensional models as wel", "because is suffers from collapse even in the time dimension i would expect the optimal proposal parameters learnt from a iwae-type objective will collapse to something close to the the standard elbo"], "labels": ["QSN", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example have you tried learning proposals for the lg-ssm in section  using the is objective as proposed in", "might this be a typo in", "you still propose to learn the proposal parameters using smc but with lower number of particl", "i suspect this lower number of particles might be model-depend", "minor comments- section  first paragraph last sentence that - than"], "labels": ["QSN", "QSN", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- section   using which formulation in two places in the firsth and second paragraph was a bit confus", "- page  second line just i", "- perhaps you can clarify the last sentence in the second paragraph of section  about computational graph not influencing gradient upd", "- section  stochastic variational inference hoffman et al  uses natural gradients and exact variational solution for local latents so i don't think k= reduces to thi", "summarythis paper tackles the cross-task and cross-domain transfer and adaptation problem"], "labels": ["CRT", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close in kl divergence and the distributions from dissimilar pairs of data to be farther apart in kl diverg", "what's similar vs dissimilar is trained with a binary classifi", "pros the citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learn", "learning to output the k class membership probability and the loss in eqn  seems novel", "cons the authors overclaim to be state of the art"], "labels": ["SMY", "SMY", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example table  doesn't compare against two recent methods which report results exactly on the same dataset", "i checked the numbers in table  and the numbers aren't on par with the recent method", "unsupervised pixel-level domain adaptation with generative adversarial networks bousmalis et al cvpr and  learning transferrable representations for unsupervised domain adaptation sener et al nip", "authors selectively cite and compare sener et al only in svhn-mnist experiment in sec  but not in the office- experiments in sec", "there are some typos in the related works section and the inferece procedure isn't clearly explain"], "labels": ["CRT", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["perhaps the authors can clear this up in the text after sec", "assessmentborderline refer to the cons section abov", "the main significance of this paper is to propose the task of generating the lead section of wikipedia articles by viewing it as a multi-document summarization problem", "linked articles as well as the results of an external web search query are used as input documents from which the wikipedia lead section must be gener", "further preprocessing of the input articles is required using simple heuristics to extract the most relevant sections to feed to a neural abstractive summar"], "labels": ["SUG", "FBK", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a number of variants of attention mechanisms are compared including the transofer-decoder and a variant with memory-compressed attention in order to handle longer sequ", "the outputs are evaluated by rouge-l and test perplex", "there is also a a-b testing setup by human evaluators to show that rouge-l rankings correspond to human preferences of systems at least for large rouge differ", "this paper is quite original and clearly written", "the main strength is in the task setup with the dataset and the proposed input sources for generating wikipedia articl"], "labels": ["DIS", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main weakness is that i would have liked to see more analysis and comparisons in the evalu", "evaluationcurrently only neural abstractive methods are compar", "evaluationcurrently only neural abstractive methods are compar", "i would have liked to see the rouge performance of some current unsupervised multi-document extractive summarization methods as well as some simple multi-document selection algorithms such as sumbas", "do redundancy cues which work for multi-document news summarization still work for this task"], "labels": ["DFT", "DFT", "CRT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["extractiveness analysisi would also have liked to see more analysis of how extractive the wikipedia articles actually are as well as how extractive the system outputs ar", "does higher extractiveness correspond to higher or lower system rouge scor", "this would help us understand the difficulty of the problem and how much abstractive methods could be expected to help", "a further analysis which would be nice to do though i have less clear ideas how to do it would be to have some way to figure out which article types or which section types are amenable to this setup and which are not", "a further analysis which would be nice to do though i have less clear ideas how to do it would be to have some way to figure out which article types or which section types are amenable to this setup and which are not"], "labels": ["DFT", "QSN", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries eg baidu wiktionary which is not caught by clone detect", "i have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries eg baidu wiktionary which is not caught by clone detect", "in this case the problem could become less interesting as no real analysis is required to do well her", "overall i quite like this line of work", "but i think the paper would be a lot stronger and more convincing with some additional work"], "labels": ["SUG", "DIS", "CRT", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i think the paper would be a lot stronger and more convincing with some additional work", "----after reading the authors' response and the updated submission i am satisfied that my concerns above have been adequately addressed in the new version of the pap", "this is a very nice contribut", "the paper considers a problem of adversarial examples applied to the deep neural network", "the authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal or noisy exampl"], "labels": ["CRT", "APC", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more precisely the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points see sect", "based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal and noisy ones during the test tim", "in other words the paper proposes a particular approach for the adversarial def", "it turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality it is called the local intrinsic dimensionality lid definit", "moreover there is a known empirical estimator of lid based on the k-nearest neighbour"], "labels": ["SMY", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose to use this estimator in computing the intrinsic dimensionalities for the test time exampl", "for every test-time example x the resulting algorithm  computes lid estimates of x activations computed for all intermediate layer of dnn", "these values are finally used as features in classifying adversarial examples from normal and noisy on", "the authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks  datasets mnist cifar and svhn and compare their novel adversarial detection technique to  other ones recently reported in the literatur", "the experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to  other methods across all attacks and datasets see t"], "labels": ["DIS", "DIS", "DIS", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["interestingly the authors also test whether adversarial attacks can bypass lid-based detection methods by incorporating lid in their design", "preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the tim", "in other words the proposed technique is rather stable and can not be easily exploit", "i really enjoyed reading this pap", "all the statements are very clear the structure is transparent and easy to follow"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the writing is excel", "i found only one typo page  we also note that", "otherwise i don't actually have any comments on the text", "unfortunately i am not an expert in the particular field of adversarial examples and can not properly assess the conceptual novelty of the proposed method", "however it seems that it is indeed novel and given rather convincing empirical justifications i would recommend to accept the pap"], "labels": ["APC", "CRT", "DIS", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality & significancethe authors build upon value iteration networks the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network", "the authors point out that the original value iteration networkud tamar  did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network", "it seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update step", "extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problem", "the work is therefore original and signific"], "labels": ["SMY", "SMY", "APC", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the algorithm is evaluated on the original obstacle grids from tamar  and larger grids generated to test scal", "the authors prop and mvprop are able to solve the grids with much higher reliability at the end of training and converge much fast", "the m in mvprop in particular seems to be very useful in scaling up to the large grid", "the authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over tim", "qualitythe symbol d_{rew} is never defined u what does ucnewud stand for"], "labels": ["SMY", "SMY", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it appears to be the number of latent convolutional filters or channels generated by the state embedding network", "section  sentence  the final layer representing the encoding is given as  r^{d_rew  x d_x x d_y }", "based on the description  in the first paragraph of section  it sounds like d_rew might be the number of channels or filters in the last convolutional lay", "in equation  it wasnut obvious to me that the expression max_a q_{ij}^{k-} q^{k} corresponds to an actual oper", "the h phi x  v^{k-}  sort of makes sense u  value is only calculated with respect to only the observation of the maze obstacles but the policy pi is calculated with respect to the joint  observation and agent st"], "labels": ["DIS", "DIS", "DIS", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the expression    h_{aid}  phi v    =     wa   [ phio  v ]      +   bmakes sense and reminds me of the value iteration network work where we take the previous value function combine it with the reward function and use convolution to compute the expectation the weights wa encode the effect of transit", "i gather the tensor wa = r^{|a| x d_{rew} x d_x x d_y } both converts the feature embedding phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same tim", "i didnut understand the r^in r&out representation in section  these are given by the domain", "i did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decis", "a bit more detail defining terms explaining their intuitive role and how the output of one module feeds into the next would be help"], "labels": ["SMY", "QSN", "QSN", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["post revision comments- i didn't reread the whole thing -  just used the diff tool", "- it looks like the typos in the equations got fixed- the new phrase enables to learn to plan seems pretty awkward", "the paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reason", "the idea presented seems to have merit", "however i found the presentation lack"], "labels": ["SMY", "DIS", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["many sentences are poorly written making the paper hard to read especially when not familiar with the presented method", "the experimental section could be organized bett", "i didn't like that two types of experiment are now presented in parallel", "finally the paper stops abruptly without any final discussion and/or conclus", "in this work the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sent"], "labels": ["CRT", "SUG", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task", "the authors evaluate their technique using three morphologically rich languages french polish and russian and obtain promising result", "the morphological agreement task would be an interesting contribution of the paper with wider potenti", "but one concern that i have is regarding the evaluation metrics used for it", "firstly word accuracy rate doesn't seem appropriate as it does not measure morphological agr"], "labels": ["APC", "APC", "SUG", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["secondly sentence accuracy wrt the sentences from which the normalized sentences are derived is not indicative of morphological agreement even wrong sentences in the output could be perfectly valid in terms of agr", "a grammatical error rate fraction of grammatically wrong sentences produced would probably be a better measur", "a grammatical error rate fraction of grammatically wrong sentences produced would probably be a better measur", "another concern i have is regarding the quality of the baseline additional variants of the baseline models should be considered and the best one report", "another concern i have is regarding the quality of the baseline additional variants of the baseline models should be considered and the best one report"], "labels": ["CRT", "SUG", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically in the conversation task have the authors considered switching the order of normalized answer and context in the input", "also the word order of the normalized answer and/or context could be reversed as is done in sequence-to-sequence translation model", "also many experimental details are missing from the draft", "-- what are the sizes of the train/test sets derived from the opensubtitles databas", "-- details of the validation sets used to tune the model"], "labels": ["QSN", "SUG", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- in section  no details of the question-answer corpus are provid", "how many pairs were extract", "how many were used for training and test", "-- in section  how many assessors participated in the evaluation and how many questions were evalu", "-- in some of the tables eg    which show example sentences from polish russian and french please provide some more information in the accompanying text on how to interpret these examples since most readers may not be familiar with these languag"], "labels": ["DFT", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros-- efficient model", "-- proposed architecture is general enough to be useful for other sequence-to-sequence problem", "cons-- evaluation metrics for the morphological agreement task are unsatisfactori", "-- it would appear that the baselines could be improved further using standard techniqu", "- this paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statist"], "labels": ["APC", "DIS", "CRT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key idea is that the agent learns a shared representations for tasks with different visual statist", "- a lot of important references  touching on very similar ideas are miss", "for eg unsupervised pixel-level domain adaptation with generative adversarial networks using simulation and domain adaptation to improve efficiency of deep robotic grasping schema networks zero-shot transfer with a generative causal model of intuitive phys", "- this paper has a lot of orthogonal detail", "for instance sec  reviews the history of games and ai which is besides the key point and does not provide any literary context"], "labels": ["SMY", "DFT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- only single runs for the results are shown in plot", "how statistically valid are the result", "- in the last section authors mention the intent to do future work on atari and other env", "given that this general idea has been discussed in the literature several times it seems imperative to at least scale up the experiments before the paper is ready for publ", "the work is motivated by a real challenge of neuroimaging analysis how to increase the amount of data to support the learning of brain decod"], "labels": ["DIS", "QSN", "SMY", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the contribution seems to mix two objectives on one hand to prove that it is possible to do data augmentation for fmri brain decoding on the other hand to design or better to extend a new model to be more precise two model", "concerning the first objective the empirical results do not provide meaningful support that the generative model is really effect", "the improvement is really tiny and a statistical test not included in the analysis probably wouldn't pass a significant threshold", "this analysis is missing a straw man", "it is not clear whether the difference in the evaluation measures is related to the greater number of examples or by the specific generative model"], "labels": ["SMY", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["concerning the contribution of the model one novelty is the conditional formulation of the discrimin", "the design of the empirical evaluation doesn't address the analysis of the impact of this new formul", "it is not clear whether the supposed improvement is related to the conditional formul", "figure  and figure  illustrate the brain maps generated for collection  with icw-gan and for collection  with acd-gan", "figure  and figure  illustrate the brain maps generated for collection  with icw-gan and for collection  with acd-gan"], "labels": ["DIS", "CRT", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear how the authors operated the choices of these figur", "from the perspective of neuroscience a reader  would expect to look at the brain maps for the same collection with different method", "the pairwise brain maps would support the interpretation of the generated data", "it is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding classification relies on artifacts or confound", "minor comments- typos a first application or this = a first application of this p- qualitative quality p"], "labels": ["CRT", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor comments- typos a first application or this = a first application of this p- qualitative quality p", "summarythis paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep network", "in this method two new types of kernels are developed namely the spatial-wise and channel-wise sparse-complementary kernel", "the authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the paramet", "by adding more filters or layers in the model while keeping the same flops and parameters the models with the proposed method outperform the regular convolution model"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is easy to follow and the idea is interest", "however the novelty of the paper is limited and the experiments are not suffici", "however the novelty of the paper is limited and the experiments are not suffici", "strengths the authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolut", "the authors implement the proposed sparse-complementary convolution on nvidia gpu and achieved competitive speed under the same computational load to regular convolut"], "labels": ["APC", "DFT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors demonstrated that given the same resource budget the wider networks with the proposed method are more efficient than the deeper networks due to the nature of gpu parallel mechan", "weak points the novelty of this paper is limit", "the main idea is to design complementary kernels that cover the same receptive field as the regular convolut", "however the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernel", "moreover the experiments are not sufficient to support the argu"], "labels": ["SMY", "CRT", "APC", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example how is the performance of a model containing sw-sc or cw-sc without deepening or widening the network", "without such experiment it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernel", "the relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear", "which kernel is better and how to choose between them in a deep network", "there is no experimental proof in the pap"], "labels": ["SMY", "SMY", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed two kernels introduce sparsity in the spatial and channel dimension respect", "the two methods are used separ", "is it possible to combine them togeth", "the proposed method only considers the uc+-shapeud and ucx-shapeud sparse pattern", "given the same receptive field with multiple complementary kernels is the kernel shape important for the train"], "labels": ["SMY", "SMY", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the same receptive field with multiple complementary kernels is the kernel shape important for the train", "there is no experimental result to verify thi", "as mentioned in the paper there are many methods which introduce sparsity in the convolution layer such as ucrandom kernelsud uclow-rank approximated kernelsud and ucmixed-shape kernelsud", "however there is no experimental comparison with these method", "however there is no experimental comparison with these method"], "labels": ["QSN", "DFT", "SMY", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the paper the author mentioned another sparse-complementary baseline sc-seq which applies sparse kernels sequenti", "it yields smaller receptive field than the proposed method when the model depth is very smal", "indeed when the model goes deeper the receptive field becomes very close to that of the proposed method", "in the experiments it is strange that this method can also achieve comparable or better result", "so what is the advantage of the proposed ucscud method compared to the ucsc-sequd method"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so what is the advantage of the proposed ucscud method compared to the ucsc-sequd method", "figure  is hard to understand", "this figure only shows that training shallower networks is more effective than training the deeper networks on gpu", "however it does not mean training the wider networks is more efficient than training the deeper on", "the paper deals with ucfixing gans at the computational levelud in a similar sprit to f-gans and wgan"], "labels": ["QSN", "DFT", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the fix is very specific and restrict", "it relies on the logistic regression model as the discriminator and the dual formulation of logistic regression by jaakkola and haussl", "comments  experiments are performed by restricting alternatives to also use a linear classifier for the discrimin", "it is mentioned that results are expected to be lower than those produced by methods with a multi-layer classifier as the discriminator eg shen et al wasserstein distance guided representation learning for domain adaptation ganin et al domain-adversarial training of neural network", "considering this is an unsupervised domain adaption problem how do you set the hyper-parameters lambda and the kernel width"], "labels": ["APC", "DIS", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the ucreverse validationud method described in ganin et al domain-adversarial training of neural networks jmlr  might be help", "the ucreverse validationud method described in ganin et al domain-adversarial training of neural networks jmlr  might be help", "minor comments on the upper-bound of the distance alpha_i instead of alpha^top and please label the axes in your figur", "the paper describes an empirical evaluation of some of the most common metrics to evaluate gans inception score mode score kernel mmd wasserstein distance and loo accuraci", "the paper is well written clear organized and easy to follow"], "labels": ["SUG", "DIS", "SUG", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given that the underlying application is image generation the authors move from a pixel representation of images to using the feature representation given by a pre-trained resnet which is key in their results and further comparison", "they analyzed discriminability mode collapsing and dropping robustness to transformations efficiency and overfit", "although this work and its results are very useful for practition", "it lacks in two aspect", "first it only considers a single task for which gans are very popular"], "labels": ["SMY", "SMY", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["second it could benefit from a deeper maybe theoretical analysis of some of the quest", "some of the conclusions could be further clarified with additional experiments eg sec  uwhile the reason that rms also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the imagenet datasetu", "this paper presents methods for query completion that includes prefix correction and some engineering details to meet particular latency requirements on a cpu", "regarding the latter methods what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be mad", "the final threshold for 'successful' speedups feels somewhat arbitrary -- why ms in particular"], "labels": ["SUG", "CRT", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in any case these methods are useful to document but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the pap", "while the idea of integrating the spelling error probability into the search for completions is a sound one the specific details of the model being pursued feel very ad hoc which diminishes the ultimate impact of these result", "specifically estimating the log probability to be proportional to the number of edits in the levenshtein distance is really not the right thing to do at al", "under such an approach the unedited string receives probability one which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive norm", "even under the assumption that a normalized edit probability is not particularly critical an issue that was not raised at all in the paper let alone assessed the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural languag"], "labels": ["APC", "SMY", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the probability p_ of 'pkoe' versus p_ of 'zoze' as likely versions of 'poke' as say the prefix of pokemon as in your example should be such that p_  p_ not equal as they are in your model", "probabilistic models of string distance have been common since ristad and yianlios in the late s and there are proper probabilistic models that would work with your same dynamic programming algorithm as well as improved models with some modest state split", "and even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model", "it may very well end up that your very simple model does as well as a well estimated model but that is something to establish in your paper not assum", "that such shortcomings are not noted in the paper is troublesome particularly for a conference like iclr that is focused on learned models which this is not"], "labels": ["CRT", "SUG", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as the primary contribution of the paper is this method for combining correction with completion this shortcoming in the paper is pretty seri", "some other commentsyour presentation of completion cost versus edit cost separation in section  is not particularly clear partly since the methods are discussed prior to this point as extension of possibly corrected prefix", "in fact it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation or frankly the way in which the experimental results are describ", "the notation that you use is a bit sloppy and not everything is introduced in a clear way", "for example the s_m notation is introduced before indicating that s_i would be the symbol in the i_th position which you use in sect"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also you claim that s_ is the empty string but isn't it more correct to model this symbol as the beginning of string symbol", "also you claim that s_ is the empty string but isn't it more correct to model this symbol as the beginning of string symbol", "if not what is the difference between s_m and s_m", "if s_ is start of string the s_m is of length m+ not length m", "you spend too much time on common well-known information such as the lstm equ"], "labels": ["CRT", "QSN", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["you don't need them but also why number if you never refer to them lat", "also the dynamic programming for levenshtein is foundational not required to present that algorithm in detail unless there is something specific that you need to point out there which your section  modification really doesn't require to make that point", "is there a specific use scenario for the prefix splitting other than for the evaluation of unseen prefix", "this doesn't strike me as the most effective way to try to assess the seen/unseen distinction since as i understand the procedure you will end up with very common prefixes alongside less common prefixes in your validation set which doesn't really correspond to true 'unseen' scenario", "i think another way of teasing apart such results would be recommend"], "labels": ["QSN", "SUG", "QSN", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["you never explicitly mention what your training loss is in sect", "overall while this is an interesting and important problem and the engineering details are interesting and reasonably well-motivated the main contribution of the paper is based on a pretty flawed approach to modeling correction probability which would limit the ultimate applicability of the method", "the authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity namely the benefit brought by training classifiers on mixtures of real/generated data compared to training on real data onli", "despite the the idea of exploiting generative models to perform data augmentation is interesting using it as an evaluation metric does not constitute an innovative enough contribut", "despite the the idea of exploiting generative models to perform data augmentation is interesting using it as an evaluation metric does not constitute an innovative enough contribut"], "labels": ["CRT", "DIS", "APC", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition there is a fundamental matter which the paper does not address when evaluating a generative model one should always ask himself what purpose the data is generated for", "in addition there is a fundamental matter which the paper does not address when evaluating a generative model one should always ask himself what purpose the data is generated for", "if the aim is to have realistic samples a visual turing test is probably the best metr", "if instead the purpose is to exploit the generated data for classification well in this case an evaluation of the impact of artificial data over training is a good opt", "prosthe idea is interest"], "labels": ["SMY", "QSN", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons the authors did not relate the proposed evaluation metric to other metrics cited eg the inception score or a visual turing test as discussed in the introduct", "it would be interesting to understand how the different metrics rel", "moreover the new metric is introduced with the following motivation uc[visual turing test and inception score] do not indicate if the generator collapses to a particular mode of the data distributionud", "the mode collapse issue is never discussed elsewhere in the pap", "the mode collapse issue is never discussed elsewhere in the pap"], "labels": ["DFT", "APC", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the mode collapse issue is never discussed elsewhere in the pap", "n only two datasets were considered both extremely simple generating mnist digits is nearly a toy task nowaday", "different works on gans make use of cifar- and svhn since they entail more variability those two could be a good start", "the authors should clarify if the method is specifically designed for gans and va", "the authors should clarify if the method is specifically designed for gans and va"], "labels": ["FBK", "APC", "APC", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if not section  should contain several other works as in t", "one of the main statements of the paper ucour approach imposes a high entropy on py and gives unbiased indicator about entropy of both py|x and px|yud is never proved nor discuss", "n equation  the proposed metric is not convincing taking the maximum over tau implies training many models with different fractions of generated data which is expens", "further how many tauus one should evalu", "in order to evaluate a generative model one should test on the generated data only tau= i believ"], "labels": ["DFT", "DFT", "DFT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in order to evaluate a generative model one should test on the generated data only tau= i believ", "in the worst case the generator experiences mode collapse and performs badli", "differently it can memorize the training data and performs as good as the baseline model", "if it does actual data augmentation it should perform bett", "the protocol of section  looks inconsistent with the aim of the work which is to evaluate data augmentation capability of generative model"], "labels": ["DFT", "DFT", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact the limit of training with a fixed dataset is that the model useesu the data multiple times across epochs with the risk of memor", "in fact the limit of training with a fixed dataset is that the model useesu the data multiple times across epochs with the risk of memor", "in the proposed protocol the model useesu the generated data d_gen which is fixed before training multiple time across epoch", "in the proposed protocol the model useesu the generated data d_gen which is fixed before training multiple time across epoch", "this clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant vari"], "labels": ["SMY", "DIS", "SUG", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant vari", "minor section  might be more readable it divided in two exploitation and evalu", "this paper presents an iterative approach to sparsify a network already during train", "during the training process the amount of connections in the network is guaranteed to stay under a specific threshold", "this is a big advantage when training is performed on hardware with computational limitations in comparison to post-hoc sparsification methods that compress the network after train"], "labels": ["DIS", "APC", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is derived by considering the rewiring of an artificial neural network as a stochastic process", "this perspective is based on a recent model in computational biology but also can be interpreted as a sequential monte carlo sampling based stochastic gradient descent approach", "references to previous work in this area are missing eg[] de freitas et al sequential monte carlo methods to train neural networkmodels neural computation [] welling et al bayesian learning via stochastic gradient langevin dynamics icml", "especially the stochastic gradient method in [] is strongly related to the existing approach", "positive aspects- the presented approach is well grounded in the theory of stochastic process"], "labels": ["SMY", "DIS", "DFT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process-", "by keeping the temperature parameter of the stochastic process high it can be directly applied to online transfer learn", "- the method is specifically designed for online learning with limited hardware ressourc", "negative aspects- the presented approach is outperformed for moderate compression levels by han's pruning method for % connectivity on mnist fig  a and by l-shrinkage for % connectivity on cifar- and timit fig  b&c", "especially the results on mnist suggest that this method is most advantageous for very high compression level"], "labels": ["DIS", "DIS", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however in these cases the overall classification accuracy has already dropped significantly which could limit the practical applic", "- a detailled discussion of the relation to previously existing very similar work is missing see abov", "technical remarksfig   and  are referenced on the pages following the page containing the figur", "readibility could be slightly increased by putting the figures on the respective pag", "this paper focuses on accelerating rnn by applying the method from blelloch"], "labels": ["CRT", "DFT", "CRT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the application is straightforward and thus technical novelty of this paper is limit", "but the results are impress", "one concern is the proposed technique is only applied for few types of rnns which may limit its applications in practic", "could the authors comment on this potential limit", "this paper applies gated convolutional neural networks [] to speech recognition using the training criterion asg []"], "labels": ["CRT", "APC", "DFT", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is fair to say that this paper contains almost no novelti", "this paper starts by bashing the complexity of conventional hmm systems and states the benefits of their approach", "however all of the other grapheme-based end-to-end systems enjoy the same benefit as ctc and asg", "prior work along this line includes [    ]", "using mfsc or more commonly known as log mel filter bank outputs has been pretty common since []"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["having a separate subsection  discussing this seems unnecessari", "arguments in section  are weak because again all other grapheme-based end-to-end systems have the same benefit as ctc and asg", "it is unclear why discriminative training such as mmi smbr and lattice-free mmi is mentioned in sect", "discriminative training is not invented to overcome the lack of manual segmentations and is equally applicable to the case where we have manual segment", "the authors argue that asg is better than ctc in section  because it does not use the blank symbol and can be faster during decod"], "labels": ["CRT", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however once the transition scores are introduced in asg the search space becomes quadratic in the number of characters while ctc is still linear in the number charact", "in addition asg requires additional forward-backward computation for computing the partition function second term in eq", "there is no reason to believe that asg can be faster than ctc in both training and decod", "the connection between asg ctc and marginal log loss has been addressed in [] and it does make sense to train asg with the partition funct", "otherwise the objective won't be a proper probability distribut"], "labels": ["DIS", "DIS", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the citation style in section  seems off", "also see [] for a great description of how beam search is done in ctc", "details about training such as the optimizer step size and batch size are miss", "does no batching in section  means a batch size of one utter", "in the last paragraph of section  why is there a huge difference in real-time factors between the clean and other set"], "labels": ["CRT", "DIS", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["something is wrong unless the authors are using different beam widths in the two set", "the paper can be significantly improved if the authors compare the performance and decoding speed against ctc with the same gated convnet", "it would be even better to compare ctc and asg to seqseq-based models with the same gated convnet", "similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj", "none of the comparison in table  is really meaningful because none of the other systems have parameters as many as  layers of convolut"], "labels": ["QSN", "QSN", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why does ctc fail when trained without the blank", "is there a way to fix it besides using asg", "it is also unclear why speaker-adaptive training is not need", "at which layer do the features become speaker invari", "can the system improve further if speaker-adaptive features are used instead of log mel"], "labels": ["QSN", "QSN", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper would be much stronger if the authors can include these experiments and analys", "[] r collobert c puhrsch g synnaeve wavletter an end-to-end convnet-based speech recognition system [] y dauphin a fan m auli d grangier language modeling with gated convolutional nets [] a graves and n jaitly towards end-to-end speech recognition with recurrent neural network", "[] a maas z xie d jurafsky a ng lexicon-free conversational speech recognition with neural network", "[] y miao m gowayyed f metze eesen end-to-end speech recognition using deep rnn models and wfst-based decoding [] d bahdanau j chorowski d serdyuk p brakel y bengio end-to-end attention-based large vocabulary speech recognit", "[] w chan n jaitly q le o vinyals listen attend and spel"], "labels": ["SUG", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] a graves a mohamed g hinton speech recognition with deep recurrent neural network", "[] h tang l lu l kong k gimpel k livescu c dyer n smith s renals end-to-end neural segmental models for speech recognit", "this work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster which represents a set of similar data point eg sentence according to a metr", "by employing the cluster this work propose a joint source/target modeling by varying how sampling is performed eg draw independently or conditionally and how the cluster are constructed eg model-wise or non-model", "experiments on german/english and chinese/english show gains over other reinforcement learning method"], "labels": ["DIS", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if my understanding is correct the motivation is investigate alternative combination of how a cluster is constructed eg sampling and model-based scor", "however one of the problems of this paper is clar", "- the notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology eg clust", "also cluster-to-cluster might not fit wel", "- it is hard to map system-{abcd} to the underlying proposed methods described in t"], "labels": ["SMY", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also i feel algorithm  is spurious given that it merely switch by system", "probably better to introduce branch for key methods parallel sampling/ translation broadcasting and inadaptive or adaptive model", "this paper addresses the problem of one class classif", "the authors suggest a few techniques to learn how to classify samples as negative out of class based on tweaking the gan learning process to explore large areas of the input space which are out of the objective class", "the suggested techniques are nice and show promising result"], "labels": ["DIS", "SUG", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i feel a lot can still be done to justify them even just one of them", "for instance the authors manipulate the objective of g using a new parameter alpha_new and divide heuristically the range of its valu", "but in the experimental section results are shown only for a  single value alpha_new= the authors also suggest early stopping but again as far as i understand only a single value for the number of iterations was test", "the writing of the paper is also very unclear with several repetitions and many typos eg'we first introduce you a''architexture''future work remain to''it self'i believe there is a lot of potential in the approaches presented in the pap", "in my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discuss"], "labels": ["SUG", "SMY", "SMY", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# summarythis paper proposes a neural network framework for solving binary linear programs binary lp", "the idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example binary lp", "in order to store such information the paper proposes an external memory with non-differentiable reading/writing oper", "this network is trained through supervised learning for the output and reinforcement learning for discrete oper", "the results show that the proposed network outperforms the baseline handcrafted solver and the seq-to-seq network baselin"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[pros]- the idea of approximating a binary linear program solver using neural network is new", "[cons]- the paper is not clearly written eg problem statement notations architecture descript", "so it is hard to understand the core idea of this pap", "- the proposed method and problem setting are not well-justifi", "- the results are not very convinc"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# novelty and significance- the problem considered in this paper is new", "but it is unclear why the problem should be formulated in such a way", "to my understanding the network is given a set of input problem and output solution pairs and should predict the solution given a new problem", "i do not see why this should be formulated as a sequential decision problem", "instead we can just give access to all input/output examples in a non-sequential way and allow the network to predict the solution given the new input like q&a task"], "labels": ["APC", "CRT", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this does not require any memory because all necessary information is available to the network", "- the proposed method seems to require a set of input/output examples even during evaluation if my understanding is correct which has limited practical appl", "- the proposed method seems to require a set of input/output examples even during evaluation if my understanding is correct which has limited practical appl", "# quality- the proposed reward function for training the memory controller sounds a bit arbitrari", "the entire problem is a supervised learning problem and the memory controller is just a non-differentiable decision within the neural network"], "labels": ["DIS", "DFT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this case the reward function is usually defined as the sum of log-likelihood of the future predictions see [kelvin xu et al] for training hard-attention because this matches the supervised learning object", "it would be good to justify empirically the proposed reward funct", "- the results are not fully-convinc", "if my understanding is correct the ltmn is trained to predict the baseline solver's output", "but the ltmn significantly outperforms the baseline solver even in the training set"], "labels": ["DIS", "SUG", "CRT", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can you explain why this is poss", "# clarity- the problem statement and model description are not described wel", "is the network given a sequence of program/solution input", "if yes is it given during evaluation as wel", "many notations are not formally defin"], "labels": ["QSN", "CRT", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the output o_t of the network", "is it the optimal solution x_t", "there is no mathematical definition of memory addressing mechanism used in this pap", "- the overall objective function is miss", "[reference]- kelvin xu et al show attend and tell neural image caption generation with visual attent"], "labels": ["QSN", "QSN", "DFT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this very well written paper covers the span between w-gan and va", "for a reviewer who is not an expert in the domain it reads very well and would have been of tutorial quality if space had allowed for more detailed explan", "the appendix are very useful and tutorial paper material especially a", "while i am not sure description would be enough to reproduce and no code is provided every aspect of the architecture if not described if referred as similar to some previous work", "there are also some notation shortcuts not explained in the proof of theorems that can lead to initial confusion but they turn out to be non-ambigu"], "labels": ["APC", "DIS", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one that could be improved is pp_x p_g where one loses the fact that the second random variable is i", "this work contains plenty of novel material which is clearly compared to previous work- the main consequence of the use of wasserstein distance is the surprisingly simple and useful theorem", "i could not verify its novelty but this seems to be a great contribut", "- blending gan and auto-encoders has been tried in the past but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max", "- the use of mmd in the context of gans has also been tri"], "labels": ["DFT", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors claim that their use in the latent space makes it more practiv", "the experiments are very convincing both numerically and visu", "source of confusion in algorithm  and  tilde{z} is sampled from q_thz|xi some one is lead to believe that this is the sampling process as in vaes while in reality q_thz|xi is deterministic in the experi", "source of confusion in algorithm  and  tilde{z} is sampled from q_thz|xi some one is lead to believe that this is the sampling process as in vaes while in reality q_thz|xi is deterministic in the experi", "summarythe authors reinvent a  years old technique for adapting a global or component-wise learning rate for gradient desc"], "labels": ["SMY", "APC", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the technique can be derived as a gradient step for the learning rate hyperparameter or it can be understood as a simple and efficient adaptation techniqu", "general impressionone central problem of the paper is missing novelti", "the authors are well aware of this they still manage to provide added valu", "despite its limited novelty this is a very interesting and potentially impactful pap", "i like in particular the detailed discussion of related work which includes some frequently overlooked precursors of modern method"], "labels": ["SMY", "CRT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["criticismthe experimental evaluation is rather solid but not perfect", "it considers three different problems logistic regression a convex problem and dense as well as convolutional networks that's a solid spectrum", "however it is not clear why the method is tested only on a single data set mnist", "since it is entirely general i would rather expect a test on a dozen different data set", "that would also tell us more about a possible sensitivity wrt the hyperparameters alpha_ and beta"], "labels": ["CRT", "SMY", "DFT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the extensions in section  don't seem to be very us", "in particular i cannot get rid of the impression that section  exists for the sole purpose of introducing a convergence theorem", "analyzing the actual adaptive algorithm would be very interest", "in contrast the present result is trivial and of no interest at all since it requires knowing a good parameter setting which defeats a large part of the value of the method", "minor pointspage  bottom use citep for duchi et "], "labels": ["CRT", "CRT", "SUG", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["none of the figures is legible on a grayscale printout of the pap", "please do not use color as the only cue to identify a curv", "in figure  top row please display the learning rate on a log scal", "page  line  in section  the the unintended repetit", "end of section  an increase from  to  is hardly worth reporting - or am i missing someth"], "labels": ["CRT", "SUG", "SUG", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["end of section  an increase from  to  is hardly worth reporting - or am i missing someth", "the authors has addressed my concerns so i raised my r", "the authors has addressed my concerns so i raised my r", "the paper is grounded on a solid theoretical motivation and the analysis is sound and quite interest", "there are no results on large corpora such as  billion tokens benchmark corpus or at least medium level corpus with  million token"], "labels": ["CRT", "APC", "FBK", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the corpora the authors choose are quite small the variance of the estimates are high and similar conclusions might not be valid on a large corpu", "[] provides the results of character level language models on enwik dataset which shows regularization doesn't have much effect and needs less tun", "results on this data might be more convinc", "the results of mos is very good", "but the computation complexity is much higher than other baselines in the experiments the embedding dimension of mos is slightly smaller but the number of mixture i"], "labels": ["DFT", "DFT", "SUG", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this will make it less usable i think it's necessary to provide the training time comparison", "finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for bleu or w", "finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for bleu or w", "[] melis guebor chris dyer and phil blunsom", "on the state of the art of evaluation in neural language model"], "labels": ["SUG", "SUG", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["arxiv preprint arxiv", "[] joris pelemans noam shazeer ciprian chelba sparse non-negative matrix language modeling  transactions of the association for computational linguistics vol   pp -", "[] shazeer et al  outrageously large neural networks the sparsely-gated mixture-of-experts layer iclr", "this paper is concerned with both security and machine learn", "assuming that data is encoded transmited and decoded using a vaethe paper proposes a man-in-middle attack that alters the vae encoding of the input data so that the decoded output will be misclassifi"], "labels": ["DIS", "DIS", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the objectives are to  fool the autoencoder the classification output of the autoencoder is different from the actual class of the input", "make minimal change in the middle so that the attack is not detect", "this paper is concerned with both security and machine learning but there is no clear contributions to either field", "from the machine learning perspective the proposed attacking method is standard without any technical novelti", "from the security perspective the scenarios are too simplist"], "labels": ["SMY", "SMY", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the encoding-decoding mechanism being attacked is too simple without any security enhanc", "this is an unrealistic scenario", "for applications with security concerns there should have been methods to guard against man-in-the-middle attack and the paper should have at least considered some of them", "for applications with security concerns there should have been methods to guard against man-in-the-middle attack and the paper should have at least considered some of them", "without considering the state-of-the-art security defending mechanism it is difficult to judge the contribution of the paper to the security commun"], "labels": ["DIS", "DIS", "SUG", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not a security expert but i doubt that the proposed method are formulated based on well founded security concepts and idea", "for example what are the necessary and sufficient conditions for an attacking method to be undetect", "are the criteria about the magnitude of epsilon given on section  necessary and suffici", "is there any reference for them", "why do we require the correspondence between the classification confidence of tranformed and original data"], "labels": ["CRT", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would it be enough to match the distribution of the confid", "summary this work is about prototype networks for image classif", "the idea is to jointly embed an image and a confidence measure into a latent space and to use these embeddings to define prototypes together with confidence estim", "a gaussian model is used for representing these confidences as covariance matric", "within a class the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix s-c and this s_c defines the tensor in the mahalanobis metric for measuring the distances to the prototyp"], "labels": ["QSN", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["evaluationclarity i found the paper difficult to read", "in principle the idea seems to be clear", "but then the description and motivation of the model remains very vagu", "for instance what is the the precise meaning of an image-specific covariance matrix supported by just one point", "what is the motivation to just average the inverse covariance matrices to compute s_c"], "labels": ["CRT", "APC", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding spac", "novelty honestly i had difficulties to see which parts of this work could be sufficiently novel", "the idea of using a gaussian model and its associated mahalanobis metric is certainly interest", "but also a time-honored concept", "the experiments focus very specifically on the omniglot dataset and it is not entirely clear to me what  should be concluded from the results pres"], "labels": ["QSN", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are you sure that there is any significant improvement over the models in snell et al mishra et al munkhandalai & yu finn et ", "this paper presents a method for classifying tumblr posts with associated images according to associated single emotion word hashtag", "the method relies on sentiment pre-processing from glove and image pre-processing from incept", "my strongest criticism for this paper is against the claim that tumblr post represent self-reported emotions and that this method sheds new insight on emotion representation and my secondary criticism is a lack of novelty in the method which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module fused in an output lay", "the authors claim that the hashtags represent self-reported emotions but this is not true in the way that psychologists query participants regarding emotion words in psychology studi"], "labels": ["QSN", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead these are emotion words that a person chooses to broadcast along with an associated announc", "as the authors point out hashtags and words may be used sarcastically or in different ways from what is understood in emotion theori", "it is quite common for everyday people to use emotion words this way eg using #love to express strong approval rather than an actual feeling of lov", "in their analysis the authors claimucthe  emotions retained were those with high relative frequencies on tumblr among the panas-x scale watson & clark ud", "however five of the words the authors retain bored annoyed love optimistic and pensive are not in fact found in the panas-x scalereference the panas-x scale https//wikiaaltofi/download/attachments//panas-x-scale_specpdf also the longer version that the authors cited https//wwwpsychologyuiowaedu/faculty/clark/panas-xpdf"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it should also be noted that the panas positive and negative affect scale scale and the panas-x the ucxud is for extended scale are questionnaires used to elicit from participants feelings of positive and negative affect they are not collections of core emotion words but rather words that are colloquially attached to either positive or negative senti", "for example panas-x includes words likeucstrongud ucactiveud uchealthyud ucsleepyud which are not considered emotion words by psycholog", "if the authors stated goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment they should be aware that this is exactly what panas is designed to do - not to infer the latent emotional state of a person except to the extent that their affect is positive or neg", "the work of representing emotions had been an field in psychology for over a hundred years and it is still continu", "https//enwikipediaorg/wiki/contrasting_and_categorization_of_emotionsone of the most popular theories of emotion is the theory that there exist ucbasicud emotions anger disgust fear happiness enjoyment sadness and surprise paul ekman cited by the author"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these are short duration sates lasting only second", "they are also fairly specific for example ucsurpriseud is sudden reaction to something unexpected which is it exactly the same as seeing a flower on your car and expressing ucwhat a nice surpris", "ud  the surprise would be the initial reaction of ucwhatus that on my car  is it dangerousud but after identifying the object as non-threatening the emotion of ucsurpriseud would likely pass and be replaced with appreci", "the circumplex model of emotions posner et al  the authors refer to actually stands in opposition to the theories of ekman", "from the cited paper by posner et al  the circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological system"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this model stands in contrast to theories of basic emotions which posit that a discrete and independent neural system subserves every emot", "from my reading of this paper it is clear to me that the authors do not have a clear understanding of the current state of psychologyus view of emotion representation and this work would not likely contribute to a new understanding of the latent structure of peoplesu emot", "in the pca result it is not clear that the first axis represents valence as sad has a slight positive on this scale and sad is one of the emotions most clearly associated with negative val", "with respect to the rest of the paper the level of novelty and impact is ok", "but not good enough"], "labels": ["DIS", "CRT", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this analysis does not seem very different from twitter analysis because although tumblr posts are allowed to be longer than twitter posts the authors truncate the posts to  charact", "additionally the images do not seem to add very much to the classif", "the authors algorithm also seems to be essentially a combination of two other previously published algorithm", "for me the novelty of this paper was in its application to the realm of emotion theory but i do not feel there is a contribution her", "this paper is more about classifying tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional st"], "labels": ["CRT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper presents a new network architecture for learning a regression of probability distribut", "the distribution output from a given node is defined in terms of a learned conditional probability function and the output distributions of its input nod", "the conditional probability function is an unnormalized distribution with the same form as the boltzman distribution and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bin", "by letting the conditional distribution between nodes be unnormalized and using an energy function that incorporates child nodes independently the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level", "under these dynamics and discretization the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribut"], "labels": ["SMY", "SMY", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these gradients are in turn used to calculate updates for the network parameters with respect to the jensen shannon divergence between the predicted distribution and a target distribut", "the approach is evaluated on three tasks two synthetic and one real world", "the baselines are the state of the art triple basis estimator be or a standard mlp that represents the output distribution using a softmax over quantil", "on both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines although be does achieve better overall perform", "on a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions the proposed approach significantly outperforms both baselin"], "labels": ["SUG", "SMY", "SMY", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however this experiment uses be outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is do", "notes to authorsi'm not familiar with be but the fact that it is used outside of its intended use case for the stock data is worri", "how does be perform at predicting the ftse distribution at time t + k from the ftse distribution at time t onli", "do the multiple input distributions actually help", "you use a kernel density estimate with a gaussian kernel function to estimate the stock market pdf but then you apply your network directly to this estimate what would happen if you built more complex networks using the kernel values themselves as input"], "labels": ["CRT", "CRT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could you also run experiments on the real-world datasets used by the be pap", "what is the structure of the drn that uses  ^ parameters from fig", "the width of the network is bounded by the two input distributions so is this network just incredibly deep", "also is it reasonable to assume that both the drn and mlp are overfitting the toy task when they have access to an order of magnitude more parameters than datapoint", "it would be nice if section  was expanded to actually define the cost gradients for the network parameters either in line or in an appendix"], "labels": ["QSN", "QSN", "QSN", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper explores gan training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus nois", "the paper explores in detail several practically useful versions of the linear measurement model such as blurring linear projection masking etc and establishes identifiability conditions/theorems for the underlying model", "the ambientgan approach advocated in the paper amounts to learning end-to-end differentiable generator/discriminator networks that operate in the measurement spac", "the experimental results in the paper show that this works much better than reasonable baselines such as trying to invert the measurement model for each individual training sample followed by standard gan train", "the theoretical analysis is satisfactori"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of ambientgan train", "however it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of ambientgan train", "for example if the condition number for the linear measurement model is high one would expect that recovering the target real distribution is more difficult", "the condition in theorem  is a step in this direction showing that the required number of samples for correct recovery increases with the probability of missing data", "it would be great if theorems  and  also came with similar quantitative bound"], "labels": ["SUG", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program", "three nn architectures are explored which leverage program semantics rather than pure syntax", "the approach is validated using programming assignments from an online course and compared against syntax based approaches as a baselin", "the problem considered by the paper is interest", "though it's not clear from the paper that the approach is a substantial improvement over previous work"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is in part due to the fact that the paper is relatively short and would benefit from more detail", "i noticed the following issues the learning task is based on error patterns but it's not clear to me what exactly that means from a software development standpoint", "terms used in the paper are not defined/explain", "for example i assume gru is gated recurrent unit but this isn't st", "treatment of related work is lack"], "labels": ["SUG", "CRT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the cai et al paper from iclr  is not consid", "if i understand dependency reinforcement embedding correctly a rnn is trained for every trace if so is this scal", "i believe the work is very promis", "but this manuscript should be improved prior to publ", "the main contribution of this paper is a particular taylor expansion of the outputs of a resnet which is shown to be exact at almost all points in the input spac"], "labels": ["DFT", "QSN", "APC", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this expression is used to develop a new layer called a ucwarp layerud which essentially tries to compute several layers of the residual network using the taylor expansion expression u however in this expression things can be done in parallel and interestingly the authors show that the gradients also decouple when the resnet model is close to a local minimum in a certain sense which may motivate the decoupling of layers to begin with", "this expression is used to develop a new layer called a ucwarp layerud which essentially tries to compute several layers of the residual network using the taylor expansion expression u however in this expression things can be done in parallel and interestingly the authors show that the gradients also decouple when the resnet model is close to a local minimum in a certain sense which may motivate the decoupling of layers to begin with", "finally the authors stack these warp layers to create a ucwarped resnetud which they show does about as well as an ordinary resnet but has better parallelization properti", "to me the analytical parts of the paper are the most interesting particularly in showing how the gradients approximately decoupl", "however there are several weaknesses to the paper or maybe just things i didnut understand"], "labels": ["SMY", "SUG", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first  a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model which i am afraid i simply was not able to follow", "some of the notation is confusing here u for example presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix which the notation suggests to be the cas", "it is also never precisely spelled out what the final theoretical guarantee is preferably the authors would do this in the form of a proposition or theorem", "throughout the authors write out equations as if the weights in all layers are equal but this is confusing even if the authors say that this is what they are doing since their explanation is not very clear", "the confusion is particularly acute in places where derivatives are taken because the derivatives continue to be taken as if the weights were untied but then written as if they happened to be the sam"], "labels": ["CRT", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally the experimental results are okay", "but perhaps a bit preliminari", "i have a few recommendations here* it would be stronger to evaluate results on a larger dataset like ilsvrc", "* the relative speed-up of warpnet compared to resnet needs to be better explained u the authors break the computation of the warpnet onto two gpus but itus not clear if they do this for the vanilla resnet as wel", "in batch mode the easiest way to parallelize is to have each gpu evaluate half the batch"], "labels": ["APC", "CRT", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in batch mode the easiest way to parallelize is to have each gpu evaluate half the batch", "even in a streaming mode where images need to be evaluated one by one there are ways to pipeline execution of the residual blocks and i do not see any discussion of these alternatives in the pap", "* in the experimental results k is set to be  and the authors only mention in passing that they have tried larger k in the conclus", "it would be good to have a more thorough experimental evaluation of the trade-offs of setting k to be higher valu", "a few remaining questions for the authors* there is a parallel submission presumably by different authors called ucresidual connections encourage iterative inferenceud which contains some related insight"], "labels": ["DIS", "DFT", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wonder what are the differences between the two taylor expansions and whether the insights of this paper could be used to help the other paper and vice versa", "* on implementation - the authors mention using tensorflowus auto-differenti", "my question here is u are gradients being re-used intelligently as suggested in sect", "* i notice that the analysis about the vanishing hessian could be applied to most of the popular neural network architectures available now", "how much of the ideas offered in this paper would then generalize to non-resnet set"], "labels": ["QSN", "SMY", "SMY", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper aims to synthesize programs in a java-like language from a task description x that includes some names and types of the components that should be used in the program", "the paper argues that it is too difficult to map directly from the description to a full program so it instead formulates the synthesis in two part", "first the description is mapped to a sketch y containing high level program structure but no concrete details about", "eg variable names afterwards the sketch is converted into a full program prog by stochastically filling in the abstract parts of the sketch with concrete instanti", "the paper presents an abstraction method for converting a program into a sketch a stochastic encoder-decoder model for converting descriptions to trees and rejection sampling-like approach for converting sketches to program"], "labels": ["SMY", "SMY", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimentally it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program ast", "the data is derived from an online repository of ~ android app", "and from that were extracted ~k methods which makes the data very respectable in terms of realisticness and scal", "this is one of the strongest points of the pap", "one point i found confusing is how exactly the combinatorial concretization step work"], "labels": ["SMY", "SMY", "SMY", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["am i correct in understanding that this step depends only on y and that given i", "prog is conditionally independent of x", "if this is correct how many progs are consistent with a typical i", "some additional discussion of why no learning is required for the pprog | y step would be appreci", "i'm also curious whether using a stochastic latent variable z is necessari"], "labels": ["QSN", "QSN", "QSN", "FBK", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would the approach work as well using a more standard encoder-decoder model with determinstic z", "some discussion of grammar variational autoencoder kusner et al would probably be appropri", "overall i really like the fact that this paper is aiming to do program synthesis on programs that are more like those found in the wild", "while the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel", "i think this paper adds an interesting new take on the pattern it has a very different abstraction than say deepcoder and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques in my opinion"], "labels": ["QSN", "APC", "APC", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary of the paper the paper analysis how well difference target probation dtp - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like cifar- and imagenet", "the dtp algorithm is slightly adapted to make it more biologically plausible by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation leading to the variant sdtp and by making the optimisation of both involved losses parallel", "furthermore only feedforward and locally connected networks cnn are considered since their architecture is considered more biologically plausible than convolutional neural network", "while on mnist and cifar dtp and sdtp performed as well as backprop they perform worse on imagenet", "furthermore it becomes clear that without cnn structure no really good performance is achieved neither on cifar nor on imagenet"], "labels": ["SMY", "SMY", "SMY", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the paper is nicely written and good to follow", "- suggested modifications from dtp to stdp increase its biological plausibility without making its performance wors", "- the worse performance compared to backprop and cnns underlines the open question how to yield biologically plausible and efficient algorithms and network architectur", "cons- the title of the paper seems to general to me since target propagation is the only algorithm compared against backpropag", "- since the adaptions to dtp are rather small the work does not contain much novelti"], "labels": ["APC", "SUG", "QSN", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it can rather be seen as an interesting empirical study with negative result", "minor comments- page  the reference list could also include  http//wwwmitpressjournalsorg/doi/abs//neco_a_ and  https//arxivorg/abs/-", "page  ucthe the degreeud  ucspecified as u followed byud -  ucas u followed byud - this notation probably stems from the code but same and valid could be nicer described as uc paddingud and ucno paddingud", "for example- page   ucapplying bp to the brainud sounds strange to m", "the paper presents a series of definitions and results elucidating details about the functions representable by relu networks their parametrisation and gaps between deep and shallower net"], "labels": ["SUG", "SUG", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is easy to read", "although it does not seem to have a main focus exponential gaps vs optimisation vs universal approxim", "the paper makes a nice contribution to the details of deep neural networks with relu", "although i find the contributed results slightly overst", "the d results are not difficult to derive from previous result"], "labels": ["APC", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network", "the optimisation method appears close to brute force and is limited to  lay", "theorem  appears to be easily deduced from the results from montufar pascanu cho bengio", "for d inputs each layer will multiply the number of regions at most by the number of units in the layer leading to the condition wu geq w^{k/ku}", "theorem  is simply giving a parametrization of the functions removing symmetries of the units in the lay"], "labels": ["DIS", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the list at the top of page  note that the function classes might be characterized in terms of countable properties such as the number of linear regions as discussed in mpcb but still they build a continuum of funct", "similarly in page  ``moreover for fixed nks our functions are smoothly parameterized''", "this should not be a surpris", "in the last paragraph of section  ``m = w^k-'' this is a very big first lay", "this also seems to subsume the first condition sgeq  w^k- +wk- for the network discussed in theorem"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the last paragraph of section  ``to the best of our knowledge''", "in the construction presented here the networkus size is essentially in the layer of size m", "under such conditions corollary  of mpcb also reads as s^n", "here it is irrelevant whether one artificially increases the depth of the network by additional very narrow layers which do not contribute to the asymptotic number of unit", "the function class zonotope is a composition of two part"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to consider also a single construction instead of the composition of two construct", "theorem  ii it would be nice to have a construction where the size becomes m + wk when ku=k", "section  while interesting appears to be somewhat disconnected from the rest of the pap", "in theorem  explain why the two layer case is limited to n=", "at some point in the first  pages it would be good to explain what is meant by ``harduu functions eg functions that are hard to represent as opposed to step functions etc"], "labels": ["SUG", "SUG", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces modifications the wordvec and glove loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embed", "the resulting word embeddings are evaluated on a number of standard tasks including word similarity outlier prediction sentiment detection and also on a new task for formality frustration and politeness detect", "a considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learn", "the methodologies of kiela et al  and bollegala et al  are very similar to those proposed in this work", "the main originality seems to be captured in algorithm  which computes the strength between two word"], "labels": ["SMY", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unlike prior work this is a real-valued instead of a binary quant", "because this modification is not particularly novel i believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniqu", "in this light the performance relative to the baselines is particularly import", "from the results reported in tables   and  i do not see compelling evidence that +v +a +d or +vad consistently lead to significant performance increases relative to the baseline method", "i therefore cannot recommend this paper for publ"], "labels": ["SMY", "DFT", "SUG", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a self-normalizing bipolar extension for the relu activation famili", "for every neuron out of two authors propose to preserve the negative input", "such activation function allows to shift the mean of iid variables to zeros in the case of relu or to a given saturation value in the case of elu", "combined with variance preserving initialization scheme authors empirically observe that the bipolar relu allows to better preserve the mean and variance of the activations through training compared to regular relu for a deep stacked rnn", "authors evaluate their bipolar activation on ptb and text using a deep stacked rnn"], "labels": ["SMY", "SMY", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they show that bipolar activations allow to train deeper rnn up to some limit and leads to better generalization performances compared to the relu /elu activation funct", "they also show that they can train deep residual network architecture on cifar without the use of bn", "question- which layer mean and variance are reported in figur", "what is the difference between the left and right plot", "- in table  we observe that relu-rnn and belu-rnn for very deep stacked rnn leads to worst validation perform"], "labels": ["SMY", "SMY", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be nice to report the training loss to see if this is an optimization or a generalization problem", "- how does bipolar activation compare to model train with bn on cifar", "- did you try bipolar activation function for gated recurrent neural networks for lstm or gru", "- as stated in the text belu-rnn outperforms bn-lstm for ptb", "however bn-lstm outperforms belu-rnn on text"], "labels": ["SUG", "QSN", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["do you know why the trend is not consistent across dataset", "-clarity/qualitythe paper is well written and pleasant to read", "- originalityself-normalizing function have been explored also in scaled elu however the application of self-normalizing function to rnn seems novel", "- significanceactivation function is still a very active research topic and self-normalizing function could potentially be impactful for rnn given that the normalization approaches batch norm layer norm add a significant computational cost", "in this paper bipolar activations are used to train very deep stacked rnn"], "labels": ["QSN", "APC", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the stacked rnn with bipolar activation are not competitive regarding to other recurrent architectur", "it is not clear what are the advantage of deep stacked rnn in that context", "the authors describe a new defense mechanism against adversarial attacks on classifiers eg fgsm", "they propose utilizing generative adversarial networks gan which are usually used for training generative models for an unknown distribution but have a natural adversarial interpret", "in particular a gan consists of a generator nn g which maps a random vector z to an example x and a discriminator nn d which seeks to discriminate between an examples produced by g and examples drawn from the true distribut"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the gan is trained to minimize the max min loss of d on this discrimination task thereby producing a g in the limit whose outputs are indistinguishable from the true distribution by the best discrimin", "utilizing a trained gan the authors propose the following defense at inference tim", "given a sample x which has been adversarially perturbed first project x onto the range of g by solving the minimization problem z* = argmin_z ||gz - x||_ this is done by sgd", "then apply any classifier trained on the true distribution on the resulting x* = gz*", "in the case of existing black-box attacks the authors argue convincingly that the method is both flexible and empirically effect"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the defense can be applied in conjunction with any classifier including already hardened classifiers and does not assume any specific attack model", "nevertheless it appears to be effective against fgsm attacks and competitive with adversarial training specifically to defend against fgsm", "the authors provide less-convincing evidence that the defense is effective against white-box attack", "in particular the method is shown to be robust against fgsm rand+fgsm and cw white-box attack", "however it is not clear to me that the method is invulnerable to novel white-box attack"], "labels": ["DIS", "APC", "DFT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular it seems that the attacker can design an x which projects onto some desired x* using some other method entirely which then fools the classifier downstream", "nevertheless the method is shown to be an effective tool for hardening any classifier against existing black-box attacks which is arguably of great practical valu", "it is novel and should generate further research with respect to understanding its vulnerabilities more complet", "minor commentsthe sentence starting ucunless otherwise specifieduud at the top of page  is confusing given the actual contents of tables  and  which are clarified only by looking at table  in the appendix this should be fix", "this submission does not fit iclr"], "labels": ["DIS", "APC", "APC", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the center topic does not fit iclr", "the main novelty is about using word pair embedding to improve the topic model", "the word-pair was generated by the standford dependency pars", "- many citation errors exist", "- no clear novelti"], "labels": ["FBK", "SMY", "SMY", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the experimental setup is problemat", "the authors filtered the number of words and word-pairs to very smal", "it is hard to justify any of the results after these strategi", "- the baselines are not thorough and lack proper justif", "- the experimental results are not properly presented with many overlapping figur"], "labels": ["CRT", "SMY", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["no insights can be derived from the presented result", "the paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called style memory which would presumably capture non-class inform", "the idea of learning representations that decompose into class-specific and class-agnostic parts and more generally style and content is an interesting and long-standing problem", "the results in the paper are mostly qualitative and only on mnist", "they do not show convincingly that the network managed to learn interesting class-specific and class-agnostic represent"], "labels": ["CRT", "SMY", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's not clear whether the examples shown in figures  to  are representative of the network's general behavior", "the tsne visualization in figure  seems to indicate that the style memory representation does not capture class information as well as the raw pixels but doesn't indicate whether that representation is sens", "the use of fully connected networks on images may affect the quality of the learned representations and it may be necessary to use convolutional networks to get interesting result", "the use of fully connected networks on images may affect the quality of the learned representations and it may be necessary to use convolutional networks to get interesting result", "it may also be interesting to consider class-specific representations that are more general than just the class label"], "labels": ["CRT", "CRT", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example see learning a nonlinear embedding by preserving class neighbourhood structure by salakhutdinov and hinton  which learns hidden vector representations for both class-specific and class-agnostic parts this paper should be cit", "paper summarythis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network", "it first recaps some of the setting if a neural network has a piecewise linear activation function eg relu maxout the final function computed by the network before softmax is also piecewise linear and divides up the input into polyhedral regions which are all different linear funct", "these regions also have a correspondence with activation patterns the active/inactive pattern of neurons over the entire network", "previous work [] [] has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can hav"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper improves on the upper bound given by [] and the lower bound given by []", "they also provide a tight bound for the one dimensional input cas", "finally for small networks they formulate finding linear regions as solving a linear program and use this method to compute the number of linear regions on small networks during training on mnist", "main commentsthe paper is very well written and clearly states and explains the contribut", "however the new bounds proposed theorem  theorem  seem like small improvements over the previously proposed bound"], "labels": ["APC", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with no other novel interpretations or insights into deep architectur", "the improvement on zaslavsky's theorem is interest", "the idea of counting the number of regions exactly by solving a linear program is interest", "but is not going to scale wel", "and as a result the experiments are on extremely small networks width  which only achieve % accuracy on mnist"], "labels": ["CRT", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is bett", "i would like to see the technique of counting linear regions used even approximately for larger networks where even though the results are an approximation the takeaways might be more insight", "overall while the paper is well written and makes some interesting point", "it presently isn't a significant enough contribution to warrant accept", "[] on the number of linear regions of deep neural networks  montufar pascanu cho bengio"], "labels": ["CRT", "CRT", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] on the expressive power of deep neural networks  raghu poole kleinberg ganguli sohl-dickstein", "this paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input", "the model achieves good results on babi compared to memory networks and the relation network model", "from what i understood it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an mlp whose input is just a sentence and question not two sentences and a quest", "this average is then fed to a softmax layer for answer predict"], "labels": ["DIS", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i found it difficult to understand how the model is related to relation networks since it no longer scores every combination of objects or in the case of babi sentences which is the fundamental idea behind relation network", "why is the approach not evaluated on clevr in which the interaction between two objects is perhaps more critical and was the main result of the original relation networks pap", "the fact that the model works well on babi despite its simplicity is interesting but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model which i can't agree with based solely on babi experi", "the fact that the model works well on babi despite its simplicity is interesting but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model which i can't agree with based solely on babi experi", "i'd encourage the authors to do a more detailed experimental study with more task"], "labels": ["DFT", "QSN", "DFT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i can't recommend this paper's acceptance in its current form", "other questions / comments- we use mlp to produce the attention weight without any extrinsic computation between the input sentence and the question isn't this statement false because the attention computation takes as input the concatenation of the question and sentence represent", "- writing could be cleaned up for spelling / grammar eg last  stories instead of last  sentences currently the paper is very hard to read and it took me a while to understand the model", "this paper presents an image-to-image cross domain translation framework based on generative adversarial network", "the contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retriev"], "labels": ["FBK", "QSN", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results show that the proposed method is superior for the task of exact correspondence identification and that an-gan rivals the performance of pixpix with strong supervis", "negatives the task of exact correspondence identification seems contriv", "it is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset with just the correspondence information between inputs and outputs miss", "the supervised vs unsupervised experiment on facades-labels table  is only one scenario where applying a supervised method on top of an-ganus matches is better than an unsupervised method", "more transfer experiments of this kind would greatly benefit the paper and support the conclusion that ucour self-supervised method performs similarly to the fully supervised method"], "labels": ["APC", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud positives the paper does a good job motivating the need for an explicit image matching term inside a gan framework", "the paper shows promising results on applying a supervised method on top of an-ganus match", "minor comments the paper sometimes uses l and sometimes l_ it should be l_ in all cas", "discogan should have the kim et al citation right after the first time it is used i had to look up discogan to realize it is just kim et ", "this paper proposes a method that scales reading comprehension qa to large quantities of text with much less document truncation than competing approach"], "labels": ["APC", "APC", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the model also does not consider the first mention of the answer span as gold instead formulating its loss function to incorporate multiple mentions of the answer within the evid", "the reported results were state-of-the-art* on the triviaqa dataset at the time of the submission deadlin", "it's interesting that such a simple model relying mainly on weighted word embedding averages can outperform more complex architectures however these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to rnn", "overall i found the paper interesting to read and scaling qa up to larger documents is definitely an important research direct", "on the other hand i'm not quite convinced by its experimental results more below and the paper is lacking an analysis of what the different sub-models are learn"], "labels": ["SMY", "DIS", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as such i am borderline on its accept", "* the triviaqa leaderboard shows a submission from // by chrisc that has significantly higher em/f scores than the proposed model", "why is this result not compared to in t", "detailed comments- did you consider pruning spans as in the end-to-end coreference paper of lee et al emnlp", "this may allow you to avoid truncation altogeth"], "labels": ["FBK", "CRT", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["perhaps this pruning could occur at level  making subsequent levels would be much more effici", "- how long do you estimate training would take if instead of bag-of-words level  used a bilstm encoder for spans / quest", "- what is the average number of sentences per docu", "it's hard to get an idea of how reasonable the chosen truncation thresholds are without thi", "- in figure  it looks like the exact match score is still increasing as the maximum tokens in document is increas"], "labels": ["SUG", "QSN", "QSN", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["did the authors try truncating after more words eg k", "- i would have liked to see some examples of questions that are answered correctly by level  but not by level  or  for example to give some intuition as to how each level work", "- i would have liked to see some examples of questions that are answered correctly by level  but not by level  or  for example to give some intuition as to how each level work", "- krasner misspelled multiple times as kram", "this paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network vgg due to simonyan and zisserman"], "labels": ["QSN", "SUG", "DIS", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to estimate the id the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network in which the intrinsic dimension is determined in a more or less standard way by the rank at which two consecutive singular values has a ratio exceeding some threshold", "for the convolutional layers of vgg they observe that the sum of ids for each feature map is roughly equal to the id of the matrix formed by concatenating the vectors over all feature map", "they also observe that the id drops with each successive lay", "the authors' findings are intuitively obvious and certainly not surpris", "although a thorough and careful empirical investigation of this phenomenon would be a welcome addition to the research literature this paper does not yet reach this standard"], "labels": ["SMY", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first and foremost a result for a single neural network does not constitute enough evidence to justify the authors' conclus", "second the latter half of the paper is concerned with details of the experimental results without offering any insights as to the implications for deep learn", "third the paper is not well presented and organized the introduction is scant the notational formulism is not at all clear rigorous or consistent the paper overall lacks polish with many grammatical error", "third the paper is not well presented and organized the introduction is scant the notational formulism is not at all clear rigorous or consistent the paper overall lacks polish with many grammatical error", "overall i feel that while this line of research is worthwhile at this stage the work is not yet ready for publ"], "labels": ["CRT", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i feel that while this line of research is worthwhile at this stage the work is not yet ready for publ", "this paper thoroughly analyzes an algorithmic task determining if two points in a maze are connected which requires bfs to solve by constructing an explicit convnet solution and analytically deriving properties of the loss surface around this analytical solut", "they show that their analytical solution implements a form of bfs algorithm characterize the probability of introducing bugs in the algorithm as the weights move away from the optimal solution and how this influences the error surface for different depth", "this analysis is conducted by drawing on results from the field of critical percolation in phys", "overall i think this is a good paper and its core contribution is definitely valuable it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm and in particular the role which initialization play"], "labels": ["FBK", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the analysis is very thorough and the methods described may find use in analyzing other task", "in particular this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks memory networks neural turing machines etc which try to learn reasoning tasks or algorithm", "it is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best on", "this work actually explains the role of initialization for learning bfs and how certain types of initialization lead to poor solut", "i am curious if a similar analysis could be applied to methods evaluated on the babi question-answering tasks which can be represented as graphs like the maze task and possibly yield better initialization or optimization schemes that would remove the need for multiple random se"], "labels": ["APC", "APC", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with that being said there is some work that needs to be done to make the paper clear", "in particular many parts are quite technical and may not be accessible to a broader machine learning audi", "it would be good if the authors spent more time developing intuition through visualization for example and move some of the more technical proofs to the appendix", "specifically- i think figure  in the appendix should be moved to the main text to help understand the behavior of the analytical solut", "- top of page  when you describe the checkerboard bfs please include a visualization somewhere it could be in the appendix"], "labels": ["DIS", "DIS", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- section  there is lots of math here but the main results don't obviously stand out", "i would suggest highlighting equations  and  in some way for example proposition/lemma + proof so that the casual reader can quickly see what the main results ar", "interested readers can then work through the math if they want to", "also some plots/visualizations of the loss surface given in equations  and  would be very help", "also although i found their work to be interesting after finishing the paper i was initially confused by how the authors frame their work and where the paper was head"], "labels": ["CRT", "SUG", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they claim their contribution is in the analysis of loss surfaces true and neural nets applied to graph-structured input", "this second part was confusing - although the maze can be viewed as a graph many other works apply convnets to maze environments [  ] and their work has little relation to other work on graph cnn", "here the assumptions of locality and stationarity underlying cnns are sensible and i don't think the first paragraph in section  justifying the use of the cnn on the maze environment is necessari", "however i think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms such as the neural turing machine and variants or reasoning tasks more generally for example memory-augmented networks applied to the babi task", "there are lots of small typos please fix them"], "labels": ["SUG", "CRT", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["here are a few- for l= batch size of   not a complete sent", "- right before  when the these such - when such", "- top of page  it also have a - it also has a when encountering larger dataset", "- datasets-  first sentence of  we turn to the discuss a second -", "we turn to the discussion of a second- etc"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality highclarity medium-loworiginality high", "significance medium-highreferences[] https//arxivorg/pdf/pdf[] https//arxivorg/pdf/pdf[] https//arxivorg/pdf/pdf", "this paper introduces siamese neural networks to the competing risks framework of fine and gray", "the authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pair", "while the idea of optimizing directly for the c-index directly is a good one with an approximation and with useful complementary loss function term"], "labels": ["CRT", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper leaves something to be desired in quality and clar", "related works- for your consideration is multi-task survival analysis effectively a competing risks model except that these models also estimate risk after the first competing event ie in a competing risks model the rates for other events simply go to  or near-zero please discuss", "related works- for your consideration is multi-task survival analysis effectively a competing risks model except that these models also estimate risk after the first competing event ie in a competing risks model the rates for other events simply go to  or near-zero please discuss", "also if the claim is that there are not deep learning survival analyses please see eg jing and smola", "- it would be helpful to define t_k explicitly to alleviate determining whether it is the interval time between ordered events or the absolute time since t_ it's the latt"], "labels": ["DFT", "DFT", "DIS", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consider calling k a time index instead of t_k a time interval subject x experiences cause m occurs [sic] in a time interval t_k- line after eq  do you mean accuracy term", "- i would not call reg a regularization term since it is not shrinking the coeffici", "it is a term to minimize a risk not a paramet", "- you claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experi", "- the results show only one form of comparison and the results have confidence intervals that overlap with at least one competing method in all task"], "labels": ["QSN", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["properly capturing named entities for goal oriented dialog is essential for instance location time and cuisine for restaurant reserv", "mots successful approaches have argued for separate mechanism for ne captures that rely on various hacks and trick", "this paper attempt to propose a comprehensive approach offers intriguing new ideas but is too preliminary both in the descriptions and experi", "the proposed methods and experiments are not understandable in the current way the paper is written there is not a single equation pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process", "all we have a besides text is a small figure figur"], "labels": ["SMY", "SMY", "SMY", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then we have to trust the authors that on their modified dataset the accuracies of the proposed method is around % while not using this method yields % accuraci", "the initial description section   leaves way too many unanswered questions- what embeddings are used for words detected as n", "is it the same as the generated represent", "- what is the exact mechanism of generating a representation for ne eec", "end of page - is it correct that the same representation stored in the ne table is used twic"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a to retrieve the key a vector given the value a string  as the encoder input", "b to find the value that best matches a key at the decoder stag", "- exact description of the column attention mechanism some similarity between a key embedding and embeddings representing each column multiplicative addit", "- how is the system supervis", "do we need to give the name of the column the attention-column-query attention should focus on"], "labels": ["SMY", "SMY", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["because of this unknown i could not understand the experiment setup and data formatting!", "the list goes onfor such a complex architecture the authors must try to analyze separate modules as much as poss", "as neither the qa and the babi tasks use the rnn dialog manager while not start with something that only works at the sentence level", "the q&a task could be used to describe a simpler system with only a decoder accessing the db t", "complexity for solving the babi tasks could be added lat"], "labels": ["DFT", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary  the paper applies graph convolutions with deep neural networks to the problem of variable misuse putting the wrong variable name in a program statement in graphs created deterministically from source cod", "graph structure is determined by program abstract syntax tree ast and next-token edges as well as variable/function name identity assignment and other deterministic semantic rel", "initial node embedding comes from both type and tokenized name inform", "gated graph neural networks ggnns trained by maximum likelihood objective are then run for  iterations at test tim", "the evaluation is extensive and mostly very good"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["substantial data set of m lines of cod", "reasonable baselin", "nice ablation studi", "i would have liked to see separate precision and recall rather than accuraci", "the current % accuracy is nice to se"], "labels": ["APC", "APC", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but if % of my program variables were erroneously flagged as errors the tool would be useless", "i'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings but still catches some error", "nice work creating an implementation of fast ggnns with large diverse graph", "glad to see that the code will be releas", "great to see that the method is fast---it seems fast enough to use in practice in a real id"], "labels": ["CRT", "DIS", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the model ggnn is not particularly novel but i'm not much bothered by that", "i'm very happy to see good application papers at iclr", "i agree with your pair of sentences in the conclusion although source code is well understood and studied within other disciplines such as programming language research it is a relatively new domain for deep learn", "it presents novel opportunities compared to textual or perceptual data as its local semantics are well-defined and rich additional information can be extracted using well-known efficient program analys", "i'd like to see work in this area encourag"], "labels": ["CRT", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so i recommend accept", "if it had better eg roc curve evaluation and some modeling novelty i would rate it higher stil", "small notesthe paper uses the term data flow structure without defining it", "your data set consisted of c# cod", "perhaps future work will see if the results are much different in other languag"], "labels": ["FBK", "FBK", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and sc", "the method is combination of a spatial transformer module that predicts a focal point around which a log-polar transform is perform", "the resulting log-polar image is analyzed by a conventional cnn", "i find the basic idea quite compel", "although this is not mentioned in the article the proposed approach is quite similar to human vision in that people choose where to focus their eyes and have an approximately log-polar sampling grid in the retina"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore dealing well with variations in scale is a long-standing and difficult problem in computer vision and using a log-spaced sampling grid seems like a sensible approach to deal with it", "one fundamental limitation of the proposed approach is that although it is invariant to global translations it does not have the built-in equivariance to local translations that a convnet ha", "although we do not have data on this i would guess that for more complex datasets like imagenet / ms coco where a lot of variation can be reasonably well modelled by diffeomorphisms this will result in degraded perform", "the use of the heatmap centroid as the prediction for the focal point is potentially problematic as wel", "it would not work if the heatmap is multimodal eg when there are multiple instances in the same image or when there is a lot of clutterthere is a minor conceptual confusion on page  where it is written that group-convolution requires integrability over a group and identification of the appropriate measure dg"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we ignore this detail as implementation requires application of the sum instead of integr", "when approximating an integral by a sum one should generally use quadrature weights that depend on the measure so the measure cannot be ignor", "fortunately in the chosen parameterization the haar measure is equal to the standard lebesque measure and so when using equally-spaced sampling points in this parameterization the quadrature weights should be on", "please double-check this - i'm only expressing my mathematical intuition but have not actually proven thi", "it does not make sense to say that the above convolution requires computation of the orbit which is feasible with respect to the finite rotation group but not for general rotation-dilations and then proceed to do exactly that in canonical coordin"], "labels": ["CRT", "SUG", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since the rotation-dilation group is d just like the d translation group used in convnets this is entirely feas", "the use of canonical coordinates is certainly a sensible choice for the reason given abov", "but it does not make an infeasible computation feas", "the authors may want to consider cit", "the authors may want to consider cit"], "labels": ["APC", "APC", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- warped convolutions efficient invariance to spatial transformations henriques & vedaldi", "this paper also uses a log-polar transform but lacks the focal point prediction / stn", "likewise although the paper makes a good effort to rewiev the literature on equivariance / steer", "it missed several recent works in this area- steerable cnns cohen & welling- dynamic steerable blocks in deep residual networks jacobsen et ", "it missed several recent works in this area- steerable cnns cohen & welling- dynamic steerable blocks in deep residual networks jacobsen et "], "labels": ["DIS", "DIS", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- learning steerable filters for rotation equivariant cnns weiler et ", "- learning steerable filters for rotation equivariant cnns weiler et ", "the last paper reports % error on mnist-rot which is slightly better than the ptn-cnn-b++ reported on in this pap", "the experimental results presented in this paper are quite good", "but both mnist and modelnet seem like simple / toyish dataset"], "labels": ["DFT", "CRT", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for reasons outlined above i am not convinced that this approach in its current form would work very well on more complicated problem", "if the authors can show that it does either in its current form or after improving it eg with multiple saccades or other improvements i would recommend this paper for publ", "minor issues & typos- section  psi_gh = psi_g psi_h", "i suppose you use psi for l and l' but this is not very clear", "- l_h f = fh^{-} p - coordiantes p"], "labels": ["CRT", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality the authors study non-saturating gans and the effect of two penalized gradient approach", "the authors consider a number of thought experiments to demonstrate their observations and validate these on real data experi", "clarity the paper is well-written and clear", "the authors could be more concise when reporting result", "i would suggest keeping the main results in the main body and move extended results to an appendix"], "labels": ["SMY", "SMY", "APC", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality the authors demonstrate experimentally that there is a benefit of using non-saturating gan", "more specifically the provide empirical evidence that they can fit problems where jensen-shannon divergence fail", "they also show experimentally that penalized gradients stabilize the learning process", "significance the problems the authors consider is worth exploring furth", "the authors describe their finding in the appropriate level of details and demonstrate their findings experiment"], "labels": ["APC", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however publishing this  work is in my opinion premature for the following reasons- the authors do not provide further evidence of why non-saturating gans perform better or under which mathematical conditions non-saturating gans will be able to handle cases where distribution manifolds do not overlap", "- the authors show empirically the positive effect of penalized gradients but do not provide an explanation grounded in theori", "- the authors do not provide practical recommendations how to set-up gans and not that these findings did not lead to a bullet-proof recipe to train them", "this paper studies learning to play two-player general-sum games with state markov games with imperfect inform", "the idea is to learn to cooperate think prisoner's dilemma but in more complex domains generally in repeated prisoner's dilemma one can punish one's opponent for noncooper"], "labels": ["FBK", "DFT", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper they design an apporach to learn to cooperate in a more complex game like a hybrid pong meets prisoner's dilemma gam", "this is fun but i did not find it particularly surprising from a game-theoretic or from a deep learning point of view", "from a game-theoretic point of view the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect inform", "it is basically a straightforward generalization of the idea of punishing which is common in folk theorems from game theory to give a particular equilibrium for cooperating in markov gam", "many markov games do not have a cooperative equilibrium so this paper restricts attention to those that do"], "labels": ["SMY", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even in games where there is a cooperative solution that maximizes the total welfare it is not clear why players would choose to do so", "when the game is symmetric this might be the natural solution but in general it is far from clear why all players would want to maximize the total payoff", "the paper follows with some fun experiments implementing these new game theory not", "unfortunately since the game theory was not particularly well-motivated i did not find the overall story compel", "it is perhaps interesting that one can make deep learning learn to cooperate with imperfect information but one could have illustrated the game theory equally well with other techniqu"], "labels": ["CRT", "SMY", "SMY", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in contrast the paper coco-q learning in stochastic games with side payments by sodomka et al is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learn", "i would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interest", "it should also be noted that i was asked to review another iclr submission entitled maintaining cooperation in complex social dilemmas using deep reinforcement learning which amazingly introduced the same pong playerus dilemma game as in this pap", "notice the following suspiciously similar paragraphs from the two papersfrom maintaining cooperation in complex social dilemmas using deep reinforcement learningwe also look at an environment where strategies must be learned from raw pixel", "we use the methodof tampuu et al  to alter the reward structure of atari pong so that whenever an agent scores apoint they receive a reward of  and the other player receives u"], "labels": ["DIS", "SUG", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we refer to this game as the pongplayerus dilemma ppd", "in the ppd the only jointly winning move is not to play", "however a fullycooperative agent can be exploited by a defector", "from consequentialist conditional cooperation in social dilemmas with imperfect informationto demonstrate this we follow the method of tampuu et al  to construct a version of atari pong which makes the game into a social dilemma", "in what we call the pong playerus dilemma ppd when an agent scores they gain a reward of  but the partner receives a reward of u"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus in the ppd the only jointly winningmove is not to play but selfish agents are again tempted to defect and try to score points even thoughthis decreases total social reward", "we see that ccc is a successful robust and simple strategy in thisgam", "this paper describes an attempt of improving information flow in deep networks but is used and tested here with seqseq models although it is reality unrelated to seqseq models per s", "slightly different from resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current output", "the authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the nois"], "labels": ["DIS", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this is an ok technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak", "overall this is an ok technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak", "the results seem to indicate that there were some problems with getting deeper networks to work for the baseline why is in table  baseline-l worse than baseline-l", "the results seem to indicate that there were some problems with getting deeper networks to work for the baseline why is in table  baseline-l worse than baseline-l", "for which the reason could be a multitude of issues probably related to hyper-parameter tun"], "labels": ["SMY", "FBK", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is also missing is a an analysis of the negative consequences of this technique -- for example doesn't the number of parameters increase with the depth of the network because of the concaten", "what is also missing is a an analysis of the negative consequences of this technique -- for example doesn't the number of parameters increase with the depth of the network because of the concaten", "also it would have been good to see more experiments with smaller baseline networks as well to match the smaller densenet networks in table  and", "also it would have been good to see more experiments with smaller baseline networks as well to match the smaller densenet networks in table  and", "finally the writing of the paper could be improved a lot the basic idea is not well described however many times repeated and the grammar is often wrong and also there are some typo"], "labels": ["DFT", "QSN", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally the writing of the paper could be improved a lot the basic idea is not well described however many times repeated and the grammar is often wrong and also there are some typo", "authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using lsh", "i found the paper relatively creative and generally well-founded and well-argu", "nice clear example with least squares linear regression though a little hard to tell how generalizable the given ideas are to other loss functions/function classes given the authors seem to be taking heavy advantage of the inner product", "experiments appreciated the wall clock tim"], "labels": ["CRT", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sgd comparison ucfixed learning r", "ud didn't see how the initial well constant here step size was tun", "why not use the more standard /t decay", "fig  suspicious cifar that test objective is so much better than train object", "legend backward"], "labels": ["APC", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why were so many of the chosen datasets have so few training exampl", "paper is mostly very clearly written", "though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through", "the key contribution of the paper is a new method for nonlinear dimensionality reduct", "the proposed method is more or less a modification of the drlim manifold learning algorithm hadsell chopra lecun  with a slightly different loss function that is inspired by multidimensional sc"], "labels": ["QSN", "APC", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while drlim only preserves local geometry the modified loss function presents the opportunity to preserve both local and global geometri", "the rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data the familiar swiss roll as well as a couple of synthetic image dataset", "the paper revisits mostly familiar idea", "the importance of preserving both local and global information in manifold learning is well known so unclear what the main conceptual novelty i", "this reviewer does not believe that modifying the loss function of a well established previous method that is over  years old drlim constitutes a significant enough contribut"], "labels": ["SMY", "SMY", "SMY", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover in this reviewer's experience the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold and such an estimation is simply too difficult for any reasonable dataset encountered in practic", "however the authors do not address this and instead simply use the isomap approach for approximating geodesics by graph distances which opens up a completely different set of challenges how to construct the graph how to deal with holes in the manifold how to avoid short circuiting in the all-pairs shortest path computations etc etc", "finally the experimental results are somewhat uninspir", "it seems that the proposed method does roughly as well as landmark isomap with slightly better generalization properties but is slower by a factor of x", "the horizon articulation data as well as the pose articulation data are both far too synthetic to draw any practical conclus"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an rnn to perform a navigation task", "on the positive side this is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand", "i enjoyed reading the paper which is in general clearly written", "i have a few mostly cosmetic complaints but this can easily be addressed in a revis", "on the negative side the manuscript is not written in a way that is suitable for the target iclr audience which will include for the most part readers that are not expert on the entorhinal cortex and/or spatial navig"], "labels": ["SMY", "APC", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first the contributions need to be more clearly spelled out", "in particular the authors tend to take shortcuts for some of their stat", "for instance in the introduction it is stated that previous attractor network type of models which are also recurrent networks uc[] require hand-crafted and fined tuned connectivity patterns and the evidence of such specific d connectivity patterns has been largely abs", "ud this statement is problematic for two reasons i it is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data", "this is not to say that demonstrating that these patterns can arise as a byproduct is not important on the contrari"], "labels": ["DIS", "DIS", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these are just two complementary lines of work", "in the same vein it would be silly to dismiss the present work simply because it lacks spik", "ii the authors do not seem to address one of the main criticisms they make about previous work and in particular [a lack of evidence] of such specific d connectivity pattern", "my understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connect", "i would argue that there is a lot of evidence for local inhibitory connection in the cortex"], "labels": ["DIS", "CRT", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["somewhat related to this point it would be insightful to show the pattern of local connections learned in the rnn to see how it differs from the aforementioned pattern of connect", "second the navigation task used needs to be better justifi", "why training a network to predict d spatial location from velocity input", "why is this a reasonable starting point to study the emergence of grid cel", "it might be obvious to the authors but it will not be to the iclr audi"], "labels": ["DFT", "CRT", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["dead-reckoning ie spatial localization from velocity inputs is of critical ecological relevance for many anim", "this needs to be spelled out and a reference needs to be ad", "as a side note i would have expected the authors to use actual behavioral data but instead the network is trained using artificial trajectories based on modified brownian motionud", "this seems like an important assumption of the manuscript but the issue is brushed off and not discuss", "why is this a reasonable assumption to mak"], "labels": ["DIS", "CRT", "DIS", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is there any reference demonstrating that rodent locomotory behavior in a d arena is random", "figure  seems kind of strang", "i do not understand how the ucrepresentative unitsud are selected and where the uclateud selectivity on the far right side in panel a arises if not from ucearlyud units that would have to travel ucfarud from the left sideu", "apologies if i am missing something obvi", "i found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially tr"], "labels": ["QSN", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cel", "the paper is generally well-written and the intuition is very clear", "it combines the advanced attention mechanism pointer networks and reinforce learning signal to train a sequence-to-sequence model for text summar", "the experimental results show that the model is able to achieve the state-of-the-art performance on cnn/daily main and new york times dataset", "it is a good incremental research"], "labels": ["DIS", "APC", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to u", "i would like to see the model ablation wrt repetition avoidance trick by muting the second trigram at test tim", "intuitively if the repetition issue is prominent to having decent summarization performance it might affect our judgement on the significance of using intra-attention or combined rl sign", "another thought on this is it possible to integrate the trigram occurrence with summarization reward", "another thought on this is it possible to integrate the trigram occurrence with summarization reward"], "labels": ["DFT", "SMY", "SMY", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be remov", "so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be remov", "in addition as the encoder-decoder structure gradually becomes the standard choice of sequence prediction i would suggest the authors to add the sum of parameters into model ablation for refer", "suggested referencesbahdanau et al  an actor-critic algorithm for sequence predict", "actor-critic on machine translationmiao & blunsom  language as a latent variable discrete generative models for sentence compress"], "labels": ["DFT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["mixture pointer mechanism + reinforc", "in this paper the authors look to improve neural architecture search nas which has been successfully applied to discovering successful neural network architectures albeit requiring many computational resourc", "the authors propose a new approach they call efficient neural architecture search enas whose key insight is parameter shar", "in nas the practitioners have to retrain for every new architecture in the search process but in enas this problem is avoided by sharing parameters and using discrete mask", "in both approaches reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metr"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since we can encode a neural network as a sequence the policy can be parameterized as an rnn where every step of the sequence corresponds to an architectural choic", "in their experiments enas achieves test set metrics that are almost as good as nas yet require significantly less computational resources and tim", "the authors present two enas models one for cnns and another for rnn", "initially it seems like the controller can choose any of b operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connect", "however in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large so the authors use only one restriction to reduce the size of the state spac"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a limitation as the model space is not as flexible as one would desire in a discovery task", "moreover their best results and those they choose to report in the abstract are due to fixing  parallel branches at every layer combined with a  x  convolution and using enas to learn the skip connect", "thus they are essentially learning the skip connections while using a human-selected model", "enas for rnns is similar while nas searches for a new architecture the authors use a recurrent highway network for each cell and use enas to find the skip connect", "thus it seems like the term efficient neural architecture search promises too much since in both tasks they are essentially only using the controller to find skip connect"], "labels": ["DFT", "APC", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although finding an appropriate architecture for skip connections is an important task finding an efficient method to structure rnn cells seems like a significantly more important go", "overall the paper is well-written and it brings up an important idea that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process", "moreover using binary masks to control network path essentially corresponding to training different models is a neat idea", "it is also impressive how much faster their model performs on tasks without sacrificing much perform", "the main limitation is that the best architectures as currently described are less about discovery and more about human input"], "labels": ["SMY", "APC", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- finding a more efficient search path would be an important next step", "the paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text", "for this purpose the method learns joint embeddings of symbolic data images and text to predict the links in a knowledge graph", "the multimodal embeddings are evaluated on newly created datasets which extend the movielens-k and yago- with multimodal inform", "the paper is written well good to understand and technically sound"], "labels": ["SUG", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i especially liked the general idea of using multiple modalities to improve embeddings of relational data", "this direction is not only interesting because of the improvements it brings for link prediction tasks but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embed", "the technical novelty of the paper is somewhat limited as the proposed method consists of a mostly straightforward combination of existing method", "with regard to related work recently [] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction task", "this reference should be included in the related work"], "labels": ["APC", "APC", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors mention also in the last sentence of section  that previous approaches cannot handle missing data or uncertainti", "this claim needs to be discussed clearer as it is not clear to me why this would be the cas", "with regard to the evaluation overall i found the evaluation to be good especially with regard to the different abl", "however it would be nice to see results for more sophisticated models than distmult which due to its symmetry shouldn't be used on directed graphs anyway as the improvements that can be gained might be less for these model", "it would also be interesting to see how predictions using only the non-symbolic modalities would do eg in t"], "labels": ["SMY", "SUG", "APC", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore section  would clearly benefit from a better analysis and discussion as it isn't very informative in its current form and the analysis is quite hand-wavy eg two of the predicted titles for die hard have something to do with dying and being buri", "further comments- the proposed method to incorporate numerical data seems quite ad hoc", "what are the motivations for this particular approach", "- are the image features fixed or learn", "in the later case how much do the results change with pretrained cnns eg on imagenet"], "labels": ["SUG", "DFT", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- p we use an appropriate encoder is repeated twic", "- since the datasets are newly introduced it would be good to provide a more detailed analysis of their characterist", "[] thoma et al towards holistic concept representations embedding relational knowledge visual attributes and distributional word semant", "the below review addresses the first revision of the pap", "the revised version does address my concern"], "labels": ["SMY", "SUG", "SUG", "CNT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the fact that the paper does not come with substantial theoretical contributions/justification still stands out", "---the authors present a variant of the adversarial feature learning afl approach by edwards & storkey", "afl aims to find a data representation that allows to construct a predictive model for target variable y and at the same time prevents to build a predictor for sensitive variable ", "the key idea is to solve a minimax problem where the log-likelihood of a model predicting y is maximized and the log-likelihood of an adversarial model predicting s is minim", "the authors suggest the use of multiple adversarial models which can be interpreted as using an ensemble model instead of a single model"], "labels": ["APC", "SMY", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in eq", "while there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversari", "while there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversari", "hence lambda in eq  may need to be chosen differently depending on the adversarial model without tuning lambda for each method the empirical experiments seem unfair", "hence lambda in eq  may need to be chosen differently depending on the adversarial model without tuning lambda for each method the empirical experiments seem unfair"], "labels": ["DFT", "DFT", "DIS", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this may also explain why for example the baseline method with one adversary effectively fails for opp-l", "this may also explain why for example the baseline method with one adversary effectively fails for opp-l", "a better comparison would be to plot the performance of the predictor of s against the performance of y for varying lambdas the area under this curve allows much better to compare the various method", "there are little theoretical contributions basically instead of a single adversarial model - eg a single-layer nn or a multi-layer nn - the authors propose to train multiple adversarial models on different views of the data", "an alternative interpretation is to use an ensemble learner where each learner is trained on a different overlapping feature set"], "labels": ["SUG", "DIS", "SUG", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversari", "tuning the architecture of the single multi-layer nn adversary might be as good", "in short in the current experiments the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods this renders the comparison unfair", "given that there is also no theoretical argument why an ensemble approach is expected to perform bett", "i recommend to reject the pap"], "labels": ["DFT", "QSN", "DIS", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis work is about model evaluation for molecule generation and design", "benchmarks are proposed small data sets are expanded to a large standardized data set and it is explored how to apply new rl techniques effectively for molecular design", "on the positive sid", "the paper is well written quality and clarity of the work are good", "the work provides a good overview about how to apply new reinforcement learning techniques for sequence gener"], "labels": ["SMY", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is investigated how several rl strategies perform on a large standardized data set", "different rl models like hillclimb-mle ppo gan ac are investigated and discuss", "an implementation of  suggested benchmarks of relevance for de novo design will be provided as open source as an openai gym", "an implementation of  suggested benchmarks of relevance for de novo design will be provided as open source as an openai gym", "on the negative sid"], "labels": ["SMY", "DIS", "SMY", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no new novel contribution on the methods sid", "minor commentssection  see fig u see figpage just before equation  the th", "this paper proposes a method for learning the sentence representations with sentences dependencies inform", "it is more like a dependency-based version skip-thought on the sentence level", "the idea is interesting to me but i think this paper still needs some improv"], "labels": ["DFT", "SMY", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the introduction and related work part are clear with strong motivations to m", "but section  and  need a lot of detail", "my comments are as followsi this paper claims that this is a general sentence embedding method however from what has been described in section  i think this dependency is only defined in html format docu", "what if i only have pure text document without these html structure inform", "so i suggest the authors do not claim that this method is a general-purpose sentence embedding model"], "labels": ["APC", "DFT", "DIS", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ii the authors do not have any descriptions for figure   equation  is also very confus", "iii the experiments are insufficient in terms of details how is the loss calculated how is the detection accuracy calcul", "iii the experiments are insufficient in terms of details how is the loss calculated how is the detection accuracy calcul", "this paper proposes a learning-to-learn approach to training inference networks in vaes that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribut", "the basic approach follows andrychowicz et al  but there are some extra considerations in the context of learning an inference algorithm"], "labels": ["DFT", "DFT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network above and beyond the limitations imposed by the variational famili", "this source of error is often ignored in the literatur", "although there are some exceptions that may be worth mentioning* hjelm et al  https//arxivorg/pdf/pdf observe it for directed belief networks admittedly a different model class", "* the ladder vae paper by sonderby et al  https//arxivorg/pdf/pdf uses an architecture that reduces the work that the encoder network needs to do without increasing the expressiveness of the variational approxim", "* the structured vae paper by johnson et al  https//arxivorg/abs/ also proposes an architecture that reduces the load on the inference network* a very recent paper by krishnan et al https//arxivorg/pdf/pdf posted to arxiv days before the iclr deadline is probably closest it also examines using iterative optimization but no learning-to-learn to improve training of va"], "labels": ["APC", "CRT", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they remark that the benefits on binarized mnist are pretty minimal compared to the benefits on sparse high-dimensional data like text and recommendations this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent vari", "i think this is good and potentially important work", "although i do have some questions/concerns about the results in table  see below", "some more specific commentsfigure  i think this might be clearer if you unrolled a couple of iterations in a and c", "dempster et al  is not the best reference for this section that paper only considers the case where the e and m steps can be done in closed form on the whole dataset"], "labels": ["CRT", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a more relevant reference would be stochastic variational inference by hoffman et al  which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm", "section  the statement pz=nzmu_psigma_p doesnut quite match the formulation of rezende&moham", "first in the case where there is only one layer of latent variables there is almost never any reason to use anything but a normal i prior since the first weight matrix of the decoder can reproduce the effects of any mean or covari", "second in the case where there are two or more layers the joint distribution of all z need not be gaussian or even unimodal since the means and variances at layer n can depend nonlinearly on the value of z at layer n+", "an added bonus of eliminating the mu_p sigma_p you could get rid of one subscript in mu_q and sigma_q which would reduce notational clutt"], "labels": ["SUG", "APC", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why not have mu_{qt+} depend on sigma_{qt} as well as mu_{qt}", "table  these results are strange in a few ways* the gap between the standard and iterative inference network seems very small  nats at most", "this is much smaller than the gap in figure a", "* the mnist results are suspiciously good overall given that itus ultimately a gaussian approximation and simple fully connected architectur", "iuve read a lot of papers evaluating that sort of model/variational distribution as a baseline and i donut think iuve ever seen a number better than ~ nat"], "labels": ["QSN", "CRT", "CRT", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- algorithm  has a lot of problem specific hyperparametes that may be difficult to get right", "not clear how important they are- they analyze the simpler analytically and likely computationally boolean hyperparameter case each hyperparameter is binari", "not a realistic set", "in their experiments they use these binary parameter spaces so i'm not sure how much i buy that it is straightforward to use continuous valued polynomi", "- interesting idea but i think it's more theoretical than pract"], "labels": ["DFT", "CRT", "SMY", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["feels like a hammer in need of a nail", "the paper is motivated with building robots that learn in an open-ended way which is really interest", "what it actually investigates is the performance of existing image classifiers and object detector", "i could not find any technical contribution or something sufficiently mature and interesting for presenting in iclr", "some issues- submission is supposed to be double blind but authors reveal their identity at the start of sect"], "labels": ["DFT", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- implementation details all over the place section  is called implementation but at that point no concrete idea has been proposed so it seems too early for talking about tensorflow and kera", "this work proposes a variation of the densenet architecture that can cope with computational resource limits at test tim", "the paper is very well written experiments are clearly presented and convincing and most importantly the research question is exciting and often overlook", "my only major concern is the degree of technical novelty with respect to the original densenet paper of huang et ", "the authors add a hierarchical multi-scale structure and show that densenet can better cope with it than resnet eg fig"], "labels": ["CRT", "SMY", "APC", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they investigate pros and cons in detail adding more valuable analysis in the appendix", "however this work is basically an extension of the densenet approach with a new problem statement and additional in-depth analysi", "however this work is basically an extension of the densenet approach with a new problem statement and additional in-depth analysi", "some more minor comments -tplease enlarge fig", "-ti did not fully grasp the details in the first solution paragraph on p"], "labels": ["FBK", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["please extend and describe in more detail", "in conclusion this is a very well written paper that designs the network architecture of densenet such that it is optimized to include cpu budgets at test tim", "i recommend acceptance to iclr", "detecting out of distribution examples is important since it lets you know when neural network predictions might be garbag", "the paper addresses this problem with a method inspired by adversarial training and shows significant improvement over best known method previously published in iclr"], "labels": ["DFT", "APC", "FBK", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["previous method used at the distribution of softmax scores as the measur", "highly peaked - confidence spread out - out of distribut", "the authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step", "the small step is in the direction of gradient when top class activation is taken as the object", "this is also the gradient used to determine influence of predictors and it's the gradient term used for adversarial training fast gradient sign method"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their experiments show improvement across the board using densenet on collection of small size dataset tiny imagenet cifar lsun", "their experiments show improvement across the board using densenet on collection of small size dataset tiny imagenet cifar lsun", "for instance at % threshold detect % of out of distribution examples their error rate goes down from % for the best known method to % which is significant enough to prefer their method to the previous work", "for instance at % threshold detect % of out of distribution examples their error rate goes down from % for the best known method to % which is significant enough to prefer their method to the previous work", "the paper introduces a neural translation model that automatically discovers phras"], "labels": ["SMY", "APC", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way", "however the clarity of the paper could be improv", "the local reordering layer has the ability to swap inputs however how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating oth", "are all segments translated independently or do you carry over the hidden state of the decoder rnn between seg", "in figure  both a brnn and swan layer are shown is there another rnn in the swan layer or does the brnn emit the final outputs after the segments have been determin"], "labels": ["APC", "SUG", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a supervised variant of kohonen's self-organizing mapsom ie trained by gradient descent with gradient obtained bybackprop using a grid of rbf neurons which respond to the input only ifthey are in the grid-neighborhood of the 'winner' neuron closest to theinput", "this in itself is not even new but the authors replace a linearoutput layer with squared error proposed in another earlier paper by asoftmax layer with cross-entropi", "unsuprisingly this leads to an improv", "the title is mislead", "there is nothing deep in this architectur"], "labels": ["CRT", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it isa shallow architecture with a single rbf-like hidden lay", "there is atiny ounce of novelty in that the authors propose to improve a supervisedversion of the som by using what should have been used in the first placeaccording to modern good practic", "and the whole paper seems as if writtencirca  or", "another misleading thing is the term self-organizingused throughout which is roughly synonym to learning according to me andnot something uniquely belonging to the som family of models as used bythe author", "as an example of time-travel to the past the authors talkabout rbms and stacks of auto-encoders as if that was the deep learningstate-of-the-art"], "labels": ["CRT", "CRT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors even call these methods 'recent'! clearly notthe cas", "unfortunately it's not just talk they are also the point ofcomparison in the experiments ie there are no comparison with moderndeep learning method", "even the datasets are outdated from the ", "vocabulary is wrong in other places for example the word semi-supervisedis wrongly understood and us", "semi-supervised means that one combineslabeled and unlabeled data"], "labels": ["CRT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["where the label 'semi-supervised' is used page is actually wrong yes the labels are used but of course it is the*gradients* which show up in the update not the labels themselvesdirectli", "it's also not true that there is little research in understandingthe formation of internal represent", "there is a whole subfields ofpapers trying to interpret the features learned by deep networks and muchwork designing learning frameworks and objectives to achieve betterrepresentations eg to better disentangle the underlying factor", "it's also common practice to analyze the representations learned inmany deep learning pap", "the paper uses much space to show how to compute gradients in the proposedarchitecture there is obviously no need for this in a day and age wheregradients are automatically derived by softwar"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the cherry on the sundae are the experimental result", "how could the authorsget % on mnist with an mlp of any kind", "it does not seem right at al", "even a linear regression would get at least half of that", "as there are notenough experimental details to judge it's hard to figure out the problembut this ppaper is clearly not publishable at any of the quality machinelearning venues for weakness in originality quality of the writingand poor experi"], "labels": ["APC", "QSN", "CRT", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper a deep predictive coding network for learning latent representations considers learning of a generative neural network", "the network learns unsupervised using a predictive coding setup", "a subset of the cifar- image database  images horses and ships are used for train", "then images generated using the latent representations inferred on these images on translated images and on images of other objects are shown", "it is then claimed that the generated images show that the network has learned good latent represent"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have some concerns about the paper maybe most notably about the experimental result and the conclusions drawn from them", "the numerical experiments are motivated as a way to understand the capacity of the network with regards to modeling the external environment abstract", "and it is concluded in the final three sentences of the paper that the presented network can infer effective latent representations for images of other objects ie of objects that have not been used for training and further that in this regards the network is better than most existing algorithms []", "i expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggest", "i expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggest"], "labels": ["CRT", "DIS", "DIS", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also some at least quantifiable if not benchmarked outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sect", "as a matter of fact all images shown including those in the appendix are blurred versions of the original images except of one single image fig  last row nd image and that is not commented on", "if these are the generated images then some reconstruction is done by the network fine but also not unsurprising as the network was told to do so by the used objective funct", "what precisely do we learn her", "i would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network"], "labels": ["DFT", "CRT", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how can the reader conclude any functioning from these imag", "using the right objective function reconstructions can also be obtained using random not learned generative fields and relatively basic model", "the fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is to my mind not at all shown her", "what would be a good measure for an effective latent representation that substantiates the claims mad", "the reconstruction of unseen images is claimed central but as far as i could see figures   and  are not even referred to in the text nor is there any objective measure discuss"], "labels": ["QSN", "DIS", "DFT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["studying the relation between predictive coding and deep learning makes sense but i do not come to the same strong conclusions as the authors by considering the experimental results - and i do not see evidence for a sophisticated latent representation learned by the network", "i am not saying that there is none but i do not see how the presented experimental results show evidence for thi", "furthermore the authors stress that a main distinguishing feature of their approach top of page  is that in their network information flows from latent space to observed space eg in contrast to cnn", "that is a true statement but also one which is true for basically all generative models eg of standard directed graphical models such as wake-sleep approaches hinton et al  deep sbns and more recent generative models used in gans goodfellow et ", "any of these references would have made a lot of sens"], "labels": ["DFT", "DFT", "DIS", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with my evaluation i do not want to be discouraging about the general approach", "but i can not at all give a good evaluation given the current experimental results unless substantial new evidence which make me evaluate these results differently is provided in a discuss", "minor- no legend for fig -notes - notedhave focus", "this paper presents a practical methodology to use neural network for recommending products to users based on their past purchase histori", "the model contains three components a predictor model which is essentially a rnn-style model to capture near-term user interests a time-decay function which serves as a way to decay the input based on when the purchase happened and an auto-encoder component which makes sure the user's past purchase history get fully utilized with the consideration of time decay"], "labels": ["DIS", "DIS", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and the paper showed the combination of the three performs the best in terms of precision@k and pcc@k and also with good scal", "it also showed good online a/b test performance which indicates that this approach has been tested in real world", "two small concerns in section  i am not fully sure why the proposed predictor model is able to win over lstm", "as lstm tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model", "some insights might be useful ther"], "labels": ["SMY", "APC", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the title of this paper is weird", "suggest to rephrase unreasonable to something more posit", "this paper proposes a variant of hierarchical hidden markov models hmms where the chains operate at different time-scales with an associate d spectral estimation procedure that is computationally effici", "the model is applied to artificially generated data and to high-frequency equity data showing promising result", "the proposed model and method are reasonably original and novel"], "labels": ["CRT", "SUG", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written and the method reasonably well explain", "i would add an explanation of the spectral estimation in the appendix rather than just citing rodu et ", "additional experimental results would make it a stronger pap", "it would be great if the authors could include the code that implements the model", "this  paper is  on ab important topic  unsupervised learning on unaligned data"], "labels": ["APC", "SUG", "DFT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper shows that is possible to learn the between domains mapping using gan only without a reconstruction cyclic loss", "the paper postulates that learning should happen on shallower networks first then on a deeper network that uses the gan cost function and regularizing discrepancy between the deeper and the small network", "i did not get the time to go through the proofs but they handle the fully connected case as far as i understand", "please find my comments are belowoverall it is an interesting  but long pap", "the claims are a bit strong for cnn and need further theoretical and experimental verif"], "labels": ["SMY", "SMY", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the number of layer as a complexity is not appropriate  as we need to take in account many parameters  the pooling or the striding for the resolution the presence or the absence of residual connections for content preservation the number of feature maps more experimentation is need", "prosimportant and challenging topic to analyze and any progress on unsupervised learning is interest", "consi have some questions on the shallow/deep in the context of cnn and to what extent the cyclic cost is not needed or it is just distilled from the shallow training - arguably the shallow to deep distillation can be understood as a reconstruction cost  since the shallow network will keep a lot of the spatial inform", "if the deep network match the shallow one  this can be understood as a form of ucdistilled content uc loss and the disc of the deep one will take care of the texture  style content is this intuition correct - original cyclic reconstruction constraint is in the pixel space using l norm usually the regularizer introduced matches in a feature space  which is known to produce better results as a ucperceptual lossud can the author comment on this is this what is really happening here moving from cyclic constraint on pixels to a  cyclic constraint in a feature space  shallow network-  *", "spatial resolution*  the analysis seems to be done with respect to dnn not to a  cnn"], "labels": ["DIS", "APC", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["did you study the effect of the architectures in terms of striding and pooling how it affects the result", "i think just counting number of layers as a complexity is not reasonable when we deal with images with respect to  what preserves contents and what matches texture or styl", "- have you tried resnets generators and discriminators  at various depths  with padding so that the spatial resolution is preserv", "- depth versus width another measure that is missing is also the number of feature maps how wide is the network  how does this interplays with the depth", "regularizing deeper networks in the experiments of varying the length did you see if the results can be stabilized using dropout with deep networks and small feature maps between training g and h"], "labels": ["QSN", "DIS", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how do you initialize h fully at random  seems the paper is following implementation by kim et al what happens if the discriminator is like in cycle gan acting on pixel", "pixel gan rather then only giving a global score for the whole imag", "this paper studies a new architecture dualac", "the author give strong and convincing justifications based on the lagrangian dual of the bellman equation although not new introducing this as the justification for the architecture design is plaus", "there are several drawbacks of the current format of the paper the algorithm is vagu"], "labels": ["DIS", "QSN", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["alg  line  'closed form' there is no closed form in eq", "it is just an mc approxim", "line  decay o/t^beta this is indeed vague albeit easy to understand", "the algorithm requires that every step is crystal clear", "also there are several format error which may be due to compiling eg line  of abstract'dual-ac ' an extra spac"], "labels": ["CRT", "SMY", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are many format errors like this throughout the pap", "the author is suggested to do a careful format check", "the author is suggested to explain more about the necessity of introducing path regularization and sda the current justification is reasonable but too brief", "the experimental part is ok to me but not very impress", "overall this seems to be a nice paper to m"], "labels": ["CRT", "SUG", "SUG", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is written well and clear", "the core contribution of the paper is the illustration that under the assumption of flat or curved decision boundaries with positive curvature small universal adversarial perturbations exist", "pros the intuition and geometry is rather clearly pres", "cons references to caffenet  and lenet even though the latter is well-known are miss", "in the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries there is no description of how these networks were train"], "labels": ["SMY", "SMY", "APC", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear why the authors have decided to use out-dated -layer lenet  and nin network in network architectures instead of more recent and much better performing architectures and less complex than nin architectur", "it would be nice to see how the behavior and boundaries look in these cas", "the conclusion is speculativeour analysis hence shows that to construct classifiers that are robust to universal perturbations itis key to suppress this subspace of shared positive directions which can possibly be done throughregularization of the objective funct", "this will be the subject of future work", "it is clear that regularization should play a significant role in shaping the decision boundari"], "labels": ["CRT", "DIS", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unfortunately the paper does not provide details at the basic level which algorithms  architectures hyper-parameters or regularization terms are us", "all these factors should play a very significant role in the experimental validation of their hypothesi", "notes i did not check the proofs of the theorems in detail", "this paper introduces the lr-net which uses the reparametrization trick inspired by a similar component in va", "although the idea of reparametrization itself is not new applying that for the purpose of training a binary or ternary network and sample the pre-activations instead of weights is novel"], "labels": ["CRT", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from the experiments we can see that the proposed method is effect", "it seems that there could be more things to show in the experiments part", "for example since it is using a multinomial distribution for weights it makes sense to see the entropy wrt training epoch", "also since the reparametrization is based on the lyapunov central limit theorem which assumes statistical independence a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram", "also in the literature of low precision networks people are concerning both training time and test time computation demand"], "labels": ["APC", "DIS", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since you are sampling the pre-activations instead of weights i guess this approach is also able to reduce training time complexity by an ord", "thus a calculation of train/test time computation could highlight the advantage of this approach more boldli", "this paper proposes to use neural network and gradient descent to automatically design for engineering task", "it uses two networks parameterization network and prediction network to model the mapping from design parameters to fit", "it uses back propagation gradient descent to improve the design"], "labels": ["SUG", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is evaluated on heat sink design and airfoil design", "this paper targets at a potentially very useful application of neural networks that can have real world impact", "however i have three main concerns presentation the organization of the paper could be improv", "it mixes the method the heat sink example and the airfoil example throughout the entire pap", "sometimes i am very confused about what is being describ"], "labels": ["SMY", "APC", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my suggestion would be to completely separate these three parts present a general method first then use heat sink as the first experiment and airfoil as the second experi", "this organization would make the writing much clear", "in the paragraph above section  the paper made two arguments i might be wrong but i do not agree with either of them in gener", "first of all neural networks are good at generalizing to examples outside their train set this depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample spac", "this is especially critical if a deep neural network is used since overfitting is a real issu"], "labels": ["SUG", "SUG", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["second it is easy to imagine a hybrid system where a network is trained on a simulation and fine tuned  implementing such a hybrid system is nontrivial due to the reality gap", "there is an entire research field about closing the reality gap and transfer learn", "so i am not convinced by these two arguments made by this pap", "they might be true for a narrow field of application but in general i think they are not quite correct", "the key of this paper is to approximate the dynamics using neural network which is a continuous mapping and take advantage of its gradient comput"], "labels": ["CRT", "DIS", "CRT", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however many of dynamic systems are inherently discontinuous collision/contact dynamics or chaotic turbulent flow", "in those scenarios the proposed method might not work well and we may have to resort to the gradient free method", "it seems that the proposed method works well for heat sink problem and the steady flow around airfoil both of which do not fall into the more complex physics regim", "it would be great that the paper could be more explicit about its limit", "in summary i like the idea the application and the result of this pap"], "labels": ["DIS", "SUG", "DIS", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the writing could be improv", "but more importantly i think that the proposed method has its limitation about what kind of physical systems it can model", "these limitation should be discussed more explicitly and more thoroughli", "the paper discusses the problems of meta optimization with small look-ahead do small runs bias the results of tun", "the result is yes and the authors show how differently the tuning can be compared to tuning the full run"], "labels": ["SUG", "DFT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the greedy schedules are far inferior to hand-tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be seen with a small lookahead", "the authors show that this effect is caused by the noise in the obective funct", "pro- thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work- easy to read and follow", "cons-small issues in presentation * figure  optimal learning rate - optimal greedy learning rate also reference to theorem  for increased clar", "* the optimized learning rate in  is not described this reduces reproduc"], "labels": ["SMY", "SMY", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* figure  misses the red trajectories also it would be easier to have colors on the same log-scal", "the text unfortunately does not explain why the loss function looks so vastly different  with different look-ahead", "i would assume from the description that the colors are based  on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective lr", "typos and notationpage  last paragraph we train the all - we train allnotation page  i find abla_{theta_i} confusing when theta_i is a scalar i would propose frac{partial}{partial theta_i}page  but this would come at the expense of long-term optimization process at this point of the paper it is not clear how or why this should happen", "maybe add a sentence regarding the large/small eigenvalu"], "labels": ["DFT", "DFT", "DFT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a deep learning architecture for joint learning of feature representation a target-task mapping function and a sample re-weighting funct", "specifically the method tries to discover feature representations which are invariance in different domains by minimizing the re-weighted empirical risk and distributional shift between design", "overall the paper is well written and organized with good description on the related work research background and theoretic proof", "the main contribution can be the idea of learning a sample re-weighting function which is highly important in domain shift", "however as stated in the paper since the causal effect of an intervention t on y conditioned on x is one of main interests it is expected to add the related analysis in the experiment sect"], "labels": ["SMY", "DIS", "APC", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a model for learning to generate data conditional on attribut", "demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training tim", "the model is interesting and the results while preliminary suggest that the model is capable of making quite interesting generalizations in particular it can synthesize images that consist of settings of features that have not been seen befor", "however this paper is mercilessly difficult to read", "the most serious problems are the extensive discussion of the fully unsupervised variant rather than the semisupervised variant that is evaluated poor use of examples when describing the model nonstandard terminology ucconceptsud and uccontextud are extremely vague terms that are not defined precisely and discussions to vaguely related work that does not clarify but rather obscures what is going on in the pap"], "labels": ["SMY", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the evaluation since this paper proposes a technique for learning a posterior recognition model it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine uccontextsud that were not observed during train", "the experiments show that the generation component is quite effect", "but this is an obvious missing step", "anyway some other related worklample et al  nips fader networks i realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attribut", "but i mostly bring it up because it is an impressively clear presentation of a model and experimental set up"], "labels": ["DIS", "APC", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this work the authors propose a procedure for tuning the parameters of an hmc algorithm i guess if i have understood correctli", "ni think this paper has a good and strong point this work points out the difficulties in choosing properly the parameters in a hmc method such as the step and the number of iterations in the leapfrog integrator for inst", "in the literature specially in machine learning there is ``feveruu about hmc in my opinion partially unjustifi", "if i have understood your method is an adaptive hmc algorithm  where the parameters are updated online or is the training  done in advance please remark and clarify this point", "however i have other additional comments- eqs  and  are quite complicated i think a running toy example can help the interested read"], "labels": ["SMY", "APC", "DIS", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i suggest to compare the proposed method to other efficient methods that do not use the gradient information in some cases as multimodal posteriors the use of the gradient information can be counter-productive for sampling purposes such as multiple try metropolis mtm schem", "l martino j read on the flexibility of the design of multiple try metropolis schemes computational statistics volume  issue  pages -  adaptive techniqu", "h haario e saksman and j tamminen an adaptive metropolis algorithm bernoulli u april", "and component-wise strategies as gibbs sampling w r gilks and p wild adaptive rejection sampling for gibbs sampling appl statist vol  no  pp u u", "at least add a brief paragraph in the introduction citing and discussing this possible altern"], "labels": ["SUG", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attent", "the goal is to avoid recurrent which is sequential and hence a bottleneck during both training and infer", "authors also propose a paraphrasing based data augmentation method which helps in improving the perform", "proposed method performs better than existing models in squad dataset while being much faster in training and infer", "my commentsthe proposed model is convincing and the paper is well written"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why donut you report your model performance without data augmentation in t", "is it because it does not achieve sota", "the proposed data augmentation is a general one and it can be used to improve the performance of other models as wel", "so it does not make sense to compare your model + data augmentation against other models without data augment", "i think it is ok to have some deterioration in the performance as you have a good speedup when compared to other model"], "labels": ["QSN", "QSN", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can you mention your leaderboard test accuracy in the rebutt", "the paper can be significantly strengthened by adding at least one more reading comprehension dataset", "that will show the generality of the proposed architectur", "given the sufficient time for rebuttal i am willing to increase my score if authors report results in an additional dataset in the revis", "are you willing to release your code to reproduce the result"], "labels": ["QSN", "SUG", "SUG", "FBK", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nminor comments you mention x to x for inference speedup in abstract and then x to x speedup in intro please be consist", "n in the first contribution bullet point ucthat exclusive built uponud should be ucthat is exclusively built uponud", "the paper proposes another training objective for training neural sequence-to-sequence model", "the objective is based on alpha-divergence between the true input-output distribution q and the model distribution p", "the new objective generalizes  reward-augmented maximum likelihood raml and entropy-regularized reinforcement learning rl to which it presumably degenerates when alpha goes to  or to  respect"], "labels": ["SUG", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper has significant writing issu", "in paragraph ucmaximum likelihoodud page  the formalization of the studied problem is unclear", "do x and y denote the complete input/output spaces or do they stand for the training set examples onli", "in the former case the statement ucx is uniformly sampled from xud does not make sense because x is practically infinit", "same applies to the dirac distribution qy|x the true conditional distribution of outputs given inputs is multimodal even for machine transl"], "labels": ["DFT", "DFT", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if x and y were meant to refer to the training set it would be worth mentioning the existence of the test set", "furthermore in the same section  the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach at which stage greedy search or beam search is us", "the proposed method is evaluated on just one dataset", "crucially there is no comparison to a trivial linear combination of ml and rl which in one way or another was used in almost all prior work including gnmt bahdanau et al ranzato et ", "the paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison"], "labels": ["SUG", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to sum up i can not recommend the paper to accept", "because a an important baseline is miss", "b there are serious writing issu", "the authors discuss a direct gamma sampling method for the interpolated samples in gans and show the improvements over usual normal sampling for celeba mnist cifar and svhn dataset", "the method involves a nice albeit minor trick where the chi-squared distribution of the sum of the z_{i}^{} has its dependence on the dimensionality remov"], "labels": ["FBK", "DFT", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am not convinced by the distribution of |z^prime|^{} in the first place eqn  the samples from the gaussian will be approximately orthogonal in high dimensions but the inner product will be at least o", "thus although the |z_{}|^{} and |z_{}|^{} are chi-squared/gamma i don't think |z^prime|^{} is exactly gamma in gener", "the experiments do show that the interpolated samples are qualitatively better but a thorough empirical analysis for different dimensionalities would be welcom", "figures  and  do not add anything to the story since  is just a plot of gamma pdfs and  shows the difference between the constant kl and the normal case that is linear in d", "overall i think the trick needs to be motivated better and the experiments improved to really show the import of the d-independence of the kl"], "labels": ["DFT", "DFT", "APC", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus i think this paper is below the acceptance threshold", "thus i think this paper is below the acceptance threshold", "the paper analyzes the expressivity of convolutional arithmetic circuits convacs where neighboring neurons in a single layer have overlapping receptive field", "to compare the expressivity of overlapping networks with non-overlapping networks the paper employs grid tensors computed from the output of the convac", "to compare the expressivity of overlapping networks with non-overlapping networks the paper employs grid tensors computed from the output of the convac"], "labels": ["CRT", "FBK", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the grid tensors are matricized and the ranks of the resultant matrices are compar", "the grid tensors are matricized and the ranks of the resultant matrices are compar", "the paper obtains a lower bound on the rank of the resultant grid tensor", "and uses them to show that an exponentially large number of non-overlapping convacs are required to approximate the grid tensor of an overlapping convac", "and uses them to show that an exponentially large number of non-overlapping convacs are required to approximate the grid tensor of an overlapping convac"], "labels": ["SMY", "DIS", "DFT", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["assuming that the result carries over to convnets i find this result to be very interest", "while overlapped convolutional layers are almost universally used there has been very little theoretical justification for the sam", "this paper shows that overlapped convacs are exponentially more powerful than their non-overlapping counterpart", "the article do gans learn the distribut", "some theory and empirics considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of imag"], "labels": ["APC", "DFT", "APC", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors argue that gans in fact generate the distributions with fairly low support", "the proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching or very similar pairs in the generated sampl", "this test is expected to experimentally support the previous theoretical analysis by arora et ", "the further theoretical analysis is also performed showing that for encoder-decoder gan architectures the distributions with low support can be very close to the optimum of the specific bigan object", "the experimental part of the paper considers the celeba and cifar- dataset"], "labels": ["SMY", "SMY", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we definitely see many very similar images in fairly small sample gener", "so the general claim is support", "however if you look closely at some pictures you can see that they are very different though reported as similar", "for example some deer or truck pictur", "that's why i would recommend to reevaluate the results visually which may lead to some change in the number of near duplicates and consequently the final support estim"], "labels": ["DIS", "APC", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to sum up i think that the general idea looks very natural and the results are support", "on theoretical side the results seem fair though i didn't check the proofs and being partly based on the previous results of arora et al  clearly make a step furth", "the paper studies a combination of model-based and model-free rl", "the idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learn", "some parts of the paper lack clarity and the empirical results need improvement to support the claims see details below"], "labels": ["APC", "APC", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarity - the main idea of the proposed method is clear", "- some notations and equations are broken", "for example  the definition of bar{a} in section  is broken", "the overall objective in section  is broken", "the computation of w in algorithm  is problemat"], "labels": ["APC", "CRT", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- some details of the experiments/methods are confus", "for example  the step number k is dynamically determined by a short line search as in section  ``dynamic rolloutuu but later in the experiments section  the value of k is set to be  uniformli", "only the policy and value networks specifi", "the forward models are not specifi", "in algorithm  what exact method is used in determining if mu is converged or not"], "labels": ["CRT", "SMY", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originalitythe proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm", "an empirical comparison could be helpful but not provid", "the idea of the proposed method is related to the classic dyna methods from sutton", "a discussion on the difference would be help", "significance- the paper could compare against other relevant baselines that combine model-based and model-free rl methods such as svg stochastic value gradi"], "labels": ["SMY", "DFT", "SMY", "CNT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- to make a fair comparison the results in table  should consider the amount of data used in pre-training the forward model", "current results in table  only compare the amount of data in policy learn", "- figure  is plotted for just one random starting st", "the figure could have been more informative if it was averaged over different starting st", "the same issue is found in figur"], "labels": ["SMY", "SMY", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be helpful if the plots of other domains are provid", "- in figure  even though the diff norm fluctuates the cosine similarity remains almost const", "does it suggest the cosine similarity is not effective in measuring the state similar", "- figure   and  need confidence intervals or standard error", "pros- the research direction in combining model-based and model-free rl is interest"], "labels": ["DFT", "SMY", "QSN", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the main idea of the proposed method is clear", "cons- parts of the paper are unclear and some details are miss", "- the paper needs more discussion and comparison to relevant baseline method", "- the empirical results need improvement to support the paperus claim", "the quality of the paper is good and clarity is mostly good"], "labels": ["APC", "DFT", "DFT", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed metric is interest", "but it is hard to judge the significance without more thorough experiments demonstrating that it works in practic", "pros - clear definitions of terms - overall outline of paper is good - novel metr", "cons - text is a bit over-wordy and flow/meaning sometimes get lost", "a strict editor would be helpful because the underlying content is good - odd that your definition of generalization in gans appears immediately preceding the section titled generalisation in gans - the paragraph at the end of the generalisation in gans section is confus"], "labels": ["APC", "DFT", "APC", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think this section and the previous the objective of unsupervised learning could be combined removing some repetition adding some subtitles to improve clar", "this would cut down the text a bit to make space for more experi", "- why is your definition of generalization that the test set distance is strictly less than training set", "i would think this should be less-than-or-equ", "- there is a sentence that doesn't end at the top of p  the original gan paper showed that [ends here]"], "labels": ["SUG", "DIS", "QSN", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- should state in the abstract what your notion of generalization for gans is instead of being vague about it", "- more experiments showing a comparison of the proposed metric to others eg inception score mturk assessments of sample quality etc would be necessary to find the metric convinc", "- what is a pushforward measur", "p - the related work section is well-written and interesting but it's a bit odd to have it at the end", "earlier in the work eg before experiments and discussion would allow the comparison with mmd to inform the context of the introduct"], "labels": ["SUG", "SUG", "QSN", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- there are some errors in figures that i think were all mentioned by previous comment", "the problem of interest is to train deep neural network models with few labelled training sampl", "the specific assumption is there is a large pool of unlabelled data and a heuristic function that can provide label annotations possibly with varying levels of noises to those unlabelled data", "the adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression and also machine teach", "the student deep neural network model will learn from both labelled and unlabelled training data with the labels provided by the teacher gaussian process model"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the teacher also supplies an uncertainty estimate to each predicted label", "how about the heuristic funct", "this is used for learning initial feature representation of the student model", "crucially the teacher model will also rely on these learned featur", "labelled data and unlabelled data are therefore lie in the same dimensional spac"], "labels": ["SMY", "QSN", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specific questions to be addressedtclustering of strongly-labelled data point", "thinking about the statement uceach an expert on this specific region of data spaceud if this is the case i am expecting a clustering for both strongly-labelled data points and weakly-labelled data point", "each teacher model is trained on a portion of strongly-labelled data and will only predict similar weakly-labelled data", "on a related remark the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limit", "as well gp models are quite scalable nowadays experiments with millions to billions of data points are available in recent nips/icml papers though they are all rely on low dimensionality of the feature space for optimizing the inducing point loc"], "labels": ["DIS", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it will be informative to provide results with a single gp model", "tfrom modifying learning rates to weighting sampl", "rather than using uncertainty in label annotation as a multiplicative factor in the learning rate it is more ucintuitiveud to use it to modify the sampling procedure of mini-batches akin to baseline # sample with higher probability data points with higher certainti", "here experimental comparison with for example an svm model that takes into account instance weighting will be informative and a student model trained with logits as in knowledge distillation/model compress", "summary of the paper-------------------------------the authors propose to add  elements to the 'federatedaveraging' algorithm to provide a user-level differential privacy guarante"], "labels": ["SUG", "DIS", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the impact of those  elements on the model'a accuracy and privacy is then carefully analys", "clarity significance and correctness--------------------------------------------------clarity excel", "significance i'm not familiar with the literature of differential privacy so i'll let more knowledgeable reviewers evaluate this point", "correctness the paper is technically correct", "questions-------------- figure  adding some noise to the updates could be view as some form of regularization so i have trouble understand why the models with noise are less efficient than the baselin"], "labels": ["SMY", "APC", "DIS", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["questions-------------- figure  adding some noise to the updates could be view as some form of regularization so i have trouble understand why the models with noise are less efficient than the baselin", "clipping is supposed to help with the exploding gradients problem", "do you have an idea why a low threshold hurts the perform", "is it because it reduces the amplitude of the updates and thus simply slows down the train", "is your method compatible with other optimizers such as rmsprop or adam which are commonly used to train rnn"], "labels": ["CRT", "DIS", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros------ nice extensions to federatedaveraging to provide privacy guarante", "strong experimental setup that analyses in details the proposed extens", "experiments performed on public dataset", "cons-------non", "typos-------- section  paragraph   is given in figure  - is given in algorithm"], "labels": ["APC", "APC", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["typos-------- section  paragraph   is given in figure  - is given in algorithm", "note-------since i'm not familiar with the differential privacy literature i'm flexible with my evaluation based on what other reviewers with more expertise have to say", "the paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar result", "the authors show that these techniques can all be seen as a product of input activations and a modified gradient where the local derivative of the activation function at each neuron is replaced by some fixed funct", "a second part of the paper looks at whether explanations are global or loc"], "labels": ["CRT", "FBK", "SMY", "SMY", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a metric called sensitivity-n for that purpose and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear cas", "the behavior of each explanation wrt these properties is then tested on multiple dnn models tested on real-world dataset", "results further outline the resemblance between the compared method", "in the appendix the last step of the proof below eq  is unclear", "as far as i can see the variable g_i^lrp wasnut defined and the use of eq  to achieve this last could be better explain"], "labels": ["SMY", "SMY", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i can see the variable g_i^lrp wasnut defined and the use of eq  to achieve this last could be better explain", "there also seems to be some issues with the ordering ij where these indices alternatively describe the lower/higher layers or the higher/lower lay", "# summarythis paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's st", "this paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map", "the result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in starcraft"], "labels": ["CRT", "CRT", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[pros]- interesting problem", "[cons]- the proposed method is not much novel", "- the evaluation is a bit limited to two specific downstream prediction task", "- the evaluation is a bit limited to two specific downstream prediction task", "# novelty and significance- the problem considered in this paper is interest"], "labels": ["APC", "CRT", "DFT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the proposed method is not much novel", "- overall this paper is too specific to starcraft domain + particular downstream prediction task", "it would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction task", "alternatively it could be also interesting to consider an rl problem where the agent should reveal the hidden state of the opponent as much/quickly as poss", "# quality- the experimental result is not much comprehens"], "labels": ["CRT", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is expected to perform better than hand-designed methods on downstream prediction task", "it would be better to show an in-depth analysis of the learned model or show more results on different tasks possibly rl tasks rather than prediction task", "# clarity- i did not fully understand the learning object", "does the model try to reconstruct the state of the current time-step or the futur", "the learning objective is not clearly defin"], "labels": ["DIS", "SUG", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  the target x and y have time steps from t to t", "what is the range of t and t", "if the proposed model is doing future prediction it would be important to show and discuss long-term prediction result", "the paper devises a sparse kernel for rnns which is urgently needed because current gpu deep learning libraries eg cudnn cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune rnns so as to be able to run on devices with limited compute power eg smartphon", "unfortunately due to the low-level and gpu specific nature of the work i would think that this work will be better critiqued in a more gpu-centric confer"], "labels": ["DIS", "QSN", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity these are not contrasted by presenting the loss in accuracy caused by introducing sparsity in the main portion of the pap", "it may be the case by reducing density to % we can speedup by n fold but this observation may not have any value if the accuracy becomes  abysm", "pros- addresses an urgent and timely issue of devising sparse kernels for rnns on gpu", "- experiments show that the kernel can effectively exploit sparsity while utilizing gpu resources wel", "cons- this work may be better reviewed at a more gpu-centric confer"], "labels": ["CRT", "DIS", "APC", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- experiments in main paper only show speedups and do not show loss of accuracy due to spars", "- experiments in main paper only show speedups and do not show loss of accuracy due to spars", "paper summarythis is a very long paper  pages and i did not read it in its entireti", "the first part up to page  focuses on better understanding the exploding gradients problem and challenges the fact that current techniques to address gradient explosion work as claim", "to do so they first motivate a new measure of gradient size the gradient scale coefficient which averages the singular values of the jacobian and takes a ratio of different lay"], "labels": ["DFT", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the motivation for this measure is that it is invariant to simple rescaling of layers that preserves the funct", "i would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs eg", "they focus on linear mlps in the paper for computational simpl", "with this setup and assuming the jacobian decomposes they prove that the gsc increases exponentially proposit", "they empirically test this out for networks  layers deep and  layers wide where they find that some architectures have exploding gradients after random initialization and others do not but those that do not have other drawback"], "labels": ["SMY", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they then overview the notion of effective depth for a residual network a linear residual network can be written as a product of terms of the form i + r_i", "expanding out each term is a product of some of the r_i and some of the identities i", "if all r_i have a norm   then the terms that dominate will be those that consist of fewer r_i resulting in a lower effective depth", "this is described in veit et ", "while this analysis was originally used for residual networks they relate this to any network by letting i turn into an arbitrary initial funct"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their main theoretical result from this is that deeper networks take exponentially longer to train under certain conditions which they test out with linear networks of depth  and width", "they also propose that the reason gradients explode is because networks try to preserve their domain going forward which requires jacobians to have determinant  and leads to a higher q-norm", "main commentsthis could potentially be a very nice paper but i feel the current presentation is not ready for accept", "in particular the paper would benefit greatly from being made much shorter and having more of the important details or proof outlines for the various propositions in the main text", "in particular the paper would benefit greatly from being made much shorter and having more of the important details or proof outlines for the various propositions in the main text"], "labels": ["DIS", "DIS", "FBK", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["right now it is quite confusing to follow and i fail to see the motivation for some of the analysi", "for example the gradient scale coefficient appears to be motivated because bottom page  with other norm measurements we could take any architecture and rescale the parameters and inversely scale the gradients to make it easy to train", "but typically easy to train does not involve a specific preprocessing of gradi", "other propositions eg theorem  proposition  could do with clearer intuition leading to them", "i think the assumptions made in the results should also be clear"], "labels": ["CRT", "CRT", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's fine to have results but currently i can't tell under what conditions the results apply and under what conditions they don't eg are there any extensions of this that apply to non-linear network", "it's fine to have results but currently i can't tell under what conditions the results apply and under what conditions they don't eg are there any extensions of this that apply to non-linear network", "i also have issues with their experimental setup why choose to experiment on networks of depth  and width", "this doesn't really look anything like networks that are trained in practic", "calling these popular architectures is mislead"], "labels": ["QSN", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary i think this paper needs more work on the presentation to make clear what they are proving and under what conditions and with experiments that are closer to those used in practice to support their claim", "summarythis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a gmm end-to-end with reconstruction in a low-dimensional latent spac", "the algorithm employs a tailored loss function that involves reconstruction error on the latent space penalties on degenerate parameters of the gmm and an energy term to model the probability of observing the input sampl", "the algorithm replaces the membership probabilities found in the e-step of em for a gmm with the outputs of a subnetwork in the end-to-end architectur", "the gmm parameters are updated with these estimated responsibilities as usual in the m-step during train"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper demonstrates improvements in a number of public dataset", "careful reporting of the tuning and hyperparameter choices renders these experiments repeatable and hence a suitable improvement in the field", "well-designed ablation studies demonstrate the importance of the architectural choices made which are generally well-motivated in intuitions about the nature of anomaly detect", "criticismsbased on the performance of gmm-en the reconstruction error features are crucial to the success of this method", "little to no detail about these features is includ"], "labels": ["APC", "APC", "APC", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["intuitively the estimation network is given the latent code conditioned and some probably highly redundant information about the residual structure remaining to be model", "since this is so important to the results more analysis would be help", "why did the choices that were made in the paper yield this success", "how do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best result", "qualitythis paper does not set out to produce a novel network architectur"], "labels": ["CRT", "SUG", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the e-step output in em for a gmm", "this is interesting and novel enough in my opinion to warrant publication at iclr along with the strong performance and careful reporting of experimental design", "the paper presents a model titled the unsupervised tree-lstm in which the authors mash up a dynamic-programming chart and a recurrent neural network", "as far as i can glean the topology of the neural network is constructed using the chart of a cky pars", "when combining different constituents an energy function is computed equation  and the resulting energies are passed through a softmax"], "labels": ["DIS", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the architecture achieves impressive results on two tasks snli and the reverse dictionary of hill et ", "overall i found the paper deeply uninspir", "the authors downplay the similarity of their paper to that of le and zuidema  which  i did not appreci", "it's true that le and zuidema take a parse forest from an existing parser but it still contains an exponential number of trees as does the work in her", "note that exposition in le and zuidema  discusses the pruned case as well ie a compete parse forest"], "labels": ["APC", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors of this paper simply write le and zuidema  propose a model that takes as input a parse forest from an external parser in order to deal with uncertainti", "i would encourage the authors to revisit le and zuidema  especially section  and consider the technical innovations over the existing work", "i believe the primary difference other using an lstm instead of a convnet is to replace max-pooling with softmax-pooling do these two architectural changes matt", "i believe the primary difference other using an lstm instead of a convnet is to replace max-pooling with softmax-pooling do these two architectural changes matt", "the experiments offer no empirical comparison"], "labels": ["DIS", "SUG", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in short the insight of having an end-to-end differentiable function based on a dynamic-programming chart is pretty common -- the idea is in the air", "the authors provide yet another instantiation of such an approach but this time with an lstm", "the technical exposition is also relatively poor", "the authors could have expressed their network using a clean recursion following the parse chart but opted not to and instead  provided a round-about explanation in english", "thus despite the strong result"], "labels": ["CRT", "DIS", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would not like to see this work in the proceedings due to the lack of originality and poor technical discuss", "if the paper were substantially cleaned-up i would be willing to increase my r", "experimental results have shown that deep networks many hidden layers can approximate more complicated functions with less neurons compared to shallow single hidden layer network", "this paper gives an explicit proof when the function in question is a sparse polynomial ie a polynomial in n variables which equals a sum j of monomials of degree at most c", "in this setup theorem  says that a shallow network need at least ~  + c/n^n many neurons while the optimal deep network whose depth is optimized to approximate this particular input polynomial needs at most  ~ j*n that is linear in the number of terms and the number of vari"], "labels": ["CRT", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper also has bounds for neural networks of a specified depth k theorem  and the authors conjecture this bound to be tight conjectur", "this is an interesting result and is an improvement over lin  where a similar bound is presented for monomial approxim", "overall i like the pap", "pros new and interesting result theoretically sound", "cons nothing major"], "labels": ["SMY", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments and clarifications* what about the ability of a single neural network to approximate a class of functions instead of a single p where the topology is fixed but the network weights are allowed to vari", "could you comment on this problem", "* is the assumption that sigma has taylor expansion to order d tight", "that is are there counter examples for relaxations of this assumpt", "* as noted the assumptions of your theorems - do not apply to relu"], "labels": ["QSN", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but relus network perform well in practic", "could you provide some further comments on thi", "the paper studies learning in deep neural networks with hard activation functions eg step functions like signx", "of course backpropagation is difficult to adapt to such networks so prior work has considered different approach", "arguably the most popular is straight-through estimation hinton  bengio et al  in which the activation functions are simply treated as identity functions during backpropag"], "labels": ["DIS", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more recently a new type of straight-through estimation saturated ste hubara et al  uses [|z|] as the derivative of signz", "the paper generalizes saturated ste by recognizing that other discrete targets of each activation layer can be chosen", "deciding on these targets is formulated as a combinatorial optimization problem", "once the targets are chosen updating the weights of each layer to minimize the loss on those targets is a convex optim", "the targets are heuristically updated through the layers starting out the output using the proposed feasibility target propag"], "labels": ["SMY", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at each layer the targets can be chosen using a variety of search algorithms such as beam search", "experiments show that ftp often outperforms saturated ste on cifar and imagenet with sign and quantized activation functions reaching levels of performance closer to the full-precision activation network", "this paper's ideas are very interesting exploring an alternative training method to backpropagation that supports hard-threshold activation funct", "the experimental results are encourag", "though i have a few questions below that prevent me for now from rating the paper high"], "labels": ["SMY", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments and questions how computationally expensive is ftp", "the experiments using resnet indicate it is not prohibitively expensive but i am eager for more detail", "does hubara et al  actually compare their proposed saturated ste with the orignal ste on any task", "i do not see a comparison", "if that is so should this paper also compare with st"], "labels": ["QSN", "APC", "QSN", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how do we know if generalizing saturated ste is more worthwhile than generalizing st", "it took me a while to understand the authors' subtle comparison with target propagation where they say our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets whereas previous approaches employed continuous optim", "it seems that the difference is greater than explicitly stated that prior target propagation used continuous optimization to set *continuous targets* one could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem", "focusing on discrete targets gains the benefits of quantized network", "if i am understanding the novelty correctly it would strengthen the paper to make this difference clear"], "labels": ["QSN", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on a related note if feasible target propagation generalizes saturated straight through estimation is there a connection between continuous target propagation and the original type of straight through estim", "in table  the significance of the last two columns is unclear", "it seems that relu and saturated relu are included to show the performance of networks with full-precision activation functions which is good", "i am unclear though on why they are compared against each other bolding one or the other and if there is some correspondence between those two columns and the other pairs ie is relu some kind of analog of sste and saturated relu corresponds to ftp-sh somehow", "the paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter spectral density ratio from [shajarisales et al ] to cascades of two filters in successive layers of deep convolutional network"], "labels": ["DIS", "CRT", "APC", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors apply their newly defined measure to dcgans and plain vaes with relus and show that dependency between successive layers may lead to bad perform", "the paper proposed a possibly interesting approach", "but i found it quite hard to follow especially section  which i thought was quite unstructur", "also section  could be improved and simplifi", "it would be also good to add some more related work"], "labels": ["SMY", "APC", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ium not an expert but i assume there must be some similar idea in cnn", "from my limited point of view this seems like a sound novel and potentially useful application of a interesting idea", "if the writing was improved i think the paper may have even more impact", "smaller details some spacing issues some extra punctuation pg  uc  henceud a typo pg  uctraining of the vae did not lead to values as satisfactory as what we obtained with the ganud", "maml finn+  is recast as a hierarchical bayesian learning procedur"], "labels": ["DIS", "DIS", "SUG", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the inner task training is initially cast as point-wise max likelihood estimation and then sec improved upon by making use of the laplace approxim", "experimental evidence of the relevance of the method is provided on a toy task involving a niw prior of gaussians and the benchmark miniimagenet task", "casting maml as hb seems a good idea", "the paper does a good job of explaining the connect", "but i think the presentation could be clarifi"], "labels": ["SMY", "SMY", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the role of the task prior and how it emerges from early stopping ie a finite number of gradient descent steps sec  is original and technically non-trivial and is a contribution of this pap", "the synthetic data experiment sec and fig is clearly explained and serves to additionally clarify the proposed method", "regarding the miniimagenet experiments i read the exchange on tcml and agree with the authors of the paper under review", "however i recommend including the references to mukhdalai  and sung  in the footnote on tcml to strengthen the point more generically and show that not just tcml but other non-shallow architectures are not considered for comparison her", "in addition the point made by the tcml authors is fair nothing prevented you from and i would also recommend mentioning the reviewed paper's authors' decision not to test deeper architectures in the footnot"], "labels": ["APC", "APC", "DIS", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her dispos", "the experimental performance reported table  remains small and largely within one standard deviation of competitor method", "i am assessing this paper as  because despite the merit of the paper the relevance of the reformulation of maml and the technical steps involved in the reformulation the paper does not eg address other forms than l-maml of the task-specific subroutine ml- and the benchmark improvements are quite smal", "i think the approach is good and fruit", "# suggestions on readability* i have the feeling the paper inverts $alpha beta$ from their use in finn  step size for meta- vs task-train"], "labels": ["SUG", "DIS", "FBK", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is unfortunate and will certainly confuse readers i advise carefully changing this throughout the entire paper eg algo  eq  last eq in sec eq in text below eq etc*", "i advise avoiding the use of the symbol f which appears in only two places in algo  and the end of sec", "this is in part because f is given another meaning in finn  but also out of general parsimony in symbol us", "could leave the output of ml- implicit by writing ml-theta t_j in the $sum_j$ if absolutely needed use another symbol than f*", "maybe sec can be clarified in its structure by re-ordering points on the quadratic error function and early stopping eg avoiding to split them between end of  and"], "labels": ["CRT", "SUG", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* sec machine learning and deep learning i would definitely avoid this formulation seems to tail in with all the media nonsense on what's the difference between ml and dl", "in addition the formulation seems to contrast ml with hierarchical bayesian modeling which does not make sense/ is wrong and confus", "# typos* sec second parag did you really mean in the architecture or loss function unclear", "* sec over a family* common structure so that not such that* orthgonal* sec suggestion clarify that theta and phi are in the same space* sec suggestion task-specific parameter $phi_j$ is distinct from  parameters $phi_{j'} j' eq j}", "* unless an approximate  is provided the use of the subjunctive here is definitely dated -"], "labels": ["QSN", "CRT", "QSN", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* sec task-specific parameters $phi_j$ i would avoid writing just phi altogether to distinguish in usage from theta", "* gaussian-noised* approximation of the it objective* before eq that solves well it doesn't really solve the minimisation in that it is not a minimum reformulate thi", "* sec innaccurate* well approximated* sec an curvature* amari * for the the laplace* on^  what is n", "* sec ravi and l * for the th", "the paper proposes combining classification-specific neural networks with auto-encod"], "labels": ["CRT", "QSN", "QSN", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruct", "the training objective is then changed to minimize the sum of the classification loss as measured by cross-entropy for instance and the reconstruction error as measured by ell- error as is done in training auto-encod", "the authors minimize the loss function by greedy layer-wise training as is done in several prior work", "the authors then perform other experiments on the learned representations in the output layer those corresponding to classification + those corresponding to reconstruct", "for example the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very differ"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors also observe that interpolating between two reconstruction-feature vectors by convex combinations seems to interpolate well between the two corresponding imag", "while the experimental results are interest", "they are not striking especially when viewed in the context of the tremendous amount of work on auto-encod", "training the classification-features along with reconstruction-features does not seem to give any significantly new insight", "this is quite an interesting paper thank y"], "labels": ["SMY", "APC", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["here are a few commentsi think this style of writing theoretical papers is pretty good where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix", "however i would have appreciated a little bit more details about the proofs in the main text maybe more details about the construct that is involv", "i can appreciate though that this a fine line to walk", "also in the appendix please restate the lemma that is being proven", "otherwise one will have to scroll up and down all the time to understand the proof"], "labels": ["APC", "SUG", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the paper could also discuss a bit more in detail the results provid", "for example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nic", "though regardless i think the findings speak for themselves and this seems an important step forward in understanding neural net", "****************i had reduced my score based on the observation made by reviewer  regarding the talk montufar at sampta", "could the authors prioritize clarification to that point !"], "labels": ["SUG", "SUG", "APC", "FBK", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- thanks for the clarification and adding this cit", "this paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm", "the affect lexical seems to be a very interesting resource although i'm not sure what it means to call it 'state of the art' and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and senti", "the justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to m", "- the statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least other then them both being participle / adject"], "labels": ["APC", "SMY", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- affect in language seems to me to be a very contextual phenomenon", "only a tiny subset of words have intrinsic and context-free affect", "most affect seems to me to come from the use of words in phrasal and extra-linguistic contexts so a more context-dependent model in which affect is computed over phrases or sentences would seem to be more appropriate consider words like 'expensive' 'wicked' 'elimination'", "the model proposes several applications sentiment prediction predicting email tone word similarity where the affect-based embeddings yield small improv", "however in different cases taking different flavours of affect information v a or d produces the best score so it is not clear what to conclude about what sort of information is most us"], "labels": ["DIS", "DIS", "SUG", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not surprising to me that an algorithm that uses both wordnet and running text to compute word similarity scores improves over one that uses just running text", "it also not surprising that adding information about affect improves the ability to predict sentiment and the tone of email", "to understand the importance of the proposed algorithm rather than just the addition of additional data i would like to see comparison with various different post-processing techniques using wordnet and the affect lexicon ie not just bollelaga et al including some much simpler baselin", "for instance what about averaging wordnet path-based distance metrics and distance in word embedding space for word similarity and other ways of applying the affect data to email tone predict", "summarythis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural network"], "labels": ["DFT", "DFT", "SUG", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposal is to replace the non-linearity in half of the units in each layer with its bipolar version -- one that is obtained by flipping the function on both ax", "the technique is tested on deep stacks of recurrent layers and on convolutional networks with depth of  showing that improved results over the baseline networks are obtain", "claritythe paper is easy to read", "the plots in fig  and the appendix are quite helpful in improving present", "the experimental setups are explained in detail"], "labels": ["SMY", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality and significancethe main idea from this paper is simple and intuit", "however the experiments to support the idea do not seem to match the motivation of the pap", "as stated in the beginning of the paper the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient desc", "however the presented results focus on the performance on held-out data instead of improvements in training spe", "this is especially the case for the rnn experi"], "labels": ["APC", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the cifar- experiment the training loss curves do show faster initial progress in learn", "however it is unclear that overall training time can be reduced with the help of this techniqu", "to evaluate this speed up effect the dependence on the choice of learning rate and other hyperparameters should also be consid", "nevertheless it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cas", "the method appears to improve the training for moderately deep convolutional networks without batch normalization although this is tested on a single dataset"], "labels": ["SMY", "CRT", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but is not practically useful yet since the regularization benefits of batch normalization are also taken away", "overview the authors present results from several state-of-the-art generative models trained on a facial dataset for learning a general facial identity spac", "strengths the paper in general is well written and easy to ready i appreciate the idea of the turing test and qualitative results presented are quite impressive also the use of  diverse state-of-the-art generative models is also a strong point", "weaknesses while the strengths mentioned above are obvious i had the impression through the whole paper that a whole part is miss", "weaknesses while the strengths mentioned above are obvious i had the impression through the whole paper that a whole part is miss"], "labels": ["CRT", "SMY", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my list of concerns are the following     the authors state as their first contribution the presentation of a novel dataset", "this is nic", "but i see the data is actually already available as the photographic work of an artist", "so what's the authors' contribut", "as i understand from the paper it is just compiling this already available data"], "labels": ["DIS", "APC", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the major problem of this paper in my opinion is the total lack of technical detail", "in this sense the results cannot by any means by reproduc", "while the authors use a set of very novel generative model", "there is absolutely no detail on how do they train them", "we are just shown some very impressive qualitative results which are indeed admirable but without further details i cannot judge them as true or not"], "labels": ["CRT", "CRT", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i strongly recommend to the authors to provide technical details of topologies used hyper parameters and any other important detail that would help a third party research to reproduce these result", "also it is really hard to understand how could they obtain such impressive result by doing an unsupervised training on a dataset containing  samples taking into account the high capacity of the models they are us", "in section  the authors mention that facial landmarks have been detected using a 'pre-trained ensemble-of-regresion-trees detector gerbrands '", "i know very well the facial alignment literature and i do not understand this refer", "this i do not think is a reference to a facial alignment method bu t rather a set of general purpose linear algebra method"], "labels": ["SUG", "CRT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["taking into account this major weaknesses i cannot accept this paper and i do not think it is worth discussing results and applications in sections  before authors detail explain and clarify how exactly they have obtained these result", "taking into account this major weaknesses i cannot accept this paper and i do not think it is worth discussing results and applications in sections  before authors detail explain and clarify how exactly they have obtained these result", "this paper presents a method to cope with adversarial examples in classification tasks leveraging a generative model of the input", "given an accurate generative model of the input this approach first projects the input onto the manifold learned by the generative model the idea being that inputs on this manifold reflect the non-adversarial input distribut", "this projected input is then used to produce the classification prob"], "labels": ["CRT", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors test their method on various adversarially constructed inputs with varying degrees of nois", "questions/comments- i am interested in unpacking the improvement of defense-gan over the magnet auto-encoder based method", "is the magnet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data", "if the decoder from the magnet approach were treated purely as a generative model and the same optimization-based projection approach proposed in this work was followed would the results be compar", "- is there anything special about the gan approach versus other generative approach"], "labels": ["SMY", "DIS", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in the black-box vs white-box scenarios can the attacker know the gan paramet", "is that what is meant by the defense network in experiments bullet", "- how computationally expensive is this approach take compared to magnet or other adversarial approach", "quality the method appears to be technically correct", "clarity this paper clearly written"], "labels": ["QSN", "QSN", "QSN", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["both method and experiments are presented wel", "originality i am not familiar enough with adversarial learning to assess the novelty of this approach", "significance i believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold", "i think this kernel has the potential to be explored further eg computational speed-up projection metr", "this paper applies the word pairs instead to bag of words to current rbm model"], "labels": ["APC", "FBK", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the word pairs are extracted using stanford pars", "the word pairs are further filtered and clustered to improve the represent", "the experiments show improvement over baselin", "however i think this paper has limited contribution and novelti", "and the experiments also need to be improv"], "labels": ["SMY", "SMY", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the detailed comments are as follows- the main contribution of this paper is to apply word pairs instead of words to rbm model", "however the main techniques such as rbm parser to extract word pairs tf-idf for filtering and k-means for clustering are all existing standard techniqu", "it is more like an application of these methods and has limited contribution and novelti", "- for experiments they apply k-means clustering in the process so k is one parameter to tune k needs to be tuned on validation set instead of testing set", "this paper simply presents the results of different parameter k on testing set directli"], "labels": ["SMY", "DIS", "DFT", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the structure of section  needs to be improv", "- the structure of section  needs to be improv", "instead of listing each step in each subsection a general introduction picture should be introduced first more intuition is also needed for each step", "instead of listing each step in each subsection a general introduction picture should be introduced first more intuition is also needed for each step", "- some figures and tables are overlapping in the experiments just keep one is enough"], "labels": ["SUG", "DFT", "SUG", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the format of reference should be fixed in this pap", "this paper proposes a lightweight neural network architecture for reading comprehension which  only consists of feed-forward nets  aggregates information from different occurrences of candidate answers and demonstrates good performance on triviaqa where documents are generally pretty long", "overall i think it is a nice demonstration that non-recurrent models can work so wel", "but i also donut find the results strikingly surpris", "it is also a bit hard to get the main takeaway messag"], "labels": ["DFT", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems that multi-loss is important highlight that! summing up multiple mentions of the same candidate answers seems to be important this paper should be cited text understanding with the attention sum reader network https//arxivorg/abs/", "it seems that multi-loss is important highlight that! summing up multiple mentions of the same candidate answers seems to be important this paper should be cited text understanding with the attention sum reader network https//arxivorg/abs/", "but all the other components seem to have been demonstrated previously in other pap", "an important feature of this model is it is easier to parallelize and speed up the training/testing process", "however i donut see any demonstration of this in the experiments sect"], "labels": ["SUG", "DIS", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also i am a bit disappointed by how uccascadesud are actually impl", "i was expecting some sophisticated ways of combining information in a cascaded way finding the most relevant piece of information and then based on what it is obtained so far trying to find the next piece of relevant information and so on", "the proposed model just simply sums up all the occurrences of candidate answers throughout the full docu", "-layer cascade is really just more like stacking several layers where each layer captures information of different granular", "i am wondering if the authors can also add results on other rc datasets eg squad and see if the model can generalize or not"], "labels": ["CRT", "SUG", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper describes an implementation of reduced precision deep learning using a  bit integer represent", "this field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activ", "these schemes have generally achieved close-to-sota accuracy for small networks on datasets such as mnist and cifar-", "however for larger networks resnet vgg etc on large dataset such as imagenet a significant accuracy drop are report", "in this work the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve sota on  large networks on the imagenet-k dataset"], "labels": ["SMY", "SMY", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["using a int as opposed to fp has the advantage of enabling the use of new simd mul-acc instructions such as qvnni", "the reported accuracy numbers show convincingly that int weights and activations can be used without loss of accuracy in large cnn", "however i was hoping to see a direct comparison between fp and int", "the paper is written clearly and the english is fin", "this paper introduces related memory network rmn an improvement over relationship networks rn"], "labels": ["SMY", "SMY", "DFT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rmn avoids growing the relationship time complexity as suffered by rn santoro et ", "rmn reduces the complexity to linear time for the babi dataset", "rn constructs pair-wise interactions between objects in rn to solve complex tasks such as transitive reason", "rmn instead uses a multi-hop attention over objects followed by an mlp to learn relationships in linear tim", "comments for the authorthe paper addresses an important problem since understanding object interactions are crucial for reason"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however how widespread is this problem across other models or are you simply addressing a point problem for rn", "for example entnet is able to reason as the input is fed in and the decoding costs are low", "likewise other graph-based networks which although may require strong supervision are able to decode quite cheapli", "the relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism and an mlp", "it would not be fair to claim superiority over rn since you only evaluate on babi while rn also demonstrated results on other task"], "labels": ["QSN", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for more complex tasks even over just text it is necessary to show that you outperform rn w/o considering all objects in a pairwise fashion", "more specifically rn uses an mlp over pair-wise interactions does that allow it to model more complex interactions than just selecting two hops to generate attention weight", "showing results with multiple hops  would be useful her", "more details are needed about figur", "is this on babi as wel"], "labels": ["SUG", "QSN", "SUG", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how did you generate these stories with so many sent", "another clarification is the babi performance over entnet which claims to solve all task", "your results show  failed tasks is this your reproduction of entnet", "finally what are the savings from reducing this time complex", "some wall clock time results or flops of train/test time should be provided since you use multiple hop"], "labels": ["QSN", "SMY", "QSN", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this paper feels like a small improvement over rn", "without experiments over other datasets and wall clock time results it is hard to appreciate the significance of this improv", "one direction to strengthen this paper is to examine if rmn can do better than pair-wise interactions and other baselines for more complex reasoning task", "this is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description sketch of the task to be achiev", "this is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of android app"], "labels": ["SMY", "DFT", "SUG", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previ", "this paper has many strengths the writing is clear and the paper is well-motiv", "the proposed algorithm is described in excellent detail which is essential to reproduc", "as stated previously the approach is validated with a large number of real android project", "the fact that the language generated is non-trivial java-like is a substantial plus good discussion of limit"], "labels": ["APC", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this paper is a valuable addition to the empirical software engineering community and a nice break from more traditional approaches of learning abstract syntax tre", "paper proposes a shallow model for approximating stacks of resnet layers based on mathematical approximations to the resnet equations and experimental insights and uses this technique to train resnet-like models in half the time on cifar- and cifar-", "while the experiments are not particularly impress", "i liked the originality of this pap", "the paper presents an analysis and characterization of relu networks with a linear final layer via the set of functions these networks can model especially focusing on the set of uchardud functions that are not easily representable by shallower network"], "labels": ["APC", "SMY", "CRT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it makes several important contributions including extending the previously published bounds by telgarsky et al to tighter bounds for the special case of relu dnns giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs and giving a procedure for searching for globally optimal solution of a -hidden layer relu dnn with linear output layer and convex loss", "i think these contributions warrant publishing the paper at iclr", "the paper is also well written a bit dense in places but overall well organized and easy to follow", "a key limitation of the paper in my opinion is that typically dnns do not contain a linear final lay", "it will be valuable to note what if any of the representation analysis and global convergence results carry over to networks with non-linear softmax eg final lay"], "labels": ["APC", "FBK", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like d^nw would like hearing authorsu comments in case ium missing some simplif", "one minor suggestion for improving readability is to explicitly state whenever applicable that functions under consideration are pwl", "for example adding pwl to theorems and corollaries in section  will help", "similarly would be good to state wherever applicable the dnn being discussed is a relu dnn", "this is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural network"], "labels": ["CRT", "SUG", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the contributions here are  experiments testing the dtp algorithm on more difficult dataset", "proposing a minor modification of the dtp algorithm at the output lay", "and  testing the dtp algorithm on locally-connected architectur", "these are all novel contributions but each one seems incremental in the context of previous work on this and similar algorithms eg nokland direct feedback alignment provides learning in deep neural networks  baldi et al learning in the machine the symmetries of the deep learning channel", "summary this paper studied the conditional image generation with two-stream generative adversarial network"], "labels": ["SMY", "SMY", "SMY", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more specifically this paper proposed an unsupervised learning approach to generate  foreground region conditioned on class label and  background region without semantic meaning in the label", "more specifically this paper proposed an unsupervised learning approach to generate  foreground region conditioned on class label and  background region without semantic meaning in the label", "during training two generators are competing against each other to hallucinate foreground region and background region with a physical gating oper", "an auxiliary uclabel difference costud was further introduced to encourage class information captured by the foreground gener", "experiments on mnist svhn and celeba datasets demonstrated promising generation results with the unsupervised two-stream generation pipelin"], "labels": ["SMY", "DIS", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["== novelty/significance ==controllable image generation is an important task in representation learning and computer vis", "== novelty/significance ==controllable image generation is an important task in representation learning and computer vis", "i also like the unsupervised learning through gating function and label difference cost", "however considering many other related work mentioned by the paper the novelty in this paper is quite limited for example layered generation section  has been explored in yan et al  vaes and vondrick et al  gan", "== detailed comments ==the proposed two-stream model is developed with the following two assumptions  single object in the scene and  class information is provided for the foreground/object region"], "labels": ["SMY", "DIS", "APC", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the proposed method learns to distinguish foreground and background in an unsupervised fashion it is limited in terms of applicability and generaliz", "for example i am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as ms-coco lsun and imagenet", "given the proposed method is controllable image generation i would assume to see the following ablation studies keeping two latent variables from z_u z_l z_v fixed while gradually changing the value of the other latent vari", "however i didnut see such detailed analysis as in the other papers on controllable image gener", "in figure  and figure  the boundary between foreground and background region is not very sharp"], "labels": ["SMY", "FBK", "SMY", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it looks like equation  and   are insufficient for foreground and background separation triplet/margin loss could work bett", "also in celeba experiment it is not a well defined experimental setting since only binary label smiling/non-smiling is condit", "is it possible to use all the binary attributes in the dataset", "also please either provide more qualitative examples or provide some type of quantitative evaluations through user study  dataset statistics or down-stream recognition task", "also please either provide more qualitative examples or provide some type of quantitative evaluations through user study  dataset statistics or down-stream recognition task"], "labels": ["DFT", "DFT", "QSN", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i believe the paper is interesting but not ready for publ", "overall i believe the paper is interesting but not ready for publ", "i encourage authors to investigate  more generic layered generation process and  better unsupervised boundary separ", "hopefully the suggested studies will improve the quality of the paper in the future submiss", "== presentation ==the paper is readable but not well polish"], "labels": ["DFT", "FBK", "SUG", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["== presentation ==the paper is readable but not well polish", "-- in figure  the ucgud on the right should be ucgud-- section  ucx_fud should be ucx_fud-- the motivation of having ucz_vud should be introduced earlier-- section  please use either ucalphaud or ucalphaud but not both-- section  the dataset information is incorrect uc imagesud should be uc imagesudmissing reference-", "- neural face editing with intrinsic image disentangling shu et al in cvpr", "-- domain separation networks bousmalis et al in nip", "n-- unsupervised image-to-image translation networks liu et al in nip"], "labels": ["CRT", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper includes several controlled empirical studies comparing mc and td methods in predicting of value function with complex dnn function approxim", "such comparison has been carried out both in theory and practice for simple low dimensional environments with linear and rkhs value function approximation showing how td methods can have much better sample complexity and overall performance compared to pure mc method", "this paper shows some results to the contrary when applying rl to complex perceptual observation spac", "the main results include in a rollout update a mix of mc and td update ie a rollout of   and  horizon outperforms either extrem", "this is inline with td-lambda analysis in previous work"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pure mc methods can outperform td methods when the rewards becomes noisi", "td methods can outperform pure mc methods when the return is mostly dominated by the reward in the terminal st", "mc methods tend to degrade less when the reward signal is delay", "somewhat surprising mc methods seems to be on-par with td methods when the reward is sparse and even longer than the rollout horizon", "mc methods can outperform td methods with more complex and high dimensional perceptual input"], "labels": ["APC", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors conjecture that several of the above observations can be explained by the fact that the training target in mc methods is ground truth and do not rely on bootstrapping from the current estimates as is done in a td rollout", "they suggest that training on such signal can be beneficial when training deep models on complex perceptual input spac", "the contributions of the paper are in parts surprising and overall interest", "i believe there are far more caveats in this analysis than what is suggested in the paper and the authors should avoid over-generalizing the results based on a few domains and the analysis of a small set of algorithm", "nonetheless i find the results interesting to the rl community and a starting point to further analysis of the mc methods or adaptations of td methods that work better with image observation spac"], "labels": ["DIS", "DIS", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["publishing the code as the authors mentioned would certainly help with that", "notes- i find the description of the q_mc method presented in the paper very confusing and had to consult the reference to understand the detail", "adding a couple of equations on this would improve the readability of the pap", "- the first mention of partial observability can be moved to the introduct", "- adding results for m= to table  would bring further insight to the comparison"], "labels": ["SUG", "CRT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the results for the perceptual complexity experiment seem contradictory and inconclus", "one would expect q_mc to work well in grid map domain if the conjecture put forth by the authors was to hold univers", "- in the study on reward sparsity although a prediction horizon of  is less than the average steps needed to get to a rewarding state a blind random walk might be enough to take the rl agent to a close-enough neighbourhood from which a greedy mc-based policy has a direct path to the go", "what is missing from this picture is when a blind walk cannot reach such a state eg when a narrow corridor is present in the environ", "such a case cannot be resolved by a short horizon mc method"], "labels": ["SUG", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in other words a sparse reward setting is only difficult if getting into a good neighbourhood requires long term planning and cannot be resolved by a pseudo blind random walk", "- the extrapolation of the value function approximator can also contribute to why the limited horizon mc method can see beyond its horizon in a sparse reward set", "that is even if there is no way to reach a reward state in  steps an mc value function approximation with horizon  can extrapolate from similar looking observed states that have a short path to a rewarding state enough to be better than a blind random walk", "it would have been nice to experiment with increasing model complexity to study such effect", "this paper proposes using long term memory to solve combinatorial optimization problems with binary vari"], "labels": ["CRT", "DIS", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors do not exhibit much knowledge of combinatorial optimization literature as has been pointed out by other readers and ignore a lot of previous work by the combinatorial optimization commun", "in particular evaluating on random instances is not a good measure of performance  as has already been pointed out", "the other issue is with the baseline solver which also seems to be broken since their solution quality seems extremely poor", "in light of these issues i recommend reject", "qualitythis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data"], "labels": ["CRT", "CRT", "CRT", "FBK", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this points at important limitations of current neural network architectures where architectures depend mainly on rote memor", "claritythe rationale in the paper is straightforward", "i do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered befor", "originalitywhile others have pointed out limitations before this paper considers relational networks for the first tim", "significance this work demonstrates failures of relational networks on relational tasks which is an important messag"], "labels": ["DIS", "APC", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at the same time no new architectures are presented to address these limit", "prosimportant message about network limit", "consstraightforward testing of network performance on specific visual relation task", "no new theory develop", "no new theory develop"], "labels": ["DFT", "DIS", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["conclusions drawn by testing on out of sample data may not be completely valid", "the paper proposes and evaluates a method to make neural networks for image recognition color invari", "the contribution of the paper is  - some proposed methods to extract a color-invariant represent", "- an experimental evaluation of the methods on the cifar  dataset", "- a new dataset crashed car"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- evaluation of the best method from the cifar experiments on the new dataset", "pros  - the crashed cars dataset is interest", "the authors have definitely found an interesting untapped source of interesting imag", "cons - the authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network", "- the paper is incomplete without the appendic"], "labels": ["SMY", "APC", "APC", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact the paper is referring to specific figures in the appendix in the main text", "- the authors define color invariance as a being invariant to which specific color an object in an image does have eg whether a car is red or green but they don't think about color invariance in the broader context - color changes because of lighting shad", "also the proposed methods aim to preserve the colorfullness of a color", "this is also problematic because while the proposed method works for a car that is green or a car that is red it will fail for a car that is black or white - because in both cases the colorfulness is not relev", "note that this is specifically interesting in the context of the task at hand cars and many cars being white grey silver or black"], "labels": ["DFT", "DIS", "SMY", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the difference in the results in table  could well come from the fact that in all of the invariant methods except for ord the input is a wxhx matrix but for ord and cifar the input is a wxhx matrix", "this probably leads to more parameters in the convolut", "- the results in the  figure  it's very unlikely that the differences reported are actually signific", "it appears that all methods perform approximately the same - and the authors pick a specific line k steps as the relevant one in which the rgb-input space performs best", "the proposed method does not lead to any relevant improv"], "labels": ["DIS", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure / are very hard to read i am still not sure what exactly they are trying to say", "minor comments  - section  called for is network - called for is a network", "- section  and and - and", "- section  appendix - appendix c - section  their exists many - there exist many - section  these transformation - these transformations - section  what does the wallpaper groups refer to  - section  are a groups - are groups - section  reference to a non-existing figure - section /training  iterations = steps  - section /training longer as suggested - longer than suggest", "this paper provides a reasonably comprehensive generalization to vaes and adversarial auto-encoders through the lens of the wasserstein metr"], "labels": ["CRT", "SUG", "SUG", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by posing the auto-encoder design as a dual formulation of optimal transport the proposed work supports the use of both deterministic and random decoders under a common framework", "in my opinion this is one of the crucial contributions of this pap", "while the existing properties of auto-encoders are preserved stability characteristics of w-gans are also observed in the proposed architectur", "the results from mnist and celeba datasets look convincing though could include additional evaluation to compare the adversarial loss with the straightforward mmd metric and potentially discuss their pros and con", "the results from mnist and celeba datasets look convincing though could include additional evaluation to compare the adversarial loss with the straightforward mmd metric and potentially discuss their pros and con"], "labels": ["SMY", "APC", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results from mnist and celeba datasets look convincing though could include additional evaluation to compare the adversarial loss with the straightforward mmd metric and potentially discuss their pros and con", "in some sense given the challenges in evaluating and comparing closely related auto-encoder solutions the authors could design demonstrative experiments for cases where wassersterin distance helps and may be  its potential limit", "the closest work to this paper is the adversarial variational bayes framework by mescheder et", "which also attempts at unifying vaes and gan", "while the authors describe the conceptual differences and advantages over that approach it will be beneficial to actually include some comparisons in the results sect"], "labels": ["DIS", "SMY", "SMY", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of the paper is to use a gan-like training to learn a novelty detection approach", "in contrast to traditional gans this approach does not aim at convergence where the generator has nicely learned to fool the discriminator with examples from the same data distribut", "the goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outli", "to establish such a behavior the authors propose early stopping as well as other heurist", "i like the idea of the pap"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however this paper needs a revision in various aspects which i simply list in the follow", "* the authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines svdd/oneclasssvm without pca gaussian mixture model knfst kernel density estimation etc* the model selection using the auc of inlier accepted fraction is not well motivated in my opinion", "this model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data", "the latter is important for the gan-like train", "* the experiments are not sufficient especially for multi-class classification tasks it is easy to sample various experimental setups for outlier detection this allows for robust performance comparison"], "labels": ["SUG", "CRT", "SMY", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* with the imbalanced training as described in the paper it is quite natural that the confidence threshold for the classification decision needs to be adapted not equal to", "* there are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed such as the discriminator training from multiple gener", "* a cross-entropy loss for the autoencoder does not make much sense in my opinion", "minor comments* citations should be fixed use citep to enclose them in", "* the term ai-related task sounds a bit too broad"], "labels": ["DIS", "CRT", "QSN", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the authors could skip the paragraph in the beginning of page  on the auc performance auc is a standard choice for evaluation in outlier detect", "* where is t", "* there are quite a lot of typo", "*after revision statement*i thank the authors for their revision but i keep my r", "the clarity of the paper has improv"], "labels": ["SUG", "QSN", "CRT", "FBK", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the experimental evaluation is lacking realistic datasets and further simple baselines as also stated by the other review", "this paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence sampl", "the paper claims superior results using the described method", "overall there are a few problems with the pap", "the arguments for using clusters instead of single sentences are question"], "labels": ["CRT", "SMY", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper claims several times that mle training for nmt faces over-training or data sparsity -- while that can be true depending on the corpus and model used there are well-known remedies for that for example regularization via dropout almost everybody uses that", "it is not clear why that is not used or at least compared to the method pres", "the writing of the paper is often unclear and sometimes grammatically wrong typos etc but that aside there are some made up words/concepts what is 'golden centroid augmentation or model centroid augment", "the reason for attention is not to better memorize input information it is to be able to attend to certain regions in the input", "the reason to use rl is to focus on optimizing directly for bleu score or other metrics instead of likelihood but not for improving on the train/test loss discrep"], "labels": ["DIS", "DFT", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are lots more examples of unclear statements in this paper -- it should be heavily improv", "section  and  are very hard/impossible to understand it is not clear how the formulas help the reader to better understand the concept in any way", "the results presented in this paper given the complexity of the method are just not great -- for example wmt en-de is  bleu reported by you while much older papers report for example  bleu google's neural machine translation system -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that", "finally what is missing most is simply why a much simpler method just generate some data using a trained system and use that as additional training data with details on how much etc -- is not directly compared to this very complicated looking method", "summary this paper tackles the issue of combining td learning methods with function approxim"], "labels": ["DFT", "CRT", "CRT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed algorithm constrains the gradient update to deal with the fact that canonical td with function approximation ignores the impact of changing the weights on the target of the td learning rul", "results with linear and non-linear function approximation highlight the attributes of the method", "quality the quality of the writing notation motivation and results analysis is low", "i will give a few examples to highlight the point", "the paper motivates that td is divergent with function approximation and then goes on to discuss mspbe methods that have strong convergence results without addressing why a new approach is need"], "labels": ["SMY", "SMY", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are many missing references etd htd mirror-prox methods retrace abq q-sigma", "this is a very active area of research and the paper needs to justify their approach", "the paper has straightforward technical errors and naive statements eg the equation for the loss of td takes the norm of a scalar", "the paper claims that it is not well-known that td with function approximation ignores part of the gradient of the msve there are many oth", "the experiments have serious issu"], "labels": ["CRT", "SUG", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["exp seems to indicate that the new method does not converge to the correct solut", "the grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discuss", "again exp provides little information about the experimental setup", "clarity the clarity of the text is fine though errors make things difficult sometim", "for example the bhatnagar  reference should be maei"], "labels": ["CRT", "CRT", "CRT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality as mentioned above this is a very active research area and the paper makes little effort to explain why the multitude of existing algorithms are not suit", "significance because of all the things outlined above the significance is below the bar for this round", "the paper extends the prototypical networks of snell et al nips  for one shot learn", "snell et al use a soft knn classification rule typically used in standard metric learning work eg nca mcml over learned instance projections ie distances are computed over the learned project", "each class is represented by a class prototype which is given by the average of the projections of the class inst"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["classification is done with soft k-nn on the class prototyp", "the distance that is used is the euclidean distance over the learned representations ie z-c^tz-c where z is the projection of the x instance to be classified and c is a class prototype computed as the average of the projections of the support instances of a given class", "the present paper extends the above work to include the learning of a mahalanobis matrix s for each instance in addition to learning its project", "thus now the classification is based on the mahalanobis distance z-c^t s_c z-c", "on a conceptual level since s_c should be a psd matrix it can be written as the square of some matrix ie s_c = a_c^ta_c then the mahanalobis distance becomes a_c z - a_c c^t  a_c z-a_c c ie in addition to learning a projection as it is done in snell et al the authors now learn also a linear transformation matrix which is a function of the support points ie the ones which give rise to the class prototyp"], "labels": ["SMY", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the interesting part here is that the linear projection is a function of the support point", "i wonder though if such a transformation could not be learned by the vanilla prototypical networks simply by learning now a projection matrix a_z as a function of the query point z", "i am not sure i see any reason why the vanilla prototypical networks cannot learn to project x directly to a_z z and why one would need to do this indirectly through the use of the mahalanobis distance as proposed in this pap", "on a more technical level the properties of the learned mahalanobis matrix ie the fact that it should be psd are not really discussed neither how this can be enforced especially in the case where s is a full matrix even though the authors state that this method was not further explor", "on a more technical level the properties of the learned mahalanobis matrix ie the fact that it should be psd are not really discussed neither how this can be enforced especially in the case where s is a full matrix even though the authors state that this method was not further explor"], "labels": ["DIS", "DIS", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if s is diagonal then the s generation methods a b c in the end of section  will make sure that s is psd i do not think that this is the case with d though", "in the definition of the prototypes the component wise weigthing eq  works when the mahalanobis matrix is diagonal even though the weighting should be done by the sqrt of it how would it work if it was a full matrix is not clear", "on the experiments side the authors could have also experimented with miniimagenet and not only omniglot as is the standard practice in one shot learning pap", "i am not sure i understand figure  in which the authors try to see what happens if instead of learning the mahalanobis matrix one would learn a projection that would have as many additional dimensions as free elements in the mahalanobis matrix", "i would expect to see a comparison of the vanilla prototypical nets against their method for each one of the different scenarios of the free parameters of the s matrix something like a ratio of accuracies of the two methods in order to establish whether learning the mahalanobis matrix brings an improvement over the prototypical nets with an equal number of output paramet"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an intriguing paper on running regressions on probability distributions ie a target distribution is expressed as a function of input distribut", "a well-written manuscript", "though the introduction could have motivated the problem a little better ie why would we want to do thi", "the novelty in the paper is implementing such a regression in a layered network", "the paper shows how the densities at each nodes are computed and normalis"], "labels": ["SMY", "APC", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow", "the paper uses three problems to illustrate the idea -- a synthetic dataset a mean reverting stochastic process and a prediction problem on stock indic", "my only two reservations of this paper is the illustration on the stock index data -- it seems to me returns on individual constituent stocks of an index are used as samples of the return on the index itself", "but this cannot be true when the index is a weighted sum of the constituent asset", "secondly it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into  bins for numerical reason"], "labels": ["APC", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- does the smoothing that results from this give any advantage over a histogram of the returns in  bin", "the paper presents a multi-modal cnn model for sentiment analysis that combines images and text", "the model is trained on a new dataset collected from tumblr", "positive aspects+ emphasis in model interpretability and its connection to psychological findings in emot", "+ the idea of using tumblr data seems interesting allowing to work with a large set of emotion categories instead of considering just the binary task positive vs neg"], "labels": ["QSN", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weaknesses- a deeper analysis of previous work on the combination of image and text for sentiment analysis both datasets and methods and its relation with the presented work is necessari", "- the proposed method is not compared with other methods that combine text and image for sentiment analysi", "-  the study is limited to just one dataset", "the paper presents interesting ideas and findings in an important challenging area", "the main novelties of the paper are  the use of tumblr data"], "labels": ["DFT", "DFT", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed cnn architecture combining images and text using word embed", "i missed a related work section where authors clearly mention previous works on similar dataset", "some related works are mentioned in the paper but those are spread in different sect", "it's hard to get a clear overview of the previous research datasets methods and contextualization of the proposed approach in relation with previous work", "i think authors should cite sentibank"], "labels": ["APC", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also at some point authors should compare their proposal with previous work", "more comments- some figures could be more complete to see more examples in fig    would help to understand better the dataset and the challeng", "- in table  for example it would be nice to see the performance on the different emotion categori", "- it would be interesting to see qualitative visual results on recognit", "i like this work"], "labels": ["DFT", "CRT", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i think authors should improve the aspects i mention for its publ", "this paper investigates a new approach to prevent a given classifier from adversarial exampl", "the most important contribution is that the proposed algorithm can be applied post-hoc to already trained network", "hence the proposed algorithm stochastic activation pruning can be combined with algorithms which prevent from adversarial examples during the train", "the proposed algorithm is clearly describ"], "labels": ["FBK", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are issues in the present", "in section - the problem setting is not suitably introduc", "in particular one sentence that can be misleadingucgiven a classifier one common way to generate an adversarial example is to perturb the input in direction of the gradientuudyou should explain that given a classifier with stochastic output the optimal way to generate an adversarial example is to perturb the input proportionally to the gradi", "in particular one sentence that can be misleadingucgiven a classifier one common way to generate an adversarial example is to perturb the input in direction of the gradientuudyou should explain that given a classifier with stochastic output the optimal way to generate an adversarial example is to perturb the input proportionally to the gradi", "the practical way in which the adversarial examples are generated is not known to the play"], "labels": ["DFT", "CRT", "SUG", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the practical way in which the adversarial examples are generated is not known to the play", "an adversary could choose any polici", "the only thing the player knows is the best adversarial polici", "in section  i do not understand why the adversary uses only the sign and not also the value of the estimated gradi", "does it come from a high vari"], "labels": ["CRT", "SUG", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if it is the case you should explain that the optimal policy of the adversary is approximated by ucfast gradient sign methodud", "if it is the case you should explain that the optimal policy of the adversary is approximated by ucfast gradient sign methodud", "in comparison to dropout algorithm sap shows improvements of accuracy against adversarial exampl", "sap does not perform as well as adversarial training but sap could be used with a trained network", "sap does not perform as well as adversarial training but sap could be used with a trained network"], "labels": ["SMY", "CRT", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this paper presents a practical method to prevent a classifier from adversarial examples which can be applied in addition to adversarial train", "the presentation could be improv", "the presentation could be improv", "the paper proposes an approach to train generators within a gan framework in the setting where one has access only to degraded / imperfect measurements of real samples rather than the samples themselv", "broadly the approach is to have a generator produce the full real data pass it through a simulated model of the measurement process and then train the discriminator to distinguish between these simulated measurements of generated samples and true measurements of real sampl"], "labels": ["SMY", "SUG", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by this mechanism the proposed method is able to train gans to generate high-quality samples from only imperfect measur", "the paper is largely well-written and well-motivated the overall setup is interesting i find the authors' practical use cases convincing---where one only has access to imperfect data in the first place and the empirical results are convinc", "the theoretical proofs do make strong assumptions in particular the fact that the true distribution must be uniquely constrained by its marginal along the measur", "however in most theoretical analysis of gans and neural networks in general i view proofs as a means of gaining intuition rather than being strong guarantees---and to that end i found the analysis in this paper to be inform", "i would make a  suggestions for possible further experimental analysis it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions for instance slight differences in the blur kernels noise variance etc"], "labels": ["APC", "APC", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["especially in the kind of settings the paper considers i imagine it may sometimes also be hard to accurately model the measurement function of a device or it may be necessary to use a computationally cheaper approximation for train", "i think a study of how such mismatches affect the training procedure would be instructive perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample qu", "this paper misses the point of what vaes or gans in general are used for", "the idea of using vaes is not to encode and decode images or in general any input but to recover the generating process that created those images so we have an unlimited source of sampl", "the use of these techniques for compressing is still unclear and their quality today is too low"], "labels": ["DIS", "SUG", "DFT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sens", "so the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sens", "but letus assume that at some point they can be used as the authors propos", "in which one person encodes an image send the latent variable to a friend but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without know", "now if the sender believes the sample can be tampered with if the sender codes z with his private key would not make the attack useless"], "labels": ["SUG", "CRT", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think this will make the first attack useless", "the other two attacks require that the foe is inserted in the middle of the training of the va", "this is even less doable because the encoder and decoder are not train remot", "they are train of the same machine or cluster in a controlled manner by the person that would use the system", "once it is train it will give away the decoder and keep the encoder for sending inform"], "labels": ["CRT", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a new design of kernels in convolutional neural network", "the idea is to have sparse but complementary kernels with predefined patterns which altogether cover the same receptive field as dense kernel", "because of the sparsity of such kernels deeper or wider networks can be designed at the same computational cost as networks with dense kernel", "strengths- the complementary kernels come at no loss compare to standard ones- the resulting wider networks can achieve better accuracies than the original onesweaknesses- the proposed patterns are clear for x kernels but no solution is proposed for other dimensions- the improvement over the baseline is not very impressive- there is no comparison against other strategies such as xk and kx kernels eg ioannou et al detailed comments- the separation into + and x patterns is quite clear for x kernel", "however two such patterns would not be sufficient for x or x kernel"], "labels": ["SMY", "SMY", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this idea would have more impact if it generalized to arbitrary kernel dimens", "- the improvement over the original models are of the order of less than  perc", "i understand that such improvements are not easy to achieve but one could wonder if they are not due to the randomness of initialization/mini-batch", "it would be more meaningful to report average accuracies and standard deviations over several runs of each experi", "- section  briefly discusses the comparison with using x and x kernels mentioning that an empirical comparison is beyond the scope of this pap"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to me this comparison is a must in fact the discussion in this section is not very clear to me as it mentions additional experiments that i could not find maybe i misunderstood the author", "what i would like to see is the results of a model based on the method of ioannou et al  with the same number of flop", "- in section  the authors review ideas of so-called random kernel spars", "note that the work of wen et al  and that of alvarez & salzmann nips  do not really impose random sparsity but rather aim to cancel out entire kernels thus reducing the size of the model and not requiring implementation overhead", "they also do not require pre-training and re-training but just a single training procedur"], "labels": ["DFT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["note also that these methods often tend not to decrease accuracy but rather even increase it by a similar magnitude to that in this paper for a more compact model", "- in the context of random sparsity it would be worth citing the work of collins & kohli  memory bounded deep convolutional network", "- i am not entirely convinced by the discussion of the grouped sparsity method in section  in fact the order of the channels is arbitrary since the kernels are learnt", "therefore it seems to me that they could achieve the same result maybe the authors can clarify thi", "- is there a particular reason why the central points appears in both complementary kernels + and x"], "labels": ["SMY", "SMY", "DFT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- why did the authors change the training procedure of resnets slightly compared to the original paper ie k training images instead of k training + k valid", "did the baseline original model reported here also use k", "what would the results be with k", "- fig  is not entirely clear to m", "what was the width of each lay"], "labels": ["SMY", "SMY", "QSN", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the original one or the modified on", "- it would be interesting to report the accuracy of a standard resnet with *width as a comparison as well as the runtime of such a model", "- in table  i find it surprising that there is an actual speedup for the model with larger width", "i would have expected the same runtim", "how do the authors explain thi"], "labels": ["SMY", "APC", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a method to learn a policy on visually different but otherwise identical gam", "while the idea would be interesting in gener", "unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario", "any sort of remotely convincing experiment is left to 'future work'", "the experimental setup is x grid world with different basic shape or grey level rend"], "labels": ["SMY", "APC", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am quite convinced that any somewhat correctly setup vanilla deep rl algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box", "figure  looks to me like the baseline is actually doing much better than the proposed method", "figure  looking at those d pcas i am not sure any of those method really abstracts the rendering away", "anyway it would be good to have a quantified metric on this which is not just eyeballing pca scatter plot", "this paper proposes to use d conditional gan models to generatefmri scan"], "labels": ["DIS", "QSN", "QSN", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["using the generated images paper reports improvementin classification accuracy on various task", "one claim of the paper is that a generative model of fmridata can help to caracterize and understand variability of scansacross subject", "article is based on recent works such as wasserstein gans and ac-gansby odena et ", "despite the rich literature of this recent topic the related worksection is rather convinc", "model presented extends iw-gan by using d convolution and alsoby supervising the generator using sample label"], "labels": ["APC", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["major- the size of the generated images is up to xx which is limitedabout half the size of the actual resolution of fmri data", "as aconsequence results on decoding learning task using low resolutionimages can end up worse than with the actual data as pointed out", "what it means is that the actual impact of the work is probably limit", "- generating high resolution images with gans even on faces for whichthere is almost infinite data is still a challeng", "here a few thousanddata points are us"], "labels": ["SMY", "CRT", "DFT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so it raises too concerns first is it enough", "using so-called learning curves is a good way to answer thi", "secondis what are the contributions to the state-of-the-art of the methods introduc", "said differently as thereis no classification results using images produced by an anothergan architecture it is hard to say that the extra complexityproposed here which is a bit contribution of the work is actuallynecessari", "minor- fonts in figure  are too smal"], "labels": ["QSN", "DIS", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper presents interesting algorithms for minimizing softmax with many class", "the objective function is a multi-class classification problem using softmax loss and with linear model", "the main idea is to rewrite the obj as double-sum using the dual formulation and then apply sgd to solve it", "at each iteration sgd samples a subset of training samples and label", "the main contribution of this paper is  proposing a u-max trick to improve the numerical stability and  proposing an implicit sgd approach"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems the implicit sgd approach is better in the experimental comparison", "i found the paper quite interesting but meanwhile i have the following comments and quest", "i found the paper quite interesting but meanwhile i have the following comments and quest", "- as pointed out by the authors the idea of this formulation and doubly sgd is not new", "raman et al  has used a similar trick to derive the double-sum formulation and solved it by doubly sgd"], "labels": ["APC", "APC", "QSN", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors claim that  the algorithm in raman et al has an onkd cost for updating u at the end of each epoch", "however since each epoch requires at least onkd time anyway sometimes larger as in proposition  is another onkd a significant bottleneck", "also since the formulation is similar to raman et al  a comparison is need", "- i'm confused by proposition  and  in appendix e the formulation of the update is derived but why we need newton to get log/epsilon time complex", "i think most first order methods instead of newton will have linear converge log/epsilon tim"], "labels": ["DIS", "QSN", "SUG", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also i guess we are assuming the obj is strongly convex", "- the step size is selected in one dataset and used for all others this might lead to divergence of other algorithms since usually step size depends on data", "as we can see ove nce and is diverges on wiki-small which may be fixed if the step size is chosen for each data in practice we can choose using subsamples for each data", "- all the comparisons are based on epochs but the competing algorithms are quite different and can have very different running time for each epoch", "for example implicit sgd has another iterative solver for each update therefore the timing comparison is needed in this paper to justify that implicit sgd is fast"], "labels": ["QSN", "DFT", "SUG", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the claim that implicit sgd never overshoots the optimum needs more supports is it proved in some previous pap", "- the presentation can be improv", "- the presentation can be improv", "i think it will be helpful to state the algorithms explicitly in the main pap", "the authors provide a novel interesting and simple algorithm capable of training with limited memori"], "labels": ["QSN", "SUG", "DFT", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the algorithm is well-motivated and clearly explained and empirical evidence suggests that the algorithm works wel", "however the paper needs additional examination in how the algorithm can deal with larger data inputs and output", "second the relationship to existing work needs to be explained bett", "prothe algorithm is clearly explained well-motivated and empirically support", "conthe relationship to stochastic gradient markov chain monte carlo needs to be explained bett"], "labels": ["APC", "DFT", "CRT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the update form was first introduced in [] the annealing scheme was analyzed in [] and the reflection step was introduced in []  these relationships need to be explained clearli", "the evidence is presented on very small input data", "with something like natural images the parameterization is much larger and with more data the number of total parameters is much larg", "is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increas", "this would require a smaller ratio of included paramet"], "labels": ["DFT", "CRT", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] welling m and teh yw  bayesian learning via stochastic gradient langevin dynam", "in proceedings of the th international conference on machine learning icml-pp -", "[] chen c carlson d gan z li c and carin l  may", "bridging the gap between stochastic gradient mcmc and stochastic optim", "in artificial intelligence and statisticspp -"], "labels": ["DFT", "DFT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] patterson s and teh yw", "stochastic gradient riemannian langevin dynamics on the probability simplex", "in advances in neural information processing systems pp -", "thanks for an interesting pap", "the paper evaluates popular gan evaluation metrics to better understand their properti"], "labels": ["DFT", "DFT", "DFT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the novelty of this paper is a bit hard to assess", "however i found their empirical evaluation and experimental observations to be very interest", "if the authors release their code as promised the off-the-shelf tool would be a very valuable contribution to the gan commun", "in addition to existing metrics it would be useful to add frechet inception distance fid and multi-scale structural similarity ms-ssim", "have you considered approximations to wasserstein distance eg danihelka et al proposed using an independent wasserstein critic to evaluate gan"], "labels": ["CRT", "APC", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["have you considered approximations to wasserstein distance eg danihelka et al proposed using an independent wasserstein critic to evaluate gan", "comparison of maximum likelihood and gan-based training of real nvpshttps//arxivorg/pdf/pdf", "how sensitive are the results to hyperparamet", "it would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters cf appendix g in https//arxivorg/pdf/pdf", "do you think it would be useful to compare other generative models eg vaes using these evaluation metr"], "labels": ["QSN", "DIS", "QSN", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some of the metrics don't capture perceptual similarity but i'm curious to hear what you think", "the authors extend the resnext architectur", "they substitute the simple add operation with a selection operation for each input in the residual modul", "the selection of the inputs happens through gate weights which are sampled at train tim", "at test time the gates with the highest values are kept on while the other ones are shut"], "labels": ["DIS", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors fix the number of the allowed gates to k out of c possible inputs c is the multi-branch factor in the resnext modul", "they show results on cifar- and imagenet as well as mini imagenet", "they ablate the choice of k the binary nature of the gate weight", "pros+ the paper is well written and the method is well explain", "+ the authors ablate and experiment on large scale dataset"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- the proposed method is a simple extension of resnext", "- the gains are reason", "yet not sota and come at a price of more complex training protocols see below", "- generalization to other tasks not shown", "the authors do a great job walking us through the formulation and intutition of their proposed approach"], "labels": ["CRT", "APC", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they describe their training procedure and their sampling approach for the gate weight", "however the training protocol gets complicated with the introduction of gate weight", "in order to train the gate weights along with the network parameters the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen", "in order to train the gate weights along with the network parameters the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen", "this makes training of such networks cumbersom"], "labels": ["SMY", "CRT", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition the authors report a loss in performance when the gates are not discretized to {}", "this means that a liner combination with the real-valued learned gate parameters is suboptim", "could this be a result of suboptimal possibly compromised train", "while the cifar- results look promising the imagenet-k results are less impress", "the gains from introducing gate weights in the input of the residual modules vanish when increasing the network s"], "labels": ["CRT", "DIS", "QSN", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["last the impact of resnext/resnet lies in their ability to generalize to other task", "have the authors experimented with other tasks eg object detection to verify that their approach leads to better performance in a more diverse set of problem", "paper proposes a weak synchronization approach to synchronous sgd with the goal of improving even with slow parameter serv", "this is an improvement on earlier proposals eg revisiting synchronous sgd that allow for slow work", "empirical results on resnet on cifar show promising results for simulations with slow workers and servers with the proposed approach"], "labels": ["DIS", "QSN", "DFT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["issues with the paper- since the paper is focused on empirical results having results only for resnet on cifar is very limiting- empirical results are based on simulations and not real workload", "the choice of simulation constants % delayed and delay time seems somewhat arbitrary as wel", "- for the simulated results the comparisons seem unfair since the validation error is differ", "it will be useful to also provide time to a certain accuracy that all of them get to eg the validation error of  reached by the  important cas", "overall the paper proposes an interesting improvement to this area of synchronous training however it is unable to validate the impact of this propos"], "labels": ["DFT", "APC", "DFT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overalli had a really hard time reading this paper because i found the writing to be quite confus", "for this reason i cannot recommend publication as i am not sure how to evaluate the paperus contribut", "summarythe authors study state space models in the unsupervised learning cas", "we have a set of observed variables y we posit a latent set of variables x the mapping from the latent to the observed variables has a parametric form and we have a prior over the paramet", "we want to infer a posterior density given some data"], "labels": ["CRT", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose an algorithm which uses sequential monte carlo + autoencod", "they use a reinforce-like algorithm to differentiate through the monte carlo", "the contribution of this paper is to add to this a method which uses  different elbos for updating different sets of paramet", "the authors show the aesmc works better than importance weighted autoencoders and the double elbo method works even better in some experi", "the proposed algorithm seems novel"], "labels": ["SMY", "SMY", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i do not understand a few points which make it hard to judge the contribut", "note that here i am assuming full technical correctness of the paper and still cannot recommend accept", "is the proposed contribution of this paper just to add the double elbo or does it also include the aesmc that is should this paper subsume the anonymized pre-print mentioned in the intro", "this was very unclear to m", "the introduction/experiments section of the paper is not well motiv"], "labels": ["DIS", "CRT", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the problem the authors are trying to solve with aesmc over existing method", "is it scal", "is it purely to improve likelihood of the fitted model see my questions on the experiments in the next sect", "the experiments feel lack", "there is only one experiment comparing the gains from aesmc alt to a simpler  method of iwa"], "labels": ["QSN", "QSN", "QSN", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we see that they do better but the magnitude of the improvement is not obvious should i be looking at the elbo scores as the sole judg", "does aesmc give a better generative model", "the authors discuss the advantages of smc and say that is scales better than other method", "it would be good to show this as an experimental result if indeed the quality of the learned representations is compar", "the submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and cost"], "labels": ["QSN", "QSN", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this type of study is important to give perspective to non-standardized performance scores reported across separate publ", "and indeed the results here are interesting as they favour relatively simpler structur", "i have a favourable impression of this pap", "i have a favourable impression of this pap", "but would hope another reviewer is more familiar with the specific application domain than i am"], "labels": ["SMY", "APC", "APC", "FBK", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes an approach to generating the first section of wikipedia articles and potentially entire articl", "first relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a td-idf-based rank", "then abstractive summarization is performed using a modification of transformer networks vasvani et ", "a mixture of experts layer further improves perform", "the proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network"], "labels": ["SMY", "SMY", "SMY", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on its own this modification improves perplexity on longer sequences but not the rouge score however the architecture enables memory-compressed attention which is more scalable to long input sequ", "it is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given", "computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of train", "in the task setup the information retrieval-based extractive stage is crucial to performance but this contribution might be less important to the iclr commun", "it willl also be hard to reproduce without significant computational resources even if the urls of the dataset are made avail"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the training data is significantly larger than the cnn/dailymail single-document summarization dataset", "the paper presents strong quantitative results and qualitative exampl", "unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments especially with regards to the quality of the generated output in comparison to the output of the extractive stag", "in some of the examples the system output seems to be significantly shorter than the reference so it would be helpful to quantify this as well how much the quality degrades when the model is forced to generate outputs of a given minimum length", "while the proposed approach is more scalable it is hard to judge the extend of thi"], "labels": ["CRT", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so while the performance of the overall system is impress", "it is hard to judge the significance of the technical contribution made by the pap", "---the additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper and i would like to see the paper accept", "an approach to adjust inference speed power consumption or latency by using incomplete dot products mcdanel et al  is investig", "the approach is based on `profile coefficientsu which are learned for every channel in a convolution layer or for every column in the fully connected lay"], "labels": ["APC", "CRT", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["based on the magnitude of this profile coefficient which determines the importance of this `filteru individual components in a neural net are switched on or off mcdanel et al  propose to train such an approach in a stage-by-stage mann", "different from a recently proposed method by mcdanel et al  the authors of this submission argue that the stage-by-stage training doesnut fully utilize the deep net perform", "to address this issue a `loss aggregationu is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are us", "the method is evaluated on the mnist and cifar- datasets and shown to outperform work on incomplete dot products by mcdanel et al  by % in the low resource regim", "summaryuuin summary i think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method"], "labels": ["SMY", "SMY", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results are preliminary and should be extended to cifar- and imagenet to be convinc", "in addition the writing should be improved as it is often ambigu", "see below for detailsreviewuuuuu experiments are only provided on very small datasets according to my opinion this isnut sufficient to illustrate the effectiveness of the proposed approach", "as a reader i wouldnut want to see results on cifar- and imagenet using multiple network architectures eg alexnet and vgg", "usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsist"], "labels": ["SUG", "SUG", "DFT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more specifically while the profile coefficient is applied for every input element in eq  itus applied based on output channels in eq  this seems inconsistent and a comment like `these two approaches however are equivalent with negligible difference induced by the first hidden layeru is more confusing than clarifi", "the writing should be improved significantly and statements should be made more precise eg `from now on x% dp where leq x geq  means the x% of terms used in dot productsu while sentences like those can be deciphered they arenut that app", "the loss functions in eq  should be made more precise it remains unclear whether the profile coefficients and the weights are trained jointly separately incrementally etc", "the loss functions in eq  should be made more precise it remains unclear whether the profile coefficients and the weights are trained jointly separately incrementally etc", "algorithm  and algorithm  call functions that arenut described/defin"], "labels": ["DFT", "SUG", "SUG", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["baseline numbers for training on datasets without incomplete dot products should be provid", "prosthis is a great paper - i enjoyed reading it", "the authors lay down a general method for addressing various transfer learning problems transferring across domains and tasks and in a unsupervised fashion", "the paper is clearly written and easy to understand", "even though the method combines the previous general learning frameworks the proposed algorithm for  learnable clustering objective lco is novel and fits very well in this framework"], "labels": ["DFT", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cas", "cons/suggestions - the authors should discuss in more detail the limitations of their approach it is clear that when there is a high discrepancy between source and target domains that the similarity prediction network can fail", "summarythis paper proposed an extension of the dynamic coattention network dcn with deeper residual layers and self-attent", "it also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span", "the resulting dcn+ model achieved significant improvement over dcn"], "labels": ["APC", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["strengthsthe model and the mixed objective is well-motivated and clearly explainednear state-of-the-art performance on squad dataset according to the squad leaderboard", "other questions and commentsthe ablation shows  improvement on em with mixed object", "it is interesting that the mixed objective which targets f also brings improvement on em", "the authors propose a new network architecture for rl that contains some relevant inductive biases about plan", "this fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task"], "labels": ["APC", "APC", "FBK", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed architecture performs something analogous to a full-width tree search using an abstract model learned end-to-end", "this is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nod", "the final backup value is the q-value prediction for a given state or can represent a policy through a softmax", "i thought the paper was clear and well-motiv", "the architecture and various associated tricks like state vector normalization are well-described for reproduc"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental results seem promising but i wasnut fully convinced of its conclus", "in both domains treeqn and atreec are compared to a dqn architecture but it wasnut clear to me that this is the right baselin", "indeed treeqn and atreec share the same conv stack in the encoder i think but also have the extra capacity of the tree on top", "can the performance gain we see in the push task as a function of tree depth be explained by the added network capac", "same comment in atari but there itus not really obvious that the proposed architecture is help"], "labels": ["DFT", "DFT", "DIS", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["baselines could include unsharing the weights in the tree removing the max backup having a regular mlp with similar capacity etc", "page  the auxiliary loss on reward prediction seems appropriate but itus not clear from the text and experiments whether it actually was necessari", "is it that makes interpretability of the model easier like we see in fig c or does it actually lead to better perform", "despite some shortcomings in the result section i believe this is good work and worth communicating as i", "this paper deals with early stopping but the contributions are limit"], "labels": ["SUG", "DFT", "QSN", "FBK", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this work would fit better a workshop as a preliminary result furthermore it is too short", "following a short review section per sect", "intro the name sfc is misleading as the method consists in stopping early the training with an optimized learning schedule schem", "furthermore the work is not compared to the appropriate baselin", "proposal the first motivation is not clear"], "labels": ["CRT", "CNT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the training time of the feature extractor has never been a problem for transfer learning tasks for example once it is trained you can reuse the architecture in a wide range of task", "besides the training time of a cnn on cifar or even imagenet is now quite smallfor reasonable architectures which allows fast benchmark", "the second motivation wrt ib seems interest", "but this should be empirically motivatedeg figures in the subsection  and this is not don", "but this should be empirically motivatedeg figures in the subsection  and this is not don"], "labels": ["DIS", "DIS", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the section  is quite long and could be compressed to improve the relevance of this experimental sect", "all the accuraciesunsup dict unsup etc on cifar/cifar are reported from the paper oyallon & mallat  ignoring - years of research that leads to new numerical result", "furthermore this supervised technique is only compared to unsupervised or predefined methods which is is not fair and the training time of the scattering transform is not reported for exampl", "finally extracting features is mainly useful on imagenet for realistic images and this is not reported her", "i believe re-thinking new learning rate schedules is interest"], "labels": ["CRT", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i recommend the rejection of this pap", "the key contributions of this paper area proposes to reduce the vocabulary size in large sequence to sequence mapping tasks eg translation by first mapping them into a standard form and then into their correct morphological form", "b they achieve this by clever use of character lstm encoder / decoder that sandwiches a bidirectional lstm which captures context", "c they demonstrate clear and substantial performance gains on the opensubtitle task", "andd they demonstrate clear and substantial performance gains on a dialog question answer task"], "labels": ["FBK", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their analysis in section  shows one clear advantage of this model in the context of long sequ", "as an aside the authors should correct the numbering of their figures there is no figure  and provide better captions to the tables so the results shown can easily understood at a gl", "the only drawback of the paper is that this does not advance representation learning per se though a nice application of current model", "the authors prove a generalization guarantee for deepneural networks with relu activations in terms of margins of theclassifications and norms of the weight matric", "they compare thisbound with a similar recent bound proved by bartlett et "], "labels": ["APC", "SUG", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["whilestrictly speaking the bounds are incomparable in strength theauthors of the submission make a convincing case that their new boundmakes stronger guarantees under some interesting condit", "the analysis is eleg", "it uses some existing tools but brings themto bear in an important new context with substantive new ideas neededthe mathematical writing is excel", "very nice pap", "i guess that networks including convolutional layers are covered bytheir analysi"], "labels": ["APC", "APC", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it feels to me that these tend to be spars", "but thattheir analysis still my provides some additional leverage for suchlay", "some explicit discussion of convolutional layers may behelp", "the paper introduces two alternatives to value iteration network vin proposed by tamar et al vin was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environ", "the authors propose two new updates value propagation vprop and max propagation mvprop which are roughly speaking additive and multiplicative versions of the update used in the bellman-ford algorithm for shortest path"], "labels": ["APC", "SUG", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approaches are evaluated in grid worlds with and without other ag", "i had some difficulty to understand the paper because of its presentation and writing see below", "in tamar's work a mapping from observation to reward is learn", "it seems this is not the case for vprop and mvprop given the gradient updates provided in p", "as a consequence those two methods need to take as input a new reward function for every new map"], "labels": ["SMY", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is that correct", "i think this could explain the better experimental resultsin the experimental part the results for vin are worse than those reported in tamar et al's pap", "why did you use your own implementation of vin and not tamar et al's which is publicly shared as far as i know", "i think the writing needs to be improved on the following points- the abstract doesn't fit well the content of the pap", "for instance its variants is confusing because there is only other variant to vprop"], "labels": ["QSN", "CRT", "QSN", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["adversarial agents is also misleading because those agents act like automata", "- the authors should recall more thoroughly and precisely the work of tamar et al on which their work is based to make the paper more self-contained eg  is hardly understand", "- the writing should be careful eg value iteration is presented as a learning algorithm which in my opinion is not", "pi^* is defined as a distribution over state-action space and then pi is defined as a funct", "- the mathematical writing should be more rigor"], "labels": ["DFT", "SUG", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["eg pt s to a to s' pi  s to aa denotes a set and its cardinalin  shouldn't it be phio", "eg pt s to a to s' pi  s to aa denotes a set and its cardinalin  shouldn't it be phio", "all the new terms should be explainedp definition of t and r shouldn't v_{ij}^k depend on q_{aij}^kt_{aij} should be definedin the definition of h_{aij} should phi and b be indexed by a", "all the new terms should be explainedp definition of t and r shouldn't v_{ij}^k depend on q_{aij}^kt_{aij} should be definedin the definition of h_{aij} should phi and b be indexed by a", "- the typos and other issues should be fixedp k iterationwith capablepclose pour ours^{t+} should be defined like the other termsthe state is represented by the coordinates of the agent and d environment observation should appear much earlier in the pap"], "labels": ["SUG", "QSN", "SUG", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pi_theta described in the previous sections notation pi_theta appears the first time herex -  times ofbv_{theta^t w^t}", "pthe thefig's captionwhat does both cases refer to they are three model", "referenceset alyi wu", "summary of the paper---------------------------the paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learn", "the covered framework is limited to regularization paramet"], "labels": ["DIS", "QSN", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these hyper-parameters noted $lambda$ are updated along the training of model parameters $theta$ by relying on the generalization performance validation error", "the paper proposes a dynamical system including the dynamical update of $theta$ and the update of the gradient $y$ derivative of $theta$ wrt to the hyper-paramet", "the main contribution of the paper is to propose a way to re-initialize $y$ at each update of $lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system", "experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach", "comments-------------- the materials of the paper sometimes may be quite not easy to follow"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nevertheless the paper is quite well written", "- the main contributions of the paper can be seen as an incremental version of franceschi et al  based on the proposal in luketina et ", "as such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitor", "- one motivation of the approach is to fix the slow convergence of the method in franceschi et ", "the paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discuss"], "labels": ["APC", "APC", "APC", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the goal of the paper is to address automatically the learning of regularization paramet", "unfortunately algorithm  involves several other hyper-parameters namely clipping factor $r$ constant $c$ or $eta$ which choices are not clearly discuss", "it turns that the paper trades a set of hyper-parameters for another one which tuning may be tedi", "this fact weakens the scope of the online hyper-parameter optimization approach", "- it may be helpful to indicate the standard deviations of the experimental result"], "labels": ["SMY", "CRT", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors analyze training of residual networks using large cyclic learning rates clr", "the authors demonstrate a fast convergence with cyclic learning rates and b evidence of large learning rates acting as regularization which improves performance on test sets u this is called ucsuper-convergenceud", "however both these effects are only shown on a specific dataset architecture learning algorithm and hyper parameter set", "some specific comments by sections related work this section loosely mentions other related works on sgd topology of loss function and adaptive learning r", "the authors mention loshchilov & hutter in next section but do not compare it to their work"], "labels": ["SMY", "SMY", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors do not discuss a somewhat contradictory claim from nips  as pointed out in the public comment http//papersnipscc/paper/-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networkspdf", "super-convergence this is a well explained section where the authors describe the lr range test and how it can be used to understand potential for super-convergence for any architecture the authors also provide sufficient intuition for super-converg", "since clrs were already proposed by smith  the originality of this work would be specifically tied to their application to residual unit", "it would be interesting to see a qualitative analysis on how the residual error is impacting super-converg", "regularization while fig  demonstrates the regularization property the reference to fig a with better test error compared to typical training methods could simply be a result of slower convergence of typical training method"], "labels": ["DFT", "APC", "SMY", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["optimal lrs figb shows results for  iterations whereas the text says  seems like a typo in scaling the plot", "figs  and  illustrate only one cycle one increase and one decrease of clr it would be interesting to see cases where more than one cycle is required and to see what happens when the lr increases the second tim", "experiments this is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work", "however the fact that the results only applies to cifar- dataset and could not be observed for imagenet or other architectures is disappointing and heavily takes away from the significance of this work", "overall the work is presented as a positive result in very specific conditions but it seems more like a negative result"], "labels": ["DIS", "SUG", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical back", "recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure similar to how the spatial axes form a plan", "this paper proposes a method to uncover this structure from the filters of a trained convnet", "the method uses an infogan to learn the distribution of filters by varying the latent variables of the gan one can traverse the manifold of filt", "the effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filt"], "labels": ["SUG", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of empirically studying the manifold / topological / group structure in the space of filters is interest", "a priori using a gan to model a relatively small number of filters seems problematic due to overfitting but the authors show that their infogan approach seems to work wel", "my main concerns arecontrolsto generate the visualizations two coordinates in the latent space are varied and for each variation a figure is produc", "to figure out if the gan is adding anything it would be nice to see what would happen if you varied individual coordinates in the filter space x-space of the gan or varied the magnitude of filters or filter plan", "since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the gan i would expect to see similar plots for these baselin"], "labels": ["APC", "APC", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["lack of new insightsthe visualizations produced in this paper are interesting to look at but it is not clear what they tell us other than something non-trivial is going on in these networks in fact it is not even clear that the transformations being visualized are indeed non-linear in pixel space note that even a d diffeomorphism which is a non-linear map on r^ is a linear operator on the space of *functions* on r^ ie on the space of imag", "in any case no attempt is made to analyze the results or provide new insights into the computations performed by a trained convnet", "interpretationthis is a minor point but i would not say as the paper does that the method captures the invariances learned by the model but rather that it aims to show the variability captured by the model", "a relu net is only invariant to changes that are mapped to zero by the relu or that end up in the kernel of one of the linear lay", "the presented method does not consider this and hence does not analyze invari"], "labels": ["CRT", "CRT", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor issues- in the last equation on page  the right-hand side is missing a min max", "this paper presents a lifelong learning method for learning word embed", "given a new domain of interest the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain or standard pre-trained embed", "the general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to iclr", "however this submission needs to be improved in terms of clarity and its experi"], "labels": ["DFT", "SMY", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in terms of clarity the paper has a large number of typos i list a few at the end of this review and more significantly at several points in the paper is hard to tell what exactly was done and whi", "when presenting algorithms starting with an english description of the high-level goal and steps of the algorithm would be help", "what are the inputs and outputs of the meta-learner and how will it be used to obtain embeddings for the new domain", "the paper states the purpose of the meta learning is to learn a general word context similarity from the first m domains but i was never sure what this m", "further some of the paper's pseudocode includes unexplained steps like invert by domain index and scanco-occurr"], "labels": ["CRT", "SUG", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in terms of the experiments the paper is missing some important baselines that would help us understand how well the approach work", "first besides the glove common crawl embeddings used here there are several other embedding sets including the other glove embeddings released along with the ones used here and the google news wordvec embeddings that should be consid", "also the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuraci", "see think globally embed locallyulocally linear meta-embedding of words bollegala et al https//arxivorg/pdf/pdfthat paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there but their baselines of concatenation of pre-trained embedding sets should be compared against in the submiss", "beyond trying other embeddings the paper should also compare against simpler combination approaches including simpler variants of its own approach"], "labels": ["CRT", "SUG", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what if we just selected the one past domain that was most similar to the new domain by some measur", "and how does the performance of the technique depend on the setting of m", "investigating some of these questions would help us understand how well the approach works and in which set", "minorsecond paragraph glovec should be glovegiven many domains with uncertain noise for the new domain -- not clear what uncertain noise means perhaps uncertain relevance would be more clear", "the text refers to a figure  which does not exist probably means figur"], "labels": ["QSN", "QSN", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i didn't understand the need for both figures figure  is almost contained within figur", "when m is introduced it would help to say that m  n and justify why dividing the n domains into two chunks of m and n-m domains is necessari", "from the first m domain corpus - from the first m domain", "may not helpful - may not be helpfulvocabularie - vocabularysystem first retrieval - system first retriev", "comments on revisions i appreciate the authors including the new experiments against concatenation baselin"], "labels": ["CRT", "SUG", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the concatenation does fairly comparably to ll in tables &", "ll wins by a bit more in t", "given these somewhat close/inconsistent wins it would help the paper to include an explanation of why and under what conditions the ll approach will outperform concaten", "summary this paper explores how to handle two practical issues in reinforcement learn", "the first is including time remaining in the state for domains where episodes are cut-off before a terminal state is reached in the usual way"], "labels": ["APC", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the second idea is to allow bootstrapping at episode boundaries but cutting off episodes to facilitate explor", "the ideas are illustrated through several well-worked micro-world experi", "overall the paper is well written and polish", "they slowly worked through a simple set of ideas trying to convey a better understanding to the reader with a focus on performance of rl in practic", "my main issue with the paper is that these two topics are actually not new and are well covered by the existing rl form"], "labels": ["SMY", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that is not to say that an empirical exploration of the practical implications is not of value but that the paper would be much stronger if it was better positioned in the literature that exist", "that is not to say that an empirical exploration of the practical implications is not of value but that the paper would be much stronger if it was better positioned in the literature that exist", "the first idea of the paper is to include time-remaining in the st", "this is of course always possible in the mdp form", "if it was not done as in your examples the state would not be markov and thus it would not be an mdp at al"], "labels": ["SUG", "CRT", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition the technical term for this is finite horizon mdps in many cases the horizon is taken to be a constant h", "it is not surprising that algorithms that take this into account do better as your examples and experiments illustr", "the paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case to motivate your work", "see dynamic programming and optimal control and references too it", "the second idea is that episodes may terminate due to time out but we should include the discounted value of the time-out termination state in the return"], "labels": ["DIS", "APC", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i could not tell from the text but i assume the next transition to the start state is fully discounted to zero otherwise the value function would link the values of s_t and the next state which i assume you do not w", "the impact of this choice is s_t is no longer a termination state and there is a direct fully discounted transition to the start st", "this is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic rl frameworks eg rl glu", "if we treat the value of s_t as zero or consider gamma on the transition into the time-out state as zero then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior", "the literature might not be totally clear about this but it is very well discussed in a recent icml paper white  []another way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the horde paper []"], "labels": ["CRT", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this setting the behavior policy can have terminations and episodes in the classic sense perhaps due to time out", "however the agent's continuation function gamma  s - [] can specify weightings on states representing complex terminations or not completely independent of the behavior policy or actual state transition dynamics of the underlying mdp", "to clearly establish your contributions the authors must do a better job of relating their work to [] and []", "[] white unifying task specification in reinforcement learning martha white international conference on machine learning icml", "[] sutton r s modayil j delp m degris t pilarski p m white a & precup d"], "labels": ["DIS", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["horde a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interact", "in the th international conference on autonomous agents and multiagent systems  --", "small comments that did not impact paper scoring eq  we usually don't use the superscript gamma", "eq usually we talk about truncated n-step returns include the value of the last state to correct the return", "you should mention this last paragraph of page  should not be in the intro"], "labels": ["DIS", "DIS", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  why is the behavior policy random instead of epsilon greedi", "it would be useful to discuss the average reward setting and how it relates to your work", "fig  what does good performance look like in this domain", "i have no reference point to understand these graph", "page  second par outlines alternative approaches but they are not presented as such confus"], "labels": ["QSN", "SUG", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a new sampling based approach for inference in latent variable model", "they apply this approach to multi-modal several intentions imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural network", "the proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampl", "the scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior", "the objective to be maximized is a lower bound to /alpha * the likelihood"], "labels": ["SMY", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality the empirical results including a video of an actual robotic arm system performing the task looks good", "this reviewer is a bit sceptical to the methodolog", "i am not convinced that the proposed bound will have low enough vari", "it is mentioned in a footnote that variational autoencoders were tested but that they fail", "since the variational bound has much better sampling properties due to recognition network reparameterization trick and bounding to get log likelihoods instead of likelihoods it is hard to believe that it is harder to get to work than the proposed framework"], "labels": ["APC", "CRT", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the recently proposed continuous relaxation of random variables seemed relev", "clarity the paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribut", "clarity the paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribut", "significance hard to say", "new method proposed and shown to work well in one cas"], "labels": ["APC", "APC", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["too early to tell about signific", "pro challenging and relevant problem solved better than other approach", "new latent variable model bound that might work better than classic approach", "con not entirely convincing that it should work better than already existing method", "missing some investigation of the properties of the estimator on simple problem to be compared to standard method"], "labels": ["DIS", "SMY", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a model for solving the wikisql dataset that was released rec", "the main issues with the paper is that its contributions are not new", "* the first claimed contribution is to use typing at decoding time they don't say why but this helps search and learn", "restricting the type of the decoded tokens based on the programming language has already been done by the neural symbolic machines of liang et ", "then krishnamurthy et al expanded that in emnlp  and used typing in a grammar at decoding tim"], "labels": ["SMY", "CRT", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i don't really see why the authors say their approach is simpler it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple but in general sql is not regular", "of course even for cfg this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by guu et al  for the scone dataset and more recently for cnlvr by goldman et ", "so at least  papers have done that in the last year on  different datasets and it is now close to being common practice so i don't really see this as a contribut", "* the authors explain that they use a novel loss function that is better than an rl based function used by zhong et ", "if i understand correctly they did not implement zhong et al only compared to their numbers which is a problem because it is hard to judge the role of optimization in the result"], "labels": ["CRT", "CRT", "CRT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold", "the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all token", "the authors re-invent this and find it works better than randomly choosing a gold token or taking the max", "but again this is something that has been done already in the context of pointer networks and other work like see  et al  for summarization and jia et al  for semantic pars", "* as for the good results - the data is new so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final perform"], "labels": ["CRT", "SUG", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in general i tend to agree that using rl for this task is probably unnecessary when you have the full program as supervis", "this paper proposes a new method for estimating optimal transport plans and maps among continuous distributions or discrete distributions with large support s", "first the paper proposes a dual algorithm to estimate kantorovich plans ie a coupling between two input distributions minimizing a given cost function using dual functions parameterized as neural network", "then an algorithm is given to convert a generic plan into a monge map a deterministic function from one domain to the other following the barycenter of the plan", "the algorithms are shown to be consistent and demonstrated to be more efficient than an existing semi-dual algorithm"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["initial applications to domain adaptation and generative modeling are also shown", "these algorithms seem to be an improvement over the current state of the art for this problem set", "although more of a discussion of the relationship to the technique of genevay et al would be useful how does your approach compare to the full-dual continuous case of that paper if you simply replace their ball of rkhs functions with your class of deep network", "the consistency properties are nic", "though they don't provide much insight into the rate at which epsilon should be decreased with n or similar properti"], "labels": ["SMY", "APC", "SUG", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proofs are clear and seem correct on a superficial readthrough i have not carefully verified them", "the proofs are mainly limited in that they don't refer in any way to the class of approximating networks or the optimization algorithm but rather only to the optimal solut", "although of course proving things about the actual outcomes of optimizing a deep network is extremely difficult it would be helpful to have some kind of understanding of how the class of networks in use affects the solut", "in this way your guarantees don't say much more than those of arjovsky et al who must assume that their critic function reaches the global optimum essentially you add a regularization term and show that as the regularization decreases it still works but under seemingly the same kind of assumptions as arjovsky et al's approach which does not add an explicit regularization term at al", "though it makes sense that your regularization might lead to a better estimator you don't seem to have shown so either in theory or empir"], "labels": ["APC", "DFT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the performance comparison to the algorithm of genevay et al is somewhat limited it is only on one particular problem with three different hyperparameter set", "the performance comparison to the algorithm of genevay et al is somewhat limited it is only on one particular problem with three different hyperparameter set", "also since genevay et al propose using sag for their algorithm it seems strange to use plain sgd how would the results compare if you used sag or saga/etc for both algorithm", "in discussing the domain adaptation results you mention that the l regularization works very well in practice but don't highlight that although it slightly outperforms entropy regularization in two of the problems it does substantially worse in the oth", "do you have any guesses as to why this might b"], "labels": ["DFT", "CRT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for generative modeling you do have guarantees that *if* your optimization and function parameterization can reach the global optimum you will obtain the best map relative to the cost funct", "but it seems that the extent of these guarantees are comparable to those of several other generative models including wgans the sinkhorn-based models of genevay et al  https//arxivorg/abs// or eg with a different loss function the mmd-based models of li swersky and zemel icml  / dziugaite roy and ghahramani uai", "the different setting than the fundamental gan-like setup of those models is intriguing but specifying a cost function between the source and the target domains feels exceedingly unnatural compared to specifying a cost function just within one domain as in these other model", "minorin  what is the purpose of the - term in r_", "it seems to just subtract a constant  from the regularization term"], "labels": ["DIS", "APC", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks shallow linear networks deep linear networks and shallow relu nonlinear network", "the analytical forms of the critical points have direct implications on the values of the corresponding loss functions achievement of global minimum and various landscape properties around these critical point", "the paper is well organized and well written", "authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical point", "this technique generalizes existing work under full relaxation of assumpt"], "labels": ["SMY", "DIS", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the linear network with one hidden layer it generalizes the work baldi & hornik  with arbitrary network parameter dimensions and any data matric", "in the deep linear networks it generalizes the result in kawaguchi  under no assumptions on the network parameters and data matric", "moreover it also provides new characterization for shallow relu nonlinear networks which is not discussed in previous work", "the results obtained from the analytical forms of the critical points are interest", "but one problem is that how to obtain the proper solution of equ"], "labels": ["DIS", "DIS", "DIS", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the example  authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow relu nonlinear networks by properly choosing these matrices satisfi", "it will be interesting to see how to choose these matrices for all the studied networks with some concrete exampl", "this paper deals with the problem of learning nonlinear operators using deep learn", "specifically the authors propose to extend deep neural networks to the case where hidden layers can be infinite-dimension", "they give results on the quality of the approximation using these operator networks and show how to build neural network layers that are able to take into account topological information from data"], "labels": ["DIS", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments on mnist using the proposed deep function machines dfm are provid", "the paper attempts to make progress in the region between deep learning and functional data analysis fda this is interest", "unfortunately the paper requires significant improvements both in terms of substance and in terms of present", "my main concerns are the following one motivation of dfm is that in many applications data is a discretization of a continuous process and then can be represented by a funct", "fda is the research field that formulated the ideas about the statistical data analysis of data samples consisting of continuous functions where each function is viewed as one sample el"], "labels": ["SMY", "APC", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper fails to consider properly the work in its fda context", "operator learning has been already studied in fda see for eg the problem of functional regression with functional respons", "indeed the functional model considered in the linear case is very similar to eq  or eq  moreover extension to nonparametric/nonlinear situations were also studi", "the authors should add more information about previous work on this topic so that their results can be understood with respect to previous studi", "n the computational aspects of dfm are not clear in the pap"], "labels": ["CRT", "DIS", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from a practical computational perspective the algorithm will be implemented on a machine which processes on finite representations of data", "the paper does not clearly provide information about how the functional nature and the infinite dimensional can be handled in practice in fda generally this is achieved via basis function approxim", "some parts of the paper are hard to read sections  and  are not easy to understand", "maybe adding a section about the notation and developing more the intuition will improve the reading of the manuscript", "the experimental section can be significantly improv"], "labels": ["DIS", "DFT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it will be interesting to compare more dfm with its discrete counterpart", "also other fda approaches for operator learning should be discussed and compared to the proposed approach", "pros the paper proposes a ucbi-directional block self-attention network bi-blosanud for sequence encoding which inherits the advantages of multi-head vaswani et al  and disan shen et al  network but is claimed to be more memory-effici", "the paper is written clearly and is easy to follow", "the source code is released for duplicability the main originality is using block or hierarchical structures ie the proposed models split the an entire sequence into blocks apply an intra-block san to each block for modeling local context and then apply an inter-block san to the output for all blocks to capture long-range depend"], "labels": ["SUG", "SUG", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off", "cons- methodology of the paper is very incremental compared with previous models  - many of the baselines listed in the paper are not competitive eg  for snli state-of-the-art results are not included in the pap", "- the paper argues advantages of the proposed models over cnn by assuming the latter only captures local dependency which however is not supported by discussion on or comparison with hierarchical cnn", "- the block splitting as detailed in appendix is rather arbitrary in terms of that it potentially divides coherent language segments apart", "this is unnatural eg compared  with alternatives such as using linguistic segments as block"], "labels": ["APC", "DFT", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the main originality of paper is the block style however the paper doesnut analyze how and why the block brings improv", "- the main originality of paper is the block style however the paper doesnut analyze how and why the block brings improv", "-if we remove intra-block self-attention but only keep token-level self-attention whether the performance will be significantly wors", "the paper addresses an important problem in multitask learn", "but its current form has several serious issu"], "labels": ["DFT", "FBK", "QSN", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although i get the high-level goal of the paper i find sec  which describes the technical approach nearly incomprehens", "there are many things unclear for example-  it starts with talking about multiple tasks and then immediately talks about a filter f without defining what the kind of network is being address", "- also it is not clear what l_grad is it looks like a loss but equation  seems to define it to be the difference between the gradient norm of a task and the average over all task", "it is not clear how it is used in particular it is not clear how it is used to update the task weight", "- equation  seems sloppy ucjud appears as a free index on the right side but it doesnut appear on the left sid"], "labels": ["DFT", "CRT", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a result i am unable to understand how the method works exactly and unable to judge its quality and origin", "the toy experiment is not convinc", "- the evaluation metric is the sum of the relative losses that is the sum of the original losses weighted by the inverse of the initial loss of each task", "this is different from the sum of the original losses which seems to be the one used to train the ucequal weightud baselin", "a more fair baseline is to directly use the evaluation metric as the training loss - the curves seem to have not converg"], "labels": ["CRT", "CRT", "SMY", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments on nyuv involves non-standard settings without a good justif", "so it is not clear if the proposed method can make a real difference on state of the art system", "and the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks eg depth", "however the method works by normalizing the norms of the gradients which does not necessarily prevent overfitting u it can in fact magnify gradients of certain tasks and cause over-training and over-fit", "so the performance gain is likely dataset dependent and what happens on nyu depth can be a fluke and does not necessarily generalize to other dataset"], "labels": ["CRT", "DFT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have read comments and rebuttal - i do not have the luxury of time to read in depth the revis", "it seems that the authors have made an effort to accommodate reviewers' com", "i upgraded the r", "-----------------------------------------------------------------------------------------------------------------------summary the paper considers the use of natural gradients for learn", "the added twist is the substitution of the kl divergence with the wasserstein distance as proposed in gan train"], "labels": ["DIS", "APC", "FBK", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors suggest that wasserstein regularization improves generalization over sgd with a little extra cost", "the paper is structured as follows kl divergence is used as a similarity measure between two distribut", "regularizing the objective with kl div seems promising but expens", "we usually approximate the kl div with its nd order approximation - this introduces the hessian of the kl divergence known as fisher information matrix", "however computing and inverting the fisher information matrix is computationally expens"], "labels": ["SMY", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one solution is to approximate the solution f^{-} j using gradient desc", "however still we need to calculate f", "there are options where f could be formed as the outer product of a collection gradients of individual examples 'empirical fisher'", "this paper does not move towards fisher information but towards wasserstein distance after a good initialization via sgd is obtained the inner loop continues updating that point using the wasserstein regularized object", "no large matrices need to be formed or inverted however more passes needed per outer step"], "labels": ["SMY", "DIS", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["importancesomewhat lack of originality and poor experiments lead to low import", "claritythe paper needs major revision wrt presenting and highlighting the new main point", "eg one needs to get to page  to understand that the paper is just based on the wgan ideas in arjovsky et al but with a different application not gan", "originality/noveltythe paper based on wgan motivation proposes wasserstein distance regularization over kl div regularization for training of simple models such as neural network", "beyond this the paper does not provide any futher original idea"], "labels": ["CRT", "CRT", "CRT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so slight to no novelti", "main comments would the approximation of c_ by its second-order taylor expansion that also introduces a hessian help", "this would require the combination of two hessian matric", "experiments are really demotivating it is not clear whether using plain sgd or the proposed method leads to better result", "overallreject"], "labels": ["CRT", "QSN", "SUG", "CRT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper consider a method for weight normalization of layers of a neural network", "the weight matrix is maintained normalized which helps accuraci", "however the simplest way to normalize on a fully connected layer is quadratic adding squares of weights and taking square root", "the paper proposes fastnorm which is a way to implicitly maintain the normalized weight matrix using much less comput", "essentially a normalization vector is maintained an updated separ"], "labels": ["SMY", "APC", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros   natural method to do weight normalization efficeintli", "cons   a very natural and simple solution that is fairly obvi", "limited experi", "this paper presents a variational inference algorithm for models that containdeep neural network components and probabilistic graphical model pgmcompon", "the algorithm implements natural-gradient message-passing where the messagesautomatically reduce to stochastic gradients for the non-conjugate neuralnetwork compon"], "labels": ["APC", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors demonstrate the algorithm on a gaussian mixturemodel and linear dynamical system where they show that the proposed algorithmoutperforms previous algorithm", "overall i think that the paper proposes someinteresting idea", "however in its current form i do not think that the noveltyof the contributions are clearly presented and that they are not thoroughlyevaluated in the experi", "the authors propose a new variational inference algorithm that handles modelswith deep neural networks and pgm compon", "however it appears that theauthors rely heavily on the work of khan & lin  that actually providesthe algorithm"], "labels": ["SMY", "APC", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i can tell this paper fits inference networks intothe algorithm proposed in khan & lin  which boils down to i using aninference network to generate potentials for a conditionally-conjugatedistribut", "and ii introducing new pgm parameters to decouple the inferencenetwork from the model paramet", "these ideas are a clever solution to workinference networks into the message-passing algorithm of khan & lin", "but i think the authors may be overselling these ideas as a brand new algorithm", "i think if the authors sold the paper as an alternative to johnson et al that doesn't suffer from the implicit gradient problem the paper would fit intothe existing literature bett"], "labels": ["DIS", "DIS", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another concern that i have is that there are a lot of conditiona-conjugacyassumptions baked into the algorithm that the authors only mention at the endof the presentation of their algorithm", "additionally the authors briefly statethat they can handle non-conjugate distributions in the model by just usingconjugate distributions in the variational approxim", "though one could dothis the authors do not adequately show that one should or that one can do thiswithout suffering a lot of error in the posterior approxim", "i think thatwithout an experiment the small section on non-conjugacy should be remov", "finally i found the experimental evaluation to not thoroughly demonstrate theadvantages and disadvantages of the proposed algorithm"], "labels": ["CRT", "DIS", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the algorithm was appliedto the two models originally considered in johnson et al  and theproposed algorithm was shown to attain lower mean-square errors for the twomodel", "the experiments do not however demonstrate why the algorithm isperforming bett", "the experiments do not however demonstrate why the algorithm isperforming bett", "for instance is the johnson et al  algorithmsuffering from the implicit gradi", "it also would have been great to haveconsidered a model that the johnson et al  algorithm would not workwell on or could not be applied to show the added applicability of the proposedalgorithm"], "labels": ["DIS", "DFT", "CRT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also have some minor comments on the paper- there are a lot of typo", "- the first two sentences of the abstract do not really contribute anything to the pap", "what is a powerful model", "what is a powerful algorithm", "- dnn was used in section  without being defin"], "labels": ["CRT", "CRT", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- using p as an approximate distribution in section  is confusing not", "because p was used for the distributions in the model", "summary this paper proposes to use the latent representations learned by a model-free rl agent to learn a transition model for use in model-based rl specifically mct", "the paper introduces a strong model-free baseline win rate ~% in the minirts environment and shows that the latent space learned by this baseline does include relevant game inform", "they use the latent state representation to learn a model for planning which performs slightly better than a random baseline win rate ~%"], "labels": ["CRT", "CRT", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- improvement of the model-free method from previous work by incorporating information about previously observed states demonstrating the importance of memori", "- interesting evaluation of which input features are important for the model-free algorithm such as base hp ratio and the amount of resources avail", "cons- the model-based approach is disappointing compared to the model-free approach", "quality and claritythe paper in general is well-written and easy to follow and seems technically correct", "though i found some of the figures and definitions confusing specifically- the terms for different forward models are not defined eg matchpi matcha etc"], "labels": ["APC", "APC", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i can infer what they mean based on figure  but it would be helpful to readers to define them explicitli", "- in figure b it is not clear to me what the difference between the red and blue curves i", "- in figure  it would be helpful to label which color corresponds to the agent and which to the rule-based ai", "- the caption in figure  is malformat", "- in figure  the baseline of hat{h_t}=h_{t-} seems strange---i would find it more useful for figure  to compare to the performance if the model were not used ie if hat{h_t}=h_t to see how much performance suffers as a result of model error"], "labels": ["SUG", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originalityi am unfamiliar with the minirts environment but given that it is only published in this year's nips and that i couldn't find any other papers about it on google scholar it seems that this is the first paper to compare model-free and model-based approaches in this domain", "however the model-free approach does not seem particularly novel in that it is just an extension of that from tian et al  plus some additional featur", "the idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling see below", "significancei feel the paper overstates the results in saying that the learned forward model is usable in mct", "the implication in the abstract and introduction at least as i interpreted it is that the learned model would outperform a model-free method but upon reading the rest of the paper i was disappointed to learn that in reality it drastically underperform"], "labels": ["DIS", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the baseline used in the paper is a random baseline which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice such as the model-free approach", "this paper presents three observations to understand binary network in courbariaux hubara et ", "my main concerns are on the usage of the given observ", "can the observations be used to explain more recent work", "indeed courbariaux hubara et al  is a good and pioneered work on the binary network"], "labels": ["CRT", "SMY", "SMY", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however as the authors mentioned there are more recent works which give better performance than this on", "for example we can use +  - to approximate the weight", "besides [a] has also shown a carefully designed post-processing binary network can already give very good perform", "so how can the given observations be used to explain more recent work", "how can the given observations be used to improve courbariaux hubara et "], "labels": ["CRT", "DIS", "APC", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors call their findings theori", "from this perspective i wish to see more mathematical analysis rather than just doing experiments and showing some interesting observ", "from this perspective i wish to see more mathematical analysis rather than just doing experiments and showing some interesting observ", "besides giving interesting observations is not good enough", "i wish to see how they can be used to improve binary network"], "labels": ["DIS", "DFT", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["reference[a] network sketching exploiting binary structure in deep cnns cvpr", "summary of paperthe paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic task", "reviewthe paper is clearly written", "it is sometimes difficult to communicate ideas in this area so i appreciate the author's effort in choosing good not", "using an architecture to learn how to split the input find solutions then merge these is novel"], "labels": ["DIS", "SMY", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["previous work in using recursion to solve problems cai  used explicit supervision to learn how to split and recurs", "the ideas and formalism of the merge and partition operations are valuable contribut", "the experimental side of the paper is less strong", "there are good results on the convex hull problem which is promis", "there should also be a comparison to a k-means solver in the k-means section as an additional baselin"], "labels": ["SMY", "APC", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm also not sure tsp is an appropriate problem to demonstrate the method's effect", "perhaps another problem that has an explicit divide and conquer strategy could be used instead", "it would also be nice to observe failure cases of the model", "this could be done by visually showing the partition constructed or seeing how the model learned to merge solut", "this is a relatively new area to tackle so while the experiments section could be strengthened i think the ideas present in the paper are important and worth publish"], "labels": ["APC", "SUG", "SUG", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["questions what is rho on pag", "i assume it is some nonlinearity but this was not specifi", "on page  it says the merge block takes as input two sequ", "i thought the merge block was defined on set", "typos author's names should be enclosed in parentheses unless part of the sent"], "labels": ["QSN", "QSN", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe then should be removed in the sentence scale invariance then exploiting on pag", "this paper proposes a model of structured alignments between sentences as a means of comparing two sentences by matching their latent structur", "overall this paper seems a straightforward application of the model first proposed by kim et al  with latent tree attent", "in section  the formula for pc|x looks wrong c_{ijk} are indicator vari", "but where are the scores for each span"], "labels": ["CRT", "SMY", "SMY", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think it should be c_{ijk} * delta_{ijk} under the summations instead", "in the same section the expression for alpha_{ij} seems to assume that delta_{ijk} = dlta_{ij} regardless of k ie there are no production rule scores transit", "this seems rather limiting can you comment on that", "in the answer selection and nli experiments the proposed model does not beat the sota and is only marginally better than unstructured decomposable attention this is rather disappoint", "the plots in fig  with the marginals on cky charts are not very enlighten"], "labels": ["SUG", "DFT", "QSN", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how do this marginals help solving the nli task", "minor comments- sec  language is inherently tree structured -- this is debat", "- page  laf  bad formatted refer", "this paper aims to provide a continuous variant of cnn", "the main idea is to apply cnn on hilbert maps of the data"], "labels": ["QSN", "DIS", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the data is mapped to a continuous hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix", "a convolutional hilbert layer algorithm is introduced and evaluated on image classification data set", "the paper is well written and provides some new insights on incorporating kernels in cnn", "the kernel matrix in eq  is not symmetric and the kernel function in eq  is not defined over a pair of input", "in this case the projections of the data via the kernel are not necessarily in a rkh"], "labels": ["SMY", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the connection between hilbert maps and rkhs in that sense is not clear in the pap", "the size of a kernel matrix depends on the sample s", "in large scale situations working with the kernel matrix can be computational expens", "it is not clear how this issue is addressed in this pap", "in section  how mu_i and sigma_i are comput"], "labels": ["CRT", "DIS", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how the proposed approach can be compared to convolutional kernel networks nips paper of mairal et ", "this paper provides a survey of attribute-aware collaborative filt", "in particular it classifies existing methods into four different categories according to the representation of the interactions of users items and attribut", "in particular it classifies existing methods into four different categories according to the representation of the interactions of users items and attribut", "furthermore the authors also provide the probabilistic interpretation of the model"], "labels": ["QSN", "SMY", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition preliminary experiments comparing among different categories are also provid", "in addition preliminary experiments comparing among different categories are also provid", "there have existed several works which also provide surveys of attribute-aware collaborative filt", "hence the contribution of this paper is limited although the authors claim two differences between their work and the existing on", "hence the contribution of this paper is limited although the authors claim two differences between their work and the existing on"], "labels": ["SMY", "DIS", "DIS", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the advantages and disadvantages of different categories are not systematically compared and hence the readers cannot get insightful comments and suggestions from this survey", "nin general survey papers are not very suitable for publication at confer", "the main emphasis of this paper is how to add background knowledge so as to improve the performance of nlu specifically qa and nli system", "they adopt the sensible perspective that background knowledge might most easily be added by providing it in text format", "however in this paper the way it is added is simply by updating word representations based on this extra text"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this seems too simple to really be the right way to add background knowledg", "in practice the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma done by linguistic preprocess", "the paper never says exactly how not even if you read the supplementary materi", "this seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!", "as the paper notes this type of sharing is the main delta in this paper from simply using a standard deep lstm which the paper claims to not work on these data sets though i'm not quite sure couldn't be made to work with more tun"], "labels": ["CRT", "APC", "DFT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pp - the main thing of note seems to be that sharing of representations between words with the same lemma which the tables refer to as reading is worth a lot -% in every case rather more than use of background knowledge typically -%", "a note on the qa results the qa results are certainly good enough to be in the range of good system", "but none of the results really push the sota", "the best squad devset results are shown as several percent below the sota", "in the table the triviaqa results are shown as beating the sota and that's fair wrt published work at the time of submiss"], "labels": ["DIS", "APC", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but other submissions show that all of these results are below what you get by running the drqa chen et al  system off-the-shelf on triviaqa so the real picture is perhaps similar to squad especially since drqa is itself now considerably below the sota on squad", "similar remarks perhaps apply to the nli result", "p in the additional nli results it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data and the covarying dimensionality of the word vectors is much smal", "but the fact that the ideas of this paper have quite little or even negative effects when run on the full data with full word vectors on top of the esim model again draws into question whether enough value is being achieved from the world knowledg", "biggest question - are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledg"], "labels": ["CRT", "DIS", "APC", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor notes - the paper was very well written/edit", "the only real copyediting i noticed was in the conclusion and be used u and can be used that rely on u that relies on", "- should reference to manning et al  better be to manning et al  since the context here appears to be ir system", "- on p above sec  what is u", "was that meant to be z"], "labels": ["APC", "APC", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- on p i'm a bit suspicious of the is additional knowledge us", "experiment which trains with knowledge and then tests without knowledg", "it's not surprising that this mismatch might hurt performance even if the knowledge provided no incremental value over what could be gained from standard word vectors alon", "- in the supplementary material the paper notes that the numbers are from the best result from  run", "this seems to me a little less good experimental practice than reporting an average of k runs preferably for k a bit bigger than"], "labels": ["QSN", "DIS", "CRT", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper applies a predictive coding version of the sigma-delta encoding scheme to reduce a computational load on a deep learning network", "whereas neither of these components are new to my knowledge nobody has combined all three of them previ", "the paper is generally clearly written and represents a valuable contribut", "the authors may want to consider the following comments i did not really understand the analogy with stdp in neuroscience because it relies on the assumption that spiking of the post-synaptic neuron encodes the backpropagating error sign", "i am not aware of any evidence for thi"], "labels": ["SMY", "DIS", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given that the authorsu algorithm does not reproduce the sign-flip in the stdp rule i would suggest revise the corresponding part of the pap", "certainly the claim in the discussion ucshow these to be equivalent to a form of stdp u a learning rule first observed in neuroscienceud is inappropri", "if the authorsu encoding scheme really works i feel that they could beef up their experimental results to demonstrate its unqualified advantag", "the paper could benefit greatly from better integration with the existing literatur", "a sigma-delta model of spiking neurons has a long history in neuroscience starting with the work of shin please note that these papers are much older than the ones you cite shin j adaptive noise shaping neural spike encoding and decoding neurocomputing  - p - shin jthe noise shaping neural coding hypothesis a brief history and physiological implications neurocomputing   p -"], "labels": ["SUG", "CRT", "SUG", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["shin jh adaptation in spiking neurons based on the noise shaping neural coding hypothesi", "neural networks  - p -", "more recently the noise-shaping hypothesis has been tested with physiological datachklovskii d b & soudry d", "neuronal spike generation mechanism as an oversampling noise-shaping a-to-d convert", "in advances in neural information processing systems pp - see figure a for the circuit implementing a predictive sigma-delta encoder discussed by youb"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is more appropriate to refer to encoding a combination of the current value and the increment as a version of predictive coding in signal processing rather than the proportional derivative scheme in control theory because the objective here is encoding not control", "also predictive coding has been commonly used in neurosciencesrinivasan mv laughlin sb dubs a  predictive coding a fresh view of inhibition in the retina", "proc r soc lond b biol sci  u pmid", "using leaky neurons for encoding and decoding is standard see egbharioke arjun and dmitri b chklovskii automatic adaptation to fast input changes in a time-invariant neural circuit", "plos computational biology   "], "labels": ["CRT", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the application of these ideas to spiking neurons including learning please see a recent paperdenueve sophie alireza alemi and ralph bourdoukan", "the brain as an efficient and robust adaptive learner neuron   -", "minorpenultimate paragraph of the introduction section ucget getud - get", "first paragraph of the experiments section udso that so thatud - so that", "the paper tries to maintain the accuracy of bits network while uses possibly less than bits weight"], "labels": ["DIS", "DIS", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper misses some more recent reference eg [ab]", "the author should also have a discussion on them", "indeed alexnet is a good seedbed to test binary method", "however it is more interesting and important to test on more advanced network", "so i wish to see a section on testing with resnet and googlenet"], "labels": ["DFT", "DFT", "SUG", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed the authors have commented alexnet with batch-normalization alexnet-bn is the standard model  acceptance that improvements made to accuracy transfer well to more modern architectur", "so please show that", "the paper wants to find a good trade-off on speed and accuraci", "the authors have plotted such trade-off on space vs accuracy in figure b then how about speed vs accuraci", "my concern is that one-bit system is already complicated to impl"], "labels": ["SMY", "DFT", "FBK", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed the authors have discussed their implementation in section  so how their method works in practic", "indeed the authors have discussed their implementation in section  so how their method works in practic", "one example is section  in [courbariaux et al ]", "is trade-off between  to  bits really import", "compared with bits or ternary network the proposed method at most achieving / compression ratio and / speedup based on their t"], "labels": ["SMY", "QSN", "SMY", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is such improvement really import", "reference[a] trained ternary quantization iclr [b]", "extremely low bit neural network squeeze the last bit out with admm", "in this paper the authors study the relationship between training gans and primal-dual subgradient methods for convex optim", "their technique can be applied on top of existing gans and can address issues such as mode collaps"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors also derive a gan variant similar to wgan which is called the approximate wgan", "experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collaps", "this is a strong contribut", "in table  the difference between inception scores for dcgan and this approach seems significant to ignor", "in table  the difference between inception scores for dcgan and this approach seems significant to ignor"], "labels": ["SMY", "SMY", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should explain more poss", "there is a typo in page  u for all these varaints -vari", "my main concern for this paper is that the description of the visual concepts is completely unclear for m", "at some point i thought i did understand it but then the next equation didnt make sense anymor", "if i understand correctly f_p is a representation of *all images* of a specific layer *k* at/around pixel p according to last line of pag"], "labels": ["DFT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that would make sense given that then the dimensions of the vector f_p is a scalar activation value per image for that image in layer k around pixel p", "then f_v is one of the centroids named vc", "however this doesnt seem to be the case given that it is impossible to construct vc activations for specific images from this definit", "so it should be something else but it does not become clear what this f_p i", "this is crucial in order to follow / judge the rest of the pap"], "labels": ["APC", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["still i give it a tri", "section  is the second most important section of the paper where properties of vcs are discuss", "it has a few shortcom", "first iit is unclear why coverage should be = and firerate ~  according to the motivation firerate should equal to coverage that is each pixel f_p is assigned to a single vc centroid", "second vcs tent to occur for a specific class that seems rather a bold statement from a  class  vcs experiment where the class sensitivity is in the order -%"], "labels": ["DIS", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the second experiment which shows the spatial clustering for the car wheel vc is unclear how is the name car wheel assigned to the vc", "also the second experiment which shows the spatial clustering for the car wheel vc is unclear how is the name car wheel assigned to the vc", "that has have to be named after the em process given that em is unsupervis", "finally the cost effectiveness training c how come that the same car wheel as in b is discovered by the em clustering is that coincid", "or is there some form of supervision involv"], "labels": ["QSN", "CRT", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor remarks- table  the reported results of the matching network are different from the results in the paper of viny", "- it is unclear what the influence of the smoothing is and how the smoothing parameter is estimated / set", "- the vcs are introduced for few-shot classification unclear how this is different from previous few-shot methods sect", "- x patches have a plausible size within a x image this is rather large do semantic parts really cover % of the imag", "- how are the networks trained with what objective how validated which training imag"], "labels": ["DIS", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the influence of the layer on the perform", "- influence of the clustering method on vcs eg k-means gaussian von-mises the last one is propos", "on a personal note i've difficulties with part of the writ", "for example the introduction is written rather arrogant not completely the right word sorry for that with a sentence like we have only limited insights into why cnns are effective seems overkill for the main research bodi", "the used visual concepts vcs were already introduced by other works wangt' and is not a novelti"], "labels": ["QSN", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the authors refer to another paper about using vcs for detection which is also under submission somewher", "finally the introduction paragraph of section  is rather bold resembles the learning process of human b", "not so sure that is true and it is not supported by a reference or an experi", "in conclusionthis paper presents a method for creating features from a pre-trained convnet", "it clusters features from a specific pooling layer and then creates a binary assignment between per image extracted feature vectors and the cluster centroid"], "labels": ["DIS", "QSN", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these are used in a -nn classifier and a smoothed naive bayes classifi", "the results show promising result", "yet lack exploration of the model at least to draw conclusions like we address the challenge of understanding the internal visual cues of cnn", "i believe this paper needs to focus on the working of the vcs for few-shot experiments showing the influences of some of the choices layer network layout smoothing clustering etc", "moreover the introduction should be rewritten and the the background section of vcs sect  should be clarifi"], "labels": ["DIS", "APC", "DFT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore i rate the current manuscript as a reject", "after rebuttalthe writing of the paper greatly improved still missing insights see comments below", "therefore i've upgraded my rating and due to better understanding now als my confid", "this paper revisits a subject that i have not seen revisited empirically since the s the relative performance of td and monte-carlo style methods under different values for the rollout length", "furthermore the paper performs controlled experiments using the vizdoom environment to investigate the effect of a number of other environment characteristics such as reward sparsity or perceptual complex"], "labels": ["FBK", "APC", "FBK", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the most interesting and surprising result is that finite-horizon monte carlo performs competitively in most tasks with the exception of problems where terminal states play a big role it does not do well at all on pong! and simple gridworld-type representations and outperforms td approaches in many of the more interesting set", "there is a really interesting experiment performed that suggests that this is the case due to finite-horizon mc having an easier time with learning perceptual represent", "they also show as a side result that the reward decomposition in dosvitskiy & koltun oral presentation at iclr  is not necessary for learning a good policy in vizdoom", "overall i find the paper important for furthering the understanding of fundamental rl algorithm", "however my main concern is regarding a confounding factor that may have influenced the results q_mc uses a multi-headed model trained on different horizon lengths whereas the other models seem to have a single prediction head"], "labels": ["APC", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["may this helped q_mc have better perceptual cap", "a couple of other questions- i couldn't find any mention of eligibility traces - whi", "[overview]this paper proposed a new generative adversarial network called c-gan for generating images in a composite mann", "in c-gan the authors exploited two generators one g is for generating context images and the other one g is for generating semantic cont", "to generate the semantic contents the authors introduced a conditional gan scheme to force the generated images to match the annot"], "labels": ["QSN", "DFT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after generating both parts in parallel they are combined using alpha blending to compose the final imag", "this generated image is then sent to the discrimin", "the experiments were conducted on three datasets mnist svhn and ms-celeba", "the authors showed qualitative results on all three datasets demonstrating that ac-gan could disentangle the context part from the semantic part in an image and generate them separ", "[strenghts]this paper introduced a layered-wise image generation which decomposed the image into two separate parts context part and semantic part"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["corresponding to these two parts are two gener", "to ensure this the authors introduced three strategies adding semantic labels the authors used image semantic labels as the input and then exploited a conditional gan to enforce one of the generators to generate semantic parts of imag", "as usual the label information was added as the input of generator and discriminator as wel", "adding label difference cost the intuition behind this loss is that changing the label condition should merely affect the output of g", "based on this outputs of gc should not change much when flipping the input label"], "labels": ["SMY", "SMY", "APC", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["adding exclusive prior the prior is that the masks of context part m and semantic part m should be exclusive to each oth", "therefore the authors added another loss to reduce the sum of component-wise multiplication between m and m", "decomposing the semantic part from the context part in an image based on a generative model is an interesting problem", "decomposing the semantic part from the context part in an image based on a generative model is an interesting problem", "however to my opinion completing it without any supervision is challenging and meaningless"], "labels": ["SMY", "SMY", "APC", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors proposed a conditional way to generate images composit", "it is an interesting extension of previous works such as kwak & zhang  and yang", "[weaknesses]this paper proposed an interesting and intuitive image generation model", "however there are several weaknesses exist", "there is no quantitative evaluation and comparison"], "labels": ["SMY", "APC", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is no quantitative evaluation and comparison", "from the limited qualitative results shown in fig- we can hardly get a comprehensive sense about the model perform", "the authors should present some quantitative evaluations in the paper which are more persuasive than a number of exampl", "to do that i suggest the authors exploited evaluation metrics such as inception score to evaluate the overall generation perform", "also in yang  the authors proposed adversarial divergence which is suitable for evaluating the conditional gener"], "labels": ["CRT", "DFT", "DFT", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["hence i suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated imag", "this should be a good indicator to show whether the proposed c-gan could generate more realistic images which facilitate the training of a classifi", "the authors should try more complicated datasets like cifar-", "recently cifar- has become a popular dataset as a testbed for evaluating various gan", "it is easy to train since its low resolution but also means a lot since it a relative complicated scen"], "labels": ["SUG", "SMY", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would suggest the authors also run the experiments on cifar-", "the authors did not perform any ablation studi", "apart from several generation results based on c-gan iicould not found any generation results from ablated model", "as such i can hardly get a sense of the effects of different losses and know about the relative performance in the whole gan spectrum", "i strongly suggest the authors add some ablation studi"], "labels": ["SUG", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should at least compare with one-layer conditional gan", "the proposed model merely showed two-layer generation result", "there might be two reasons one is that it is hard to extend it to more layer generation as i know and the other one reason is the inflexible formulation to compose an image in  and formula", "the authors should try some datasets like mnist-two in yang  for demonstr", "please show f m f m separately instead of showing the blending results in fig fig fig fig fig and fig"], "labels": ["SUG", "DFT", "DIS", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see what kind of context image and foreground image c-gan has generated so that i can compare it with previous works like kwak & zhang  and yang", "i did not understand very well the label difference loss in", "reducing the different between g_cz_u z_v z_l and g_cz_u z_v z_l^f seems not be able to force g and g to generate different parts of an imag", "g takes all the duty  can still obtain a lower l_ld", "from my point of view the loss should be added to g to make g less prone to the variation of label inform"], "labels": ["SMY", "DFT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor typos and textual errors in fig should the right generator be g rather than g in  and  please add numbers to the equ", "minor typos and textual errors in fig should the right generator be g rather than g in  and  please add numbers to the equ", "[summary]this paper proposed an interesting way of generating images called c-gan", "it generates images in a layer-wise mann", "to separate the context and semantic part in an image the authors introduced several new techniques to enforce the generators in the model undertake different duti"], "labels": ["QSN", "CRT", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the experiments the authors showed qualitative results on three datasets mnist svhn and celeba", "however as i pointed out above the paper missed quantitative evaluation and comparison and ablation studi", "taking all these into account i think this paper still needs more works to make it solid and comprehensive before being accept", "motivated via talor approximation of the residual network on a local minima this paper proposed a warp operator that can replace a block of a consecutive number of residual lay", "while having the same number of parameters as the original residual network the new operator has the property that the computation can be parallel"], "labels": ["SMY", "DFT", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as demonstrated in the paper this improves the training time with multi-gpu parallelization while maintaining similar performance on cifar- and cifar-", "one thing that is currently not very clear to me is about the rotational symmetri", "the paper mentioned rotated filters but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution lay", "the rotation of the filters as d images or images with depth seem to be quite different from rotating a general n-dim vectors in an abstract euclidean spac", "it would be helpful to make the description here more explicit and clear"], "labels": ["APC", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper adds an interesting twist on top of recent unpaired image translation work", "a domain-level translation function is jointly optimized with an instance-level matching object", "this yields the ability to extract corresponding image pairs out of two unpaired datasets and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered match", "i think this is a promising direction but the current paper has unconvincing results and itus not clear if the method is really solving an important problem yet", "my main criticism is with the experiments and result"], "labels": ["APC", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments focus almost entirely on the setting where there actually exist exact matches between the two image set", "even the partial matching experiments in section  only quantify performance on the images that have exact match", "this is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact match", "it feels rather contrived to focus so much on the datasets with exact matches sinc", "these datasets actually come as paired data and in actual practice supervised translation can be run directli"], "labels": ["SMY", "SMY", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["itus hard to imagine datasets that have exact but unknown matches i welcome the authors to put forward some such scenario", "when exact matches exist simpler methods may be sufficient such as matching edg", "there is no comparison to any such simple baselin", "i think finding analogies that are not exact matches is much more compel", "quantifying performance in this case may be hard and the current paper only offers a few qualitative result"], "labels": ["DIS", "DIS", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["iud like to see far more results and some attempt at a metr", "one option would be to run user studies where humans judge the quality of the match", "the results shown in figure  donut convince me not just because they are qualitative and few but also because ium not sure i even agree that the proposed method is producing better result", "for example the discogan results have some artifacts but capture the texture better in row", "i was also not convinced by the supervised second step in section  given that the first step achieves % alignment accuracy itus no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on % correct data"], "labels": ["SUG", "SUG", "CRT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in other words this section does not really add much new information beyond what we could already infer given that the first stage alignment was so success", "what i think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact match", "for example the shoes and handbags dataset or even better domain adaptation datasets like sim to r", "iud like to see more discussion of why the second stage supervised problem is benefici", "would it not be sufficient to iterate alpha and t iterations enough times until alpha is one-hot and t is simply training against a supervised objective equ"], "labels": ["CRT", "SUG", "DIS", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor comments in the intro it would be useful to have a clear definition of ucanalogyud for the present context", "page  a link should be provided for the putin example as it is not actually in zhu et ", "page  ucweakly supervised mappingud u i wouldnut call this weakly supervised rather iud say itus just another constraint / prior similar to cycle-consistency which was referred to under the ucunsupervisedud sect", "page  and throughout itus hard to follow which variables are being optimized over when", "for example in eqn  it would be clearer to write out the min over optimization vari"], "labels": ["SUG", "SUG", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["page  the maps dataset was introduced in isola et al  not zhu et ", "page  the following sentence is confusing and should be clarifi", "ucthis shows that the distribution matching is able to map source images that are semantically similar in the target domain", "ud page  ucthis shows that a good initialization is important for this task", "ud u isnut this more than initi"], "labels": ["CRT", "DIS", "DFT", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rather removing the distributional and cycle constraints changes the overall objective being optim", "in figure  are the outputs the matched training images or are they outputs of the translation funct", "throughout the paper some citations are missing enclosing parenthes", "the paper proposes to address the quadratic memory/time requirement of relation network rn by sequentially attending via multiple layers on objects and gating the object vectors with the attention weights of each lay", "the proposed model obtains state of the art in babi story-based qa and babi dialog task"], "labels": ["SMY", "QSN", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the model achieves the state of the art in babi qa and dialog i think this is a significant achievement given the simplicity of the model", "- the paper is clearly written", "cons- i am not sure what is novel in the proposed model", "while the authors use notations used in relation network eg 'g' i don't see any relevance to relation network", "rather this exactly resembles end-to-end memory network memnn and gmemnn"], "labels": ["APC", "APC", "CRT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["please tell me if i am missing something but i am not sure of the contribution of the pap", "of course i notice that there are small architectural differences but if these are responsible for the improvements i believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaning", "question- what is the exact contribution of the paper with respect to memnn and gmemnn", "this paper is about low-precision training for convnet", "it proposed a dynamic fixed point scheme that shares the exponent part for a tensor and developed procedures to do nn computing with this format"], "labels": ["CRT", "SUG", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is shown to achieve matching performance against their fp counter-parts with the same number of training iterations on several state-of-the-art convnets architectures on imagenet-k", "according to the paper this is the first time such kind of performance are demonstrated for limited precision train", "potential improvementst  - please define the terms like fprop and wtgrad at the first occur", "- for reference please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline default fp train", "this paper proposed to use affect lexica to improve word embed"], "labels": ["SMY", "APC", "DFT", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they extended the training objective functions of wordvec and glove with the affect inform", "the resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysi", "their experimental results showed that their proposed embeddings outperformed standard wordvec and glov", "in sum it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setup", "some detailed comments are-talthough the use of affect lexica is innovative the idea of extending the training objective function with lexica information is not new"], "labels": ["SMY", "SMY", "SMY", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some detailed comments are-talthough the use of affect lexica is innovative the idea of extending the training objective function with lexica information is not new", "almost the same method was proposed in ka nguyen s schulte im walde nt vu integrating distributional lexical contrast into word embeddings for antonym-synonym distinction in proceedings of acl", "-talthough the lexicons for valence arousal and dominance provide different information their combination did not perform best", "do the authors have any intuition whi", "-tin figure  the authors picked four words to show that valence is helpful to improve glove word beddings it is not convincing enough for m"], "labels": ["DIS", "CRT", "CRT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see to the top k nearest neighbors of each of those word", "this paper presents defense-gan a gan that used at test time to map the input generate an image gz close in msegz x to the input image x by applying several steps of gradient descent of this ms", "the gan is a wgan trained on the train set only to keep the gener", "the goal of the whole approach is to be robust to adversarial examples without having to change the downstream task classifier only swapping in the gz for the x", "+ the paper is easy to follow"], "labels": ["SUG", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ it seems but i am not an expert in adversarial examples to cite the relevant litterature that i know of and compare to reasonably established attacks and defens", "+ simple/directly applicable approach that seems to work experiment", "but- a missing baseline is to take the nearest neighbour of the perturbed x from the training set", "- only mnist-sized images and mnist-like k train set  labels datasets mnist and f-mnist", "- between sec and  sec to reconstruct an mnist-sized imag"], "labels": ["DIS", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["magnet results were very often worse than no defense in table  could you comment on that", "magnet results were very often worse than no defense in table  could you comment on that", "- in white-box attacks it seems to me like l steps of gradient descent on msegz x should be directly extended to l steps of at least fgsm-based attacks at least as a control", "the authors propose first applying dependency parsing to documents then using pairs of words connected via dependency as features in a similarity metr", "while intriguing a lot more work would be required to publish this at iclr"], "labels": ["QSN", "CRT", "DIS", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["namely the following questions need to be answered does using linked-word-pairs truly raise the state of the art", "unlike what is stated in the abstract the experimental results only compare rbms with and without this featur", "rbms are not state-of-the-art in topic modeling therefore itus difficult to assess whether this is help", "if linked words does improve topic modeling why does it do so", "there needs to be some sort of error analysis to show why this idea improves rather than simply stating metr"], "labels": ["QSN", "SMY", "DFT", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are words that are linked via a dependency better than commonly co-occuring word", "experiments need to be done to show that a full dependency parse is actually required rather than simply looking for co-occuring word", "how is this work related to the extensive work in nlp in applying parsing to various task", "a quick search reveals [] probabilistic modeling of dependency parses to create bayesian topic models directly and [] creating a semantic vector space from a dependency parse i suspect there are oth", "citations in [] could be a good place to start"], "labels": ["QSN", "SUG", "QSN", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["citations in [] could be a good place to start", "can the selection of word pairs be done automatically from data rather than pre-computed with a known dependency pars", "after all this is submitted to the international conference on learning representations --- feature engineering papers can easily be published at emnlp icml etc an excellent iclr paper would show some way to either a use dependency parsing only at training time to provide a hint or b not require dependency parsing at al", "a few suggestions for experimentsa i would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation just using logtf-idf as a distance metr", "by stripping away more advanced modeling that could reveal whether the dependency bi-gram has util"], "labels": ["DIS", "QSN", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b the authors may wish to consider applying lsa to both bag of words and dependency-bigrams using logtf-idf weighting for both", "from what iuve seen logtf-idf lsa seems to perform about as well as lda", "plain lsa takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on lsa at al", "c reiterating point  above to really show whether the power of the dependency parse is being used i would strongly suggest doing a null experiment with co-occuring nearby word", "references[] boyd-graber j l & blei d m  syntactic topic models in advances in neural information processing systemspp -[] paduf s and lapata m  dependency-based construction of semantic space models computational linguistics  pp-"], "labels": ["SUG", "SMY", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a new space for reasoning about human ident", "it proposes a new dataset based on an artist's work and compares existing methods in terms of the realism of the synthetic faces they can cr", "pros+ the results are very pleasing visu", "+ the authors show that one of the existing methods can fairly successfully fool humans to believe its synthetic results are actual human fac", "cons- there is no new methodology propos"], "labels": ["SMY", "SMY", "APC", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- if the main contribution is the dataset perhaps the claim that it is uniquely diverse could be justified with some quantitative arguments / statistics comparing to other dataset", "- since there are existing methods to generate images from a textual description eg zhang iccv  stackgan fig  merits a comparison to thos", "- it would have been convincing to see an experiment showing actual use of the proposed method for navigating the face space eg for finding criminals based on a descript", "questions- inventing plausible fine details while preserving identity -- since identity is created and there is no ground truth where does the line between fine detail and new identity li", "- some notation is not defined in the equation on the last pag"], "labels": ["SUG", "APC", "SUG", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors describe a method for performing query completion with error correction using a neural network that can achieve real-time perform", "the method described uses a character-level lstm and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain error", "details are also given on how the authors are able to achieve realtime complet", "overall itus nice a nice study of the query completion appl", "the paper is well explained and itus also nice that the runtime is shown for each of the algorithm block"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could imagine this work giving nice guidelines for others who also want to run query completion using neural network", "the final dataset is also a good size m search queri", "my major concerns are perhaps the fit of the paper for iclr as well as the thoroughness of the final experi", "much of the paper provides background on lstms and edit distance which granted are helpful for explaining the idea", "but much of the realtime completion section is also standard practice eg maintaining previous hidden states and grouping together the different g"], "labels": ["APC", "APC", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so the paper feels directed to an audience with less background in neural net lm", "secondly the experiments could have more thorough/stronger baselin", "i donut really see why we would try stochastic search and expected to see more analysis of how performance was impacted as the number of errors increased even if errors were introduced artificially and expected analysis of how different systems scale with varying amounts of data", "the fact that  hidden dimension worked best while  overfit was also surprising as character language models on datasets such as penn treebank with only  million words use hidden states far larger than that for  lay", "more regularization requir"], "labels": ["CRT", "SUG", "CRT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the paper the authors discuss several gan evaluation metr", "specifically the authors pointed out some desirable properties that gans evaluation metrics should satisfi", "for those properties raised the authors experimentally evaluated whether existing metrics satisfy those properties or not", "section  summarizes the results which concluded that the kernel mmd and -nn classifier in the feature space are so far recommended metrics to be us", "i think this paper tackles an interesting and important problem what metrics are preferred for evaluating gan"], "labels": ["SMY", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the authors showed that inception score which is one of the most popular metric is actually not preferred for several reason", "the result comparing data distributions and the distribution of the generator would be the preferred choice that can be attained by kernel mmd and -nn classifier seems to be reason", "this would not be a surprising result as the ultimate goal of gan is mimicking the data distribut", "however the result is supported by exhaustive experiments making the result highly convinc", "overall i think this paper is worthy for acceptance as several gan methods are proposed and good evaluation metrics are needed for further improvements of the research field"], "labels": ["CRT", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper explores a new approach to optimal transport", "contributions include a new dual-based algorithm for the fundamental task of computing an optimal transport coupling the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation learning a monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another and a plethora of supporting theoretical result", "the paper presents significant novel work in a straightforward clear and engaging way", "it represents an elegant combination of ideas and a well-rounded combination of theory and experi", "i should mention that i'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual-based algorithm stands in relation to existing algorithm"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["major commentsno major flaw", "the introduction is particular well written as an extremely clear and succinct introduction to optimal transport", "minor commentsin the introduction for vaes it's not the case that fx matches the target distribut", "there are two levels of sampling of the latent x and of the observed value given the lat", "the second step of sampling is ignored in the description of vaes in the first paragraph"], "labels": ["APC", "APC", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the comparison to previous work please explicitly mention the emd algorithm since it's used in the experi", "it would've been nice to see an experimental comparison to the algorithm proposed by arjovsky et al  since this is mentioned favorably in the introduct", "in  r is not defin", "suggest adding a forward reference to", "in section  it would be helpful to cite a reference to support the form of dual problem"], "labels": ["DFT", "SUG", "DFT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["perhaps the authors have just done a good job of laying the groundwork but the dual-based approach proposed in section  seems quite natur", "is there any reason this sort of approach wasn't used previously even though this vein of thinking was being explored for example in the semi-dual algorithm", "if so it would interesting to highlight the key obstacles that a naive dual-based approach would encounter and how these are overcomein algorithm  it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net", "there are many terms in r_e in  which appear to have no effect on optimization such as ax and by in the denominator and -  it seems like r_e boils down to just the entropi", "the definition of f_epsilon is made unnecessarily confusing by the omission of x and y as argu"], "labels": ["APC", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be great to mention very briefly any helpful intuition as to why f_epsilon and h_epsilon have the forms they do", "in the discussion of table  it would be helpful to spell out the differences between the different bary proj algorithms since i would've expected emd sinkhorn and alg  with r_e to all perform similarli", "in figure  some of the samples are quite non-phys", "is their any helpful intuition about what goes wrong", "what cost is used for generative modeling on mnist"], "labels": ["SUG", "SUG", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for generative modeling on mnist d vector is less clear than -dimensional vector", "the fact that the variable d is equal to  is not explicitly st", "it seems a bit strange to say the property we gain compared to other generative models is that our generator is a nearly optimal map wrt this cost as if this was an advantage of the proposed method since arguably there isn't a really natural cost in the generative modeling case unlike in the domain adaptation case the latent variable seems kind of conceptually distinct from observation spac", "appendix a isn't referred to from the main text as far as i could tel", "just merge it into the main text"], "labels": ["DIS", "DIS", "APC", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors present an approach to implement deep learning directly on sparsely connected graph", "previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization using this approach sparse networks can be trained efficiently online allowing for fast and flexible learn", "further investigation is necessary to understand the full implications of the two main conceptual changes introduced here signed connections that can disappear and random walk in parameter spac", "but the initial results are quite promis", "it would also be interesting to understand more fully how performance scales to larger network"], "labels": ["SMY", "SMY", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the target connectivity could be pushed to a very sparse limit where only a fixed number of connections were added with each additional neuron then this could significantly shape how these networks are trained at very large scal", "if the target connectivity could be pushed to a very sparse limit where only a fixed number of connections were added with each additional neuron then this could significantly shape how these networks are trained at very large scal", "perhaps the heuristics for initializing the connectivity matrices will be insufficient but could these be improved in further work", "as a last minor comment the authors should specify explicitly what the shaded areas are in fig bc", "as a last minor comment the authors should specify explicitly what the shaded areas are in fig bc"], "labels": ["SUG", "DIS", "QSN", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is interest", "but needs more work and should provide clear and fair comparison", "per se the model is incrementally new", "but it is not clear what the strengths are and the presentations needs to be done more car", "but it is not clear what the strengths are and the presentations needs to be done more car"], "labels": ["APC", "CRT", "APC", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in detail- please fix several typos throughout the manuscript and have a native speaker and preferably an asr expert proofread the pap", "introduction- please define hmm/gmm model and other abbreviations that will be introduced later it cannot be assumed that the reader is familiar with all of them asg is used before it is defin", "- the standard units that most asr systems use can be called senones and they are context dependent sub-phonetic units see http//sslieewashingtonedu/~mhwang/ not phonetic st", "also the units that generate the alignment and the units that are trained on an alignment can be different i can use a system with  states to write alignments for a system with  states - this needs to be correct", "- when introducing cnns please also cite waibel and tdnns - they are *the same* as -d cnns and predate them"], "labels": ["CRT", "CRT", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they have been extended to -d later on spatio-temporal tdnn", "- the most influential deep learning paper here might be seide li yu interspeech  on cd-dnn-hmms rather than overview articl", "- many papers get rid of the hmm pipeline i would add https//arxivorg/abs/ which predates deep speech", "- what is a sequence-level variant of ctc", "ctc is a sequence training criterion"], "labels": ["DIS", "DIS", "DIS", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the reason that deep speech  is better on noisy test sets is not only the fact they trained on more data but they also trained on noisy matched data", "- how is this an end-to-end approach if you are using an n-gram language model for decod", "architecture- mfsc are log filterbank", "- d cnns would be tdnn", "- figure  can you plot the various transition types normalized un-normalized  in the plot"], "labels": ["QSN", "QSN", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not sure if it would help but it might", "- maybe provide a reference for hmm/gmm and em forward backward train", "- mmi was also widely used in hmm/gmm systems not just nn system", "- the blank states do *not* model garbage frames if one wants to interpret them they might be said to model non-stationary frames between ctc peaks but these are different from silence garbage nois", "- what is the relationship of the presented asg criterion to mmi"], "labels": ["DIS", "SUG", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the form of equation  looks like an mmi criterion to m", "experiments- many of the previous comments still hold please proofread", "- you say there is no complexity incrase when using logadd - how do you measure this number of operations is there an implementation of logadd that is absolutely as fast as add", "- there is discussion as to what i-vectors model speaker or environment information - i would leave out this discussion entirely here it is enough to mention that other systems use adaptation and maybe re-run an unadapted baselien for compars", "- there are techniques for incremental adaptation and a constrained mllr feature adaptation approaches that are very eficient if one wnats to get into thi"], "labels": ["QSN", "APC", "QSN", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it may also be interesting to discuss the role of the language model to see which factors influence system perform", "- some of the other papers might use data augmentation which would increase noise robustness did not check but this might explain some of the results in t", "- i am confused by the references in the caption of table  - surely the waibel reference is meant to be for tdnns and should appear earlier in the paper while p-norm came later povey used it first for asr i think and is related to maxout", "- can you also compare the training tim", "conculsion- can you show how your approach is not so computationally expensive as rnn based approaches either in terms of flops or measured tim"], "labels": ["SUG", "DIS", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper abstracts two recently-proposed rnn variants into a family of rnns called the linear surrogate rnns which satisfy  blelloch's criteria for parallelizable sequential comput", "the authors then propose an efficient parallel algorithm for this class of rnns which produces speedups over the existing implements of quasi-rnn sru and lstm", "apart from efficiency results the paper also contributes a comparison of model convergence on a long-term dependency task due to hochreiter and schmidhub", "a novel linearized version of the lstm outperforms traditional lstm on this long-term dependency task and raises questions about whether rnns and lstms truly need the nonlinear structur", "the paper is written very well with explanation as opposed to obfuscation as the go"], "labels": ["SMY", "APC", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["linear surrogate rnns is an important concept that is useful to understand rnn variants today and potentially other future novel architectur", "the paper provides argument and experimental evidence against the rotation used typically in rnn", "while this is an interesting insight and worthy of further discussion such a claim needs backing up with more large-scale experiments on real dataset", "while the experiments on toy tasks is clearly useful the paper could be significantly improved by adding experiments on real tasks such as language model", "in this paper the authors propose a new approach for learning underlying structure of visually distinct gam"], "labels": ["SMY", "SMY", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed approach combines convolutional layers for processing input images asynchronous advantage actor critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of gam", "the network architecture is suitably described and seems reasonable to learn simultaneously similar games which are visually distinct", "however the authors do not explain how this architecture can be used to do the domain adapt", "indeed if some games have been learnt by the proposed algorithm the authors do not precise what modules have to be retrained to learn a new gam", "this is a critical issue because the experiments show that there is no gain in terms of performance to learn a shared embedding manifold see da-drl versus baseline in figur"], "labels": ["SMY", "APC", "DFT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if there is a gain to learn a shared embedding manifold which is plausible this gain should be evaluated between a baseline that learns separately the games and an algorithm that learns incrementally the gam", "moreover in the experimental setting the games are not similar but simply the sam", "my opinion is that this paper is not ready for publ", "the interesting issues are referred to future work", "the problem of numerical instability in applying sgd to soft-max minimization is the motiv"], "labels": ["SUG", "CRT", "FBK", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would have been helpful if the authors could have made a formal stat", "since the main contributions are two algorithms for stable sgd it is not clear how one can formally say that they are stable for this a formal problem statement is necessari", "since the main contributions are two algorithms for stable sgd it is not clear how one can formally say that they are stable for this a formal problem statement is necessari", "the discussion around eq  is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithm", "the proposed algorithms are variants of sgd but it is not clear why they should converge faster than existing strategi"], "labels": ["SUG", "SUG", "CRT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some parts of the text are badly written see for example the following linesee paragraph before sec since the converge of sgd isinversely proportional to the magnitude of its gradients lacoste-julien et al  we expect theformulation to converge faster which could have shed more light on the matt", "the title is also misleading in using the word exact", "i have understand it correct the proposed sgd method solves the optimization problem to an additive error", "nin summary the algorithms are novel variants of sgd", "but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are miss"], "labels": ["CRT", "CRT", "DIS", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the choice of word exact is also not clear", "qualitythis is a very clear contribution which elegantly demonstrates the use of extensions of gan variants in the context of neuroimag", "claritythe paper is well-written", "methods and results are clearly describ", "the authors state significant improvements in classification using generated data"], "labels": ["DFT", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these claims should be substantiated with significance tests comparing classification on standard versus augmented dataset", "originalitythis is one of the first uses of gans in the context of neuroimag", "significance the approach outlined in this paper may spawn a new research direct", "proswell-written and original contribution demonstrating the use of gans in the context of neuroimag", "consthe focus on neuroimaging might be less relevant to the broader ai commun"], "labels": ["SUG", "APC", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presented interesting ideas to reduce the redundancy in convolution kernel", "they are very close to existing algorithm", "tthe sw-sc kernel figure  a is an extension of the existing shaped kernel figure  c", "tthe cw-sc kernel figure  c is very similar to interleaved group convolut", "the cw-sc kernel can be regarded as a redundant version of interleaved group convolutions []"], "labels": ["APC", "APC", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this pap", "i would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this pap", "[] interleaved group convolutions ting zhang guo-jun qi bin xiao and jingdong wang iccv", "http//openaccessthecvfcom/content_iccv_/papers/zhang_interleaved_group_convolutions_iccv__paperpdf", "this paper proposes to re-formulate the gan saddle point objective for a logistic regression discriminator as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression where the dual function can be obtained in closed form when the discriminator is linear"], "labels": ["DFT", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems figure  while a gradient approach often works well for a minimization formul", "the dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the hessian is y_i y_j x_i x_j thus highlighting the pairwise similarities between the points x_i & x_j here the labels represent whether the point x comes from the samples a from the target distribution or b from the proposal distribut", "this paper then compare this objective with the mmd distance between the samples a & b", "it points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the mmd distance an interesting analogy but also showing the limited power of the adversarial logistic distance for getting good generating distributions given eg that the mmd has been observed in the past to perform poorly for face generation [dziugaite et al uai ]", "from this analogy one could expect the method to improves over mmd but not necessarily significantly in comparison to an approach which would use more powerful discrimin"], "labels": ["APC", "DIS", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation with experiments on a visual adaptation task", "they observe better performance for their approach in comparison to adda improved wgan and mmd when restricting the discriminators to be a linear classifi", "== evaluation i found this paper quite clear to read and enjoyed reading it", "the observations are interesting despite being on the toyish sid", "i am not an expert on gans for domain adaptation and thus i can not judge of the quality of the experimental comparison for this application but it seemed reasonable apart for the restriction to the linear discriminators which is required by the framework of this pap"], "labels": ["DIS", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one concern about the paper but this is an unfortunate common feature of most gan papers is that it ignores the vast knowledge on saddle point optimization coming from the optimization commun", "the instability of a gradient method on non-strongly convex-concave saddle point problems like the bilinear form of figure  is a well-known property and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this pap", "moreover the transformation to the minimization form crucially required the closed form computation of the dual function with w* just defined above equation  and this is limited to linear discriminators  thus ruling out the use of the proposed approach to more powerful discriminators like deep neural net", "thus the significance appears a bit limited to m", "== other comments note that da b'_theta is *equal* to min_alpha max_w    above equation  it is not just an upper bound"], "labels": ["CRT", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a standard result coming from the fact that the fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective a", "see eg section  of [collins et al jmlr ] this is for the more general multiclass logistic regression problem but  is just the binary special case of equation  in the [collins  ] refer", "and note that the wu defined in this reference is the lambda*w*alpha optimal relationship defined in this paper but without the /lambda factor because of just slightly different writing the point though is that strong duality holds there and thus one really has equ", "[collins et al jmlr ] michael collins amir globerson terry koo  xavier carreras peter l bartlett exponentiated gradient algorithms for conditional random fields and max-margin markov networks  jmlr  [dziugaite et al uai ] gintare karolina dziugaite daniel m roy and zoubin ghahramani training generative neural networks via maximum mean discrepancy optimization in uai", "language models are important components to many nlp task"], "labels": ["APC", "DIS", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the rnn's hidden st", "this paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmax", "the use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factor", "pros--the paper is very well written and easy to follow", "the ideas build up on each other in an intuitive way"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--the idea behind the paper is novel translating language modeling into a matrix factorization problem is new as far as i know", "--the maths is very rigor", "--the experiment section is thorough", "cons--to claim sota all models need to be given the same capacity same number of paramet", "cons--to claim sota all models need to be given the same capacity same number of paramet"], "labels": ["APC", "APC", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in table  the baselines have a lower capac", "this is an unfair comparison", "--i suspect the proposed approach is slower than the baselin", "there is no mention of computational cost", "there is no mention of computational cost"], "labels": ["CRT", "CRT", "CRT", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["reporting that would help interpret the numb", "the sota claim might not hold if baselines are given the same capac", "but regardless of this the paper has very strong contributions and deserves acceptance at iclr", "this paper revisits an interesting and important trick to automatically adapt the steps", "they consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the steps"], "labels": ["SUG", "CRT", "FBK", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["such simple trick alleviates the effort in tuning stepsize and can be incorporated with popular stochastic first-order optimization algorithms including sgd sgd with nestrov momentum and adam surprisingly it works well in practic", "although the theoretical analysis is weak that theorem  does not reveal the main reason for the benefits of such trick considering their performance i vote for accept", "but before that there are several issues need to be address", "the derivation of the update of alpha relies on the expectation formul", "i would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick"], "labels": ["APC", "FBK", "DIS", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the derivation of the multiplicative rule of hd relies on a reference i cannot find please include this part for self-contain", "as the authors claimed the maclaurin etal  is the most related work however they are not compared in the experi", "moreover the empirical comparisons are only conducted on mnist", "to be more convincing it will be good to include such competitor and comparing on practical applications on cifar/ and imagenet", "minors in the experiments results figures after adding the new trick the sgd algorithms become more stable ie the variance diminish"], "labels": ["SUG", "CRT", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could you please explain why such phenomenon happen", "the idea is clearly stated but lacks some details and i enjoyed reading the pap", "i understand the difference between [kos+] and the proposed scheme but i could not understand in which situation the proposed scheme works bett", "from the adversary's standpoint it would be easier to manipulate inputs than latent vari", "on the other hand i agree that sample-independent perturbation is much more practical than sample-dependent perturb"], "labels": ["QSN", "APC", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  the attack methods # and # should be detailed mor", "i could not imagine how vae and t are trained simultan", "in section  the authors listed a couple of loss funct", "how were these loss functions are combin", "the final optimization problem that is used for training of the propose vae should be formally defin"], "labels": ["SUG", "DIS", "SMY", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the detailed specification of the vae should be detail", "from figures in figure  and figure  i could see that the proposed scheme performs successfully in a qualitative mann", "however it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselin", "for example can the proposed scheme can be compared with [kos+] or some other sample-dependent attack", "also can you experimentally show that attacks on latent variables are more powerful than attacks on input"], "labels": ["SUG", "APC", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents an interesting extension to snell et al's prototypical networks by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings mean", "this paper presents an interesting extension to snell et al's prototypical networks by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings mean", "uncertainty may be particularly important in the few-shot learning case this paper examines when it is helpful to extract more information from limited number of input sampl", "however several important concepts in the paper are not well explained or motiv", "for example it is a bit misleading to use the word covariance throughout the paper when the best model only employs a scalar estimate of the vari"], "labels": ["SMY", "APC", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a related and potentially technical problem is in computing the prototype's mean and variance section  eq  and  are not well motivated and the claim of optimal under eq is not explain", "more importantly eq  and  do not use any covariance information off-diagonal elements of s --- as a result the model is likely to ignore the covariance structure even when using full covariance estim", "the distance function eq  is d mahalanobis distance instead of linear euclidean dist", "while the paper emphasises the importance of the form of loss function the loss function used in the model is given without explanation and using cross-entropy over distances looks hacki", "in addition the experiments are too limited to support the claimed benefits from encoding uncertainti"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since the accuracies on omniglot data from recent models are already close to perfect it is unclear whether the marginally improved number reported here is signific", "in addition more analysis may better support existing claim", "for example showing subsampled images indeed had higher uncertainty rather than only the histogram for all data point", "pros-interesting problem and interesting direct", "the authors present  architectures for learning representations of programs from execution trac"], "labels": ["CRT", "CRT", "CRT", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the variable trace embedding the input to the model is given by a sequence of variable valu", "the state trace embedding combines embeddings for variable traces using a second recurrent encod", "the dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the gru to compute the new hidden state of a vari", "the authors evaluate their architectures on the task of predicting error patterns for programming assignments from microsoft devx an introduction to c# offered on edx and problems on the microsoft codehunt platform", "they additionally use their embeddings to decrease the search time for the sarfgen program repair system"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a fairly strong pap", "the proposed models make sense and the writing is for the most part clear though there are a few places where ambiguity aris", "- the variable evidence in equation  is never defin", "- the authors refer to predicting the error patterns but again don't define what an error pattern i", "the appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors is this correct"], "labels": ["APC", "APC", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it is not immediately clear from figures  and  that the architectures employed are in fact recurr", "- figure  seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable is this correct", "nassuming that the authors can address these clarity issues i would in principle be happy for the paper to appear", "this paper propose a simple method for guarding trained models against adversarial attack", "the method is to prune the networkus activations at each layer and renormalize the output"], "labels": ["CRT", "QSN", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["itus a simple method that can be applied post-training and seems to be effect", "itus a simple method that can be applied post-training and seems to be effect", "itus a simple method that can be applied post-training and seems to be effect", "nthe paper is well written and easily to follow", "method description is clear"], "labels": ["SMY", "APC", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the analyses are interesting and done wel", "i am not familiar with the recent work in this area so can not judge if they compare against sota methods but they do compare against various other method", "could you elaborate more on the findings from fig c seems that  the dense model perform best against randomly perturbed imag", "could you elaborate more on the findings from fig c seems that  the dense model perform best against randomly perturbed imag", "could you elaborate more on the findings from fig c seems that  the dense model perform best against randomly perturbed imag"], "labels": ["APC", "FBK", "SMY", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["would be good to know if the authors have any intuition why is that the cas", "would be good to know if the authors have any intuition why is that the cas", "there are some interesting analysis in the appendix against some other methods it would be good to briefly refer to them in the main text", "i would be interested to know more about the intuition behind the proposed method", "i would be interested to know more about the intuition behind the proposed method"], "labels": ["SUG", "QSN", "SUG", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method", "also would like to see some notes about computation complexity of sampling multiple times from a larger multinomi", "again i am not familiar about different kind of existing adversarial attacks the paper seem to be mainly focus on those from goodfellow et ", "would be good to see the performance against other forms of adversarial attacks as well if they exist", "would be good to see the performance against other forms of adversarial attacks as well if they exist"], "labels": ["SUG", "DFT", "FBK", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next stateus valu", "the constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next stateus valu", "it is shown empirically that the constrained update does not diverge on bairdus counter example and improves performance in a grid world domain and cart pole over dqn", "this paper is reasonably read", "the derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potenti"], "labels": ["SMY", "SMY", "APC", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key issue is that the justification for the constrained gradients is lack", "what is the effect in terms of convergence in modifying the gradient in this way", "it seems highly problematic to simply remove a whole part of the gradient to reduce effect on the next st", "for example if we are minimizing the changes our update will make to the value of the next state what would happen if the next state is equivalent to the current state or equivalent in our feature spac", "in general when we project our update to be orthogonal to the maximal change of the next states value how do we know it is a valid direction in which to upd"], "labels": ["DFT", "QSN", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would have liked some analysis of the convergence results for td learning with this constraint or some better intuition in how this effects learn", "at the very least a mention of how the convergence proof would follow other common proofs in rl", "this is particularly important since gtd provides convergent td updates under nonlinear function approximation the role for a heuristic constrained td algorithm given convergent alternatives is not clear", "for the experiments other baselines should be included particularly just regular q-learn", "the primary motivation comes from the use of a separate target network in dqn which seems to be needed in atari though i am not aware of any clear result that demonstrates why rather just from informal discuss"], "labels": ["SUG", "QSN", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since you are not running experiments on atari here it is invalid to simply assume that such a second network is need", "a baseline of regular q-learning should be included for these simpler domain", "the results in bairdus counter example are discouraging for the new constraint", "because we already have algorithms which better solve this domain why is your method advantag", "the point of showing your algorithm not solve bairdus counter example is unclear"], "labels": ["CRT", "SUG", "CRT", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are also quite a few correctness errors in the paper and the polish of the plots and language needs work as outlined below", "there are several mistakes in the notation and background sect", "ucif we consider td-learning using function approximation the loss that is minimized is the squared td error", "uc this is not true rather td minimizes the mean-squared project bellman error", "further l_td is strangely defined why a squared norm for a scalar valu"], "labels": ["SUG", "CRT", "SMY", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the definition of v and delta_td wrt to v seems unnecessary since you only use q", "as an additional somewhat unimportant point the td-error is usually defined as the negative of what you hav", "in the function approximation case the value function and q functions parameterized by theta are only approximations of the expected return", "defining the loss wrt the state and taking the derivative of the state wrt to theta is a bit odd", "likely what you meant is the q function at state s_t"], "labels": ["CRT", "DIS", "SMY", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also are ignoring the gradient of the value at the next step", "if so this further means that this is not a true gradi", "there is a lot of white space around the plots which could be used for larger more clear figur", "the lack of labels on the plots makes them hard to understand at a glance and the overlapping lines make finding certain algorithmus performance much more difficult", "i would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plot"], "labels": ["QSN", "DIS", "SUG", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["examples of odd language choicest-tucthe idea also does not immediately scale to nonlinear function approxim", "bhatnagar et al  propose a solution by projecting the error on the tangent plane to the function at the point at which it is evalu", "uc - the paper you give exactly solves for the nonlinear function approximation cas", "what do you mean does not scale to nonlinear function approxim", "also maei is the first author on this pap"], "labels": ["CRT", "SUG", "SMY", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["t-tucthough they do not point out this insight as we haveud - this seems to be a bit overreach", "- ucthe gradient at s_{t+} that will change the value the mostud  - this is too colloqui", "i think you simply mean the gradient of the value function for the given s_t but its not clear", "this paper proposed a nmt system that expands each sentence pair to two groups of similar sent", "the idea of using similar sentence pairs as cluster-to-cluster translation is interest"], "labels": ["CRT", "DIS", "DIS", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental results seem promising but the presentation can be improv", "some parts of the paper are hard to read", "major what is the model/baseline in t", "what is the intuition in adding target cluster entropy in eq", "in the adaptive cluster i am a bit confused on the target of the parametric models where are x y of px|x* py|y* from"], "labels": ["APC", "DFT", "QSN", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the adaptive cluster i am a bit confused on the target of the parametric models where are x y of px|x* py|y* from", "is it from pretrained models it wasn't clear until i read the algorithm also why are px|x* called target cluster and py|y* called source clust", "is it from pretrained models it wasn't clear until i read the algorithm also why are px|x* called target cluster and py|y* called source clust", "in section  the name cluster is a bit confusing with the one in section  what's the relationship the symbols cy* and cx* are not used afterward", "in the conclusion it claims the system is efficient in helping current model what do you mean by effici"], "labels": ["QSN", "DFT", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the improvements of wmt are relatively small does it mean the proposed methods are not beneficial when there are large amounts of sentence pair", "what's the reward used in the experi", "in the monte-carlo sampling how many pairs are sampl", "minor  in table  where is sigma defin", "the notation d for a dataset in section  is confusing with d in system d"], "labels": ["QSN", "QSN", "QSN", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there is some redundancy between systems a b c d and in the algorithm  i wonder whether it can be simplifi", "in section  backward nmt x|y - backward nmt px|i", "it will be great to show detailed derivation for example from eq  to eq", "some recent results on wmt de-en are missing such ashttps//arxivorg/abs/", "the idea of using gans for outlier detection is interesting and the problem is relev"], "labels": ["CRT", "DFT", "SUG", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i have the following concerns about the quality and the significance- the proposed formulation in equation  is question", "the authors say that this is used to generate outliers and since it will generate inliers when convergence the authors propose the technique of early stopping in section  to avoid converg", "however then what is learned though the proposed formul", "since this approach is not straightforward more theoretical analysis of the proposed method is desir", "- in addition to the above point i guess the expectation is needed as the original formulation of gan"], "labels": ["CRT", "SMY", "QSN", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defin", "- in experiments although the authors say lots of datasets are used only two datasets are used which is not enough to examine the performance of outlier detection method", "moreover outliers are artificially generated in these datasets hence there is no evaluation on pure real-world dataset", "to achieve the better quality of the paper i recommend to add more real-world datasets in experi", "- as discussed in section  there are already many outlier detection methods such as distance-based outlier detection methods but they are not compared in experi"], "labels": ["DIS", "CRT", "CRT", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data this is not always correct", "please see the paper  -- zimek a schubert e kriegel h-p a survey on unsupervised outlier detection in high-dimensional numerical data statistical analysis and data mining   this paper shows that the performance gets even better for higher dimensional data if each feature is relev", "i recommend to add some distance-based outlier detection methods as baselines in experi", "- since parameter tuning by cross validation cannot be used due to missing information of outliers it is important to examine the sensitivity of the proposed method with respect to changes in its parameters a_new lambda and oth", "otherwise in practice how to set these parameters to get better results is not obvi"], "labels": ["CRT", "SUG", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the clarity of this paper is not high as the proposed method is not well explain", "in particular please mathematically formulate each proposed technique in sect", "* since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough the originality is not high", "minor comments- p l in the third paragraph architexture - architectur", "- what does cor of corgan mean"], "labels": ["CRT", "SUG", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after revisionthank you to the authors for their response and revis", "although the paper has been improved i keep my rating due to the insufficient experimental evalu", "the authors investigate a modified input layer that results in color invariant network", "the proposed methods are evaluated on two car dataset", "it is shown that certain color invariant input layers can improve accuracy for test-images from a different color distribution than the training imag"], "labels": ["APC", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed assumptions are not well motivated and seem arbitrari", "why is using a permutation of each pixels' color a good idea", "the paper is very hard to read", "the message is unclear and the experiments to prove it are of very limited scop", "ie one small dataset with the only experiment purportedly showing generalization to red car"], "labels": ["CRT", "QSN", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some examples of specific issues- the abstract is almost incomprehensible and it is not clear what the contributions ar", "- some references to figures are missing the figure number eg  first paragraph", "- it is not clear how many input channels the color invariant functions use eg p does it use only one channel and hence has fewer paramet", "- are the training and testing sets all disjoint sec - at random points figures are put in the appendix even though they are described in the paper and seem to show key results eg tested on nored-test", "- sec  the explanation for why the accuracy drops for all models is not clear"], "labels": ["CRT", "DFT", "QSN", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is it because the total number of training images drop", "if that's the case the whole experimental setup seems flaw", "- sec  the authors refer to the order net beating the baseline however from fig  right most it appears as if all models beat the baselin", "in the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set", "is that fig  @%"], "labels": ["QSN", "CRT", "DIS", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the baseline seems to be best performing on all cars and non-red carsin order to be at an appropriate level for any publication the experiments need to be much more general in scop", "the authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual rel", "they then evaluate several existing network architectures on these tasks and show that results are not as impressive as others might have assumed they would b", "they show that while recent approaches eg relational networks can generalize reasonably well on some tasks these results do not generalize as well to held-out-object scenarios as might have been assum", "clarity  the paper is fairly clearly written"], "labels": ["SUG", "SMY", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think i mostly followed it", "quality  i'm intrigued by but a little uncomfortable with the generalization metrics that the authors us", "the authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image condit", "the authors state that    the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem not over multiple splits of the same dataset", "taken literally this would rule out a lot of modern machine learning even obviously very good work"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the other hand it's clear that at some point generalization needs to occur in testing ability to understand relationship", "i'm a little worried that it's in the eye of the beholder whether a given generalization should be expected to work or not", "there are essentially three scenarios of generalization discussed in the paper        a various generalizations of image parameters in the psvrt dataset", "b various hold-outs of the image parameters in the sort-of-clevr dataset", "c from sort-of-clevr objects to psvrt bit pattern"], "labels": ["APC", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the result that existing architectures didn't do very well at these generalizations especially b and c *may* be important -- or it may not", "perhaps if cnn+rn were trained on a quite rich real-world training set with a variety of real-world three-d objects beyond those shown in sort-of-clevr it would generalize to most other situations that might be encount", "after all when we humans generalize to understanding relationships exactly what variability is present in our training sets as compared to our testing situ", "how do the authors know that humans are effectively generalizing rather than just interpolating within their very rich training set", "it's not totally clear to me that if totally naive humans who had never seen spatial relationships before were evaluated on exactly the training/testing scenarios described above that they would generalize particularly well eith"], "labels": ["CRT", "CRT", "DIS", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i don't think it can just be assumed a priori that humans would be super good this form of gener", "so how should authors handle this critic", "what would be useful would either be some form of positive control", "either human training data showing very effective generalization if one could somehow make novel relationships unfamiliar to humans or a different network architecture that was obviously superior in generalization to cnn+rn", "if such were present i'd rate this paper significantly high"], "labels": ["DIS", "CRT", "CRT", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also i can't tell if i really fully believe the results of this pap", "i don't doubt that the authors saw the results they report", "however i think there's some chance that if the same tasks were in the hands of people who *wanted* cnns or cnn+rn to work well the results might have been differ", "i can't point to exactly what would have to be different to make things work because it's really hard to do that ahead of actually trying to do the work", "however this suspicion on my part is actually a reason i think it might be *good* for this paper to be published at iclr"], "labels": ["CRT", "CRT", "DIS", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this will give the people working on eg cnn+rn somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctli", "i myself am very curious about what would happen and would love to see this exchange catalyz", "originality and significance  the area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should b", "however as the authors here note there's been some recent work eg santoro  in the area", "i think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful and is a somewhat novel contribut"], "labels": ["APC", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors provide a method for learning from demonstrations where several modalities of the same task are given", "the authors argue that in the case where several demonstrations exists and a deterministic ie regular network is given the network learns some average policy from the demonstr", "the paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or program", "they put the this specific work in the right context of imitation learning and irl", "afterward the authors argue that deterministic network cannot adequately several mod"], "labels": ["SMY", "SMY", "SMY", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors cover in section  related topics and indeed the relevant literature includes behavioral cloning irl  imitation learning gail and va", "i find that recent paper by tamar et ", "on value iteration networks is highly relevant to this work the authors there learn similar tasks ie similar modalities using the same network", "even the control task is very similar to the current proposed task in this pap", "the authors argue that their contribution is -fold  does not require robot  rollouts  does not require label for a task  work within raw image input"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["again tamar et al  deals with this  point", "i went over the math", "it seems right and valid", "indeed snn is a good choice for adding bayesian context to a task", "also i see the advantage of referring only to the good quantiles when need"], "labels": ["SMY", "SMY", "APC", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is indeed a good method for dealing with the vari", "it is indeed a good method for dealing with the vari", "i must say that i was impressed with the authors making the robot succeed in the tasks in hand although reaching to an object is fairly simple task", "my concerns are as follows seems like that the given trajectories are naturally divided with different tasks ie a single trajectory consists only a single task", "my concerns are as follows seems like that the given trajectories are naturally divided with different tasks ie a single trajectory consists only a single task"], "labels": ["SUG", "DFT", "APC", "SMY", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for me this is not the pain point in this task", "the pain point is knowing when tasks are begin and end", "i'm not sure and i haven't seen evidence in the paper or other references that snn is the only optimal method for this context", "why not adding non bayesian context not label to the task will not work as wel", "the robot task is impress"], "labels": ["FBK", "SMY", "DFT", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but proving the point and for the ease of comparing to different tasks and since we want to show the validity of the work on more than  trials isn't showing the task on some simulation is better for understanding the different regimes that this method has advantag", "i know how hard is to make robotic tasks work    ium not sure that the comparison of the suggested architecture to one without any underlying additional variable z or context ie non-bayesian setup is fair", "vanilla nn indeed may fail miser", "so the comparison should be to any other work that can deal with similar environment but different detail", "to summarize i like the work and i can see clearly the motiv"], "labels": ["SMY", "DFT", "DFT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i think some more work is needed in this work comparing to the right current state of the art and show that in principal by demonstrating on other simpler simulations domains that this method is better than other method", "summarythis paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion", "the proposed approach will be useful when the new domain does not have enough data avail", "the baselines chosen are  no embeddings  generic embeddings from english wiki common crawl and combining data from previous and new domain", "empirical performance is shown on  downstream tasks product-type classification sentiment classification and aspect extract"], "labels": ["DFT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed embeddings just barely beat the baseline on product classification and sentiment classification but significantly beat them on aspect extraction task", "commentsthe paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be address", "there is reasonable novelty in the proposed method compared to the existing literatur", "but i was a little disappointed by the paper as several details of the model were unclear to me and the paper's writing could definitely be improved to make things clear", "in the meta-learner section  the authors talk about word features u{_w_{ijk}}u{_w_{ij'k}}"], "labels": ["SMY", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is unclear what these word features are are they one-hot encodings or embeddings or something els", "it would really help if the paper gave some expository exampl", "in algorithm  how do you deal with vocabulary items in the new domain that do not exist in the previous domains ie when the intersection of v_i and v_{n+} is the null set", "this is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen", "the results in table  are a little confus"], "labels": ["QSN", "SUG", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselin", "what is driving those difference in result", "typos in section  is depicted in figure  and figure  i think you mean figure  and figure  as there is no figur", "the paper proposes an approach to learning a distribution over filters of a cnn", "the method is based on a adversarial training the generator produces filters and the discriminator aims to distinguish the activation maps produced by real filters from those produced by the generated on"], "labels": ["QSN", "QSN", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros the general task of learning distributions over network weights is interest", "to my knowledge the proposed approach is new", "cons experimental evaluation is very substandard the experiments on invariances seem to be the highlight of the paper but they basically do not tell me anyth", "- figures  and  take  pages but what should one see ther", "- there are no quantitative result"], "labels": ["APC", "APC", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could there be a way to measure the invari", "- can the results be applied to some practical task why are the results interesting and/or us", "the experiments are restricted to a single dataset - mnist  the authors mention that ucthe test accuracy obtained by following the above procedure is of  against a test accuracy of  for the real cnnud - these are very poor accuracies for mnist so even the mnist results do not seem convinc", "presentation is suboptimal and many details are missing for instance architectures of networks are not provid", "to conclude while the general direction is interesting and the proposed method might work the experimental evaluation is very poor and the paper absolutely cannot be accepted for publ"], "labels": ["QSN", "QSN", "CRT", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a device placement algorithm to place operations of tensorflow on devic", "pros it is a novel approach which trains the placement end to end", "the experiments are solid to demonstrate this method works very wel", "the writing is easy to follow", "this would be a very useful tool for the community if open sourc"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons it is not very clear in the paper whether the training happens for each model yielding separate agents or a shared agent is trained and used for all kinds of model", "the latter would be more excit", "the adjacency matrix varies size for different graphs so i guess a separate agent is trained for each graph", "however if the agent is not shared why not just use integer to represent each operation in the graph since overfitting would be more desirable in this cas", "averaging the embedding is hard to understand especially for the output sizes and number of output"], "labels": ["DFT", "APC", "DFT", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n it is not clear how the adjacency information is us", "the majority of the paper is focused on the observation that  making policies that condition on the time step is important in finite horizon problems and a much smaller component on that  if episodes are terminated early during learning say to restart and promote exploration that the values should be bootstrapped to reflect that there will be additional rewards received in the true infinite-horizon setting is true and is well known", "this is typically described as finite horizon mdp planning and learning and the optimal policy is well known to be nonstationary and depend on the number of remaining time step", "there are a number of papers focusing on this for both planning and learning though these are not cited in the current draft", "i donut immediately know of work that suggests bootstrapping if an episode is terminated early artificially during training but it seems a very reasonable and straightforward thing to do"], "labels": ["DFT", "DIS", "DIS", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experi", "reviewprosthis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm", "they extend the scheme to allow the use of different scaling parameters and to m-bit quant", "experiments demonstrate the proposed scheme outperforms the state-of-the-art method", "the experiments are complete and the writing is good"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consalthough the work seems convincing it is a little bit straight-forward derived from the original binarization scheme hou et al  to tenarization or m-bit since there are some analogous extension ideas lin et al b li & liu b", "algorithm  and section  and  can be seen as additive complementari", "the paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning r", "unfortunately this paper feels to be hastily written and can only be read when accompanied with several references as key parts clr are not described and thus the work can not be reproduced from the pap", "unfortunately this paper feels to be hastily written and can only be read when accompanied with several references as key parts clr are not described and thus the work can not be reproduced from the pap"], "labels": ["DFT", "SUG", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum", "in this setting a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as wel", "however this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledg", "in the end the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment especially as the only slightly theoretical contribution in section  does not predict the observed behavior", "personally i would not use the term convergence in this setting at all as the runs are very short and thus we might not be close to any region of converg"], "labels": ["DIS", "DFT", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss which is not shown at al", "the results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss thus having a similar effect as early stop", "pros- many experiments which try to study the effect", "cons-the described phenomenon seems to depend strongly on the problem surface and might never be encountered on any problem aside of cifar-", "- only single runs are shown considering the noise on those the results might not be reproduc"], "labels": ["DFT", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-experiments are not described in detail-experiment design feels ad-hoc and unstructured-the role and value of the many lr-plots remains unclear to m", "form- the paper does not maker clear how the exact schedules work", "the terms are introduced but the paper misses the most basic formulas- figures are not properly described eg axes in figures  a and b- explicit references to code are made which require familiarity with the used frameworkif at all publish", "# summary of paperthe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi  were some estimates are warm restarted to increase the stability of the method", "# summary of reviewi find the contribution to be incremental and the validation weak"], "labels": ["DFT", "DFT", "DFT", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that i would consider necessary on an optimization-based contribut", "none of my comments are fatal but together with the incremental contribution i'm inclined as of this revision towards marginal reject", "# detailed comments the distinction between parameters and hyperparameters section  should be revis", "first the definition of parameters should not include the word paramet", "second it is not clear what parameters of the regularization mean"], "labels": ["CRT", "CRT", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["typically the regularization depends on both hyperparameters and paramet", "the real distinction between parameters and parameters is how they are estimated hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion but both are begin learnt just from different dataset", "in section  credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to maclaurin  this is not correct", "this approach was first proposed in domke  and refined by maclaurin  as correctly mentioned in maclaurin", "some quantities are not correctly specified i should not need to guess from the context or related literature what the quantities refer to"], "labels": ["SMY", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theta_k for example is undefined although i could understand its meaning from the context and sometimes used with arguments sometimes without ie both theta_klambda theta_ and theta_k are us", "the hypothesis are not correctly specifi", "many of the results used require smoothness of the second derivative eg the implicit function theorem but these are nowhere st", "the algorithm introduces too many hyper-hyperparameters although the authors do acknowledge thi", "while i do believe that projecting into a compact domain is necessary see pedregosa  assumption a the other parameters should ideally be relaxed or estimated from the evolution of the algorithm"], "labels": ["CRT", "CRT", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# minormissing  after hypergradient exactli", "we could optimization the hyperparam- typo", "references justin  domke    generic  methods  for  optimization-based modeling  ininternational conference on artificial intelligence and statist", "this paper provides a new generalization bound for feed forward networks based on a pac-bayesian analysi", "the generalization bound depends on the spectral norm of the layers and the frobenius norm of the weight"], "labels": ["DFT", "DFT", "SUG", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the resulting generalization bound is similar though not comparable to a recent result of bartlett et ", "however the technique is different since this submission uses pac-bayesian analysi", "the resulting proof is more simple and streamlined compared to that of bartlett et ", "the paper is well presented the result is explained and compared to other results and the proofs seem correct", "the result is not particularly different from previous on"], "labels": ["DIS", "DIS", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the different proof technique might be a good enough reason to accept this pap", "typos several citations are unparenthesized when they should b", "typos several citations are unparenthesized when they should b", "also after equation  there is a reference command that is not compiled properli", "this paper proposes a fast way to learn convolutional features that later can be used with any classifi"], "labels": ["FBK", "SUG", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning r", "in the evaluation the features are used with support vector machines svn and extreme learning machines on mnist and cifar/ dataset", "prosthe paper compares different classifiers on three dataset", "cons- considering an adaptive schedule of the learning decay is common practice in modern machine learn", "showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries like keras or pytorch"], "labels": ["SMY", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it is not clear why once a cnn has been trained one should want to change the last layer and use a svn or other classifi", "- there are many spelling error", "- comparing cnn based methods with hand-crafted features as in fig  and tab is not interesting anymor", "it is well known that cnn features are much better if enough data is avail", "note would the authors kindly respond to the comment below regarding kekulisation of the zinc dataset"], "labels": ["CRT", "CRT", "CRT", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["fair comparison of the data is a serious concern", "i have listed this review as a good for publication due to the novelty of ideas pres", "but the accusation of misrepresentation below is a serious one and i would like to know the author's respons", "*overview*this paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking instead of verifying the semantic correctness offline of a particular discrete structure the authors propose ucstochastic lazy attributesud which amounts to loading semantic constraints into a cfg and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid", "using bayesian optimization search over this space can yield decodings with targeted properti"], "labels": ["DIS", "APC", "CRT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["many of the ideas presented are novel", "the results presented are state-of-the art", "as noted in the paper the generation of syntactically and semantically valid data is still an open problem", "this paper presents an interesting and valuable solution and as such constitutes a large advance in this nascent area of machine learn", "*remarks on methodology*by initializing a decoding by ucguessingud a value the decoder will focus on high-probability starting regions of the space of possible structur"], "labels": ["APC", "APC", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear to me immediately how this will affect the output distribut", "since this process on average begins at high-probability region and makes further decoding decisions from that starting point the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs spac", "does this sacrifice exploration for exploitation in some quantifiable way", "some exploration of this issue or commentary would be valu", "*nitpicks*i found the notion of stochastic predetermination somewhat opaque and section  in general introduces much terminology like lazy linking that was new to me coming from a machine learning background"], "labels": ["CRT", "CRT", "QSN", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in my opinion this section could benefit from a little more expansion and conceptual definit", "the first  sections of the paper are very clearly written", "but the remainder has many typos and grammatical errors often word omiss", "the draft could use a few more passes before publ", "this paper proposed an improved version of dynamic coattention networks which is used for question answering task"], "labels": ["SUG", "APC", "CRT", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically there are  aspects to improve dcn one is to use a mixed objective that combines cross entropy with self-critical policy learning the other one is to imporve dcn with deep residual coattention encod", "the proposed model achieved stoa performance on stanford question asnwering dataset", "and several ablation experiments show the effectiveness of these two improv", "although dcn+ is an improvement of dcn i think the improvement is not increment", "one question is that since the model is compicated will the authors release the source code to repeat all the experimental result"], "labels": ["SMY", "APC", "APC", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# update after the rebuttalthank you for the rebutt", "the authors claim that the source of objective mismatch comes from n-step q-learning and their method is well-justified in -step q-learn", "however there is still a mismatch even with -step q-learning because the bootstrapped target is also computed from the treeqn", "more specifically there can be a mismatch between the optimal action sequences computed from treeqn at time t and t+ if the depth of treeqn is equal or greater than", "thus the author's response is still not convincing to m"], "labels": ["FBK", "SMY", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of q-function which makes implementation simpler compared to vpn", "however the particular method treeqn proposed in this paper introduces a mismatch in the model learning as mentioned abov", "one could argue that treeqn is learning an abstract planning rather than grounded plan", "however the fact that reward prediction loss is used to train treeqn significantly weakens this claim and there is no such an evidence in the pap", "in conclusion i think the research direction is worth pursuing but the proposed modification from vpn is not well-justifi"], "labels": ["APC", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["d# summarythis paper proposes treeqn and atreec which perform look-ahead planning using neural network", "treeqn simulates the future by predicting rewards/values of the future states and performs tree backup to construct q-valu", "atreec is an actor-critic architecture that uses a softmax over treeqn", "the architecture is trained through n-step q-learning with reward prediction loss", "the proposed methods outperform dqn baseline on d box pushing domain and outperforms vpn on atari gam"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[pros]- the paper is easy to follow", "- the application to actor-critic setting atreec is novel though the underlying idea was proposed by [o'donoghue et al schulman et al]", "[cons]- the proposed method has a technical issu", "- the proposed idea treeqn and underlying motivation are almost same as those of vpn [oh et al] but there is no in-depth discussion that shows why treeqn is potentially better than vpn", "- comparison to vpn on atari is not much convinc"], "labels": ["APC", "APC", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# novelty and significance- the underlying motivation planning without predicting observations the architecture transition/reward/value functions applied to the latent state space and the algorithm n-step q-learning with reward prediction loss are same as those of vpn", "but the paper does not provide in-depth discussion on thi", "the following is the differences that i found from this paper so it would be important to discuss why such differences are import", "the paper emphasizes the fully-differentiable tree planning aspect in contrast to vpn that back-propagates only through non-branching trajectories during train", "however differentiating treeqn also amounts to back-propagating through a single trajectory in the tree that gives the maximum q-valu"], "labels": ["SMY", "CRT", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["thus the only difference between treeqn and vpn is that treeqn follows the best estimated action sequence while vpn follows the chosen action sequence in retrospect during back-propag", "can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation see technical soundness section for discuss", "treeqn only sets targets for the final q-value after tree backup whereas vpn sets targets for all intermediate value predictions in the tre", "why is treeqn's approach better than vpn's approach", "- the application to actor-critic setting atreec is novel though the underlying idea of combining q-learning with policy gradient was proposed by [o'donoghue et al] and [schulman et al]"], "labels": ["DIS", "QSN", "DIS", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# technical soundness- the proposed idea of setting targets for the final q-value after tree backup can potentially make the temporal credit assignment difficult because the best estimated actions during tree planning does not necessarily match with the chosen act", "suppose that treeqn estimated up-right-right as the best future action sequence the during -step tree planning while the agent actually ended up with choosing up-left-left this is possible because the agent re-plans at every step and follows epsilon-greedy polici", "following n-step q-learning procedure we end up with setting target q-value based on on-policy action sequence up-left-left while back-propagating through up-right-right action sequence in the treeqn's plan", "this causes a wrong temporal credit assignment because treeqn can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen act", "so it is unclear why the proposed algorithm is technically correct or better than vpn's approach ie back-propagating through the chosen actions in the search tre"], "labels": ["CRT", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# quality- comparison to vpn on atari is not convincing because treeqn- is actually almost equivalent to vpn- but the results show that treeqn- performs much better than vpn on many gam", "since the authors took the numbers from [oh et al] rather than replicating vpn it is possible that the gap comes from implementation details eg hyperparamet", "# clarity- the paper is overall easy to follow and the description of the proposed method is clear", "the paper is clear and well written", "it is an incremental modification of prior work resnext that performs better on several experiments selected by the author comparisons are only included relative to resnext"], "labels": ["CRT", "DIS", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper is not about gating cf gates in lstms mixture of experts etc but rather about masking or perhaps a kind of block sparsity as the gates of the paper do not depend upon the input they are just fixed masking matrices see eq", "the main contribution appears to be the optimisation procedure for the binary masking tensor g", "but this procedure is not justified does each step minimise the loss", "this seems unlikely due to the sampl", "can the authors show that the procedure will always converg"], "labels": ["APC", "SMY", "QSN", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be good to contrast this with other attempts to learn discrete random variables for example the concrete distribution continuous relaxation of continuous random variables maddison et al iclr", "the authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearli", "the proposed solution seems very intuitive and the empirical evaluations demonstrates its util", "my main concern is the underlying assumption if i understand correctly that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector", "especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack techniqu"], "labels": ["SUG", "APC", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however this does not invalidate the contributions of this manuscript", "specific comments/questions- minor page  eq  i think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the euclidean set", "- section  the different pieces of the problem estimation intuition for adversarial subspaces efficiency are very well describ", "- alg  l is this where the normal exmaples are converted to adversarial examples using some attack techniqu", "- alg  l is lid_norm computed using a leave-one-out estim"], "labels": ["DIS", "CRT", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["otherwise r_ for each point is  leading to a somewhat under-estimate of the true lid of the normal points in the training set", "i understand that it is not an issue in the test set", "- section  and alg  s we do not really care about the labels/targets of the exampl", "all examples in the dataset are considered normal to start with", "is this assuming that the initial training set which is used to obtain the pre-trained dnn free of adversarial exampl"], "labels": ["QSN", "DIS", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- section  experimental setup seems like normal points in the test set would get lesser values if we are not doing the leave-one-out version of the estim", "- section  the authors have done a great job at evaluating every aspect of the proposed method", "this paper considers the task of generating wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a wikipedia page along with the content collected from web search and output is the generated content for a target wikipedia pag", "the authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attent", "in general the paper is well-written and the main ideas are clear"], "labels": ["DIS", "APC", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however my main concern is the evalu", "it would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approach", "it would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approach", "although authors argue in section  that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller  experiments could have been performed to show their impact", "although authors argue in section  that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller  experiments could have been performed to show their impact"], "labels": ["CRT", "SUG", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore i really expected to see a comparison with sauper & barzilay 's non-neural extractive approach of wikipedia article generation which could certainly strengthen the technical merit of the pap", "furthermore i really expected to see a comparison with sauper & barzilay 's non-neural extractive approach of wikipedia article generation which could certainly strengthen the technical merit of the pap", "more importantly it was not clear from the paper if there was a constraint on the output length when each model generated the wikipedia cont", "for example figure - show variable sizes of the generated output", "with a fixed reference/target wikipedia article if different models generate variable sizes of output rouge evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the refer"], "labels": ["SUG", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would have been nice to know if the proposed attention mechanisms account for significantly better results than the t-ed and t-d architectur", "did you run any statistical significance test on the evaluation result", "authors claim that the proposed model can generate fluent coherent output however no evaluation has been conducted to justify this claim", "the human evaluation only compares two alternative models for preference which is not enough to support this claim", "i would suggest to carry out a duc-style user evaluation http//www-nlpirnistgov/projects/duc/duc/quality-questionstxt methodology to really show that the proposed method works well for abstractive summar"], "labels": ["SUG", "QSN", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does figure  show an example input after the extractive stage or befor", "please clarify---------------i have updated my scores as authors clarified most of my concern", "this paper presents a modification of a numeric solution incomplete dot product idp that allows a trained network to be used under different hardware constraint", "the idp method works by incorporating a 'coefficient' to each layer fully connected or convolution which can be learned as the weights of the model are being optim", "these coefficients can be used to prune subsets of the nodes or filters when hardware has limited computational capac"], "labels": ["QSN", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the original idp method cited in the paper is based on iteratively training for higher hardware capac", "this paper improves upon the limitation of the original idp by allowing the weights of the network be trained concurrently with these coefficients and authors present a loss function that is linear combination of loss function under original or constrained network set", "they also present results for a 'harmonic' combination which was not explained in the paper at al", "overall the paper has very good motivation and signific", "however the writing is not very clear and the paper is not self-contained at al"], "labels": ["SMY", "APC", "DFT", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i was not able to understand the significance of early stopping and how this connects with loss aggregation and how the learning process differs from the original idp paper if they also have a scheduled learning set", "additionally there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in figur", "as is while results are promis", "i can't fully assess that the paper has major contribut", "updateon further consideration and reading the other reviews i'm bumping my rating up to a"], "labels": ["DFT", "DFT", "APC", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think there are still some issues but this work is both valuable and interesting and it deserves to be published alongside the naesseth et al and maddison et al work", "-----------this paper proposes a version of iwae-style training that uses smc instead of classical importance sampl", "going beyond the several papers that proposed this simultaneously the authors observe a key issue the variance of the gradient of these iwae-style bounds wrt the inference parameters grows with their accuraci", "they therefore propose using a more-biased but lower-variance bound to train the inference parameters and the more-accurate bound to train the generative model", "overall i found this paper quite interest"], "labels": ["FBK", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are a few things i think could be cleared up but this seems like good work although i'm not totally up to date on the very recent literature in this area", "some comments* section  i found this argument extremely interest", "however itus worth noting that your argument implies that you could get an o snr by averaging k noisy estimates of i_k", "rainforth et al suggest this approach as well as the approach of averaging k^ noisy estimates which the theory suggests may be more appropriate if the functions involved are sufficiently smooth which even for relu networks that are non-differentiable at a finite number of points i think they should b", "rainforth et al suggest this approach as well as the approach of averaging k^ noisy estimates which the theory suggests may be more appropriate if the functions involved are sufficiently smooth which even for relu networks that are non-differentiable at a finite number of points i think they should b"], "labels": ["APC", "APC", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper would be stronger if it compared with rainforth et alus proposed approach", "this paper would be stronger if it compared with rainforth et alus proposed approach", "this would demonstrate the real tradeoffs between bias variance and comput", "of course that involves ok^ or ok^ computation which is a weak", "but one could use a small value of k say k="], "labels": ["SUG", "DIS", "DIS", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that said i could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple exampl", "basically these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail", "* section  claim  is an interesting observ", "but propositions  and  seem to just say that the only way to get a perfectly tight smc elbo is to perfectly sample from the joint posterior", "i think thereus an easier way to make this argumentgiven an unbiased estimator hat{z} of z by jensenus inequality e[log hat{z}] u log z with equality iff the variance of hat{z} ="], "labels": ["DIS", "SUG", "APC", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the only way to get an smc estimatorus variance to  is to drive the variance of the weights to", "that only happens if you perfectly sample each particle from the true posterior conditioned on all future inform", "all of which is true as far as it goes but i think itus a bit of a distract", "the question is not ucwhatus it take to get to  varianceud but uchow quickly can we approach  varianceud", "in principle is and smc can achieve arbitrarily high accuracy by making k astronomically larg"], "labels": ["DIS", "DIS", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although [particle] mcmc is probably a better choice if one wants extremely low bia", "* section  the choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work but thereus only a very cursory discussion in the main text", "i would recommend at least summarizing the main findings of appendix a in the main text", "* a relevant missing citation turner and sahanius uctwo problems with variational expectation maximisation for time-series modelsud http//wwwgatsbyuclacuk/~maneesh/papers/turner-sahani--ildnpdf", "they discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates though in a quite different context and with a quite different analysi"], "labels": ["APC", "APC", "SUG", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* figure  what is the x-axis her", "presumably phi is not actually -dimension", "typos etc* uclearn a particular series intermediateud missing ucofud", "* ucto do so we generate on sequence ytud s/on/a/ i think", "* equation  should there be a /k in z"], "labels": ["QSN", "QSN", "CRT", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors did extensive tuning of the parameters for several recurrent neural architectur", "the results are interest", "however the corpus the authors choose are quite smal", "the variance of the estimate will be quite high i suspect whether the same conclusions could be drawn", "it would be more convincing if there are experiments on the billion word corpus or other larger datasets or at least on a corpus with  million token"], "labels": ["SMY", "APC", "DFT", "DFT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this will use significant resources and is much more difficult", "but it's also really valuable because it's much more close to real world usage of language model", "and less tuning is needed for these larger dataset", "finally it's better to do some experiments on machine translation or speech recognition and see how the improvement on bleu or wer could get", "this paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phas"], "labels": ["DIS", "APC", "SMY", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though this is an important direction to investigate there are several issues comparison with previous results is misleadingat-bit weights and floating point activations rastegari et al got % accuracy on alexnet which is better than this paper bit result of %bthubara et al got % results on -bit weights and -bit activations included also quantization first and last layer in contrast to this pap", "though this is an important direction to investigate there are several issues comparison with previous results is misleadingat-bit weights and floating point activations rastegari et al got % accuracy on alexnet which is better than this paper bit result of %bthubara et al got % results on -bit weights and -bit activations included also quantization first and last layer in contrast to this pap", "therefore it is not clear if there is a significant benefit in the proposed method which achieves % when decreasing the activation precision to bit", "therefore it is not clear that the proposed methods improve over previous approach", "it is not clear to me in which dimension of the tensors are we saving the scale factor"], "labels": ["DFT", "CRT", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear to me in which dimension of the tensors are we saving the scale factor", "if it is per feature map or neuron this eliminates the main benefits of quantization doing efficient binarized operations when doing weight*activation during the forward pass", "the review of the literature is inaccur", "the review of the literature is inaccur", "for example it is not true that courbariaux et al  ucfurther improved accuracy on small datasetsud the main novelty there was binarizing the activations which typically decreased the accuraci"], "labels": ["CRT", "SMY", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example it is not true that courbariaux et al  ucfurther improved accuracy on small datasetsud the main novelty there was binarizing the activations which typically decreased the accuraci", "also it is not clear if the scale factors introduced by xnor-net indeed allowed a significant improvement over previous work in imagenet eg see dorefa and hubara et al who got similar results using binarized weigths and activations on imagenet without scale factor", "also it is not clear if the scale factors introduced by xnor-net indeed allowed a significant improvement over previous work in imagenet eg see dorefa and hubara et al who got similar results using binarized weigths and activations on imagenet without scale factor", "lastly the statement uctypical approaches include linearly placing the quantization pointsud is inaccurate it was observed that logarithmic quantization works better in various cas", "for example see miyashita lee and murmann  and hubara et "], "labels": ["CRT", "DIS", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["%%% after author's clarification %%%this paper results seem more positive now and i have therefore have increased my score assuming the authors will revise the paper accordingli", "the paper adds few operations after the pipeline for obtaining visual concepts from cnn as proposed by wang et ", "this latter paper showed how to extract from a cnn some clustered representations of the features of the internal layers of the network working on a large training dataset", "the clustered representations are the visual concept", "this paper shows that these representations can be used as exemplars by test images in the same vein as bag of words used word exemplars to create the bag of words of unseen imag"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a simple nearest neighborhood and a likelihood model is built to assign a picture to an object class", "the results a are convincing even if they are not state of the art in all the tri", "the paper is very easy to follows and the results are explained in a very simple way", "few commentsthe authors in the abstract should revise their claims too strong with respect to a literature field which has done many advancements on the cnn interpretation see all the literature of andrea vedaldi and the literature on zero shot learning transfer learning domain adaptation and fine tuning in gener", "summarythis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sent"], "labels": ["SMY", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the span representations are weighted by the spans marginal scores given by the inside-outside algorithm", "experiments on trec-qa and snli show modest improvement over the word-based structured attention baseline parikh et ", "strengthsthe idea of using latent syntactic structure and computing cross-sentence alignment over spans is very interest", "weaknessesthe paper is  pages long", "the method did not out-perform other very related structured attention methods  kim et al   liu and lapata"], "labels": ["SMY", "SMY", "APC", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["aside from the time complexity from the inside-outside algorithm as mentioned by the authors in conclusion the comparison among all pairs of spans is on^ which is more expens", "am i missing something about the algorithm", "it would be nice to show quantitatively the agreement between the latent trees and gold/supervised syntax", "the paper claimed ucthe model is able to recover tree structures that very closely mimic syntaxud but itus hard to draw this conclusion from the two examples in figur", "this paper surveys models for collaborative filtering with user/item covari"], "labels": ["DFT", "QSN", "SUG", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the authors seems to have captured the essence of a large number of popular cf models and i found that the proposed model classification is reason", "the notation also make it easy to understand the differences between different model", "in that sense this paper could be useful to researchers wanting to better understand this field", "it may also be useful to develop further insights into current models although the authors do not go that rout", "the impact of this paper may be limited in this community since it is a survey about a fairly niche topic a subset of recommender systems that may not be of central interest at iclr"], "labels": ["APC", "APC", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think this paper would be a better fit in a recsys applied ml or information retrieval journ", "a few comments i find that there are several ways the paper could make a stronger contribut", "use the unifying notation to discuss strengths and weaknesses of current approaches ideally with insights about possible future approach", "n report the results of a large study of many of the surveyed models on a large number of dataset", "ideally further insights could be derived from these result"], "labels": ["DIS", "DIS", "SUG", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n provide a common code framework with all method", "n add a discussion on more structured sources of covariates eg social network", "this could probably more or less easily be added as a subsection using the current classif", "- a similar classification of collaborative filtering models with covariates is proposed in this thesis phttps//tspacelibraryutorontoca/bitstream////charlin_laurent__phd_thesispdf", "- the paper is well written overal"], "labels": ["SUG", "SUG", "SUG", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the current version of the paper contains several typo", "this paper proposes a model for adding background knowledge to natural language understanding task", "the model reads the relevant text and then more assertions gathered from background knowledge before determining the final predict", "the authors show this leads to some improvement on multiple tasks like question answering and natural language inference they do not obtain state of the art but improve over a base model which is fine in my opinion", "i think the paper does a fairly good job at doing what it do"], "labels": ["CRT", "SMY", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is just hard to get excited by it", "here are my major comments* the authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynam", "but then they just concept net to augment text", "this is quite a static strategy i was assuming the authors are going to use some ir method over the web to back up their motiv", "as is i don't really see how this motivation has anything to do with getting things out of a kb"], "labels": ["DIS", "SMY", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a kb is usually a pretty static entity and things are added to it at a slow pac", "* the author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing qa and nli", "specifically they take text and add common sense knowledge from concept net", "the authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysi", "however is this statement enough to cross the acceptance threshold of iclr"], "labels": ["CRT", "CRT", "DIS", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["seems a bit marginal to m", "* the author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them", "to me it is unclear why we should use this method for incorporating background knowledge and not some simpler way", "for example have another rnn read the assertions and somehow integrate that", "the process of re-creating embeddings seems like one choice in a space of many not the simplest and not very well motiv"], "labels": ["CRT", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are no comparisons to other poss", "as a result it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is us", "as is i would guess the second is more likely and so i am not convinced the architecture itself is a significant contribut", "so to conclude the paper is well-written clear and has nice results and analysi", "the conclusion is that reading background knowledge from concept net boost performance using some architectur"], "labels": ["CRT", "CRT", "CRT", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is nice to know but i think does not cross the acceptance threshold", "the paper introduces the notion of continuous convolutional neural network", "the main idea of the paper is to project examples into an rk hilbert spaceand performs convolution and filtering into that spac", "interestingly thefilters defined in the hilbert space  have parameters that are learn", "while the idea may be novel and interest"], "labels": ["FBK", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["its motivation is not clear form", "is it for space for speed for expressivity of hypothesis spac", "most data that are available for learning are in discrete forms and hopefullythey have been digitalized according to shannon theori", "this means that they bringall necessary information for rebuilding their continuous counterpart", "hence it isnot clear why projecting them back into continuous functions is of interest"], "labels": ["CRT", "QSN", "DIS", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another point that is not clear or at least misleading is the so-called hilbert map", "as far as i understand equation  is not an embedding into an hilbert space butis more a proximity space representation []", "hence the learning framework of theauthors can be casted more as a learning with similarity function than learninginto a rkhs []", "a proper embedding would have mapped $x$ into a functionbelonging to $mh$", "in addition it seems that all computations are doneinto a ell^ space instead of in the rkhs equations  and"], "labels": ["CRT", "CRT", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["learning good similarity functions is also not novel [] and equations and  corresponds to learning these similarity funct", "as far as i remember there exists also some paper from the nineties thatlearn the parameters of rbf networks but unfortunately i have not been able togoogle some of them", "part  is the most interesting part of the pap", "however it would have beengreat if the authors provide other kernel functions with closed-form convolution formula that may be relevant for learn", "the proposed methodology is evaluated on some standard benchmarks in vis"], "labels": ["CRT", "CRT", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["whileresults are pretty good", "it is not clear how the various cluster sets have been obtainedand what are their influence on the performances if they are randomly initialized it would be great to see standard deviation of performances with respect to initi", "i would also be great to have intuitions on why a single continuous filter works bettersthan  discrete ones if this behaviour is consistent accross initi", "on the overall while the idea may be of interest", "the paper lacks in motivationsin connecting to relevant previous works and in providing insights on why it work"], "labels": ["APC", "CRT", "SUG", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however performance results seem to be competitive and that's the reader maybe eager for insight", "minor comments---------------* the paper employs vocabulary that is not common in ml", "eg i am not sure whatoccupancy values or inducing points ar", "* supposingly that the authors properly consider computation in rkhs then sigma_ishould be definite positive right", "how update in  is guaranteed to be dp"], "labels": ["APC", "DIS", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this constraints may not be necessary if instead they used proximity space represent", "[] https//alexsmolaorg/papers//graherschsmopdf[] https//wwwcscmuedu/~avrim/papers/similarity-bbspdf[] a bellet a habrard and m sebban similarity learning for provably accurate sparse linear classif", "general comment==============low-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction perform", "the authors a extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights and b and algorithm to determine the rank of each convolutional filt", "the authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to vgg"], "labels": ["DIS", "DIS", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is a useful extension of existing methods but needs to evaluated more rigor", "the manuscript is hard to read due to unclear descriptions and grammatical error", "major comments============= the authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to vgg", "the authors should analyze if this also holds true for resnet and inception which are more widely used than vgg", "the authors measured the actual speedup on a single cpu intel core i"], "labels": ["SUG", "CRT", "SUG", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should measure the actual speedup also on a single gpu", "it is unclear how the actual speedup was measur", "does it correspond to the seconds per update step or the overall training tim", "in the latter case how long were models train", "how and which hyper-parameters were optim"], "labels": ["SUG", "CRT", "SUG", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors should use the same hyper-parameters for all methods jaderberg zhang rank select", "the authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for urank selectionu", "figure  the authors should show the same plot for more convolutional layers at varying depth from both vgg and resnet", "the manuscript is hard to understand and not written clearly enough", "in the abstract what does utwo-pass decompositionu uproper ranksu uthe instability problemu or usystematicu mean"], "labels": ["SUG", "SUG", "SUG", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what are uedge devicesu uvanilla parametersu", "the authors should also avoid uninformative adjectives clutter and vague terms throughout the manuscript such as uvital importanceu or ulittle room for fine-tuningu", "the authors should also avoid uninformative adjectives clutter and vague terms throughout the manuscript such as uvital importanceu or ulittle room for fine-tuningu", "minor comments============= the authors should use usignificantlyu only if a statistical hypothesis was perform", "the manuscript contains several typos and grammatical flaws eg uhave been widely applied to have the breakthroughu uthe cp decomposition factorizes the tensors into a sum of series rank-one tensor"], "labels": ["QSN", "SUG", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u uour two-pass decomposition provides the better result as compared with the original cp decompositionu", "for clarity the authors should express equation  in terms of y_ y_ y_ and y_", "equation  bottom c_in w_f h_f and c_out are undefined at this point", "this paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural network", "specifically they observe that the angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions proven to be about  degrees when the dimension goes to infinity and this demonstrated empirically to be true for the binarized weight matrices of a convenet"], "labels": ["DIS", "SUG", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["except the first layer the dot product of weights*activations in each layer is highly correlated with the dot product of binarized weights*activations in each lay", "there is also a strong correlation between binarized weights*activations and binarized weights*binarized activ", "this is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same mann", "to correct the issue with the first layer in  it is suggested to use a random rotation or simply use continues weights in that lay", "the first observation is interesting is explained clearly and convincingly and is novel to the best of my knowledg"], "labels": ["SMY", "SMY", "SMY", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the second observation is much less clear to m", "specificallyatthe author claim that uca sufficient condition for delta u to be the same in both cases is lux = fu ~ lux = guud", "however ium not sure if i see why this is true in a binarized neural net u also changes since the previous layers are also binar", "btrelated to the previous issue it is not clear to me if in figure  and  did the authors binarize the activations of that specific layer or all the lay", "if it is the first case i would be interested to know the latter it is possible that if all layers are binarized then the differences between the binarized and non-binarized version become more amplifi"], "labels": ["CRT", "DIS", "CRT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ctfor bnns where both the weights and activations are binarized shouldnut we compare weights*activations to binarized weights*binarized activ", "dtto make sure in figure  the permutation of the activations was randomized independently for each data sampl", "if not then c is not proportional the identity matrix as claimed in sect", "etit is not completely clear to me that batch-normalization takes care of the scale constant if so then why did xnor-net needed an additional scale constantperhaps this should be further clarifi", "etit is not completely clear to me that batch-normalization takes care of the scale constant if so then why did xnor-net needed an additional scale constantperhaps this should be further clarifi"], "labels": ["QSN", "QSN", "CRT", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the third observation seems less useful to m", "though a random rotation may improve angle preservation in certain cases as demonstrated in figure  it may hurt classification performance eg distinguishing between  and  in mnist", "furthermore since it uses non-binary operations it is not clear if this rotation may have some benefits in terms of resource efficiency over simply keeping the input layer non-binar", "to summarize the first part is interesting and nic", "the second part was not clear to me and the last part does not seem very us"], "labels": ["CRT", "CRT", "CRT", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["%%% after author's response %%%a my mistake perhaps it should be clarified in the text that u are the weight", "i thought that gu is a forward propagation function and therefore u is the neural input ie pre-activ", "following the author's response and revisions i have raised my grad", "general impressionoverall the revised version of the paper is greatly improv", "the new derivation of the method yields a much simpler interpretation although the relation to the natural gradient remains weak see below"], "labels": ["DIS", "DIS", "FBK", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental evaluation is now far more solid", "multiple data sets and network architectures are tested and equally important the effect of parameter settings is investig", "i enjoyed the investigation of the effect of l_ regularization on qualitative optimization behavior", "criticismmy central criticism is that the introduction of the l_ norm as a replacement of kl divergence is completely ad-hoc how it is related to kl divergence remains unclear", "it seems that other choices are equally well justified including the l_ norm in parameter space which then defeats the central argument of the pap"], "labels": ["APC", "SMY", "APC", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do believe that l_ distance is more natural in function space than in parameter space but i am missing a strict argument for this in the pap", "although related work is discussed in detail in section  it remains unclear how exactly the proposed algorithm overlaps with existing approach", "i am confident that it is easy to identify many precursors in the optimization literature but i am not an expert on thi", "it would be of particular interest to highlight connections to algorithm regularly applied to neural network train", "adadelta rmsprop and adam are mentioned explicitly but what exactly are differences and similar"], "labels": ["CRT", "CRT", "DIS", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the interpretation of figure  is off", "it is deduced that hcgd generalizes better however this is the case only at the very end of training while sgd with momentum and adam work far better initi", "with the same plot one could sell sgd as the superior algorithm", "overall also in the light of figure  the interpretation that the new algorithm results in better generalization seems to stand on shaky ground since differences are smal", "i like the experiment presented in figure  in particular"], "labels": ["QSN", "SMY", "DIS", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it adds insights that are of value even if the method should turn out to have significant overlap with existing work see above and perform only on par with thes", "it adds an interesting perspective to the discussion of how network optimization works how it handles local optima and which role they play and how the objective function landscape is perceived by different optim", "this is where i learned something new", "minor pointspage  the any typopage  ture - true typo", "the authors adapts stochastic natural gradient methods for variational inference with structured inference network"], "labels": ["APC", "DIS", "APC", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the variational approximation proposed is similar to svae by jonhson et al  but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational paramet", "the authors then extends and adapts the natural gradient method by khan & lin  to optimize all the variational paramet", "in the experiments the authors generally show improved convergence over sva", "the idea seems promis", "but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approxim"], "labels": ["SMY", "SMY", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approxim", "the main motivation seems to be that it is easier to optim", "- in the last two sentences of the updates for theta_pgm you mention that you need to do svi/vmp to compute the function eta_xtheta", "might this also suffer from non-convergence issues like you argue svae do", "or do you simply mean that computation of this is exact using regular message passing/kalman filter/forward-backward"], "labels": ["CRT", "APC", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it was not clear to me why we should use a gaussian approximation for the theta_nn paramet", "the prior might be gaussian but the posterior is not", "is this more of a simplifying assumpt", "- there has recently been interest in using inference networks as part of more flexible variational approximations for structured model", "some examples of related work missing in this area is variational sequential monte carlo by naesseth et al  / filtering variational objectives by maddison et al  / auto-encoding sequential monte carlo le et "], "labels": ["QSN", "QSN", "QSN", "CNT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-  section  paragraph nr  algorihtm - algorithm", "this high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models and presents strong empirical results across multiple evaluation task", "this high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models and presents strong empirical results across multiple evaluation task", "the approach is basically to apply self-attention at two levels such that each level only has a small fixed number of items thereby limiting the memory requirement while having negligible impact on spe", "it captures local information into so-called blocks using self-attention and then applies a second level of self-attention over the blocks themselv"], "labels": ["CRT", "SMY", "APC", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well organized and clearly written modulo minor language mistakes that should be easy to fix with further proof-read", "the paper is well organized and clearly written modulo minor language mistakes that should be easy to fix with further proof-read", "the contextualization of the method relative to cnns/rnns/transformers is good and the beneficial trade-offs between memory runtime and accuracy are thoroughly investigated and they're compel", "i am curious how the story would look if one tried to push beyond two level", "for example how effective might a further inter-sentence attention level be for obtaining representations for long docu"], "labels": ["SUG", "APC", "APC", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor points- text between eq  &  w^{} appears twice one instance should probably be w^{}", "- multiple locations eg s for nli the word is *premise* not *promise*- missing word in first sentence of s  reason __ th", "-i think title is misleading as the more concise results in this paper is about linear networks i recommend adding linear in the title ie changing the title to u deep linear network", "-i think title is misleading as the more concise results in this paper is about linear networks i recommend adding linear in the title ie changing the title to u deep linear network", "- theorems   and the observation  are nice!"], "labels": ["FBK", "FBK", "SUG", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- theorem  there is no discussion about the nature of the saddle point is it strict", "- theorem  there is no discussion about the nature of the saddle point is it strict", "does this theorem imply that the global optima can be reached from a random initi", "regardless of if this theorem can deal with these issues a discussion of the computational implications of this theorem is necessari", "- ium a bit puzzled by theorems  and  and why they are us"], "labels": ["DFT", "QSN", "QSN", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result", "since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result", "further discussion would be help", "the paper claims to develop a novel method to map natural language queries to sql", "they claim to have the following contributions  using a grammar to guide decod"], "labels": ["QSN", "CRT", "SUG", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["using a new loss function for pointer / copy mechan", "for each output token they aggregate scores for all positions that the output token can be copied from", "i am confident that point  has been used in several previous work", "although point  seems novel i am not convinced that it is significant enough for iclr", "i was also not sure why there is a need to copy items from the input question since all sql query nouns will be present in the sql table in some form"], "labels": ["SMY", "SMY", "CRT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what will happen if we restrict the copy mechanism to only copy from sql t", "the references need work there are repeated entries for the same reference one form arxiv and one from confer", "please cite the conference version if one is available many arxiv references have conference vers", "rebuttal response i am still not confident about the significance of contribution  so keeping the score the sam", "this paper studies the critical points of shallow and deep linear network"], "labels": ["QSN", "DFT", "SUG", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors give a necessary and sufficient characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima", "essentially this paper revisits a classic paper by baldi and hornik  and relaxes a few requires assumptions on the matric", "i have not checked the proofs in detail but the general strategy seems sound", "while the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in iclr", "the authors also study the analytic form of critical points of a single-hidden layer relu network"], "labels": ["APC", "DIS", "APC", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors also study the analytic form of critical points of a single-hidden layer relu network", "however given the form of the necessary and sufficient conditions the usefulness of of these results is less clear", "detailed comments- i think in the title/abstract/intro the use of neural nets is somewhat misleading as neural nets are typically nonlinear", "this paper is mostly about linear network", "while a result has been stated for single-hidden relu network"], "labels": ["DIS", "CRT", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in my view this particular result is an immediate corollary of the result for linear network", "as i explain further below given the combinatorial form of the result the usefulness of this particular extension to relu network is not very clear", "i would suggest rewording title/abstract/intro", "- theorem  is neat well done!", "- page  p_ius in proposition from my understanding the p_i have been introduced in theorem  but given their prominent role in this proposition they merit a separate definition and ideally in terms of the a_i directli"], "labels": ["DIS", "CRT", "SUG", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- theorems  prop  prop  prop  theorem  prop  and tare these characterizations computable ie given x and y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i v_i etc", "- theorems  prop  prop  prop  theorem  prop  and twould recommend a better exposition why these theorems are us", "what insights do you gain by knowing these theorems etc", "are less sufficient conditions that is more intuitive or usefulan insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient on", "- page  theorem tdoes this theorem have any computational impl"], "labels": ["QSN", "DIS", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does it imply that the global optima can be found efficiently eg are saddles strict with a quantifiable bound", "- page  proposition  seems like an immediate consequence of theorem  however given the combinatorial nature of the k_{ij} it is not clear why this theorem is us", "eg  back to my earlier comment wrt linear networks given y and x can you find the parameters of this characterization with a computationally efficient algorithm", "this paper proposes an iterative inference scheme for latent variable models that use inference network", "instead of using a fixed-form inference network the paper proposes to use the learning to learn approach of andrychowicz et "], "labels": ["QSN", "QSN", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network eg it could be rnn but the experiments uses a feed-forward network", "my main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach", "it appears that the iterative method should result in direct improvement with additional samples and inference iter", "i am supposing this is at the test tim", "it is not clear exactly when this will be us"], "labels": ["SMY", "CRT", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe an iterative approach is also possible to perform with the standard vae eg by bootstrapping over the input data and then using the iterative scheme of rezende et ", "they used this method to perform data imput", "the paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in va", "in summary the paper needs to do a better job in justifying the advantages obtained by the proposed method", "this paper discusses the application of word prediction for software keyboard"], "labels": ["SUG", "DIS", "DFT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requir", "the authors propose a simple method of mixing the global model with user specific data", "collecting the user specific models and averaging them to form the next global model", "the proposal is pract", "the proposal is pract"], "labels": ["SMY", "SMY", "SMY", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am not convinced that this is novel enough for publication at iclr", "however i am not convinced that this is novel enough for publication at iclr", "one major quest", "the authors assume that the global model will depict general english", "however it is not necessary that the population of users will adhere to general english and hence the averaged model at the next time step t+ might be significantly different from general english"], "labels": ["DFT", "CRT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forget", "the paper proposes a set of benchmarks for molecular design and compares different deep models against them", "the main contributions of the paper are  molecular design benchmarks with chembl- dataset including two molecular design evaluation criterias and comparison of some deep models using these benchmark", "the main contributions of the paper are  molecular design benchmarks with chembl- dataset including two molecular design evaluation criterias and comparison of some deep models using these benchmark", "the paper does not seem to include any method develop"], "labels": ["CRT", "SMY", "SMY", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper suffers from a lack of focu", "several existing models are discussed to some length while the benchmarks are introduced quite shortli", "the dataset is not very clearly defined it seems that there are  million training instance does this apply for all benchmark", "the dataset is not very clearly defined it seems that there are  million training instance does this apply for all benchmark", "the paper's title also does not seem to fit this feels like a survey paper which is not reflected in the titl"], "labels": ["DFT", "DFT", "DFT", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper's title also does not seem to fit this feels like a survey paper which is not reflected in the titl", "biologically lots of important atoms are excluded from the dataset for instance natrium calcium and kalium", "i don't see any reason to exlude thes", "what does biological activities on  targets mean", "what does biological activities on  targets mean"], "labels": ["FBK", "DFT", "DFT", "DFT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper discussed molecular generation and reinforcement learning but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is us", "are the test molecules somehow generated in a directed or undirected fashion", "shouldn't there also be experiments on comparing ways to generate suitable molecules and how well they match the proposed criterion", "there should be benchmarks for predicting molecular properties standard regression and for generating molecules with certain properti", "there should be benchmarks for predicting molecular properties standard regression and for generating molecules with certain properti"], "labels": ["DFT", "QSN", "QSN", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["currently it's unclear which type of problems are solved her", "table  lists  models while fig  contains  why the discrep", "in table  the plotted runs seem to differ a lot from average results eg - to  or  to", "variances should be added and preferably more than  initialisations us", "overall this is an interesting pap"], "labels": ["DFT", "DFT", "DFT", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but does not have any methodological contribution and there is also few insightful results about the compared methods nor is there meaningful analysis of the problem domain of molecules eith", "- the authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring represent", "they show that their idea is effective in reducing private information leakag", "but this idea alone might not be signifcant enough as a contribut", "- the idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduct"], "labels": ["DFT", "SMY", "SMY", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed judging from the large variance in the accuracy of predicting s in table a-c for single adversaries i suspect one of the main advantage of the current mars method comes from variance reduct", "indeed judging from the large variance in the accuracy of predicting s in table a-c for single adversaries i suspect one of the main advantage of the current mars method comes from variance reduct", "the author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction and this could also be due to the high model variance of such high capacity network", "- the definition of s the private information set is not clear there is no statement about it in the experiments section and i assume s is the subject ident", "but this makes the train-test split described in  rather odd since there is no overlap of subjects in the train-test split we need clarifications on these experimental detail"], "labels": ["DFT", "DIS", "DIS", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- judging from figure  and table  all the methods tested are not effective in hiding the private information s in the learned represent", "even though the proposed method works better the prediction accuracies of s are still high", "even though the proposed method works better the prediction accuracies of s are still high", "this paper presents simple but useful ideas for improving sentence embedding by drawing from more context", "the authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence they posit that one can obtain more information about a sentence from other governing sentences in the document such as the title of the document sentences based on html sentences from table of contents etc"], "labels": ["CRT", "DFT", "FBK", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the way i understand it previous sentence like in skipthought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context", "here are the pros of this paper useful contribution in terms of using broader context for embedding a sent", "novel and simple trick for generating oov words by mapping them to local variables and generating those vari", "outperforms skipthought in ev", "cons coreference eval no details are provided for how the data was annotated for the coreference task"], "labels": ["DIS", "APC", "APC", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is crucial to understanding the reliability of the evaluation as this is a new domain for corefer", "also the authors should make this dataset available for replic", "also why have the authors not used this embedding for eval on standard coreference datasets like ontonot", "please clarify it is not clear to me how the model learns to generate specific oov vari", "can the authors clarify how does the decoder learns to generate these word"], "labels": ["SUG", "SUG", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarifications in section  what is the performance of skip-thought with the same oov trick as this pap", "what is the exact heuristic in text styles in section  should be stated for replic", "in the paper titled faster discovery of neural architectures by searching for paths in a large model the authors proposed an efficient algorithm which can be used for efficient less resources and time and faster architecture design for neural network", "the motivation of the new algorithm is by sharing parameters across child models in the searching of archtectur", "the new algorithm is empirically evaluated on two datasets cifar- and penn treeback"], "labels": ["QSN", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--- the new algorithm is  times faster and requires only / resources and the performance gets worse only slightli", "overall the paper is well-written", "although the methodology within the paper appears to be incremental over previous nas method the efficiency got improved quite significantli", "this paper proposes to perform link prediction in knowledge bases by supplementing the original entities with multimodal information such as text description or imag", "a model based on distmult able to encode all sort of information when scoring triples is presented with experiments on  new datasets based on yago and movielen"], "labels": ["APC", "APC", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper reads well and the results appear sound", "unfortunately the contribution seems rather small to be accepted for iclr", "this is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental result", "* having only results on new datasets makes it hard to compare the objective quality of the distmult baselines and hence of the improvements due to the multimodal info", "isn't there any existing benchmark where this could have an impact"], "labels": ["APC", "CRT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the much better performance of conve is worrying ther", "it is suggested that the proposed approach could be incorporated in conve to lead to similar improvements than on distmult the paper would be much stronger with thos", "* are we sure that the textual description do not explicitly contain the information of the triple to be predict", "this would explain the massive gains in yago", "* for table  the similarities are not strik"], "labels": ["CRT", "SUG", "QSN", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what were the nearest neighboring posters in the original vgg space they should not be that bad too", "* the work on multimodal embeddings like multimodal distributional semantics by bruni et al or multi-and cross-modal semantics beyond vision grounding in auditory perception by kiela et al could be discussed/cit", "the main idea in the paper is fairly simple the paper considers sgd over an objective of the form of a sum over examples of a quadratic loss", "the basic form of sgd selects an example uniformli", "instead  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradi"], "labels": ["QSN", "SUG", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a good method that builds on classic pps sampling is to select examples with higher normed gradients with higher probability [alain et al ]", "a good method that builds on classic pps sampling is to select examples with higher normed gradients with higher probability [alain et al ]", "with quadratic loss  the gradient increases with the inner product of the parameter vector concatenated with - and the example vector x_i concatenated with the label y_i  for the current parameter vector theta  we would like to sample examples so that the probability of sampling larger inner products is larg", "the paper uses lsh structures computed over the set of exampl", "to quickly sample examples with large inner products with the current parameter vector theta"], "labels": ["SMY", "DIS", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["essentially two vectors are hashed to the same bucket with probability that increases with their cosine similar", "so we select examples in the same lsh bucket as theta for rubstness we use multiple lsh map", "strengths  simple idea that can work well in the context of sampling examples for sgdweaknesses   the novelty in the paper is limit", "the use of lsh for sampling is a common technique to sample more similar vectors with higher prob", "there are theorems  but they are trivial straightforward applications of importance sampl"], "labels": ["SMY", "SMY", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is not well written", "the presentation is much more complex that need b", "references to classic weighted sampling ar", "the application is limited to certain loss functions for which we can compute lsh structur", "this excludes nn models and even the addition of regularization to the quadratic loss can affect the effect"], "labels": ["CRT", "CRT", "CNT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the submission describes an empirical study regarding the training performanceof gans more specifically it aims to present empirical evidence that thetheory of divergence minimization is more a tool to understand the outcome oftraining ie nash equillibrium than a necessary condition to be enforceduring training itself", "the work focuses on studying non-saturating gans using the modified generatorobjective function proposed by goodfellow et al in their seminal gan paper andaims to show increased capabilities of this variant compared to the standardminimax formul", "since most theory around divergence minimization is basedon the unmodified loss function for generator g the experiments carried out inthe submission might yield somewhat surprising results compared the theori", "if i may summarize the key takeaways from sections  and  they are- gan training remains difficult and good results are not guaranteed nd bullet  point", "- gradient penalties work in all settings but why is not completely clear"], "labels": ["SMY", "SMY", "DIS", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- ns-gans + gps seems to be best sample-generating combination and faster than  wgan-gp", "- ns-gans + gps seems to be best sample-generating combination and faster than  wgan-gp", "- some of the used metrics can detect mode collaps", "the submission's counter-claims are served by example cf figure  or figure description last sentence and mostly relate to statements made in the wganpaper arjovsky et ", "as a purely empirical study it poses more new and open questions on ganoptimization than it is able to answer providing theoretical answers isdeferred to future studi"], "labels": ["SMY", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a purely empirical study it poses more new and open questions on ganoptimization than it is able to answer providing theoretical answers isdeferred to future studi", "this is not necessarily a bad thing since theextensive experiments both toy and real are well-designed convincing andcomprehens", "novel combinations of gan formulations non-saturating withgradient penalties are evaluated to disentangle the effects of formulationchang", "overall this work is providing useful experimental insights clearly motivatingfurther studi", "this paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precis"], "labels": ["DIS", "DIS", "DIS", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea is to jointly train", "- a network predicting a polar origin", "- a module transforming the image into a log-polar representation according to the predicted origin", "- a final classifier performing the desired classification task", "a not too large translation of the input image therefore does not change the log-polar represent"], "labels": ["SMY", "SMY", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["rotation and scale from the polar origin result in translation of the log-polar represent", "as convolutions are translation equivariant the final classifier becomes rotation and scale equivariant in terms of the input imag", "rotation and scale can have arbitrary precision which is novel to the best of my knowledg", "+ in my opinion this is a simple attractive approach to rotation and scale equivariant cnn", "- the evaluation however is quite limit"], "labels": ["DIS", "DIS", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the evaluation however is quite limit", "the approach is evaluated on  several variants of mnist", "the authors introduce a new variant simmnist which is created by applying random similitudes to the images from mnist", "this variant is of course very well suited to the proposed method and a bit artifici", "d voxel occupancy grids with a small resolut"], "labels": ["CRT", "DIS", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the objects can be rotated around the z-axis and the method is used to be equivariant to this rot", "- since the method starts by predicting the polar origin wouldn't it be possible to also predict rotation and scal", "then the input image could be rectified to a canonical orientation and scale without needing equivari", "my intuition is that this simpler approach would work bett", "it should at least be evalu"], "labels": ["DIS", "QSN", "SUG", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["despite these weaknesses i think this paper should be interesting for researchers looking into equivariant cnn", "the paper is motivated by the fact that in gan training it is beneficial to constrain the lipschitz continuity of the discrimin", "the authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall lipschitz continuity of the entire discriminating network and propose gradient based methods to optimize a spectrally normalized object", "i think the methodology presented in this paper is neat and the experimental results are encourag", "however i do have some comments on the presentation of the paper using power method to approximate matrix largest singular value is a very old idea and i think the authors should cite some more classical references in addition to yoshida and miyato"], "labels": ["APC", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for examplematrix analysis book by bhatiamatrix computation book by golub and van loansome recent work in theory of noisy power method might also be helpful and should be cited for examplehttps//arxivorg/abs/", "i think the matrix spectral norm is not really differentiable hence the gradients the authors calculate in the paper should really be subgradi", "please clarify thi", "it should be noted that even with the product of gradient norm the resulting normalizer is still only an upper bound on the actual lipschitz constant of the discrimin", "can the authors give some empirical evidence showing that this approximation is much better than previous approximations such as l norms of gradient rows which appear to be much easier to optim"], "labels": ["SUG", "SUG", "SUG", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation gam", "the algorithm tagged 'consequentialist conditional cooperation' uses outcome-based accumulative rewards of different strategies established during prior train", "its core benefit is its adaptiveness towards diverse opposing player strategies eg selfish prosocial ccc while maintaining maximum reward", "while the contribution is explored in all its technical complexity fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phas", "during operation it then switches its strategy depending on a dynamically-calculated threshold reward value considering variation in agent-specific policies initial game states and stochasticity of rewards relative to the total reward of the played game inst"], "labels": ["APC", "SMY", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work is contrasted to tit-for-tat approaches that require complete observability and operate based on expected future reward", "in addition to the observability approximate markov tft amtft methods are more processing-intense since they fall back on a game's q-function as opposed to learned policies making ccc a lightweight altern", "commentsthe findings suggest the effectiveness of that approach", "in all experiments ccc-based agents fare better than agents operating based on a specific strategi", "while performing worse than the amtft approach and only working well for larger number of iterations the outcome-based evaluation shows benefit"], "labels": ["DIS", "DIS", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically in the ppd game the use of ccc produces interesting results when paired with cooperate agents in the ppd game ccc-based players produce higher overall reward than pairing cooperative players see figure  d & ", "this should be explain", "to improve the understanding of the ccc-based operation it would further be worthwhile to provide an additional graph that shows the action choices of ccc agents over time to clarify behavioural characteristics and convergence perform", "however when paired with non-cooperative players in the risky ppd game ccc players lead to an improvement of pay-offs by around  percent see figure  e compared to payoff received between non-cooperative players - vs - relative to - for defect", "this leads to the question how much ccc perform compared to random policy select"], "labels": ["APC", "APC", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given its reduction of processing-intensive and need for larger number of iterations how much worse is the random choice no processing independent of iter", "this is would be worthwhile to appreciate the benefit of the proposed approach", "another point relates to the fishing gam", "the game is parameterized with the rewards of + and +", "what is the bases for these parameter choic"], "labels": ["QSN", "APC", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what would happen if the higher reward was + or more interestingly if the game was extended to allow agents to fish medium-sized fish + in addition to small and large fish", "here it would be interesting to see how ccc fares in all combinations with cooperators and defector", "overall the paper is well-written and explores the technical details of the presented approach", "the authors position the approach well within contemporary literature both conceptually and using experimental evaluation and are explicit about its strengths and limit", "presentation aspects- minor typo page  last paragraph of introduction ` will act act ident"], "labels": ["DIS", "DIS", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["'- figure  should be shifted to the next page since it is not self-explanatory and requires more context", "the authors tackle the problem of estimating risk in a survival analysis setting with competing risk", "they propose directly optimizing the time-dependent discrimination index using a siamese survival network", "experiments on several real-world dataset reveal modest gains in comparison with the state of the art", "experiments on several real-world dataset reveal modest gains in comparison with the state of the art"], "labels": ["CRT", "SMY", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors should clearly highlight what is their main technical contribut", "for example eqs - appear to be background material since the time-dependent discrimination index is taken from the literature as the authors point out earli", "however this is unclear from the writ", "- one of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risk", "it is unclear why the authors solution is able to solve such an issue specially given the modest reported gains in comparison with several competitive baselin"], "labels": ["DFT", "SMY", "DFT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is unclear why the authors solution is able to solve such an issue specially given the modest reported gains in comparison with several competitive baselin", "in other words the authors oversell their own work specially in comparison with the state of the art", "in other words the authors oversell their own work specially in comparison with the state of the art", "- the authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty ther", "the application/setting may be novel"], "labels": ["QSN", "CRT", "FBK", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but not the architecture of choic", "- from eq  to eq  the authors argue that the denominator does not depend on the model parameters and can be ignor", "however afterwards the objective does combine time-dependent discrimination indices of several competing risks with different denominator valu", "this could be problematic if the risks are unbalanc", "- the competitive gain of the authors method in comparison with other competing methods is minor"], "labels": ["DFT", "DFT", "SMY", "DFT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors introduce ft d | x as cumulative incidence function cdf at the beginning of section  however afterwards they use r^mt x which they define as risk of the subject experiencing event m before t", "is the latter a proxy for the former how are they rel", "the paper introduces an application of graph neural networks li's gated graph neural nets ggnns specifically for reasoning about programs and program", "the core idea is to represent a program as a graph that a ggnn can take as input and train the ggnn to make token-level predictions that depend on the semantic context", "the two experimental tasks were  identifying variable misuse ie identifying bugs in programs where the wrong variable is us"], "labels": ["SMY", "QSN", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and  predicting a variable's name by consider its semantic context", "the paper is generally well written easy to read and understand and the results are compel", "the proposed ggnn approach outperforms bi-lstms on both task", "because the tasks are not widely explored in the literature it could be difficult to know how crucial exploiting graphically structured information is so the authors performed several ablation studies to analyze  this out", "because the tasks are not widely explored in the literature it could be difficult to know how crucial exploiting graphically structured information is so the authors performed several ablation studies to analyze  this out"], "labels": ["SMY", "APC", "SMY", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["those results show that as structural information is removed the ggnn's performance diminishes as expect", "as a demonstration of the usefulness of their approach the authors ran their model on an unnamed open-source project and claimed to find several bugs at least one of which potentially reduced memory perform", "overall the work is important original well-executed and should open new directions for deep learning in program analysi", "i recommend it be accept", "the authors are motivated by two problems inputting non-euclidean data such as graphs into deep cnns and analyzing optimization properties of deep network"], "labels": ["DIS", "CRT", "APC", "FBK", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular they look at the problem of maze testing where given a grid of black and white pixels the goal is to answer whether there is a path from a designated starting point to an ending point", "they choose to analyze mazes because they have many nice statistical properties from percolation theori", "for one the problem is solvable with breadth first search in ol^ time for an l x l maz", "they show that a cnn can essentially encode a bfs so theoretically a cnn should be able to solve the problem", "their architecture is a deep feedforward network where each layer takes as input two images one corresponding to the original maze a skip connection and the output of the previous lay"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["layers alternate between convolutional and sigmoid", "the authors discuss how this architecture can solve the problem exactli", "the pictorial explanation for how the cnn can mimic bfs is interest", "but i got a little lost in the  cases on pag", "for example what is r"], "labels": ["DIS", "SMY", "APC", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and what is the relation of the black/white and orange squar", "i thought this could use a little more clar", "though experiments they show that there are two kinds of minima depending on whether we allow negative initializations in the convolution kernel", "when positive initializations are enforced the network can more or less mimic the bfs behavior but never when initializations can be neg", "they offer a rigorous analysis into the behavior of optimization in each of these cases concluding that there is an essential singularity in the cost function around the exact solution yet learning succumbs to poor optima due to poor initial predictions in train"], "labels": ["QSN", "SUG", "SMY", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i thought this was an impressive paper that looked at theoretical properties of cnn", "the problem was very well-motivated and the analysis was sharp and offered interesting insights into the problem of maze solv", "what i thought was especially interesting is how their analysis can be extended to other graph problems while their analysis was specific to the problem of maze solving they offer an approach -- eg that of finding bugs when dealing with graph objects -- that can extend to other problem", "i would be excited to see similar analysis of other toy problems involving graph", "one complaint i had was inconsistent clarity while a lot was well-motivated and straightforward to understand i got lost in some of the details as an example the figure on page  did not initially make much sense to m"], "labels": ["DIS", "APC", "DIS", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also in the experiments the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initi", "finally there were various typos throughout one example is neglect minimua on page  should be neglect minima", "pros rigorous analysis well motivated problem generalizable results to deep learning theori", "cons clar", "this paper proposes to extend the prototypical network nips to the semi-supervised setting with three possible strategi"], "labels": ["QSN", "CRT", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the assigned pseudo-label", "another is able to deal with the case of distractors ie  unlabeled samples not beloning toany of the known categori", "in practice this second solution is analogous to the first but a general 'distractor' classis ad", "finally the third technique learns to weight the samples according to their distance to the original prototyp", "these strategies are evaluated in a particular semi-supervised transfer learning setting  the models are first trained on some source categories with few labeled data and large unlabeled samples this setting is derived by subselectingmultiple times a large dataset then they are used on a final target task with again few labeled data and large unlabeled samples but beloning to a different set of categori"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ the paper is well written well organized and overall easy to read", "+/-  this work builds largely on previous work", "it introduces only some small technical novelty inspired by soft-k-meansclustering that anyway seems to be effect", "it introduces only some small technical novelty inspired by soft-k-meansclustering that anyway seems to be effect", "+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level ofsemantic relatedness between the source and the target setsfew notes and quest"], "labels": ["APC", "DFT", "SMY", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why for the omniglot experiment the table reports the error result", "it would be better to present accuracy as for the other tables/experi", "it would be better to present accuracy as for the other tables/experi", "i would suggest to use source and target instead of train and test -- these two last terms are confusing becauseactually there is a training phase also at test tim", "although the paper indicate that there are different other few-shot methods that could be applicable her"], "labels": ["QSN", "SUG", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["no other approach is considered besides the prothotipical network and its vari", "an further external reference could be used to give an idea of what would be the experimental result at least in the supervised cas", "this paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained cnn on images within a same categori", "the local dimensionality of this paper is the svd dimensionality on the augmented images of the same class images which classified by the neural network as high prob", "however the motivation of the analysis of this paper is unclear"], "labels": ["SUG", "SMY", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i could not understand how such analysis contributes advance of representation learn", "further the analysis of this paper is not convinc", "-      i cannot believe the sum of svd dimensionality of each feature maps becomes equals to the dimensionality to concatenated feature map", "as shown in in fig the estimated dimensions and original dimensions are very differ", "although by looking some features maps this rule might be hold as shown in fig"], "labels": ["CRT", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the analysis is done on small examples without any theoretical analysi", "-tthe authors experimented one trained cnn and tested on images on only three categories persian cat container ship and volcao", "-tthe authors experimented one trained cnn and tested on images on only three categories persian cat container ship and volcao", "it is not clear if the same rules holds to other cnns and images of other categori", "this paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data point"], "labels": ["CRT", "DFT", "CRT", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main results are two-fold if the decision boundary are flat such as linear classifiers then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correl", "if the decision boundary are curved then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curv", "the authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model", "the main issue i am having is what are the applicable insight from the analysi", "why is universal perturbation an important topic as opposed to adversarial perturb"], "labels": ["SMY", "SMY", "SMY", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does the result implies that we should make the decision boundary more flat or curved but on different directions and how to achieve that", "it might be my mis-understanding but from my reading a prescriptive procedure for universal perturbation seems not attained from the results pres", "this paper proposes a simple problem to demonstrate the short-horizon bias of the learning rate meta-optim", "- the idealized case of quadratic function the analytical solution offers a good way to understand how t-step look ahead can benefit the meta-algorithm", "- the second part of the paper seems to be a bit disconnected to the quadratic function analysi"], "labels": ["QSN", "DFT", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the second part of the paper seems to be a bit disconnected to the quadratic function analysi", "it would be helpful to understand if there is gap between gradient based meta-optimization and the best effortgiven by the analytical solut", "- unfortunately no guideline or solution is offered in the pap", "in summary the idealized model gives a good demonstration of the problem itself", "i think it might be of interest to some audiences in iclr"], "labels": ["CRT", "SUG", "CRT", "DIS", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces an appealing application of deep learning use a deep network to approximate the behavior of a complex physical system and then design optimal devices eg airfoil shapes by optimizing this network with respect to its input", "overall this research direction seems fruitful both in terms of different applications and in terms of extra machine learning that could be done to improve performance such as ensuring that the optimization doesn't leave the manifold of reasonable design", "on one hand i would suggest that this work would be better placed in an engineering venue focused on fluid dynam", "on the other hand i think the iclr community would benefit from about the opportunities to work on problems of this natur", "=quality=the authors seem to be experts in their field"], "labels": ["SMY", "APC", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they could have done a better job explaining the quality of their final results though", "it is unclear if they are comparing to strong baselin", "=clarity=the overall setup and motivation is clear", "=originality=this is an interesting problem that will be novel to most member of the iclr commun", "i think that this general approach deserves further attention from the commun"], "labels": ["SUG", "CRT", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["=major comments=* it's hard for me to understand if the performance of your method is actually good", "you show that it outperforms simulated annealing is this the state of the art", "how would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil without relying on any computational simulation at al", "* you can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the input", "why not do lots of random initializations for the optim"], "labels": ["CRT", "QSN", "QSN", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["isn't that a good way to help avoid local optima", "* i'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simul", "for example you could evaluate the deep-net-predicted drag ratio vs the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shap", "if there's a gap it suggests that your nn approximation might have not been that accur", "=minor comments=* we also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optimagenerally people add noise to the gradients not the values of the paramet"], "labels": ["QSN", "SUG", "SUG", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["see for example uses of langevin dynamics as a non-convex optimization method", "* you have a complicated method for constraining the parameters to be in [-]", "why not just enforce this constraint by doing projected gradient desc", "for the constraint structure you have projection is trivial just clip the valu", "* the gradient decent approach required roughly  iterations to converge where as the simulated annealing approach needed at least this is of course confounded by the necessary cost to construct the training set which is necessary for the gradient descent approach"], "labels": ["SMY", "DIS", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'd point out that this construction can be done in parallel so it's less of a computational burden", "* i'd like to hear more about the effects of different parametrizations of the airfoil surfac", "you optimize the coefficients of a polynomial did you try anything els", "* fig  what does 'clean gradients' mean", "can you make this more precis"], "labels": ["SUG", "SUG", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the caption for fig  should explain what each of the sub figures i", "the paper focuses on a very particular hmm structure which involves multiple independent hmm", "each hmm emits an unobserved output with an explicit duration period", "this explicit duration modelling captures multiple scales of temporal resolut", "the actual observations are a weighted linear combination of the emissions from each latent hmm"], "labels": ["SUG", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the structure allows for fast inference using a spectral approach", "i found the paper unclear and lacking in detail in several key aspect", "it is unclear to me from algorithm  how the weight vectors w are estimated this is not adequately explained in the section on estim", "the authors make the assumption that each hmm injects noise into the unobserved output which then gets propagated into the overall observ", "what are reasons for his choice of model over a simpler model where the output of each hmm is uncorrupt"], "labels": ["SMY", "DFT", "DFT", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the simulation example does not really demonstrate the ability of the mshmm to do anything other than recover structure from data simulated under an mshmm", "it would be more interesting to apply to data simulated under non-markovian or other setups that would enable richer frequency structures to be included and the ability of mshmm to capture thes", "the real data experiments shows some improvements in predictive accuracy with fast infer", "however the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantag", "overall the paper presents an interesting approach"], "labels": ["DFT", "SUG", "APC", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the work lacks matur", "furthermore simulation and real data examples to explore the properties and utility of the method are requir", "qualitythe authors introduce a deep network for predictive cod", "it is unclear how the approach improves on the original predictive coding formulation of rao and ballard who also use a hierarchy of transform", "the results seem to indicate that all layers are basically performing the sam"], "labels": ["CRT", "DFT", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["no insight is provided about the kinds of filters that are learn", "nclarityin its present form it is hard to assess if there are benefits to the current formulation compared to already existing formul", "the paper should be checked for typo", "the paper should be checked for typo", "originalitythere exist alternative deep predictive coding models such as https//arxivorg/abs/"], "labels": ["DFT", "CRT", "SUG", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this work should be discussed and compar", "significance it is hard to see how the present paper improves on classical or alternative deep predictive coding result", "prosrelevant attempt to develop new predictive coding architectur", "consunclear what is gained compared to existing work", "this paper introduces a smooth surrogate loss function for the top-k svm for the purpose of plugging the svm to the deep neural network"], "labels": ["SUG", "CRT", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea is to replace the order statistics which is not smooth and has a lot of zero partial derivatives to the exponential of averages which is smooth and is a good approximation of the order statistics by a good selection of the temperature paramet", "the paper is well organized and clearly written", "the idea deserves a publ", "on the other hand there might be better and more direct solutions to reduce the combinatorial complex", "when the temperature parameter is small enough both of the original top-k svm surrogate loss  and the smooth loss  can be computed precisely by sorting the vector s first and take a good care of the boundary around s_{[k]}"], "labels": ["APC", "APC", "APC", "DFT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written and the authors do an admirable job of motivating their primary contributions throughout the early portions of the pap", "each extension to the dual actor-critic is well motivated and clear in context", "perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice multi-step updates regularized against mc returns stochastic mirror desc", "the practical implementation section losses some of this clear organization and could certainly be clarified each part tied into algorithm  and this was itself made less high-level but these are minor gripes overal", "turning to the experimental section i think the authors did a good job of evaluating their approach with the ablation study and comparisons with ppo and trpo"], "labels": ["APC", "APC", "SUG", "SUG", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there were a few things that jumped out to me that i was surprised bi", "the difference in performance for dual-ac between figure  and figure b is significant but the only difference seems to be a reduce batch size is this right", "this suggests a fairly significant sensitivity to this hyperparameter if so", "reproducibility in continuous control is particularly problemat", "nonetheless in recent work ppo and trpo performance on the same set of tasks seem to be substantively different than what the authors get in their experi"], "labels": ["DIS", "QSN", "DIS", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'm thinking in particular ofproximal policy optimization algorithms schulman et ", "multi-batch experience replay for fast convergence of continuous action control han and sung", "in both these cases the results for ppo and trpo vary pretty significantly from what we see here and an important one to look at is the inverteddoublependulum-v task which i would think ppo would get closer to  and trpo not get off the ground", "part of this could be the notion of an iteration which was not clear to me how this corresponded to actual time step", "most likely to my mind is that the parameterization used discussed in the appendix is improving trpo and hurting ppo"], "labels": ["DIS", "DIS", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["with these in mind i view the comparison results with a bit of uncertainty about the exact amount of gain being achiev", "which may beg the question if the algorithmic contributions are buying much for their added complex", "proswell written thorough treatment of the approach", "improvements on top of dual-ac with ablation study show improv", "consempirical gains might not be very larg"], "labels": ["CRT", "QSN", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper discusses learning in a neural network with three layers where the middle layer is topographically organ", "the learning dynamics defined for the network results in specific update equations of the weights w eqn  which combine elements of supervised learning and self-organizing maps som", "the weights thus change according to the imposed neighborhood relationship and depending on the class label", "what i like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context", "i agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learn"], "labels": ["SMY", "SMY", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the general methodology also makes sense to m", "however i do have concerns including two major concerns a delimitation of results from earlier work b numerical results especially tab", "a the paper derives the main update equation of w which combines self-organization and label-sensitive learning - eqn", "this equation is then discussed and the som-like updates and the differences to previous pure soms are highlight", "the paper also states secs  and  that the the network studied here is based on hartono et al  with the main difference of the sigmoidal ouput layer being replaced by a softmax lay"], "labels": ["APC", "CRT", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is missing is a discussion of the differences regarding the later numerical experiments and a clear delimitation to hartono et al  when eqn  is discuss", "what is the major structural difference to their eqn  which is discussed along very similar lines as eqn  of this pap", "also after reading the abstract of this paper one may think that this is the first paper discussing the som / supervised learning combin", "b a further difference to hartono et al  are comparisons with multi-layer networks and the presentation and discussion of this comparison is my strongest concern", "in the first paragraph of sec  the competing deep networks are introduc"], "labels": ["DFT", "QSN", "CRT", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then it is stated the number of hidden neurons as well as the structures for the deep neural networkswere empirically tried and the results of the best settings were registered for comparison st paragraph sec", "what i do not understand are then the high classification errors reported in tab", "it is known that even basic multi-layer perceptrons mlps result in much lower classification errors eg for mnist lecun et al  is a classical example with less then % error on mnist with many later examples that improve on thes", "also the well-known original dbn paper has mnist as main example and main selling point with close to % error", "why are the classification errors for dbn and mlp in the tab  so high"], "labels": ["DIS", "CRT", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and if they are in reality much lower then competitiveness of s-rrbf in terms of classification results to these systems is question", "the table makes me having doubts regarding the competitiveness of s-rrbf", "i therefore disagree with the conclusion that this paper has shown that s-rrbfs are comparable to the best performer for most of the diverse benchmark applications last paragraph in conclus", "the feature of providing auxiliary visual information also conclusion is much more convincing but also a feature of hartono et ", "more generally putting the biological arguments aside why would a d neighborhood relationship be help"], "labels": ["CRT", "CRT", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i see a benefit in interpretation which can help", "also if there is an intrinsic d hidden structure in the data then imposing a d representation can help as a sort of a prior", "but in general there may not be a d intrinsic property or there is a higher dimensional hidden structure - so why not d or more related to this why not using an objective that would result in a dynamics similar to a growing neural gas instead of an som", "minorthe work is first introduced as multi-layer but only the single hidden layer case is actually discuss", "i would suggest to either really add multi-hidden-layer results which is not really doable in a conference revision or state multi-layer work as outlook"], "labels": ["DIS", "CRT", "QSN", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["fig  bad readability of axes label", "is a hierarchical - are hierarchicalyields - yieldtwice otherwise after eqn are can be viewedthey occurscan can readily expandedtransfer transform", "this paper presents a method for image classification given test-time computational budgeting constraint", "two problems are considered  any-time classification in which there is a time constraint to evaluate a single example and batched budgets in which there is a fixed budget available to classify a large batch of imag", "a convolutional neural network structure with a diagonal propagation layout over depth and scale is used so that each activation map is constructed using dense connections from both same and finer scale featur"], "labels": ["CRT", "CRT", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this way coarse-scale maps are constructed quickly then continuously updated with feed-forward propagation from lower layers and finer scales so they can be used for image classification at any intermediate stag", "evaluations are performed on imagenet and cifar-", "i would have liked to see the mc baselines also evaluated on imagenet --- i'm not sure why they aren't there as wel", "also on p i'm not entirely clear on how the network reduction is performed ---", "it looks like finer scales are progressively dropped in successive block"], "labels": ["SUG", "SMY", "CRT", "CRT", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i don't think they exactly correspond to those that would be needed to evaluate the full model this is lazy evalu", "a picture would help here showing where the depth-layers are divided between block", "ni was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result", "it seems this relies on having a batch that is both large and varied so that its evaluation time will converge towards the expect", "so this isn't really a hard constraint just an expected result for batches that are large and varied enough"], "labels": ["CRT", "DFT", "CRT", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is fine but could perhaps be pointed out if that is indeed the cas", "overall this seems like a natural and effective approach and achieves good result", "the authors introduce a sequential/recurrent model for generation of small graph", "the recurrent model takes the form of a graph neural network", "similar to rnn language models new symbols nodes/edges are sampled from bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden st"], "labels": ["SMY", "APC", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is very well written nicely structured provides extensive experimental evaluation and examines an important problem that has so far not received much attention in the field", "the proposed model has several interesting novelties mainly in terms of new applications/experiments and being fully auto-regressive yet also shares many similarities with the generative component of the model introduced in [] not cited both models make use of recurrent graph neural networks to learn intermediate node representations from which they predict whether new nodes/edges should be added or not", "[] speeds this process up by predicting multiple nodes and edges at once whereas in this paper such a multi-step process is left for future work", "training the generative model with fixed ground-truth ordering was similarly performed in [] ucstrong supervisionud and is thus not particularly novel", "eqs- why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop h_v - h_vu"], "labels": ["APC", "APC", "CRT", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["have the authors experimented with other variants dropping the weight sharing in either or both of these step", "ordering problem a solution for the ordering problem was proposed in [] learning a matching function between the orderings of model output and ground truth", "a short discussion of this result would make the paper strong", "for chemical molecule generation a direct comparison to some more recent work eg the generator of the grammar vae [] would be insight", "other minor points- in the definition of f_nodes what is pi"], "labels": ["QSN", "DIS", "SUG", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be good to explicitly state that boldface s is a vector of scores s_u or score vectors in case of multiple edge types for all u in v", "u- the following statement is unclear to me ucbut building a varying set of objects is challenging in the first place and the graph model provides a way to do it", "ud maybe this can be substantiated by experimental results eg a comparison against pointer networks []", "- typos in this sentence uclastly when compared using the genaric graph generation decision sequence the graph architecture outperforms lstm in nll as wel", "udoverall i feel that this paper can be accepted with some revisions as discussed above as even though it shares many similarities with previous work on a very related problem it is well-written well-presented and addresses an important problem"], "labels": ["SUG", "CRT", "QSN", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["udoverall i feel that this paper can be accepted with some revisions as discussed above as even though it shares many similarities with previous work on a very related problem it is well-written well-presented and addresses an important problem", "[] dd johnson learning graphical state transitions iclr [] r stewart m andriluka and a y ng end-to-end people detection in crowded scenes cvpr", "[] mj kusner b paige jm hernandez-lobato grammar variational autoencoder icml [] o vinyals m fortunato n jaitly pointer networks nip", "authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source inform", "the model is based on the swan architecture which is previously proposed and an additional local reordering layer to reshuffle source information to adjust those positions to the target sent"], "labels": ["FBK", "DIS", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["using the swan architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monoton", "also the concept of local reordering mechanism looks well to improve the basic swan model to reconfigure it to the situation of machine translation task", "the window size of the local reordering layer looks like the distortion limit used in traditional phrase-based statistical machine translation methods and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model small window sizes may drop information about long depend", "for example verbs in german sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sent", "since reordering windows restrict the context of each position to a limited number of neighbors it may not capture distant information enough"], "labels": ["APC", "APC", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i expected that some observations about this point will be unveiled in the paper but unfortunately the paper described only a few bleu scores with different window sizes which have not enough information about it", "i expected that some observations about this point will be unveiled in the paper but unfortunately the paper described only a few bleu scores with different window sizes which have not enough information about it", "it is useful for all followers of this paper to provide some observations about this point", "in addition it could be very meaningful to provide some experimental results on linguistically distant language pairs such as japanese and english or simply reversing word orders in either source or target sentences this might work to simulate the case of distant reord", "authors argued some differences between conventional attention mechanism and the local reordering mechanism but it is somewhat unclear that which ones are the definite difference between those approach"], "labels": ["DFT", "CRT", "SUG", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a super interesting and mysterious point of the proposed method is that it achieves better bleu than conventional methods despite no any global language models table  row", "and the language model options table  row  and footnote  may reduce the model accuracy as well as it works not so effect", "this phenomenon definitely goes against the intuitions about developing most of the conventional machine translation model", "specifically it is unclear how the model correctly treats word connections between segments without any global language model", "authors should pay attention to explain more detailed analysis about this point in the pap"], "labels": ["APC", "DIS", "DIS", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["eq  is incorrect", "according to fig  the conditional probability in the product operator should be revised to pa_t | x_{t} a_{t-} and the independence approximation to remove a_{t-} from the conditions should also be noted in the pap", "nevertheless the condition x_{t} could not be reduced because the source position is always conditioned by all previous positions through an rnn", "this work explores some approaches in the object detection field of computer vision a a soft attention map based on the activations on convolutional layers b a classification regarding the location of an object in a x grid over the image c an autoencoder that the authors claim to be aware of the multiple object instances in the imag", "these three proposals are presented in a framework of a robot vision module although neither the experiments nor the dataset correspond to this domain"], "labels": ["CRT", "SUG", "DIS", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from my perspective the work is very immature and seems away from current state of the art on object detection both in the vocabulary performance or challeng", "the proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other techniqu", "this important flaw in the evaluation prevents any fair comparison with the state of the art", "the text is also difficult to follow the three contributions seem disconnected and could have been presented in separate works with a more deeper discuss", "in particular i have serious problems understanding what is exactly the contribution of the cnn pre-trained with imagenet when learning the soft-attention map"], "labels": ["CRT", "DFT", "DFT", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the reference to a gan architecture seems very forced and out of the scop", "the reference to a gan architecture seems very forced and out of the scop", "what is the interest of the localization network  the task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the x grid", "what is the interest of the localization network  the task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the x grid", "the authors talk about an autoencoder architecture but also on a classification network where the labels correspond to the object count i could not undertstand what is exactly assessed in this sect"], "labels": ["DFT", "DIS", "DIS", "QSN", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally the authors violate the double-bind review policy by clearly referring to their previous work on experiental robot learn", "i would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vis", "the main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of jia and liang emnlp", "in that work a distractor sentence is manually added to a passage to superficially but not logically support an incorrect answ", "it was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human read"], "labels": ["CRT", "FBK", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper replaces the manual addition of a distractor sentence with a single word replacement where a narrator is trained adversarially to select a replacement to fool the question answering system", "this idea seems interesting but very difficult to evalu", "an adversarial word replacement my in fact destroy the factual information needed to answer the question and there is no control for thi", "the performance of the question answering system in the presence of this adversarial narrator is of unclear significance and the empirical results in the paper are very difficult to interpret", "no comparisons with previous work are given and perhaps cannot be given"], "labels": ["SMY", "CRT", "CRT", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a better model would be the addition of a distractor sentence as this preserves the information in the original passag", "a language model could probably be used to generate a compelling distractor", "but we want that the corrupted passage has the same correct answer as the uncorrupted passage and this difficult to guarante", "a trained narrator could learn to actually change the correct answ", "this paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the fourier basi"], "labels": ["CRT", "CRT", "CRT", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the fourier basi", "the main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity paramet", "the main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity paramet", "in the presented experiments the new spectral method outperforms the tool based on the bayesian optimization technique based on mab and random search", "their result also has an application in learning decision trees where it significantly improves the sample complexity bound"], "labels": ["DIS", "SUG", "DIS", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthe main theoretical result ie the improvement in the sample complexity when learning decision trees looks very strong", "however i find this result to be out of the context with the main theme of the pap", "i find it highly unlikely that a person interested in using harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complex", "also the theoretical results are developed for harmonica- while harmonica-q is the main method used in the experi", "also the theoretical results are developed for harmonica- while harmonica-q is the main method used in the experi"], "labels": ["APC", "DFT", "DFT", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when it comes to the experiments only one real-world experiment is pres", "it is hard to conclude which method is better based on a single real-world experi", "it is hard to conclude which method is better based on a single real-world experi", "moreover the plots are not very intuitive ie one would expect that random search takes the smallest amount of tim", "i guess the authors are plotting the running time that also includes the time needed to evaluate different configur"], "labels": ["DIS", "SUG", "DIS", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if this is the case some configurations could easily require more time to evaluate than the oth", "it would be useful to plot the total number of function evaluations for each of the methods next to the presented plot", "it is not clear what is the stopping criterion for each of the methods used in the experi", "one weakness of harmonica is that it has  hyperparameters itself to be tun", "it would be great to see how harmonica compares with some of the high-dimensional bayesian optimization method"], "labels": ["DIS", "DIS", "DFT", "DFT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["few more questionswhich problem does harmonica-q solves that is present in harmonica- and what is the intuition behind the fact that it achieves better empirical result", "how do you find best t minimizers of g_i in line  of algorithm", "eigenoption discovery through the deep successor represent", "the paper is a follow up on previous work by machado et al  showing how proto-value functions pvfs can be used to define options called uceigenoptionsud", "in essence machado et al  showed that in the tabular case if you interpret the difference between pvfs as pseudo-rewards you end up with useful opt"], "labels": ["DIS", "DIS", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they also showed how to extend this idea to the linear case one replaces the laplacian normally used to build pvfs with a matrix formed by sampling differences phis' - phis where phi are featur", "the authors of the current submission extend the approach above in two ways they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear on", "interestingly the way they do so is through the successor representation sr", "stachenfeld et al  have showed that pvfs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all srs of an mdp", "thus if we have the sr matrix we can replace the laplacian mentioned abov"], "labels": ["SMY", "SMY", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this provides benefits already in the tabular case since srs naturally extend to domains with stochastic dynam", "on top of that one can apply a trick similar to the one used in the linear case --that is  construct the matrix representing the diffusion model by simply stacking samples of the sr", "thus if we can learn the srs we can extend the proposed approach to the nonlinear cas", "the authors propose to do so by having a deep neural network similar to kulkarni et al 's deep successor represent", "the main difference is that instead of using an auto-encoder they learn features phis such that the next state s' can be recovered from it they argue that this way psis will retain information about aspects of the environment the agent has control ov"], "labels": ["DIS", "DIS", "SMY", "SMY", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a well-written paper with interesting and potentially useful insight", "i only have a few comments regarding some aspects of the paper that could perhaps be improved such as the way eigenoptions are evalu", "one question left open by the paper is the strategy used to collect data in order to compute the diffusion model and thus the opt", "in order to populate the matrix that will eventually give rise to the pvfs the agent must collect transit", "the way the authors propose to do it is to have the agent follow a random polici"], "labels": ["APC", "CRT", "QSN", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so in order to have options that lead to more direct purposeful behaviour the agent must first wander around in a random purposeless way and hope that this will lead to a reasonable exploration of the state spac", "this problem is not specific to the proposed approach though in fact any method to build options will have to resolve the same issu", "one related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built the diffusion time or the expected number of steps between any two states of an mdp when following a random walk", "first although this metric makes intuitive sense it is unclear to me how much it reflects control performance which is what we ultimately care about", "perhaps more important measuring performance using the same policy used to build the options the random policy seems somewhat unsatisfactory to m"], "labels": ["APC", "DIS", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to see why suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state spac", "in this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the pap", "conversely if the diffusion time were measured under another policy it also seems likely that options built with a random policy would not perform so well assuming that the state space is reasonably large to make an exhaustive exploration infeas", "more generally we want options built under a given policy to reduce the diffusion time of other policies preferably ones that lead to good control perform", "another point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the atari experiments described in sect"], "labels": ["DIS", "DIS", "DIS", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the last paragraph of page  the authors mention that eigenoptions are more effective in reducing the diffusion time than ucrandom optionsud built based on randomly selected sub-goals however looking at figure  the terminal states of the eigenoptions look a bit like randomly-selected  sub-go", "this is especially true when we note that only a subset of the options are shown given enough random options it should be possible to select a subset of them that are reasonably spread across the state space as wel", "interestingly one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints as explained in the second paragraph of page  instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead which is the same as having a policy learned with a discount factor of zero", "the fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very inform", "as a comparison when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic polici"], "labels": ["DIS", "DIS", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore one could argue that the proposed approach not only generate useful options it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them", "this paper develop theory to study the impact of stochastic gradient noise for sgd especially for deep neural network model", "it is shown that when the gradient noise is isotropic normal sgd converges to a distribution tilted by the original objective funct", "however when the gradient noise is non isotropic normal which is shown common in many models especially in deep neural network models the behavior of sgd is intriguing which will not converge to the tilted distribution by the original objective function sometimes more interestingly will converge to limit cycles around some critical points of the original objective funct", "the paper also provides some hints on why using sgd can get good generalization ability than gradient descendi think the finding of this paper is interesting and the technical details are correct i still have the following commentsfirst assumption  seems a bit too abstract"], "labels": ["APC", "SMY", "SMY", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not easy to see what the assumption mean", "it would be better if an example is given which is verified to satisfy the assumpt", "another comment is related to the overall content of this pap", "thought the paper point out that sgd will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal it remains to show how far away this stationary distribution is from the original distribution defined by the objective funct", "this paper suggests a simple yet effective approach for learning with weak supervis"], "labels": ["DIS", "SUG", "DIS", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper suggests a simple yet effective approach for learning with weak supervis", "this learning scenario involves two datasets one with clean data ie labeled by the true function and one with noisy data collected using a weak source of supervis", "the suggested approach assumes a teacher and student networks and builds the final representation incrementally by taking into account the fidelity of the weak label when training the student at the final step", "the fidelity score is given by the teacher after being trained over the clean data and it's used to build a cost-sensitive loss function for the stud", "the suggested method seems to work well on several document classification task"], "labels": ["APC", "DIS", "SMY", "SMY", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i liked the pap", "i would like the authors to consider the following questions - - over the last  years or so many different frameworks for learning with weak supervision were suggested eg indirect supervision distant supervision response-based constraint-based to name a few", "first i'd suggest acknowledging these works and discussing the differences to your work", "second - is your approach applicable to these framework", "it would be an interesting to compare to one of those methods  eg distant supervision for relation extraction using a knowledge base and see if by incorporating fidelity score results improv"], "labels": ["APC", "DIS", "APC", "QSN", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- can this approach be applied to semi-supervised learn", "is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework", "- the paper emphasizes that the teacher uses the student's initial representation when trained over the clean data", "is it clear that this step in need", "can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratchusing different architecture than the stud"], "labels": ["QSN", "QSN", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i went over the authors comments and i appreciate their efforts to help clarify the issues rais", "this paper views graph classification as image classification and shows that the cnn model adapted from image net can be effectively adapted to the graph classif", "the idea is interesting and the result looks promis", "but i do not understand the intuition behind the success of analogizing graph with imag", "fundamentally a convolutional filter stands for a operation within a small neighborhood on the imag"], "labels": ["APC", "SMY", "APC", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however it is unclear how it means for the graph represent", "is the neighborhood predefin", "are the graph nodes pre-ord", "i am also curious with the effect of pre-trained model from imagenet", "since the graph presentation does not use color channels  pre-trained model is used different from what it was designed to"], "labels": ["CRT", "QSN", "QSN", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would imagine the benefit of using imagenet is just to bring a random high-dimensional embed", "in addition i wonder whether it will help to fine-tune the model on the graph classification data", "could this submission show some fine-tune experi", "summary the paper provides the first evidence of effectively training large rnn based language models under the constraint of differential privaci", "the paper focuses on the user-level privacy setting where the complete contribution of a single user is protected as opposed to protecting a single training exampl"], "labels": ["DIS", "DIS", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the algorithm is based on the federated averaging and federated stochastic gradient framework", "positive aspects of the paper the paper is a very strong empirical paper with experiments comparable to industrial scal", "the paper uses the right composition tools like moments accountant to get strong privacy guarante", "the main technical ideas in the paper seem to be i bounding the sensitivity for weighted average queries and ii clipping strategies for the gradient parameters in order to control the norm", "both these contributions are important in the effectiveness of the overall algorithm"], "labels": ["SMY", "APC", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["concern the paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language model", "i did not find strong algorithmic ideas in the pap", "i found the paper to be lacking in that respect", "the paper presents a means of evaluating a neural network securely using homomorphic encrypt", "a neural network is already trained and its weights are publ"], "labels": ["DIS", "CRT", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the network is to be evaluated over a private input so that only the final outcome of the computation-and nothing but that-is finally learn", "the authors take a binary-circuit approach they represent numbers via a fixed point binary representation and construct circuits of secure adders and multipliers based on homomorphic encryption as a building block for secure g", "this allows them to perform the vector products needed per layer two's complement representation also allows for an easy implementation of the relu activation function by checking multiplying by the complement of the sign bit", "the fact that multiplication often involves public weights is used to speed up computations wherever appropri", "a rudimentary  experimental evaluation with small networks is provid"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["all of this is somewhat straightforward a penalty is paid by representing numbers via fixed point arithmetic which is used to deal with relu mostli", "this is somewhat odd it is not clear why eg garbled circuits where not used for something like this as it would have been considerably faster than fh", "there is also a work in this area that the authors do not cite or contrast to bringing the novelty into question please see the following papers and references therein", "gilad-bachrach r dowlin n laine k lauter k naehrig m and wernsing j cryptonets applying neural networks to encrypted data with high throughput and accuracy in proceedings of the rd international conference on machine learning  pp u", "secureml a system for scalable privacy-preserving machine learningpayman mohassel and yupeng zhang"], "labels": ["SMY", "CRT", "CRT", "SUG", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["shokri r and shmatikov v privacy-preserving deep learn", "inproceedings of the nd acm sigsac conference on computer and communications security  acm pp u", "the first paper is the most related also using homomorphic encryption and seems to cover a superset of the functionalities presented here more activation functions a more extensive analysis and faster decryption tim", "the second paper uses arithmetic circuits rather than he but actually implements training an entire neural network secur", "minor detailsthe problem scenario states that the model/weights is private but later on it ceases to be so weights are not encrypt"], "labels": ["SUG", "SUG", "SMY", "SMY", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["both deep learning and fhe are relatively recent paradigms deep learning is certainly not recent while gentry's paper is now  years old", "in theory this system alone could be used to compute anything securely this is informal and incorrect", "can it solve the halting problem", "however in practice the operations were incredibly slow taking up to  minutes in some cases it is unclear what operations are referred to her", "`the papers aims to provide a quality measure/test for gan"], "labels": ["CRT", "CRT", "QSN", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the objective is ambitious an deserve attent", "as gans are minimizing some f-divergence measure the papers remarks that computing a  wasserstein distance between two distributions made of a sum of diracs is not a degenerate case and is tract", "so they propose evaluate the current approximation of a distribution learnt by a gan by using this distance as a baseline performance in terms of w distance and computed on a hold out dataset", "a first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in w distance rather than just trying to minimize the desired f-diverg", "more specifically as they assess the performance in terms of w distance i would would be tempted to just minimize the given criterion"], "labels": ["SUG", "SMY", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this would be very interesting to have arguments on why being better than the dirac estimation in terms of w distance would lead to better performance for others tasks as other f-divergences or image gener", "according to the authors the core claims are/ we suggest a formalisation of the goal of gan training /generative modelling more broadly in terms of divergence minimis", "this leads to a natural testable notion of generalis", "formalization in terms of divergence minimization is not new see o bousquet & all https//arxivorg/pdf/pdf", "and i do not feel like this paper actually performs any test in a statistical sens"], "labels": ["SUG", "SMY", "DIS", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of w dist", "/ we use this test to evaluate the success of gan algorithms empirically with the wasserstein distance as our diverg", "here the distance does not seems so good because the performance in generation does not seems to only be related to w dist", "nevertheless there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictur", "i would enjoyed more digging in this direct"], "labels": ["DIS", "SMY", "CRT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors proposes to solve this issue by relying to an embedded space where the l distance makes more sense for pictures densenet", "this is of course very reasonable but i would expect anyone working on distribution over picture to work with such embed", "here i'm not sure if this papers opens a new way to improve the embedding making use on non labelled data", "one could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work", "/ we find that whether our proposed test matches our intuitive sense of gan quality depends heavily on the ground metric used for the wasserstein dist"], "labels": ["SMY", "SUG", "FBK", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this claim is highly biased by who is giving the intuitive sens", "it would be much better evaluated thought a mechanical turk test", "/ we discuss how to use these insights to improve the design of wgans more gener", "as our understanding of the gans dynamics are very coarse i feel this is not a good thing to claim that doing xxx should improve things without actually trying it", "the main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts and reduce the variance of those control variates using the reparameterization trick"], "labels": ["CRT", "SUG", "DIS", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts and reduce the variance of those control variates using the reparameterization trick", "this is laid out primarily in equations - and seems like a nice idea although i must admit i had some trouble following the maths in equ", "they include results showing that their method has better sample efficiency than trpo which their method also uses under the hood to update value function paramet", "my main issue with this paper is that the empirical section is a bit weak for instance only one run seems to be shown for both methods there is no mention of hyper-parameter selection and the measure used for generating table  seems pretty arbitrary to me how were those thresholds chosen", "my main issue with this paper is that the empirical section is a bit weak for instance only one run seems to be shown for both methods there is no mention of hyper-parameter selection and the measure used for generating table  seems pretty arbitrary to me how were those thresholds chosen"], "labels": ["APC", "APC", "APC", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition one thing i would have liked to get out of this paper is a better understanding of how much each component help", "this could have been done via empirical work for instance- explore the effect of the planning horizon and implicitly compare to svg which as the authors point out is the same as their method with a horizon of", "- show the effect of the reparameterization trick on estimator variance- compare the bias and variance of trpo estimates vs the proposed method", "the manuscript proposes a new framework for inference in rnn based upon the bayes by backprop bbb algorithm", "in particular the authors propose a new framework to sharpen the posterior"], "labels": ["APC", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the hierarchical prior in  and  frame an interesting modification to directly learning a multivariate normal variational approxim", "in the experimental results it seems clear that this approach is beneficial but it's not clear as to whi", "in particular how does the variational posterior change as a result of the hierarchical prior", "it seems that  would push the center of the variational structure back towards the map point and reduces the variance of the output of the hierarchical prior however with the two layers in the prior it's unclear what actually is happen", "it seems that  would push the center of the variational structure back towards the map point and reduces the variance of the output of the hierarchical prior however with the two layers in the prior it's unclear what actually is happen"], "labels": ["APC", "QSN", "QSN", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it", "as a final point the authors state as long as the improvement along the gradient is great than the kl loss incurredthis method is guaranteed to make progress towards optimizing l", "do the authors mean that the negative log-likelihood will be improved in this cas", "or the actual optim", "improving the negative log-likelihood seems straightforward but i am confused by what the authors mean by optim"], "labels": ["SMY", "APC", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the new evaluation metric proposed in section  is confusing and i do not understand what the metric is trying to captur", "this needs significantly more detail and explan", "also it is unclear to me what would happen when you input data examples that are opposite to the original input sequence in particular for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable output", "it's completely feasible that these outputs would just be highly uncertain and i'm not sure how you can ascribe meaning to them", "the authors should not compare to the uniform prior as a baseline for entropi"], "labels": ["CRT", "DFT", "QSN", "CRT", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it's much more revealing to compare it to the empirical likelihoods of the word", "update  in light of yoon kim's retraction of replication i've downgraded my score until the authors provide further validation ie cifar and imagenet sampl", "summarythis paper proposes vae modifications that allow for the use multiple layers of latent vari", "the modifications are  a shared en/decoder parametrization as used in the ladder vae []", "the latent variable parameters are functions of a cnn"], "labels": ["SUG", "FBK", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["and  use of a pixelcnn decoder [] that is fed both the last layer of stochastic variables and the input image as done in []", "negative log likelihood nll results on cifar  binarized mnist dynamic and static omniglot and imagenet x are report", "samples are shown for cifar  mnist and omniglot", "evaluationpros  the paperus primary contribution is experimental sota results are achieved for nearly every benchmark image dataset the exception being statically binarized mnist which is only  nats off", "this experimental feat is quite impressive and moreover in the comments on openreview yoon kim claims to have replicated the cifar result"], "labels": ["SMY", "SMY", "SMY", "APC", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i commend the authors for making their code available already via dropbox", "lastly i like how the authors isolated the effect of the concatenation via the ufame no concatenationu result", "cons  the paper provides little novelty in terms of model or algorithmic design as using a cnn to parametrize the latent variables is the only model detail unique to this pap", "in terms of experiments the cifar samples look a bit blurry for the reported nll as others have mentioned in the openreview com", "i find the authorsu claim that fame is performing superior global modeling interest"], "labels": ["APC", "APC", "APC", "CRT", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is there a way to support this experiment", "also i would have liked to see results w/o the cnn parametrization how important was this choic", "conclusionwhile the paper's conceptual novelty is low", "the engineering and experimental work required to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset is commend", "i recommend the paperus acceptance for this reason"], "labels": ["QSN", "DIS", "CRT", "APC", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[]  c sonderby et al ucladder variational autoencodersud  nips []  a van den oord et al ucconditional image generation with pixelcnn decodersud arxiv []  i gulrajani et al ucpixelvae a latent variable model for natural imagesud  iclr", "this paper proposes a compositional nearest-neighbors approach to image synthesis including results on several conditional image generation dataset", "pros- simple approach based on nearest-neighbors likely easier to train compared to gan", "- scales to high-resolution imag", "cons- requires a potentially costly search procedure to generate imag"], "labels": ["DIS", "SMY", "APC", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task", "the authors propose a probabilistic framework for semi-supervised learning and domain adapt", "by varying the prior distribution the framework can incorporate both generative and discriminative model", "the authors emphasize on one particular form of constraint on the prior distribution that is weight parameter sharing and come up with a concrete model named dauto for domain adapt", "a domain confusion loss is added to learn domain-invariant feature represent"], "labels": ["CRT", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors compared dauto with several baseline methods on several datasets and showed improv", "the paper is well-organized and easy to follow", "the probabilistic framework itself is quite straight-forward", "the paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing schem", "the proposed dauto is essentially dann+autoencod"], "labels": ["APC", "APC", "APC", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the minimax loss employed in dann and dauto is known to be prone to degenerated gradient for the gener", "it would be interesting to see if the additional auto-encoder part help address the issu", "the experiments miss some of the more recent baseline in domain adaptation such as adversarial discriminative domain adaptation tzeng eric et ", "it could be more meaningful to organize the pairs in table by target domain instead of source for example grouping - - - and - in the same block", "dauto does seem to offer more boost in domain pairs that are less similar"], "labels": ["DIS", "SUG", "DFT", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper studies the expressive power provided by overlap in convolution layers of dnn", "instead of relu networks with average/max pooling as is standard in practice the authors consider linear activations with product pool", "such networks which have been known as convolutional arithmetic circuits are easier to analyze due to their connection to tensor decomposition and provide insight into standard dnn", "such networks which have been known as convolutional arithmetic circuits are easier to analyze due to their connection to tensor decomposition and provide insight into standard dnn", "nfor these networks the authors show that overlap results in the overall function having a significantly higher rank exponentially larger than a function obtained from a network with non-overlapping convolutions where the stride = filter width"], "labels": ["SMY", "DIS", "SMY", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key part of the proof is showing a lower bound on the rank for networks with overlap", "they do so by an argument well-known in this space showing a lower bound for some particular tensor and then inferring the bound for a generic tensor", "they do so by an argument well-known in this space showing a lower bound for some particular tensor and then inferring the bound for a generic tensor", "the results are interesting overall but the paper has many caveats  the results are only for convac", "the results are interesting overall but the paper has many caveats  the results are only for convac"], "labels": ["DFT", "SMY", "DIS", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["which are arguably quite different from relu networks the non-linearity in successive non-pooling layers could be import", "n  it's not clear if the importance of overlap is too surprising or is a pressing question to understand as in the case of depth", "the rank of the tensor being high does not preclude approximation to a very good accuracy by tensors of much smaller rank", "that said the results could be of interest to those thinking about minimizing the number of connections in convnet", "as it gives some intuition about how much overlap might 'suffice'  i recommend weak accept"], "labels": ["SMY", "DFT", "DFT", "DFT", "FBK"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper concerns distributions used for the code space in implicit models eg vaes and gan", "the authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribut", "the authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribut", "the well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lin", "to correct this the authors propose to use a gamma-distribution for the norm of the latent space and uniform angle distribut"], "labels": ["SMY", "SMY", "DIS", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this results in more mass closer to the origin and the authors show both that the midpoint distribution is natural in terms of the kl divergence to the data points and experimentally that the method gives visually appealing interpol", "this results in more mass closer to the origin and the authors show both that the midpoint distribution is natural in terms of the kl divergence to the data points and experimentally that the method gives visually appealing interpol", "while the contribution of using a standard family of distributions in a standard implicit model setup is limited the paper does make interesting observations analyses and an attempt to correct the interpolation issu", "while the contribution of using a standard family of distributions in a standard implicit model setup is limited the paper does make interesting observations analyses and an attempt to correct the interpolation issu", "the paper is clearly written and presents the theory and experimental results nic"], "labels": ["SMY", "DIS", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i find that the paper can be accepted but the incremental nature of the contribution prevents a higher scor", "this paper considers a dichitomy between ml and rl based methods for sequence gener", "it is argued that the ml approach has some discrepancy between the optimization objective and the learning objective and the rl approach suffers from bad sample complex", "an alpha-divergence formulation is considered to combine both method", "unfortunately i do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this pap"], "labels": ["FBK", "SMY", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i therefore have no option but to vote for reject of this paper based on my educated guess", "below are the points that i'm particularly confused about for the ml formulation the paper made several particularly confusing remarks some of them are blatantly wrong to m", "for example  the q| distribution in eq  *cannot* really be the true distribution because the true distribution is unknown and therefore cannot be used to construct estim", "from the context i guess the authors mean empirical training distribut", "i understand that the ml objective is different from what the users really care about eg blue score but this does not seem a discrepancy to m"], "labels": ["FBK", "DFT", "DFT", "QSN", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the ml estimator simply finds a parameter that is the most consistent to the observed sequences and if it fails to perform well in some other evaluation criterion such as blue score it simply means the model is inadequate to describe the data given or the model class is so large that the give number of samples is insufficient and as a result one should change his/her modeling to make it more apt to describe the data at hand", "in summary i'm not convinced that the fact that ml optimizes a different objective than the blue score is a problem with the ml estim", "in addition i don't see at all why this discrepancy is a discrepancy between training and testing data", "as long as both of them are identically distributed then no discrepancy exist", "in point ii under the maximum likelihood section i don't understand it at all and i think both sentences are wrong"], "labels": ["DIS", "DIS", "DFT", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first the model is *not* trained on the true distribution which is unknown", "the model is trained on an empirical distribution whose points are sampled from the true distribut", "i also don't understand why it is evaluated using p_theta if i understand correctly the model is evaluated on a held-out test data which is also generated from the underlying true distribut", "for the rl approach i think it is very unclear as a formulation of an estim", "for example in eq  what is r and what is y* it is mentioned that r is a reward function but i don't know what it means and the authors should perhaps explain furth"], "labels": ["DFT", "SMY", "DIS", "CRT", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i just don't see how one obtains an estimated parameter theta from the formulation in eq  using training exampl", "the paper proposed a generalized hmc by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickli", "mixing is one of the most challenge problems for a mcmc sampler particularly when there are many modes in a distribut", "the derivations look correct to m", "in the experiments the proposed algorithm was compared to other methods eg a-nice-mc and hmc"], "labels": ["CRT", "SMY", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it showed that the proposed method could mix between the modes in the posterior", "although the method could mix well when applied to those particular experi", "it lacks theoretical justifications why the method could mix wel", "the paper proposes a novel way of causal inference in situations where in causal sem notation the outcome y = ftx is a function of a treatment t and covariates x", "the goal is to infer the treatment effect ey|t=x=x - ey|t=x=x for binary treatments at every location x"], "labels": ["DIS", "APC", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the treatment effect can be learned then forecasts of y under new policies that assign treatment conditional on x will still work and the distribution of x can also change without affecting the accuracy of the predict", "what is proposed seems to be twofold- instead of using a standard inverse probability weighting the authors construct a bound for the prediction performance under new distributions of x and new policies and learn the weights by optimizing this bound", "the goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse thus leading to a small effective sample s", "- as an additional ingredient the authors also propose representation learning by mapping x to some representation phix", "the goal is to learn the mapping phi and its inverse and the weighting function simultaneously by optimizing the derived bound on the prediction perform"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros - the problem is relevant and also appears in similar form in domain adaptation and transfer learn", "- the derived bounds and procedures are interesting and nontrivial even if there is some overlap with earlier work of shalit et ", "cons- i am not sure if iclr is the optimal venue for this manuscript but will leave this decision to oth", "- the manuscript is written in a very compact style and i wish some passages would have been explained in more depth and detail", "- the manuscript is written in a very compact style and i wish some passages would have been explained in more depth and detail"], "labels": ["APC", "APC", "DIS", "SUG", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["especially the second half of page  is at times very hard to understand as it is so dens", "- the implications of the assumptions in theorem  are not easy to understand especially relating to the quantities b_phi c^mathcal{f}_{ndelta} and d^{phimathcal{h}}_delta", "why would we expect these quantities to be small or bound", "how does that compare to the assumptions needed for standard inverse probability weight", "- i appreciate that it is difficult to find good test datasets for evaluating causal estim"], "labels": ["CRT", "CRT", "QSN", "QSN", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiment on the semi-synthetic ihdp dataset is ok even though there is very little information about its structure in the manuscript even basic information like number of instances or dimensions seems miss", "the example does not provide much insight into the main ideas and when we would expect the procedure to work more gener", "this paper proposes two contributions first applying cnns+self-attention modules instead of lstms which could result in significant speedup and good rc performance second enhancing the rc model training with passage paraphrases generated by a neural paraphrasing model which could improve the rc performance margin", "firstly i suggest the authors rewrite the end of the introduct", "the current version tends to mix everything together and makes the misleading claim"], "labels": ["APC", "CRT", "SMY", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when i read the paper i thought the speeding up mechanism could give both speed up and performance boost and lead to the  f", "but it turns out that the above improvements are achieved with at least three different ideas  the cnn+self-attention module  the entire model architecture design and  the data augmentation method", "secondly none of the above three ideas are well evaluated in terms of both speedup and rc performance and i will comment in details as follow", "the cnn+self-attention was mainly borrowing the idea from vaswani et al a from nmt to rc", "the novelty is limit"], "labels": ["DIS", "DIS", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it is a good idea to speed up the rc model", "however as the authors hoped to claim that this module could contribute to both speedup and rc performance it will be necessary to show the rc performance of the same model architecture but replacing the cnns with lstm", "only if the proposed architecture still gives better results the claims in the introduction can be considered correct", "i feel that the model design is the main reason for the good overall rc perform", "i feel that the model design is the main reason for the good overall rc perform"], "labels": ["APC", "DIS", "DIS", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however in the paper there is no motivation about why the architecture was designed like thi", "however in the paper there is no motivation about why the architecture was designed like thi", "moreover the whole model architecture is only evaluated on the squad dataset", "as a result it is not convincing that the system design has good gener", "if in  it is observed that using lstms in the model instead of cnns could give on par or better results it will be necessary to test the proposed model architecture on multiple datasets as well as conducting more ablation tests about the model architecture itself"], "labels": ["DFT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i like the idea of data augmentation with paraphras", "currently the improvement is only margin", "but there seems many other things to play with", "for example training nmt models with larger parallel corpora training nmt models with different language pairs with english as the pivot and better strategies to select the generated passages for data augment", "ni am looking forward to the test performance of this work on squad"], "labels": ["APC", "DIS", "APC", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper introduces a technique for optimizing an l penalty on the weights of a neural network", "the basic problem is empirical risk minimization with a incremental penalty for each non zero weight", "to tackle this problem this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random vari", "the authors note that this loss can also be seen as a specific variational bound of a bayesian model over the weight", "the key advantage of this method is that it gives a training time technique for sparsifying neural network computation leading to potential wins in computation time during train"], "labels": ["APC", "DFT", "APC", "DFT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results presented in the paper are convinc", "they achieve results competitive with previous methods with the additional advantage that their sparse models are available during training tim", "they show order of magnitude reductions in computation time for small models and more modest constant improvements for large model", "the hard concrete distribution is a small but nice contribution on its own", "my only concern is the lack of discussion on the relationship between this method and concrete dropout https//arxivorg/abs/"], "labels": ["APC", "APC", "APC", "APC", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the focus is apparently different these methods are clearly closely rel", "a discussion of this relationship seems really import", "nspecific comments/questions- the reduction of computation time is the key advantage and it would have been nice to see a more thorough investigation of thi", "for example it would have been interesting to see whether this method would work with structured l penalties that removed entire units as opposed to single weights or other subsets of the comput", "this would give a stronger sense of the kind of wins that are possible in this framework"], "labels": ["SMY", "APC", "APC", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- hard concrete is a nice contribution but there are clearly many possibilities for these relax", "extra evaluations of different relaxations would be appreci", "at the very least a comparison to concrete would be nic", "- in equation  the equality of the l norm with the sum of z assumes that tilde{theta} is not", "the authors describe a method for encoding text into a discrete representation / latent spac"], "labels": ["DFT", "DFT", "APC", "DIS", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on a measure that they propose they should it outperforms an alternative gumbel-softmax method for both language modeling and nmt", "the proposed method seems effective and the proposed dsae metric is nice though itus surprising if previous papers have not used metrics similar to normalized reduction in log-ppl", "the datasets considered in the experiments are also large another plu", "however overall the paper is difficult to read and parse especially since low-level details are weaved together with higher-level points throughout and are often not motiv", "the major critique would be the qualitative nature of results in the sections on ucdecipering the latent codeud and to a lesser extent ucmixed sample-beam decod"], "labels": ["SMY", "APC", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud these two sections are simply too anecdotal although it is nice being stepped through the reasoning for the single example considered in sect", "ud these two sections are simply too anecdotal although it is nice being stepped through the reasoning for the single example considered in sect", "some quantitative or aggregate results are needed and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decod", "in this paper active learning meets a challenging multitask domain reinforcement learning in diverse atari  gam", "a state of the art deep reinforcement learning algorithm ac is used together with three active learning strategies to master multitask problem sets of increasing size far beyond previously reported work"], "labels": ["APC", "DIS", "DFT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the choice of problem domain is particular to atari and reinforcement learning the empirical observations especially the difficulty of learning many different policies together go far beyond the problem instantiations in this pap", "naive multitask learning with deep neural networks fails in many practical cases as covered in the pap", "the one concern i have is perhaps the choice of distinct of atari games to multitask learn may be almost adversarial since naive multitask learning struggles in this case but in practice the observed interference can appear even with less visually diverse input", "although performance is still reduced compared to single task learning in some cas", "this paper delivers an important reference point for future work towards achieving generalist agents which master diverse tasks and represent complementary behaviours compactly at scal"], "labels": ["SMY", "CRT", "CRT", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i wonder how efficient the approach would be on dm lab tasks which have much more similar visual inputs but optimal behaviours are still distinct", "this paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network", "this work to my knowledge is the first to use a dsl closer to a full languag", "the paper is very clear and easy to follow", "one way it could be improved is if it were compared with another system"], "labels": ["DIS", "SMY", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work", "this paper examines the problem of optimizing deep networks of hard-threshold unit", "this is a significant topic with implications for quantization for computational efficiency as well as for exploring the space of learning algorithms for deep network", "while none of the contributions are especially novel", "the analysis is clear and well-organized and the authors do a nice job in connecting their analysis to other work"], "labels": ["SUG", "SMY", "APC", "CRT", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approxim", "it shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomi", "by focusing on polynomials the paper is able to use of a variety of tools eg linear algebra to investigate the representation quest", "results such as proposition  relate the representation of a polynomial up to a certain degree to the approximation quest", "here it would be good to be more specific about the domain however as approximating the low order terms certainly does not guarantee a global uniform approxim"], "labels": ["SMY", "SMY", "DIS", "SMY", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theorem  makes an interesting claim that a finite network size is sufficient to achieve the best possible approximation of a polynomial the proof building on previous results eg by lin et al that i did not verifi", "the idea being to construct a superposition of taylor approximations of the individual monomi", "here it would be good to be more specific about the domain", "also in the discussion of taylor series it would be good to mention the point around which the series is developed eg the origin", "the paper mentions that ``the theorem is false for rectified linear units relus which are piecewise linear and do not admit a taylor series''"], "labels": ["APC", "DIS", "DIS", "SUG", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper mentions that ``the theorem is false for rectified linear units relus which are piecewise linear and do not admit a taylor series''", "however a relu can also be approximated by a smooth function and a taylor seri", "theorem  seems to be implied by theorem", "similarly parts of section  seem to follow directly from the previous discuss", "in page  ```existence proofs' without explicit constructions'' this is not true with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation funct"], "labels": ["DIS", "DIS", "DIS", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-paramet", "however all compression methods such as pruning and quantization also have this concern", "for example the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserv", "therefore the novelty of the proposed method is somewhat weak", "there are a lot of new algorithms on compressing deep neural networks such as [r][r][r]"], "labels": ["SMY", "SMY", "SMY", "CRT", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the paper only did a very simple investigation on related work", "[r] cnnpack packing convolutional neural networks in the frequency domain", "[r] lcnn lookup-based convolutional neural network", "[r] xnor-net imagenet classification using binary convolutional neural network", "experiments in the paper were only conducted on several small datasets such as mnist and cifar-"], "labels": ["CRT", "SMY", "SMY", "SMY", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is necessary to employ the proposed method on benchmark datasets to verify its effectiveness eg imagenet", "summary of paper - the paper introduces the gradient scale coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur", "review - the paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect", "it is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are low", "while the authors list  contributions the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contribut"], "labels": ["SUG", "SMY", "SMY", "APC", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors do mention experiments on page  but confess that some of the results are somewhat underwhelm", "unfortunately all tables with the experimental results are left to the appendix", "as this is a mostly theoretical paper pushing experimental results to the appendix does make sens", "but the repeated references to these tables suggest that these experimental results are crucial for the authorsu overall point", "while the authors do attempt to accomplish a lot in these nearly  pages of text the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the suppl"], "labels": ["CRT", "DIS", "APC", "SUG", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are several places where it is not immediately clear why a certain block of text is included ie the proof outlines on pages  and", "at other points the authors default to an chronological narrative that can be useful at times ie page  but here seems to distract from their overall narr", "this paper has a lot of content but not all of it appears to be relevant to the authorsu central point", "furthermore the paper is nearly double the recommended page length and has a nearly  page suppl", "my biggest recommendations for this paper are for the authors to  articulate one theme and then  look at each part whether that be section paragraph or sentence and ask what does that part contribute to that them"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros - * this paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark dataset", "* at several points the authors seek to make the work accessible by offering lay explanations for their more technical point", "* the practical suggestions on page  are a true highlight and could provide an outline for possible revis", "cons - * the main narrative is lost in the text leaving a reader unsure of the authors main points and contributions as they read", "cons - * the main narrative is lost in the text leaving a reader unsure of the authors main points and contributions as they read"], "labels": ["APC", "APC", "APC", "DFT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the authorsu first contribution is hidden among the text presentation of sect", "* the paper relies heavily on the supplement to make their central point", "* it is nearly double the recommended page length with a nearly  page suppl", "minor issues - * use one style for introducing and defining terms either use italics or single quot", "the latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solv"], "labels": ["CRT", "DIS", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the citation style of authors year at times leads to awkward sentence pars", "* given that many figures have several subfigures the authors should consider using a package that will denote subfigures with lett", "* the block quotes in the introduction may be quite important for points later in the paper but summarizing the points of these quotes may be a better use of space the authors more successfully did this in paragraph  of the introduction * all long descriptions of the appendix should be carefully revisited and possibly removed due to page length consider", "* in the text figure  which is in the supplement is referenced before figure  which is in the text", "=-=-=-= response to the authorsduring the initial reviewing period i was unable to distill the significance of the authorsu contributions from the current literature in large part due to the nature of the writing styl"], "labels": ["CRT", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after reading the authors responses and consulting the differences between the versions of the paper my review remains the sam", "it should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper and that in the most recent draft the authors made the main text of the paper long", "consulting the differences between the paper revisions i was initially intrigued with the volume of differences that shown in the summary bar", "upon closer inspection i read a much stronger introduction and appreciated the summaries at the ends of sections  and", "however i did notice that the majority of these changes were superficial re-orderings of the original text"], "labels": ["FBK", "CRT", "DIS", "APC", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the limited substantive changes to the main text i did not deeply re-read the text of the paper beyond the introduct", "this paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factor", "as carefully proved and tested only one pass of the training data would help recover the parametric subspace thus network could be easily trained layer-wise and expand", "some novel contributions layer by layer feedforward training process no back-prop", "on-line settings to train parameters  guaranteed convergence in a single pass of the data"], "labels": ["CRT", "SMY", "DIS", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weakness  the assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literatur", "the proof partsection  can be extended with more details in appendix", "in synthetic data experiments table only small margins could be observed between sn f-mlp and rf-mlp and only layer  of sn performs better above all oth", "typo in table multi-l_{} denotes the l norm were written wrong", "in the synthetic data experiments on comparison with single-task and multi-task models counter-intuitive results with larger training data split anmse raises instead of decreases of multi-task models may need further explan"], "labels": ["CRT", "SUG", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the synthetic data experiments on comparison with single-task and multi-task models counter-intuitive results with larger training data split anmse raises instead of decreases of multi-task models may need further explan", "extra models like deep networks with/without matrix factorization could be ad", "as proposed model is a deep model the lack of comparison with deep methods is dubi", "in section  the real dataset is rather small thus the results on this small dataset were not convincing enough sn model outperforms the state-of-the-art with only small margin", "extensive experiments could be ad"], "labels": ["DFT", "SUG", "SUG", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the performance on one-layer subspace network with only the input features could be ad", "conclusionthough with a quite novel idea on solving multi-task censored regression problem", "the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the subspace network", "this paper introduces sparse persistent rnns a mechanism to add pruning to the existing work of stashing rnn weights on a chip", "the paper describes the use additional mechanisms for synchronization and memory load"], "labels": ["SUG", "APC", "CRT", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the evaluation in the main paper is largely on synthetic workloads ie large layers with artificial spars", "with evaluation largely over layers instead of applications i was left wondering whether there is an actual benefit on real workload", "furthermore the benefit over dense persistent rnns for opennmt application of absolute -s over dense persistent rnns did not appear significant unless you can convince me otherwis", "storing weights persistent on chip should give a sharp benefit when all weights fit on the chip", "one suggestion i have to strengthen the paper is to claim that due to pruning now you can support a larger number of methods or method configurations and to provide examples of thos"], "labels": ["SMY", "CRT", "CRT", "DIS", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to summarize the paper adds the ability to support pruning over persistent rnn", "however narang et al  already explore this idea although briefli", "furthermore the gains from the sparsity appear rather limited over real appl", "i would encourage the authors to put the nmt evaluation in the main paper and perhaps add other workload", "furthermore a host of techniques are discussed lamport timestamps memory layouts and implementing them on gpus is not trivi"], "labels": ["APC", "DIS", "CRT", "SUG", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however these are well known and the novelty or even the experience of implementing these on gpus should be emphas", "the paper seems to claims that certain convnet architectures particularly alexnet and vgg have too many paramet", "the sensible solution is leave the trunk of the convnet unchanged and to randomly sparsify the top-most weight matric", "i have two problems with these claims modern convnet architectures inception resnext squeezenet bottleneck-densenets and shufflenets don't have large fully connected lay", "the authors reject the technique of 'deep compression' as being impract"], "labels": ["DIS", "SMY", "SMY", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network", "p what does 'normalized' mean", "batch-norm", "p are you using an l weight penalti", "if not your fully-connected baseline may be unnecessarily overfitting the training data"], "labels": ["CRT", "QSN", "QSN", "QSN", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["p table  where do the choice of cl junction densities come from", "did you do a grid search to find the optimal level of sparsity at each level", "p- i had trouble following the left/right & front/back not", "p figure  how did you decide which data points to include in the plot", "a parallel aproach to dqn training is proposed based on the idea of having multiple actors collecting data in parallel while a single learner trains the model from experiences sampled from a central replay memori"], "labels": ["QSN", "QSN", "QSN", "QSN", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments on atari game playing and two mujoco continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselin", "the core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well when implemented efficiently over a large cluster which is not trivi", "i also appreciate the various experiments to analyze the impact of several settings instead of just reporting a new sota", "overall i believe this is definitely a solid contribution that will benefit both practitioners and researchers as long as they got the computational resources to do so!", "there are essentially two more things i would have really liked to see in this paper maybe for future work- using all rainbow components- using multiple learners with actors cycling between them for inst"], "labels": ["APC", "APC", "APC", "APC", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["sharing your custom tensorflow implementation of prioritized experience replay would also be a great bonus!", "minor points- figure  does not seem to be referenced in the text", "- uab in principle q-learning variants are off-policy methods ubb = not with multi-step unless you do some kind of correction! i think it is important to mention it even if it works well in practice just saying uab furthermore we are using a multi-step return ubb is too vagu", "- when comparing the gt targets for dqn vs dpg it strikes me that dpg uses the delayed weights phi- to select the action while dqn uses current weights theta i am curious to know if there is a good motivation for this and what impact this can have on the training dynam", "- in caption of fig  k should be k"], "labels": ["SUG", "DFT", "SUG", "DIS", "DFT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in appendix a why duplicate memory data instead of just using a smaller memory s", "- in appendix d it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in dqn why use a different schem", "- why store rewards and gammaus at each time step in memory instead of just the total discounted reward", "- it would have been better to re-use the same colors as in fig  for plots in the appendix", "- would fig  be more interesting with the full plot and a log scale on the x axi"], "labels": ["QSN", "QSN", "QSN", "SUG", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a model for sequence classification and sequential decision mak", "the model interweaves attention layers akin to those used by vaswani et al with temporal convolut", "the authors demonstrate superior performance on a variety of benchmark problems including those for supervised classification and for sequential decision mak", "unfortunately i am not an expert in meta-learning so i cannot comment on the difficulty of the tasks eg omniglot used to evaluate the model or the appropriateness of the baselines the authors compare against eg continuous control", "the experiment section definitely demonstrate the effort put into this work"], "labels": ["SMY", "DIS", "APC", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however my primary concern is that the model seems somewhat lacking in novelti", "namely it interweaves the vaswani style attention with with temporal convolutions along with trpo", "the authors claim that vaswani model does not incoporate positional information but from my understanding it actually does so using positional encod", "i also do not see why the vaswani model cannot be lightly adapted for sequential decision mak", "i think comparison to such a similar model would strengthen the novelty of this paper eg convolution is a superior method of incorporating positional inform"], "labels": ["CRT", "CRT", "CRT", "CRT", "SUG"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["my second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learn", "i think this information would be very useful to the community in terms of what to take away from this pap", "in retrospect i wish the authors would have spent more time doing ablation studies than tackling more task domain", "overall i am inclined to accept this paper on the basis of its experimental result", "however i am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers who are hopefully more experienced in this domain"], "labels": ["DFT", "DIS", "CRT", "FBK", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some minor feedback/questions for the authors- i would prefer mathematical equations as opposed to pseudocode formul", "- in the experiment section for omniglot when the authors say  classes for training and  for testing it sounds like the authors are performing zero-shot learn", "how does this particular model generalize to classes not seen during train", "summary of the paper---------------------------the paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets train", "the main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems related to machine learning applications as learning a neural network sharing the same properti"], "labels": ["SUG", "CRT", "QSN", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for this sake the paper casts the problem into reinforcement learning framework and relies on guided policy search gps to explore the space of states and act", "the states are represented by the iterates the gradients the objective function values derived statistics and features the actions are the update directions of parameters to be learn", "to make the formulated problem tractable some simplifications are introduced the policies are restricted to gaussian distributions family block diagonal structure is imposed on the involved paramet", "the mean of the stationary non-linear policy of gps is modeled as a recurrent network with parameters to be learn", "a hatch of how to learn the overall process is pres"], "labels": ["SMY", "SMY", "SMY", "SMY", "SMY"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach", "comments-------------- the overall idea of the paper learning how to optimize is very seducing and the experimental evaluations comparison to normal optimizers and other meta-learners tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problem", "- materials of the paper sometimes appear tedious to follow mainly in sub-sections  and", "it would be desirable to sum up the overall procedure in an algorithm", "page  the term $omega$ intervening in the definition of the policy $pi$ is not defin"], "labels": ["SMY", "APC", "CRT", "DIS", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the definitions of the statistics and features state and observation features look highly elabor", "can authors provide more intuition on these precise definit", "how do they impact for instance changing the time range in the definition of $phi$ in the performance of the meta-learn", "- figures  and  illustrate some oscillations of the proposed approach", "which guarantees do we have that the algorithm will not diverge as llbgdbgd do"], "labels": ["CRT", "SUG", "QSN", "SMY", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how long should be the training to ensure a good and stable convergence of the method", "- an interesting experience to be conducted and shown is to train the meta-learner on another dataset cifar for example and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method", "in this paper the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs in particular when they are conditionally independent 'factored'", "with the increasing focus on applying rl methods to continuous control problems and rts type games this is an important problem and this technique seems like an important addition to the rl toolbox", "the paper is well written the method is easy to implement and the algorithm seems to have clear positive impact on the presented experi"], "labels": ["QSN", "APC", "SMY", "APC", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the derivations in pages - are somewhat disconnected from the rest of the paper the optimal baseline derivation is very standard even if adapted to the slightly different situation situated here and for reasons highlighted by the authors in this paper they are not often used the 'marginalized' baseline is more common and indeed the authors adopt this one as wel", "in light of this and of the paper being quite a bit over the page limit- is this material - mostly not better suited for the appendix", "same for section  which i believe is not used in the experi", "- the experimental section is very strong regarding the partial observability experiments assuming actions are here factored as well i could see four baselines two choices for whether the baseline has access to the goal location or not and two choices for whether the baseline has access to the vector $a_{-i}$", "it's not clear which two baselines are depicted in b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baselin"], "labels": ["CRT", "QSN", "CRT", "APC", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["side note it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy'  the agent requires information to train but is not provided the information to act", "out of curiosity is it intended as an experiment to verify the need for better baselines or as a 'fair' training procedur", "- minor in equation - is the correct exponent not t'", "also since $rho_pi$ is define with a scaling $-gamma$ to make it an actual distribution i believe the definition of $eta$ should also be multiplied by $-gamma$ as well as equ", "hi this was a nice read"], "labels": ["CRT", "QSN", "QSN", "DIS", "APC"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think overall it is a good idea", "but i find the paper lacking a lot of details and to some extend confus", "here are a few comments that i havefigure  is very confusing for m", "please first of all make the figures much larg", "iclr does not have a strict page limit and the figures you have are hard to impossible to read"], "labels": ["APC", "CRT", "CRT", "CRT", "CRT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so you train in a on the steps task until k step", "is b dc in a sequence or is testing moving from plain to different th", "the plot does not explicitly account for the distillation phas", "or at least not in an intuitive way", "but if the goal is transfer then actually plaid is slower than the multitasker because it has an additional cost to pay in frames and times for the distillation phase right or is this count"], "labels": ["QSN", "QSN", "DIS", "DIS", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["going then to figure  i almost fill that the multitasker might be used to simulate two separate baselin", "indeed because the retention of tasks is done by distilling all of them jointly one baseline is to keep finetuning a model through the  stages and then at the end after collecting the  policies you can do a single consolidation step that compresses al", "so it will be quite important to know if the frequent integration steps of plaid are helpful do knowing  and  helps you learn  better or knowing  is enough", "where exactly is input injection us", "is it experiments from figur"], "labels": ["DIS", "DIS", "DIS", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what input is inject", "what do you do when you go back to the task that doesn't have the input fe", "what happens if  has semant", "please say in the main text that details in terms of architecture and so on are given in the appendix", "and do try to copy a bit more of them in the main text where reason"], "labels": ["QSN", "QSN", "QSN", "DIS", "DIS"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the role of plaid", "is it to learn a continual learning solut", "so if i have  tasks do i need to do -way distillation at the end to consolidate all skil", "will this be feas", "wouldn't the fact of having data from all the  tasks at the end contradict the traditional formulation of continual learn"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or is it to obtain a multitask solution while maximizing transfer where you always have access to all tasks but you chose to sequentilize them to improve transf", "and even then maximize transfer with respect to what", "frames required from the environ", "if that are you reusing the frames you used during training to distil", "can we afford to keep all of those frames around"], "labels": ["QSN", "QSN", "QSN", "QSN", "QSN"], "confs": [1, 1, 1, 1, 1]}
