{"abstract_id": 0, "sentences": ["so if i have  tasks do i need to do -way distillation at the end to consolidate all skil", "will this be feas", "wouldn't the fact of having data from all the  tasks at the end contradict the traditional formulation of continual learn", "or is it to obtain a multitask solution while maximizing transfer where you always have access to all tasks but you chose to sequentilize them to improve transf", "and even then maximize transfer with respect to what"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["frames required from the environ", "if that are you reusing the frames you used during training to distil", "can we afford to keep all of those frames around", "if not we have to count the distillation frames as wel", "also more baselines are need"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a simple baseline is just finetunning as going from one task to another and just at the end distill all the policies found through out the way", "or at least have a good argument of why this is suboptimal compared to plaid", "i think the idea of the paper is interesting and i'm willing to increase and indeed decrease my scor", "but i want to make sure the authors put a bit more effort into cleaning up the paper making it more clear and easy to read", "providing at least one more baseline if not more considering the other things cited by them"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the propose data augmentation and bc learning is relevant much robust than frequency jitter or simple data augment", "in equation  please check the measure of the mixtur", "why not simply use a db criteria", "the comments about applying a cnn to local features or novel approach to increase sound recognition could be completed with some iclr  work towards injected priors using chirplet transform", "the authors might discuss more how to extend their model to image recognition or at least of other modalities as suggest"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  shall be placed later on and clarifi", "discussion on mixing more than two sounds leads could be completed by associative properties we think", "this paper introduces a neural network architecture for continual learn", "the model is inspired by current knowledge about long term memory consolidation mechanisms in human", "as a consequence it uses-tone temporary memory storage inspired by hippocampus and a long term memory-ta notion of memory replay implemented by generative models vae in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt task"], "labels": ["MAJ", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall although the result are not very surprising the approach is well justified and extensively test", "it provides some insights on the challenges and benefits of replay based memory consolid", "commentst-tthe results are somewhat unsurprising as we are able to learn generative models of each tasks we can use them to train on all tasks at the same time a beat algorithms that do not use this replay approach", "-tit is unclear whether the approach provides a benefit for a particular application as the task information has to be available training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better i assum", "-tso the main benefit of the approach seems to point towards the direction of what possibly happens in real brain"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains / what we know about episodic memory consolidation the system modeled in this paper is closer to unsupervised learning as a consequence information such as task id and dictionary for balancing samples would not be available / the cortex long term memory already learns during wakefulness while in the proposed algorithm this procedure is restricted to replay-based learning during sleep", "-tdue to these differences i my view this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting which relates more to finding optimal plasticity rules for the network in an unsupervised set", "-tthe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pag", "this is a well written paper on a compelling topic how to train an automated teacher to use intuitive strategies  that would also apply to human", "the introduction is fairly strong but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy  train s on random exs  train t to pick exs for s makes sens"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["such an example would dramatically improve the paper's read", "the paper appears to be original and the related work section is quite extens", "a second significant improvement would be to add an in-depth  running example in section  so that the authors could illustrate why the br strategy makes sense algorithm", "this paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control polici", "it provides two main contributions pre-training the policy network of a ddpg agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the q-updates of the q-network to be biased towards existing act"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors use the torcs enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learn", "this paper is easy to understand", "but has a couple shortcomings and some fatal but reparable flaw", "when using rl please try to standardize your notation to that used by the community it makes things much easier to read", "i would strongly suggest avoiding your notation ax|theta and using pix subscripting theta or making conditional is somewhat less import"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["your a function seems to be the policy here which is invariable denoted pi in the rl literatur", "there has been recent effort to clean up rl notation which is presented here https//sitesualbertaca/~szepesva/papers/rlalgsinmdpspdf", "you have no obligation to use this notation but it does make reading of your paper much easier on others in the commun", "this is more of a shortcoming than a fundamental issu", "more fatally you have failed to compare your algorithm's performance against benchline implementations of similar algorithm"], "labels": ["MIN", "GEN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is almost trivial to run ddpg on torcs using the openai baselines package [https//githubcom/openai/baselines]", "i would have loved for example to see the effects of simply pre-training the ddpg actor on supervised data vs adding your mixture loss on the crit", "using the baselines would have maybe made a very compelling graph showing ddpg ddpg + actor pre-training and then your complete method", "and finally perhaps complementary to point  you really need to provide examples on more than one environ", "each of these simulated environments has its own pathologies linked to determenism reward structure and other environment particular"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["almost every algorithm i've seen published will often beat baselines on one environment and then fail to improve or even be wors on others so it is important to at least run on a series of thes", "mujoco + ai gym should make this really easy to do for reference i have no relatinship with openai", "running at least cartpole which is a very well understood control task and then perhaps reacher swimmer half-cheetah etc using a known contoller as your behavior policy behavior policy is a good term for your data-generating polici", "in terms of state of the art you are very close to todd hester et al's paper on imitation learning and although you cite it you should contrast your approach more clearly with the one in that pap", "please also have a look at some more recent work my matej vecerik todd hester & jon scholz 'leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards' for an approach that is pretty similar to your"], "labels": ["GEN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i think your intuitions and ideas are good", "but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing method", "the idea of pre-training the policy net has been tried before although i can't find a published reference and in my experience will help on certain problems and hinder on others primarily because the policy network is already 'overfit' somewhat to the expert and may have a hard time moving to a more optimal spac", "because of this experience i would need more supporting evidence that your method actually generalizes to more than one rl environ", "this paper induces latent dependency syntax in the source side for nmt"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments are made in en-de and en-ru", "the idea of imposing a non-projective dependency tree structure was proposed previously by liu and lapata  and the structured attention model by kim and rush", "in light of this i see very little novelty in this pap", "the only novelty seems to be the gate that controls the amount of syntax needed for generating each target word", "seems thin for a iclr pap"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["caption of fig  subject/object are syntactic functions not semantic rol", "i don't see how the german verb orders inflects with gend", "can you post the gold german sent", "sec  is poorly explained what is z_t do you mean u_t instead this is confus", "expressions  to  are essentially the same as in liu and lapata  not original contributions of this pap"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why is hard attention sec  necessari", "it's not differentiable and requires sampling for train", "this basically spoils the main advantage of structured attention mechanisms as proposed by kim and rush", "experimentally the gains are quite small compared to flat attention which is disappioint", "in table  it would be very helpful to display the english sourc"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["table  is confus", "the da numbers rightmost three columns are for the  or  dataset", "comparison with predicted parses by spacy are by no means gold pars", "minor comments- sec   optimization techniques like adam attention  - attention is not an optimization technique but part of a model", "- sec  abilities not its representation - comma before not"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[apologies for short review i got called in l", "marking my review as educated guess since i didn't have time for a detailed review]", "the paper proposes an algorithm to tune the momentum and learning rate for sgd", "while the algorithm does not have a theory for general non-quadratic funct", "experimental validation is extensive making it a worthy contribution in my opinion"], "labels": ["GEN", "GEN", "GEN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i have personally tried the algorithm when the paper came out and can vouch for the empirical results presented her", "the paper describes a neural end-to-end architecture to solve multiple tasks at onc", "the architecture consists of an encoder a mixer a decoder and many modality networks to cover different types of input and output pairs for different task", "the engineering endeavor is impress", "but the paper has little scientific valu"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["below are a few suggestions to make the paper strong", "it is possible that the encoder mixer and decoder are just multiplexing tasks based on the input", "one way to analyze whether this happens is to predict the identity of the task from the hidden vector", "if this is the case how to prevent it from happen", "if this does not happen what is being shared across task"], "labels": ["MIN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding spac", "why multitask learning help the model perform better is still unclear", "if the model is able to leverage knowledge learned from one task to perform another task then we expect to see either faster convergence or good performance with fewer sampl", "the authors should analyze if this is the case and if not what are we actually benefiting from multitask learn", "if the modality network is shared across multiple tasks we expect the learned hidden representation produced by the modality network is more univers"], "labels": ["MIN", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if that is the case what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task", "reporting per-token accuracies such as those in table  is problemat", "it's unclear how to compute per-token accuracies for structured prediction tasks such as speech recognition parsing and transl", "furthermore based on the results in table  the model clearly fails on the speech recognition task", "the author should also report the standard speech recognition metric word error rates wer for the speech recognition task in t"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the paper proposes to learn a soft ordering over a set of layers for multitask learning mtl ie  at every step of the forward propagation each task is free to choose its unique soft `convex'  combination of the outputs from all available lay", "this idea is novel and interest", "- the learning of such soft combination is done jointly while learning the tasks and is not set  manually cf setting permutations of a fixed number of layer per task", "- the empirical evaluation is done on intuitively related superficially unrelated and a real world  task", "the first three results are on small datasets/tasks o feature dimensions and number of  tasks and o images i distinguish two mnist digits ii  uci tasks with feature sizes  -- and number of classes -- iii  different character recognition on omniglot dataset"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the last task is real world --  attribute classification on the celeba face dataset of k  imag", "while the first three tasks are smaller proof of concept the last task could have been  more convincing if near state-of-the-art methods were us", "the authors use a resnet- which is a  smaller and lesser performing model they do mention that benefits are expected to be   complimentary to say larger model but in general it becomes harder to improve strong model", "while this does not significantly dilute the message it would have made it much more convincing  if results were given with stronger network", "- the results are otherwise convincing and clear improvements are shown with the proposed method"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the number of layers over which soft ordering was tested was fixed howev", "it would be  interesting to see what would the method learn if the number of layers was explicitly set to be  large and an identity layer was put as one of the opt", "in that case the soft ordering could  actually learn the optimal depth as well repeating identity layer beyond the option number of  lay", "overall the paper presents a novel idea which is well motivated and clearly pres", "the empirical validation while being limited in some aspects is largely convinc"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes one rl architecture using external memory for previous states with the purpose of solving the non-markov task", "the essential problems here are how to identify which states should be stored and how to retrieve memory during action predict", "the proposed architecture could identify the ukeyu states through assigning higher weights for important states and applied reservoir sampling to control write and read on memori", "the weight assigning write network is optimized for maximize the expected reward", "this article focuses on the calculation of gradient for write network and provides some mathematical clues for that"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this article compares their proposed architecture with rnn gru with  hidden unit in few toy task", "they demonstrate that proposed model could work better and rational of write network could be observ", "however it seems that hyper-parameters for rnn havenut been tuned enough", "it is because the toy task author demonstrates is actually quite similar to copy tasks that previous state should be rememb", "to my knowledge copy task could be solved easily for super long sequence through rnn model"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore empirically it is really hard to justify whether this proposed method could work bett", "also intuitively this episodic memory method should work better on long-term dependencies task while this article only shows the task with  timestep", "according to that the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough", "overall the idea of this paper is simple but interest", "via performing variational inference in a kind of online manner one can address continual learning for deep discriminative or generative networks with considerations of model uncertainti"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is written wel", "and literature review is suffici", "my comment is mainly about its importance for large-scale computer vision appl", "the neural networks in the experiments are shallow", "the main contribution of this paper area replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b propose two large margin criterion -- difference in likelihood and difference in rank wer or blue ordered hypothes", "c demonstrate performance gains two standard tasks -- an asr task on wall street journal small task and an mt task", "in addition they provide examples in figure  and  that illustrate the effect of the cost function on train", "their illustration in figure  is also helpful in seeing the impact of using a warm start with a generative model", "this paper is well written and it was easy to follow"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization lay", "this is achieved by forcing the output of some channels being constant during train", "this is achieved an adaptation of ista algorithm to update the batch-norm paramet", "the authors evaluate the performance of the proposed approach on different classification and segmentation task", "the method seems to be relatively straightforward to train and achieve good performance in terms of performance/parameter reduction compared to other methods on imagenet"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some of the hyperparameters used alpha and specially rho seem to be used very ad-hoc", "could the authors explain their choic", "how sensible is the algorithm to these hyperparamet", "nit would be nice to see empirically how much of computation the proposed approach takes during train", "how much longer does it takes to train the model with the ista based constraint"], "labels": ["MIN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this is a good paper and i believe it should be accepted given the authors are more clear on the details pointed abov", "this paper proposes to automatically recognize domain names as malicious or benign by deep networks convnets and rnns trained to directly classify the character sequence as such", "prosthe paper addresses an important application of deep networks comparing the performance of a variety of different types of model architectur", "the tested networks seem to perform reasonably well on the task", "consthere is little novelty in the proposed method/models -- the paper is primarily focused on comparing existing models on a new task"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the descriptions of the different architectures compared are overly verbose -- they are all simple standard convnet / rnn architectur", "the code specifying the models is also excessive for the main text -- it should be moved to an appendix or even left for a code releas", "the comparisons between various architectures are not very enlightening as they arenut done in a controlled way -- there are a large number of differences between any pair of models so itus hard to tell where the performance differences come from", "itus also difficult to compare the learning curves among the different models fig  as they are in separate plots with differently scaled ax", "the proposed problem is an explicitly adversarial setting and adversarial examples are a well-known issue with deep networks and other models but this issue is not addressed or analyzed in the pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in fact the intro claims this is an advantage of not using hand-engineered features for malicious domain detection seemingly ignoring the literature on adversarial examples for deep nets for example in this case an attacker could start with a legitimate domain name and use black box adversarial attacks or white box attacks given access to the model weights to derive a similar domain name that the models proposed here would classify as benign", "while this paper addresses an important problem in its current form the novelty and analysis are limited and the paper has some presentation issu", "this paper proposes a new variant of dqn where the dqn targets are computed on a full episode by a uab backward ubb update ie from end to start of episod", "the targetsu update rule is similar to a regular tabular q-learning update with high learning rate beta this allows faster propagation of rewards obtained at the end of the episode while beta= corresponds to regular dqn with no such reward propag", "this mechanism is shown to improve on q-learning in a toy d maze environment with mnist-based pixel states providing cell coordinates with beta= and on dqn and its optimality tightening variant on atari games with beta="], "labels": ["MIN", "MAJ", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the intuition behind the algorithm that one should try to speed up the propagation of rewards across multiple steps is not new in fact it has inspired other approaches like n-step q-learning eligibility traces or more recently retracelambda in deep rl", "actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay uab  programming robots using reinforcement learning and teaching ubb lin  something that is not mentioned her", "that being said to the best of my knowledge the specific algorithm proposed in this submission alg  is novel even if alg  is not alg  can be seen as a specific instance of linus algorithm with a very high learning rate and clearly only makes sense in toy deterministic environ", "in the absence of any theoretical analysis of the proposed approach i would have expected an in-depth empirical valid", "unfortunately this is not the case here in the toy environment  i am surprised by the really poor quality of the results paths - times longer than the shortest path on average have algorithms been run for a long enough tim"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or maybe the average is a bad performance measure due to outli", "i would have also appreciated a comparison to retracelambda which is a more principled way to use multi-step rewards than n-step q-learning which is technically an on-policy method", "similar remarks can be made on the atari experiments  where m frames is really low the original dqn paper had results on m frames and rainbow reports m frames in only ~x the training time reported her", "the comparison also should have included prioritized experience replay which has been shown to provide a significant boost in dqn but may be tricky to combine with the proposed algorithm", "overall comparing only to vanilla dqn and its optimality tightening variant is too limited when there have been so many other meaningful improvements over dqn this makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like rainbow for inst"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a few additional small remarks and questions- uab second there is no point in updating a one-step transition unless the future transitions have not been updated yet", "ubb should uab unless ubb be replaced by uab if ubb", "- in  is there a maximum number of steps per episode and can you please confirm that training is done independently for each maz", "- typo in eq  the - in the max should be a comma", "- there is a good amount of typos and grammar error"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though they do not harm the readability of the pap", "- citations for uab deep reinforcement learning with double q-learning ubb and uab dueling network architectures for deep reinforcement learning ubb could refer to their conference versions- uab epsilon starts from  and is annealed to  at  steps in a quadratic manner ubb please specify the exact formula", "- fig  is really confusing there seem to be typos and it is not clear why the beta updates appear in these specific cells please revise it if you want to keep it", "summary based on ideas within the context of kernel theory the authors consider post-training of nns as an extra training step which only optimizes the last layer of the network", "this additional step makes sure that the embedding or representation of the data is used in the best possible way for the considered task which is also reflected in the experi"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["according to the authors the contributions are the following post-training step keeping the rest of the nn frozen after training the method trains the last layer in order to make sure that the representation learned is used in the most efficient way", "highlighting connections with kernel techniques and rkhs optimization like kernel ridge regress", "experimental result", "claritythe paper is well-written the main ideas well-clarifi", "importancewhile the majority of papers nowadays focuses on the representation part ie how we get to phi_{l-}x this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this by itself is not enough to boost the performance universally eg if phi_{l-} is not well-trained the problem is deeper than training the last layer however it proposes an additional step that can be used in most nn architectur", "from that front ie proposing to do something different than simply training a nn i find the paper interesting that might attract some attention at the confer", "on the other hand to my humble opinion the experimental results do not show a significant gain in the performances of all networks esp figure  and table  are within the range of statistical error", "in order to state something like this universally either one needs to perform experiments with more than just mnist/cifar datasets or even more preferably prove that the algorithm performs bett", "originalityit would be great to have some more theory if any for the post-training step or investigate more cases rather than optimizing only the last lay"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments i assume the authors focused in the last layer of the nn for simplicity but is there a reason why one might want to focus only on the last lay", "one reason is convexity in w of the problem  any oth", "have the authors considered even in practice only to include training of the last  layers of the nn", "the authors state this question in the future direction but it would make the paper more complete to consider it her", "the paper describes a technique to incorporate dialog acts into neural conversational ag"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is very interesting work", "existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act", "a very important type of dialog act is switching topic often done to ensure that the conversation will continu", "the paper describes a classifier that predicts the dialog act of the next utter", "the next utterance is then generated based on this dialog act"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning this is also very interest", "the empirical evaluation demonstrates the effectiveness of the approach", "the paper is also well written", "i do not have any suggestion for improvement  this is good work that should be publish", "the authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network adding it to the ensemble and selecting the strongest networks to remain under certain definitions of a strong network"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments compare favorably against ppo and ac baselines on a variety of mujoco tasks although i would appreciate a wall-time comparison as well as training the crossover network is presumably time-consum", "it seems that for much of the paper the authors could dispense with the genetic terminology altogether - and i mean that as a compli", "there are few if any valuable ideas in the field of evolutionary computing and i am glad to see the authors use sensible gradient-based learning for gpo even if it makes it depart from what many in the field would consider evolutionary comput", "another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral clon", "i would perhaps even call this a distillation network rather than a crossover network"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in many robotics tasks behavioral cloning is known for overfitting to expert trajectories but that may not be a problem in this setting as expert trajectories can be generated in unlimited quant", "this paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question represent", "it is a well-written pap", "however i am not very convinced by its motivation the proposed model and the experimental result", "first of all the improvement is rather limit"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is only  improvement overall on the race dataset", "although it outperforms gar on  out of  categori", "but why is it worse on the other  categori", "i donut see any convincing explanations her", "secondly in terms of the development of reading comprehension models i donut see why we need to care about eliminating the irrelevant opt"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is hard to generalize to any other rc/qa task", "if the point is that the options can add useful information to induce better representations for passage/question there should be some simple baselines in the middle that this paper should compare to", "the two baselines sar and gar both only induce a representation from paragraph/question and finally compare to the representation of each opt", "maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defin", "some visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a graph neural net approach to few-shot learn", "input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node featur", "in addition to standard supervised few-shot classification both semi-supervised and active learning task variants are introduc", "the proposed approach captures several popular few-shot learning approaches as special cas", "experiments are conducted on both omniglot and miniimagenet dataset"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["strengths- use of graph neural nets for few-shot learning is novel", "- introduces novel semi-supervised and active learning variants of few-shot classif", "weaknesses- improvement in accuracy is small relative to previous work", "- writing seems to be rush", "the originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this pap"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["graph neural nets seem to be a more natural way of representing sets of items as opposed to previous approaches that rely on a random ordering of the labeled set such as the fce variant of matching networks or tcml", "others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future and this paper represents a first step in that direct", "regarding the graph i am wondering if the authors can comment on what scenarios is the graph structure expected to help", "in the case of -shot the graph can only propagate information about other classes which seems to not be very us", "though novel"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the motivation behind the semi-supervised and active learning setup could use some elabor", "by including unlabeled examples in an episode it is already known that they belong to one of the k class", "how realistic is this set-up and in what application is it expected that this will show up", "for active learning the proposed method seems to be specific to the case of obtaining a single label", "how can the proposed method be scaled to handle multiple requested label"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the paper is well-structured and related work covers the relevant pap", "but the details of the paper seem hastily written", "in the problem set-up section it is not immediately clear what the distinction between s r and t i", "stating more explicitly that s is for the labeled data etc would make this section easier to follow", "in addition i would suggest stating the reason why t= is a necessary assumption for the proposed model in the few-shot and semi-supervised cas"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regarding the omniglot dataset vinyals et al  augmented the classes so that  classes were used for training and  for test", "was the same procedure done for the experiments in the pap", "if yes please update  to make this distinction more clear", "if not please update the experiments to be consistent with the baselin", "in the experiments does the varphi mlp explicitly enforce symmetry and identity or is it learn"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regarding the omniglot baselines it appears that koch et al  edwards & storkey  and finn et al  use non-standard class splits relative to the other method", "this should probably be not", "the results for prototypical networks appear to be incorrect in the omniglot and mini-imagenet t", "according to snell et al  they should be % and % for miniimagenet", "moreover snell et al  only used  classes for training instead of  as utilized in the proposed approach"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given this i am wondering if the authors can comment on the performance difference in the -shot case even though prototypical networks is a special case of gnn", "for semi-supervised and active-learning results please include error bars for the miniimagenet result", "also it would be interesting to see -way results for omniglot as the gap between the proposed method and the baseline would potentially be wid", "other comments- in section  gc is defined in equation  but not mentioned in the text", "- in section  adding an equation to clarify the relationship with matching networks would be help"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper introduces a number of different techniques for improving exploration in deep q learn", "the main technique is to use ucb upper confidence bound to speedup explor", "the authors also introduces ensemble voting facilitate exploit", "this paper shows improvement over baselin", "but does not seem to offer significant insight or dramatic improv"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the techniques introduced are a small permutation of previous result", "the baselines are not particularly strong eith", "the paper appeared to have be rushed the presentation is not always clear", "i also have the following questions i hope the authors could help me with i failed to understand how eqn  could you please clarifi", "what is the significance of the math introduced in section  all that was proposed was  majority voting  ucb explor"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why comparing to ac+ which is not necessarily better than ac in final perform", "why not comparing to bootstrapped dqn since the proposed method is based on it", "why is the proposed method better than bootstrapped dqn since ucb does not necessarily outperform thompson sampling in the case of bandit", "if there is a section on infogain exploration why not mention it in the main text", "the paper addresses the problem of tensor decomposition which is relevant and interest"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes tensor ring tr decomposition which improves over and bases on the tensor train tt decomposition method", "tt decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a d matric", "the proposed tr method generalizes tt in that the first and last tensors are also rd-order tensors instead of nd-ord", "i think such generalization is interesting but the innovation seems to be very limit", "the paper develops three different kinds of solvers for tr decomposition ie svd als and sgd"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["all of these are well known method", "finally the paper provides experimental results on synthetic data  oscillated functions and image data few sampled imag", "i think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed method", "please refer to below for more comments and quest", "-- the rating has been upd"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros the topic is interest", "the generalization over tt makes sens", "cons the writing of the paper could be improved and more clear the conclusions on inner product and f-norm can be integrated into theorem", "and those theorems in section  are just some properties from previous definitions they are not theorem", "the property of tr decomposition is that the tensors can be shifted circular invari"], "labels": ["MAJ", "GEN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an interesting property and it seems to be the major strength of tr over tt", "i think the paper could be significantly improved by providing more applications of this property in both theory and experi", "as the number of latent tensors increase the als method becomes much worse approximation of the original optim", "any insights or results on the optimization performance vs the number of latent tensor", "also the paper mentions eq  als is optimized by solving d subproblems altern"], "labels": ["MAJ", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think this only contains a single round of optim", "should als be applied repeated each round solves d problems until converg", "what is the memory consumption for different solv", "sgd also needs to update at least d times for all d latent tensor", "why is the complexity or^ independent of the parameter d"], "labels": ["GEN", "GEN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the als is so slow if looking at the results in section  which becomes not pract", "the experimental part could be improved by providing more results and description about a guidance on how to choose from different solv", "what does iteration mean in experimental results such as t", "different algorithms have different cost for each iteration so comparing that seems not fair", "the results could make more sense by providing total time consumptions and time cost per iter"], "labels": ["MIN", "MIN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also applies to t", "why is the epsion in table  not consist", "why not choose epsion = e- and epsilon=e- for tensor", "also table  could be greatly improved by providing more ablations such as results for n= d= n= d= etc", "that could help readers to better understand the effect of tr"], "labels": ["GEN", "GEN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  could be improved by providing a curve compression vs error instead of just providing a table of sampled operating point", "the paper mentions the application of image representation but only experiment on x imag", "how does the proposed method handle large imag", "otherwise it does not seem to be a practical appl", "figure  are the rse measures computed over the whole cifar- dataset or the displayed imag"], "labels": ["MIN", "MIN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor- typo page  line  note that this algorithm use the similar strategy use - us", "* this paper models images with a latent code representation and then tries to modify the latent code to minimize changes in image space while changing the classification label", "as the authors indicate it lies in the space of algorithms looking to modify the image while changing the label eg lime etc", "* this is quite an interesting paper with a sensible go", "it seems like the method could be more informative than the other method"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are quite a number of problems as explained below* the explanation of eqs  and  is quite poor", "alpha in  seems to be gamma in alg  line  l_target is a target objective which can be a negative class probability  this assumes that the example is a positive class", "could we not also apply this to negative exampl", "or in the case of heart failure predicted bnp level -- this doesn't make sense to me -- surely it would be necessary to target an adjusted bnp level", "also specific details should be reserved until a general explanation of the problem has been mad"], "labels": ["MAJ", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the trade-off parameter gamma is a fiddle factor -- how was this set for the lung image and mnist exampl", "were these values differ", "* in typical iclr style the authors use a deep network to learn the encoder and decoder network", "it would be v interesting and provide a good baseline to use a shallow network ie pca instead and elucidate what advantages the deep network br", "* the example of / misclassification seems very specific does this method also work on say s and "], "labels": ["MIN", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why have you not reported results for these kinds of task", "* fig  better to show each original and reconstructed image close by eg above below or side-by-sid", "the reconstructions show poor detail relative to the origin", "this loss of detail could be a limit", "* a serious problem with the method is that we are asked to evaluate it in terms of images like fig  or fig"], "labels": ["MIN", "GEN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a serious study would involve domain experts and ascertain if fig  conforms with what they are looking for", "* the references section is highly inadequate -- no venues of publication are given", "if these are arxiv give the proper ref", "others are published in conferences etc eg goodfellow et al is in advances in neural information processing system", "* overall the paper contains an interesting idea but given the deficiencies raised above i judge that it falls below the iclr threshold"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* textsec  para  reconstruction loss on the validation set was similar to the reconstruction loss on the validation set", "* p  bottom -- give size of dataset* p  auc curve - roc curve* p  fig  use text over each image to better specify the details given in the capt", "summarythis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problem", "the paper studies locally open maps which preserve the local minima geometri", "hence a local minima of lfw is a local minima of ls when s=fw is a locally open map"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["theorem  provides conditions under which the multiplication x*y is a locally open map", "for a pyramidal feed forward net if the weights in each layer have full rank  input x is full rank and the link function is invertible then that local minima is a global minima", "commentsthe locally open maps behrends  is an interesting concept", "however i am not convinced that the paper is able to show stronger results about the geometry of linear/neural network", "further the claims all over the paper comparing with the existing works are over the top and not justifi"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i believe the paper needs a significant rewrit", "the results are not a strict improvement over existing work", "for neural networks nguyen and hein  assume the link function is differenti", "this paper assumes the link function is invert", "both papers can handle sigmoid/tanh but cannot handle relu"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results for linear networks are not an improvement over existing work", "paper claims to remove assumption on y but they get much weaker results as they cannot differentiate between saddle points and global minima for a critical point", "results are also written in a confusing way as stating each critical point is a saddle or a global minima", "instead the presentation can be simplified by just discussing the equivalency between local minima and global minima as the proposed framework cannot handle critical points directli", "proof of lemma  seems to have typos/mistak"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is bar{w_i} why are the first two equations just showing d_i leq d_i  how do you use this to conclude locally openness of mathcal{m}", "authors claim their result extends the results for matrix completion from ge et al   this is false claim as  is not the matrix completion problem with missing entries and the results in ge et al  do not assume any non-degeneracy conditions on w", "the authors present an evolution of the idea of fast weights training a double recurrent neural network one slow trained as usual and one fast that gets updated in every time-step based on the slow network", "the authors generalize this idea in a nice  way and present results on  experi", "on the positive side the paper is clearly written and while the fast-weights are not new the details of the presented method are origin"], "labels": ["MIN", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on the negative side the experimental results are presented on only  experiment with a data-set and task made up by the author", "the results are good but the improvements are not too large and they are measured over weak baselines implemented by the author", "for a convincing result one would require an evaluation on a number of tasks including long-studied ones like language modeling and comparison to stronger related models such as the neural turing machine or the transformer from attention is all you ne", "without comparison to stronger baselines and with results only on  task constructed by the authors we have to recommend reject", "paper summarythis paper proposes a technique to generalize deconvolution operations used in standard cnn architectur"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixel", "this work proposes to do sequential prediction of adjacent pixel features via intermediate feature maps resulting in more spatially smooth outputs for deconvolution lay", "this new layer is referred to as upixel deconvolution layeru and it is demonstrated on two tasks of semantic segmentation and face gener", "paper strengths- despite being simple technique the proposed pixel deconvolution layer is novel and interest", "this paper extends the concept of global rather than local optimization from the learning to search ls literature to rnns specifically in the formation and implementation of searnn"], "labels": ["GEN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices which traditionally results in label / transition bias from the teacher forced model", "the underlying issue mle training of rnns is well founded and referenced their introduction and extension to the ls techniques that may help resolve the issue are promising and their experiments both small and large show the efficacy of their techniqu", "i am also glad to see the exploration of scaling searnn to the iwslt' de-en machine translation dataset", "as noted by the authors it is a dataset that has been tackled by related papers and importantly a well scaled dataset", "for searnn and related techniques to see widespread adoption the scaling analysis this paper provides is a fundamental compon"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this reviewer whilst not having read all of the appendix in detail also appreciates the additional insights provided by it such as including losses that were attempted but did not result in appreciable gain", "overall i believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlight", "extremely minor typo one popular possibility from ls is go the full reduction route down to binary classif", "this paper proposed a new parametrization scheme for weight matrices in neural network based on the householder  reflectors to solve the gradient vanishing and exploding problems in train", "the proposed method improved two previous papers stronger expressive power than mahammedi et "], "labels": ["MAJ", "MAJ", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["faster gradient update than vorontsov et ", "the proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in section  in explaining the corresponding expressive pow", "the experimental results also look promis", "it would be nice if the authors can analyze the spectral properties of the saddle points in linear rnn nonlinear is better but it's too difficult i believ", "if the authors can show the strict saddle properties then as a corollary stochastic gradient descent finds a global minimum"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["active learning for deep learning is an interesting topic and there is few useful tool available in the literature it is happy to see such paper in the field", "this paper proposes a batch mode active learning algorithm for cnn as a core-set problem", "the authors provide an upper bound of the core-set loss which is the gap between the training loss on the whole set and the core-set", "by minimizing this upper bound the problem becomes a k-center problem which can be solved by using a greedy approximation method -opt", "the experiments are performed on image classification problem cifar caltech svhn datasets under either supervised setting or weakly-supervised set"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin", "moreover the authors show that -opt can save tractable amount of time in practice with a small accuracy drop", "the proposed algorithm is new and writing is clear", "however the paper is not flawless", "the proposed active learning framework is under erm and cover-set which are currently not supported by deep learn"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to validate such theoretical result a non-deep-learning model should be adopt", "the erm for active learning has been investigated in the literature such as querying discriminative and representative samples for batch mode active learning in kdd  which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this pap", "another interesting question is most of the competing algorithm is myoptic active learning algorithms the comparison is not fair enough", "the authors should provide more competing algorithms in batch mode active learn", "this paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk certain window"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the paper is clearly written", "n- the proposed method is applied to several sequence-to-sequence benchmarks and the paper show the effectiveness of the proposed method comparable to full attention and better than previous hard monotonic assign", "cons- in terms of the originality the methodology of this method is rather incremental from the prior study raffel et ", "but it shows significant gains from it", "- in terms of considering a monotonic alignment hori et al advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm"], "labels": ["MAJ", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in interspeech' also tries to solve this issue by combining ctc and attention-based method", "the paper should also discuss this method in sect", "ncomments- eq  $j$ in the denominator should be $t_j$", "this paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word maybe head the preposition and right word maybe depend", "this is an interesting idea with linguistic validity and practically possible because of the commonness and promiscuity of prepositions reflecting their primary grammatical and relational roles as function words not content word"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the resulting representations are show to be useful u they produce sota results on preposition selection by a decent margin on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations and good but not quite sota results without further linguistic features beyond pos tags on the much more studied task of preposition attachment disambigu", "overall i really liked this pap", "despite this enthusiasm i am doubtful whether this is a good paper for iclr", "and i write this as a card-carrying computational linguist", "this is partly because of the writ"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is very hard not to see the content of the introduction as addressing a linguistics/computational linguistics audience rather than the mainstream of the iclr audience you get this impression rather strongly from the start of each of the first  paragraphs of the introduct", "more profoundly this impression comes from the nature of the investigation and result", "while this paper makes a contribution to representation learning in suggesting a good way to learn a representation for prepositions it does not make any contributions to methods of representation learn", "indeed it is basically an application of the orthogonalized alternating least squares method of sharan and valiant  and more generally of the tensor decomposition ideas of numerous papers of anandkumar and colleagu", "there aren't any new technical ideas her"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the learned representations are successful for the two main performance tasks discussed abov", "the ancillary evidence provided from the paraphrasing of prepositional phrase seems highly uncompelling to m", "that is the task seems a completely valid one u one would like to be able to show that sparked off is a synonym of provoked but the actually results provided on this task seem quite uncompel", "among other things the example used in the text in section  seems bad to m", "it isn't really the case that split off something means divided something  sally split off a sliver of wood does not mean sally divided a sliver of wood"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["separated would be much clos", "indeed of the examples in table  the first  look bad the fourth isn't generally  true but valid in certain contexts the th is again wrong", "and only the th is really good", "similar remarks for the many more examples in the supplementary materi", "the most intriguing question is the one raised in the first paragraph of the conclusion while prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical the kind of concerns raised here are also applicable to a whole bunch of word types and it would be natural to want to extend the method to them"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["eg we would also like to learn synonymy with light verb like take note or pay attention means roughly notice or observe or the widely studied svo triples like rocksankship would also seem to cry out for a tensor decomposit", "it would be interesting to think about what further might be done her", "minor comments - abstract saying that wordvec and glove treat prepositions as content words seems slightly wrong really they treat them just as words since all words are treated the same u though one can argue that most words are content words and the method of modeling word meaning is generally much more appropriate for content word", "- p folklore within the nlp community i'm not sure whether this is true or not while pairwise counts have been the method of choice in recent word vector learning methods it wasn't true of older methods collobert and weston or bengio's nplm and n-gram counts for n   are widespread in pre-neural nlp", "at any rate it is rather unconvincing when the only evidence you cite is the paper by sharan and valiant where afaik neither author has ever published an nlp paper or attended  an nlp conference - p fec should be fce ng et al  should be ng et "], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents the n-gram machine a model that encodes sentences into simple symbolic representations n-grams which can be queried effici", "the authors propose a variety of tricks stabilized autoencoding structured tweaking to deal with the huge search space and they evaluate ngms on five of the  babi task", "i am overall a fan of the general idea of this paper scaling up to huge inputs is definitely a necessary research direction for qa", "however i have some concerns about the specific implementation and model discussed her", "how much of the proposed approach is specific to getting good results on babi eg conditioning the knowledge encoder on only the previous sentence time stamps in the knowledge tuple super small rnns four simple functions in the n-gram machine structure tweaking versus having a general-purpose qa model for natural languag"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["addressing some of these issues would likely prevent scaling to millions of real sentences as the scalability is reliant on programs being efficiently executed by simple string matching against a knowledge storag", "the paper is missing a clear analysis of ngm's limit", "the examples of knowledge storage from babi in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simpl", "in its current form i am borderline but leaning towards rejecting this pap", "other questions- is n-gram really the most appropriate term to use for the symbolic represent"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n-grams are by definition contiguous sequences the authors may want to consider altern", "- why focus only on extractive qa", "the evaluations are only conducted on  of the  babi tasks so  it is hard to draw any conclusions from the results as to the validity of this approach", "can the authors comment on how difficult it will be to add functions to the list in table  to handle the other  tasks or is ngm strictly for extractive qa", "- beam search is performed on each sentence in the input story to obtain knowledge tuples while the answering time may not change as shown in figure  as the input story grows the time to encode the story into knowledge tuples certainly grows which likely necessitates the tiny rnn sizes used in the pap"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how long does the encoding time take with  million sent", "- need more detail on the programmer architecture is it identical to the one used in liang et ", "this is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagon", "the authors describe two methods to obtain this structure  enforced during training  enforced through regularization and prun", "as a second contribution the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is larg"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject which somewhat lessens the strength of their argu", "quality this paper is of good qu", "clarity this paper is clear", "but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text", "originality this paper introduces block diagonal matrices to structure the weights of a neural network"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of structured matrices in this context is not new but the diagonal block structure appears to b", "significance this paper is somewhat signific", "pros - a new approach to analyzing the behavior of weight matrices during learn", "- a new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward pass", "cons- some of the figures are hard to read in particular fig  &  left and would benefit from a better layout"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it would be valuable to see experiments on bigger datasets than only mnist and cifar-", "- i understand that the main advantage of this method is the speedup however providing the final accuracy as a function of the nonzero entries for slower methods eg the sparse pruning showed in fig  a would provide a more complete pictur", "main questions- could you briefly comment on the training time in sect", "- could you elaborate on the last sentence of sect", "- you state singular values of an ip layer behave according to the mp distribution even after s of training iter"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is this a known fact or something that you observed empir", "in practice how large must the weight matrix be to observe this behavior", "nitpicks- i believe the term fully connected is more standard than inner product and would add clarity to the paper but i may be mistaken", "the authors propose a defense against attacks on the security of one-class svm based anonaly detector", "the core idea is to perform a random projection of the data which is supposed to decrease the impact from adversarial distort"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach is empirically tested on the following data mnist cifar and svhn", "the paper is moderately well written and structur", "command of related work is ok", "but some relevant refs are missing eg kloft and laskov jmlr", "the empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distort"], "labels": ["GEN", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is encourag", "what the paper really lacks in my opinion is a closer analysis of *why* the proposed approach works ie a qualitative empirical analysis toy experiment  or theoretical justif", "right now there is no theoretical justification for the approach nor even a in my opinion convincing movitation/intuition behind the approach", "also the attack model should formally introduc", "in summary i d like to encourage the authors to further investigate into their approach but i am not convinced by the manuscript in the current form"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it lacks both in sound theoretical justification and intuitive motivation of the approach", "the experiments however show clearly advantages of the approach", "again here further experiments are necessary eg varying the dose of adversarial point", "the authors proposed a supervised learning algorithm for modeling label and worker qu", "further utilize it to study one of the important problems in crowdsourcing - how much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better label"], "labels": ["MIN", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the paper was well written", "the motivation of the work is clearly explained and supported with relevant related work", "the main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion", "though limited to binary classification the paper proposed a theoretical framework extending the existing work on vc dimension to compute the upper bound on the risk", "the authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better result"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more detailed comments instead of considering multi-class classification as one-vs-all binary classification can you extend the theoretical guarantee on the risk to multi-class set up like softmax which is widely used in research nowaday", "can you introduce the risk -r in the paper before using it in theorem", "is there any limit on how many examples each worker has to label", "can you comment more on how to pick that value in real-world settings just saying sufficiently many section  is not suffici", "under the experiments different variations of majority vote em and oracle correction were used as baselin"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work sect", "for the experiments on synthetic datasets workers are randomly sampled with replacements were the scores reported based on average of multiple run", "if yes can you please report the error bar", "for the ms-coco examples can you provide more detailed results as shown for synthetic datasets majority vote is a very weak baselin", "for the novel approach and the theoretical backing i consider the paper to be a good on"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper has scope for improv", "this paper proposes a label embedding network method that learns label embeddings during the training process of deep network", "pros good empirical result", "cons  there is not much technical contribut", "the proposed approach is neither well motivated nor well presented/justified  the presentation of the paper needs to be improv"], "labels": ["MIN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["part of the motivation on page  does not make sense in particular for paragraph  if the classification task is just to separate a from b then  separation should be better than", "label embedding learning has been investigated in many previous work", "the authors however ignored all the existing works on this topic but enforce label embedding vectors as similarities between labels in section  without clear motivation and justif", "this assumption is not very natural u though label embeddings can capture semantic information and label correlations it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of label", "the paper needs to provide a clear rationale/justification for the assumptions made while clarifying the difference and reason from the literature work"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed model is not well explain", "by using the objective in eq how to learn the embeddings ", "the authors state ucin back propagation the gradient from z is kept from propagating to hud  this makes the learning process quite arbitrary under the objective in eq", "the label embeddings are not directly used for the classification hy zu_ but rather as auxiliary part of the objective  how to decide the test label", "the authors present confidence-based autodidactic returns a deep learning rl method to adjust the weights of an eligibility vector in tdlambda-like value estimation to favour more stable estimates of the st"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecturehowever the method by which these confidence estimates are refined could be better describ", "the authors describe these confidences variously as some notion of confidence that the agent has in the value function estimate and weighing the returns based on a notion of confidence has been explored earlier white & white  thomas et ", "but the exact method is difficult to piece together from what is written", "i believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c paramet", "this would then be captured by the critic gradient for the car method that appears towards the end of pag"], "labels": ["MAJ", "GEN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if so this should be stated explicitli", "there is another theoretical point that could be clear", "the variation in an autodidactic update of a value function equation  depends on a few things the in variation future value function estimates themselves being just one factor", "another two sources of variation are the uncertainty over how likely each path is to be taken and the uncertainty in immediate rewards accumulated as part of some n-step return", "in my opinion the quality of the paper would be much improved by a brief discussion of this and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captur"], "labels": ["MIN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nonetheless i believe that the paper represents an interesting and worthy submission to the confer", "i would strongly urge the authors to improve the method description in the camera read version though", "a few additional comments are as follows  u the plot in figure  is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach car over the ac td estimates and lrac truncated tdlambda estimates approach", "however the way the results are presented/plotted namely the linear plot of the shifted relative performance of car and lrac versus ac visually inflates the importance of tasks on which car and lrac perform better than ac and diminishes the importance of those tasks on which ac performs bett", "it would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal set"], "labels": ["MAJ", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u on page  when gt is first mentioned gt should really be described first before the reader is told what it is often replaced with", "u on page  where delta_t is defined the j step return td error i think the middle term should be $gamma^j vs_{t+j}$", "u on page  and  when describing the gradient for the actor and critic it would be better if these were given their own terminology but if not then use of the word respectively in each case would help", "the proposed approach gti has many free parameters number of layers l number of communities in each layer number of non-overlapping subgraphs m number of nodes in each subgraph k etc", "no analysis is reported on how these affect the performance of gti"], "labels": ["GEN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["gti uses the louvain hierarchical community detection method to identify the hierarchy in the graph and metis to partition the commun", "how important are these two methods to the success of gti", "why is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution as stated in sect", "why is the stride for the convolutional/deconvoluational layers set to  as stated in sect", "equation  has a symbol e in it"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["e is defined in section  to be all the inter-subgraph community edges identified by the louvain method for each hierarchi", "however e can be intra-community because communities are partitioned by meti", "more discussion is needed about the role of edges in ", "equation  sparsifies ie prunes the edges of a graph -- namely $re_{g}$", "however it is not clear how one selects a $re^{i}{g}$ from among the various i valu"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the symbol i is an index into $cv_{i}$ the cut-value of the i-th largest unique weight-valu", "was the edge-importance reported in section  checked against various measures of edge importance such as edge between", "table  needs more discussion in terms of retained edge percentage for ordered stag", "should one expect a certain trend in these sequ", "almost all of the experiments are qualitative and can be easily made quantitive by comparing pagerank or degree of nod"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the discussion on graph sampling does not include how much of the graph was sampl", "thus the comparisons in tables  and  are not fairthe most realistic graph generator is the bter model  see http//wwwsandiagov/~tgkolda/bter_supplement/ and http//wwwsandiagov/~tgkolda/feastpack/doc_bter_matchhtmla minor point the acronym gti is never defin", "this work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated word", "the key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part not the generated target words as input", "to achieve the goal authors investigated various techniques for inference sample fertility space for generating multiple possible transl"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for training apply knowledge distilation for better training followed by fine tuning by reinforc", "experiments for english/german and english/romanian show comparable translation qualities with speedup by non-autoregressive decod", "the motivation is clear and proposed methods are very sound", "experiments are carried out very car", "i have only minor concerns to this paper- the experiments are designed to achieve comparable bleu with improved lat"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'd like to know whether any blue improvement might be possible under similar latency for instance by increasing the model size given that inference is already  fast enough", "- it'd also like to see other language pairs with distorted word alignment eg chinese/english to further strengthen this work though  it might have little impact given that attention already capture sort of align", "- what is the impact of the external word aligner qu", "for instance it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word align", "- the positional attention is rather unclear and it would be better to revise it"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["note that equation  is simply mentioning attention computation not the proposed positional attent", "after reading the authors's rebuttal i increased my score from a  to a", "i do think the paper would benefit from experimental result", "but agree with the authors that the theoretical results are non-trivial and interesting on their own merit", "------------------------the paper presents a theoretical analysis of depth in rnns technically a variant called racs ie stacking rnns on top of one another so that h_t^l ie hidden state at time t and layer l is a function of h_t^{l-} and h_{t-}^{l}"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the work is inspired by previous results for feed forward nets and cnn", "however what is unique to rnns is their ability to model long term dependencies across tim", "to analyze this specific property the authors propose a concept called start-end rank that essentially models the richness of the dependency between two disjoint subsets of input", "specifically let s = {     t/} and e === {t/ +      t} sep_{se}y models the dependence between these two sets of time point", "specifically sep_{se}y = k means there exists g_s^k and g_e^k for k=k such that yx = sum_{k} g_s^kx_s g_e^kx_"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore sep_{se}y is the rank of a particular matricization of y with respect to the partition s", "if sep_{se}= then it is rank  and would correspond to independence if yx was a probability distribut", "a higher rank would correspond to more dependence across tim", "comment i believe if i understood the above correctly it would be easier to explain tensors/matricization first and then introduce separation rank", "since i think it much makes it clearer to explain"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["right now the authors explain separation rank first and then discuss tensors / matric", "using this concept the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow rnn", "i overall like the paper's theoretical result", "but i have the following complaints  i have the same question as the other review", "why is theorem  not a function of l"], "labels": ["GEN", "GEN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["do the papers that prove similar theorems about convnets able to handle general l", "what makes this more challeng", "i feel if comparing l= vs l= is hard the authors should be more up front about that in the introduction/abstract", "i think it would have been stronger if the authors would have provided some empirical results validating their claim", "the main insight in this paper is that lstms can be viewed as producing a sort of sketch of tensor representations of n-gram"], "labels": ["GEN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this allows the authors to design a matrix that maps bag-of-n-gram embeddings into the lstm embed", "they then show that the result matrix satisfies a restricted isometry condit", "combining these results allows them to argue that the classification performance based on lstm embeddings is comparable to that based on bag-of-n-gram embed", "i didn't check all the proof details but based on my knowledge of compressed sensing theory the results seem plaus", "i think the paper is a nice contribution to the theoretical analysis of lstm word embed"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper udeep learning for physical process incorporating prior physical knowledgeu proposesto question the use of data-intensive strategies such as deep learning in solving physical inverse problems that are traditionally solved through assimilation strategi", "they notably showhow physical priors on a given phenomenon can be incorporated in the learning process and propose an application on the problem of estimating sea surface temperature directly from a given collection of satellite imag", "all in all the paper is very clear and interest", "the results obtained on the considered problem", "are clearly of great interest especially when compared to state-of-the-art assimilation strategiessuch as the one of buerueziat"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the learning architecture is not original in itself it is shown that a proper physical regularization greatly improves the perform", "for these reasons i believe the paper has sufficient merits to be published at iclr", "that being said i believe that some discussions could strengthen the paper - most classical variational assimilation schemes are stochastic in nature notably by incorporatinguncertainties in the observation or physical evolution model", "it is still unclear how those uncertainties can be integrated in the model", "- assimilation methods are usually independent of the type of data at hand"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear how the modellearnt on one particular type of data transpose to other data sequ", "notably the question of transferand generalization is of high relevance her", "does the learnt model performs well on other dataset for instanceacquired on a different region or at a distant tim", "i believe this type of issue has to be examinated for this type of approach to be widely use in inverse physical problem", "this paper considers the task of aspect sentiment classification which entails categorizing texts with respect to the sentiment expressed concerning particular aspects eg television resolution or pric"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["toward this end the authors adopt a memory-network based approach", "the idea is to explicitly model interactions between aspects and words expressing sentiment about them", "this is achieved via an attention mechan", "overall this paper does seem to identify a concrete problem and i liked the use of explicit aspect embeddings for sentiment analysi", "however the contribution is relatively minor her"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore the approaches did not seem particularly well motivated in my view inconsistent and seemingly erroneous notation in places complicate the pictur", "moreover the experimental evaluation is small considering only two dataset", "my feeling is that this work is a bit preliminary at pres", "but could be expanded into a nice contribution eventu", "specific comments---- i had some trouble following the notation in places and i think this is due to a bit of sloppi"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically     target aspect vectors live in r^v but it's not clear to me what v represents this is the number of distinct aspect", "this should be clarified ideally with concrete exampl", "are the x_  x_n here word embeddings or one-hot encod", "i believe the latter but this implies that the vocabulary dimension v is the same as the number of aspects since a is apparently shar", "this seems a bit surprising or at least would seem to warrant further explan"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in eq  below it actually seems words are embedded separately via c although again the dimensions are not provid", "regardless more discussion regarding what the ax_i embeddings are meant to capture in contrast to the cx_i vectors would be appreci", "what is u is equ", "as far as i can tell this term is undefin", "in equation  this is mysteriously replaced with v_t which is the target aspect embedding and so may have been the intention for u all along"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in equation  on the rhs in the expansion the alpha_ w c_i should be alpha_ w c_", "the mistake is repeated for the following two terms - equations  and  suggest that despite their ordinal structure sentiment labels are treated as unstructured at predict tim", "this seems like a missed opportunity to capitalize on structure to bias predictions neutral sentiment is closer to positive than is negative after all but the model does not know this as currently specifi", "- the authors write alpha_i w c_i does not explicitly depend on the target word t", "i'm not sure i follow though because the alpha terms do indeed depend on the word $t$ as per equation  which includes v_t a vector representation of the target aspect"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think what the authors intend to note here is that the w parameters are independent of this or as expounded upon in the concrete example that follows that context words and weights w both ar", "in any case this statement should be clarifi", "- it would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of section  would be to add an additional layer which would facilitate interactions between the attention-weighted word embedding alpha_i c_i and aspect embedding v_t", "however the authors do not seem to have considered this straight-forward approach", "why - surely d_t = dt should be d_t = d v_t in the interaction term it subsect"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the authors write diwdt generates slightly better results and we adopt it in our experi", "-- please clarify better results on a development set i hop", "- the evaluation is smal", "the authors use only two datasets comprising only a few thousand data points and hence the test sets comprise - inst", "it is therefore hard to draw anything conclusive from these results especially given how many moving parts there are here from model initializations to training procedur"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regardless the authors should report the micro-f in addition to the macro-f", "smaller comments---- as a stylistic thing i would suggest not pluralizing attention ie remove attent", "- in eq  please be explicit as to whether the '+' is here a concatentation or an actual element-wise sum", "eq  clarifies this implicitly but would be good to state outright", "this paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car set"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a new learning strategy called q-masking which couples well a defined low level controller with a high level tactical decision making polici", "the authors rightly say that one of the skills an autonomous car must have is the ability to change lanes however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicl", "real vehicles also decouple wayfinding with local vehicle control similar to the strategy employed her", "to make a stronger case for this research being relevant to the real autonomous driving problem the authors would need to compare their algorithm to a real algorithm and prove that it is more ucdata effici", "ud  this is a difficult comparison since the sensing strategies employed by real vehicles u lidar computer vision recorded labeled real maps are vastly different from the slot car model proposed by the author"], "labels": ["GEN", "GEN", "GEN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in term of impact this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving cas", "in this paper the authors investigate ucthe use and placeud of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses q-learning to learn uchigh level tactical decisionsud and introduce ucq-maskingud a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the q-valu", "the authors claim that ucby relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing which makes it a possibility to perform training directly on real system", "ud  i am not sure what is meant by this since in this paper the authors never test their algorithm on real systems and in real systems it is not possible to completely eliminate collis", "if it were this would be a much sought breakthrough"], "labels": ["MIN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["additionally for their experiment authors use the sumo top view driving simul", "this choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sens", "this paper presents a learning algorithm that can ucoutperform a greedy baseline in terms of efficiencyud and uchumans driving the simulator in terms of safety and successud within their top view driving gam", "the game can be programmed to have an ucnud lane highway where n could reasonable go up to five to represent larger highway", "the authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target random speed within these limit"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cars follow a fixed model of behavior do not collide with each other and cannot switch lan", "it is unclear if the simulator extends beyond a single straight section of highway as shown in figur", "the agent is tasked with driving the ego-car down the n-lane highway and stopping at ucthe exitud in the right hand lane d km from the start posit", "the authors use deep q learning from mnih et al  to learn their optimal polici", "they use a sparse reward function of + for reaching the goal and -xlane difference from desired lane as a penalty for failure  this simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collis"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors limit what the car is able to do u for example it is not allowed to take actions that would get it off the highway", "this makes the high level learning strategy more efficient because it does not have to explore these possibilities q-mask", "the authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these act", "in the real world however it is unlikely that any low level controller would be able to do this perfectli", "in terms of evaluation the authors do not compare their result against any other method"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["instead using only one set of test parameters the authors compare their algorithm to a ucgreedy baselineud policy that is specified a ucalways try to change lanes to the right until the lane is correctud then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front", "it seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison", "so given a fixed policy and these constraints it is not surprising that it underperforms the q-masked q-learning algorithm", "with respect to the comparison vs human operators of the car simulation the human operators were not expert", "they were only given uca few trialsud to learn how to operate the controls before the test"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it was reported that the human participants ucdid not feel comfortableud with the low level controller on possibly indicating that the user experience of controlling the car was less than id", "with the low level controller off collisions became poss", "it is possibly not a fair claim to say that human drivers were ucless safeud but rather that it was difficult to play the game or control the car with the safety module on", "this could be seen as a game design issu", "it was not clear from this presentation how the human participants were rewarded for their perform"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in more typical hci experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated including compensation reward are specifi", "overall this paper presents an overly simplified game simulation with a weak experimental result", "this paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network", "this penalty is meant to disentangle the latent representation by removing shared covariance between each dimens", "while the proposed penalty is described as a novel contribution there are multiple instances of previous work which use the same type of penalty cheung et al  cogswell et "], "labels": ["MIN", "MAJ", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["like this work cheung et al  propose the xcov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder model", "cogswell et al  also proposes a similar penalty decov to this work for reducing overfitting in supervised learn", "the novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covari", "intuitively this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruct", "but given the similarity to previous work the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cogswell et al  explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily resc", "it would be helpful to understand how this approach avoids this issu", "- ie  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities so what prevents this trivial solut", "there doesn't appear to be a definition of the l penalty this paper compares against and it's unclear why this is a reasonable baselin", "the evaluation metrics this work uses mapc cvr tdv ud need to be justified more in the absence of their use in previous work"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while they evaluate their method on non-toy dataset such as cifar they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at al", "again the utility of the evaluation metrics proposed in this work is unclear", "the toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real dataset", "our method has no penalty on the performance on tasks evaluated in the experiments while it does disentangle the data", "this needs to be expanded in the results as all the results presented appear to show mean squared error increasing when increasing the weight of the regularization penalti"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial techniqu", "the task is interesting and relevant especially for in low-resource language pair set", "nthe paper however misses comparison against important work from the literature that is very relevant to their task u decipherment ravi  nuhn et al  ravi & knight  and other approaches like cca", "the former set of works while focused on machine translation also learns a translation table in the process", "besides the authors also claim that their approach is particularly suited for low-resource mt and list this as one of their contribut"], "labels": ["GEN", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["previous works have used non-parallel and comparable corpora to learn mt models and for bilingual lexicon induct", "the authors seem aware of corpora used in previous works tiedemann  yet provide no comparison against any of these method", "while some of the bilingual lexicon extraction works are cited haghighi et al  artetxe et al  they do not demonstrate how their approach performs against these baseline method", "such a comparison even on language pairs which share some similarities eg orthography is warranted to determine the effectiveness of the proposed approach", "the proposed methodology is not novel it rehashes existing adversarial techniques instead of other probabilistic models used in earlier work"], "labels": ["GEN", "GEN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for the translation task it would be useful to see performance of a supervised mt baseline many tools available in open-source that was trained on similar amount of parallel training data k pairs and see the gap in performance with the proposed approach", "the paper mentions that the approach is ucunsupervisedud", "however it relies on bootstrapping from word embeddings learned on wikipedia corpus which is a comparable corpus even though individual sentences are not aligned across languag", "how does the quality degrade if word embeddings had to be learned from scratch or initialized from a different sourc", "the authors claim three contributions in this paper  they introduce the framework of softmax q-distribution estimation through which they are able to interpret the role the payoff distribution plays in raml"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically the softmax q-distribution serves as a smooth approximation to the bayes decision boundari", "the raml approximately estimates the softmax q-distribution and thus approximates the bayes decision rul", "algorithmically they further propose softmax q-distribution maximum likelihood sqdml which improves raml by achieving the exact bayes decision boundary asymptot", "through one experiment using synthetic data on multi-class classiufbcation and one using real data on image captioning they show that sqdml is consistently as good or better than raml on the task-speciufbc metrics that is desired to optim", "i found the first contribution is sound and it reasonably explains why raml achieves better performance when measured by a specific metr"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given a reward function one can define the bayes decision rul", "the softmax q-distribution eqn  is defined to be the softmax approximation of the deterministic bayes rul", "the authors show that the raml can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation eqn", "of course the moving-out is biased but the replacing is unbias", "the second contribution is partially valid"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although i doubt how much improvement one can get from sqdml", "the authors define the empirical q-distribution by replacing the expectation in eqn  with empirical expectation eqn", "in fact this step can result in biased estimation because the replacement is inside the nonlinear funct", "when x is repeated sufficiently in the data this bias is small and improvement can be observed like in the synthetic data exampl", "however when x is not repeated frequently both raml and sqdml are bias"], "labels": ["MIN", "GEN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiment in section  do not validate significant improvement eith", "the numerical results are relatively weak", "the synthetic experiment verifies the reward-maximizing property of raml and sqdml", "however from figure  we can see that the result is quite sensitive to the temperature tau", "is there any guidelines to choose tau"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for experiments in section  all of them are to show the effectiveness of raml which are not very relevant to this pap", "these experiment results show very small improvement compared to the ml baselines see table  and", "these results are also lower than the state of the art perform", "a few questions the author may want to check whether  can be called a bayes decision rul", "this is a direct result from definition of conditional prob"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["no bayesian elements like prior or likelihood appears her", "in the implementation of sqdml one can sample from  without exactly computing the summation in the denomin", "compared with the n-gram replacement used in the paper which one is bett", "the authors may want to write eqn  in the same conditional form of eqn  and eqn", "this will make the comparison much more clear"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is theorem  trying to convey", "although tau goes to  there is still a gap between q and q'", "this seems to suggest that for small tau q' is not a good approximation of q", "are the assumptions in theorem  reason", "there are several typos in the proof of theorem"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in section  the authors write the rewards we directly optimized in training token-level accuracy for ner and uas for dependency parsing are more stable wrt uc than the evaluation metrics f in ner illustrating that in practice choosing a training reward that correlates well with the evaluation metric is import", "could you explain it in more detail", "the authors present a novel evolution scheme applied to neural network architecture search", "it relies on defining an expressive search space for conducting optimization with a constrained search space that leads to a lighter and more efficient algorithm", "to balance these constraints they grow sub-modules in a hierarchical way to form more and more complex cel"], "labels": ["MIN", "MIN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["hence each level is limited to a small search space while the system as a whole converges toward a complex structur", "to speed up the search they focus on finding cells instead of an entire network", "in evaluation time they insert these cells between layers of a network comparable in size to known network", "they find complex cells that lead to state-of-the-art performance on benchmark dataset cifar- and imagenet", "they also claim that their method is reaching a new milestone in evolutionary search strategies perform"], "labels": ["GEN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound", "it could lead to new insight on automating design of neural networks for given problem", "in addition the authors present results that appear to be on par with the state-of-the-art with architecture search on cifar- and imagenet benchmark dataset", "the paper presents a good work and is well articul", "however it could benefit from additional details and a deeper analysis of the result"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the key idea is a smart evolution schem", "it circumvents the traditional tradeoff between search space size and complexity of the found model", "the method is also appealing for its use of some kind of emergence between two levels of hierarchi", "in fact it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecul", "thought the paper could benefit from a more detailed analysis of the architectures found by the algorithm"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redund", "are the cells found interpret", "the authors should try to give their opinion about the design obtain", "the implementation seems technically sound the experiments and results section shows that the authors are confident and the evaluation seems correct however paragraphs on the architectures could be a bit clearer for the reader the diagram could be more complete and reflect better the descript", "during evaluation what is a step"], "labels": ["MIN", "MIN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a batch or an epoch or oth", "the method seems relatively efficient as it took  hours to converge in a field traditionally considered as heavy in terms of computation but at the requirement of using  gpu", "it raises questions on the usability of the method for small lab", "at some point we will have to use insights from this search to stop early when no improvement is expect", "also authors claim that their method consume less computation time than reinforcement learn"], "labels": ["MIN", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be supported by some quantitative result", "the paper would greatly benefit from a deeper comparison over other techniqu", "for instance it could describe more the advantages over reinforcement learn", "an important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search", "it could have taken more spaces in the pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am also concerned the computational efficiency of the results obtained with this method on current processor", "indeed the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand", "exploiting the structure of the gpus cache size sequential accesses etc allows to get best possible performance from the hardware at hand", "does the solution obtained with the optimization can be run as effici", "a short analysis forward pass time of optimized cells vs popular models could be an interesting addition to the pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a general comment over this kind of approach but i think it should be address", "this paper presents an impressive set of results on predicting lung pathologies from chest x-ray imag", "authors present two architectures one based on densenet and one based on densenet + lstm on output dimensions ie similar to nade model and compare it to state of the art on the chest x-ray classif", "experiments are clearly described and results are significantly better compared to state of the art", "the only issue with this paper is that their proposed method in practice is not tractable for inference on estimating probability of a single output a task which would be critical in medical domain"], "labels": ["MIN", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["considering that their paper is titled as a work to use dependencies among labels not being able to evaluate their network's and lack of interpretable evaluation results on this model in the experiment section is a major limit", "on the other hand there are many alternative models where one could simply use multi-task learning and shared parameter to predict multiple outcomes extremely effici", "to be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels i would need to see how the much simpler multi-task setting works as wel", "that said the paper has several positive aspects in all areasoriginality - the paper presents first combination of densenets with lstm-based output factor", "writing clarity - the paper is very well written and clear"], "labels": ["MAJ", "GEN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality - apart from the missing multi-task baseline the results are significantly better than state of the art and experiments are well don", "significance - apart from the issue of intractable inference which is arguably a large limitation of this work", "the paper leaves me guessing which part is a new contribution and which one is already possible with conceptors as described in the jaeger  report", "figure  in the paper is identical to the one in the short version of the jaeger report but is missing an explicit refer", "figure  is almost identical again a reference to the original would be bett"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["conceptors can be trained with a number of approaches as described both in the  jaeger tech report and in the jmlr paper including ridge regress", "what i am missing here is a clear indication what is an original contribution of the paper and what is already possible using the original approach", "the fact that additional conceptors can be trained does not appear new for the approach described her", "if the presented approach was an improvement over the original conceptors the evaluation should compare the new and the original vers", "the evaluation also leaves me a little confused in an additional dimension the paper title and abstract suggested that the contribution is about overcoming catastrophic forget"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the evaluation shows that the approach performs better classifying mnist digits than another approach", "this is nice but doesn't really tell me much about overcoming catastrophic forget", "it is rather difficult to evaluate the manuscript a large part of the manuscript reviews various papers from the active vision domain and subsequently proposes that this can directly be modeled using fristonus free energy principle essentially by ucanalogyud as the authors st", "this extends up to page  i would argue that this is quite a stretch as the free energy principle is essentially blind to the idea of rewards and preferable states such that all tasks are essentially evaluated in terms surprise reduct", "this is very much different from large part of the cited classic active vision literatur"], "labels": ["MAJ", "MIN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors furthermore introduce a simplification of the setting ie that nothing changes in a scene during saccadic exploration which is rather unusual for active vision problem", "the authors provide some detail about the actual implementation of their model section  but the in depth details required at iclr are miss", "no comparisons to other gaze selection models or saliency models are given", "furthermore the manuscript seems to suggest that the simulation results are somehow related to human vision as it is stateducthe model provides apparently realistic saccades for they cover the full range of the image and tend to point over regions that contain class-characteristic pixelsudbut no actual comparisons or evaluations are provid", "the paper proposes to add a rotation operation in long short-term memory lstm cel"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it performs experiments on babi tasks and showed that the results are better than the simple baselines with original lstm cel", "there are a few problems with the paperfirstly the title and abstract discuss modifying memories but the content is only about a rotation oper", "perhaps the title should be rotation operation in long short-term memori", "secondly the motivation of adding the rotation operation is not properly justifi", "what does it do that a usual lstm cell could not learn"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["does it reduce the excess representational power compared to the lstm cell that could result in better model", "or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible befor", "this is not clear at all after reading the pap", "besides the idea of using a rotation operation in recurrent networks has been explored before []", "finally the task babi and baseline models lstm from a keras tutorial are too weak"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there have been recent works that nearly solved the babi tasks to perfection eg [][][][] and many oth", "the paper presented a solution that is weak compared to these recent result", "in a summary the main idea of adding rotation to lstm cells is not properly justified in the paper and the results presented are quite weak for publication in iclr", "[] sainbayar sukhbaatar jason weston rob fergus end-to-end memory networks nips [] caiming xiong stephen merity richard socher dynamic memory networks for visual and textual question answering icml [] mikael henaff arthur szlam yann lecun recurrent orthogonal networks and long-memory tasks icml  [] caglar gulcehre sarath chandar kyunghyun cho yoshua bengio dynamic neural turing machine with soft and hard addressing schemes iclr [] mikael henaff jason weston arthur szlam antoine bordes yann lecun tracking the world state with recurrent entity networks iclr", "the paper presents a proof of the self normalization of nce as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however it seems that in equation  the authors assume that the noise distribution is a unigram model over word", "however one is allowed to use any noise distribution in nce and convergence should be quicker with those distributions that are close to the true distribut", "does the argument hold for general noise distribut", "with this assumption they can borrow easily from goldberg and levy  for the proof", "in experiments they find that while nce does result in self-normalization it is inversely correlated with perplexity which is a bit surpris"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is interesting but lacks strong empirical result", "it could be stronger if they could exploit some of their findings to improve language modeling over a strong baselin", "as one can see by the title the originality application of dcnn and significance limited to atm domain is very limit", "if this is still enough for iclr the paper could be okay", "however even so one can clearly see that the architecture the depth the regularization techniques and the evaluation are clearly behind the state of the art"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["especially for this problem domain drop-out and data augmentation should be investig", "only one dataset is used for the evaluation and it seems to be very limited and smal", "moreover it seems that the same subjects even if it is other pictures may appear in the training set and test set as they were randomly select", "looking into the referece to get the details of the dataset -  from a workshop of the ieee international conference on computer vision workshops iccvw  reveals that it has only  subjects and  disguis", "this makes it even likely that the same subject with the same disguise appears in the training and test set"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a very bad manner which unfortunately is often performed by deep learning researchers with limited pattern recognition background is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is report", "as such you perform an optimization of the paramerter #iterations on the test set making it a validation set and not an independent test set", "minor issuesmake sure that the capitalization in the references is correct atm should be capital eg by putting {atm} - and many more th", "the paper demonstrates the need and usage for flexible priors in the latent space alongside current priors used for the generator network", "these priors are indirectly induced from the data - the example discussed is via an empirical diagonal covariance assumption for a multivariate gaussian"], "labels": ["GEN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental results show the benefits of this approach", "the paper provides for a good read", "comments how do the pag scores differ when using a full covariance structur", "diagonal covariances are still very restrict", "the results are depicted with a latent space of  dimens"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it will be informative to see how the model holds in high-dimensional set", "and when data can be spars", "you could consider giving the discriminator real data etc in fig  for completeness as a graphical summari", "this paper proposes a feature augmentation method for one-shot learn", "the proposed approach is very interest"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the method needs to be further clarified and the experiments need to be improv", "details the citation format used in the paper is not appropriate which makes the paper especially the related work section very inconvenient to read", "the approach based on the discussion in the related work section and the approach section it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features as shown by [x_i x_i^a] in", "however under one-shot learning wonut this  make each class still have only one instance for train", "moreover the augmenting features x_i^a regardless a=f g or h are in the same space as the original features x_i hence x_i^a is rather an augmenting instance than additional features what makes feature augmentation better than instance augment"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear how will the vocabulary-information be exploit", "in particular how to ensure the semantic space u to be same as the vocabulary semantic spac", "how to generate the neighborhood in neighhat{u}_i on pag", "in the experiments  the authors didnut compare the proposed method with existing state-of-the-art one-shot learning approaches which makes the results not very convinc", "the results are reported for different numbers of augmented inst"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarification is need", "the paper presents a method for improving the diversity of generative adversarial network gan by promoting the gnet's weights to be as informative as poss", "this is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra nod", "numerical experiments that demonstrate the diversity increment on the generated samples are shown", "concernsthe paper is hard do tear and it is deficit to identify the precise contribution of the author"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["such contribution can in my opinion be summarized  in a potential of the formwith$$r_bre = a r_me+ b r_ac = a sum_k  sum_i s_{ki}^   +  b sum_{kl} sum_i { s_{ki} s_{li} }   $$note that my version of r_me is different to the one proposed by the authors but it could have the same effectwhere a and b are parameters that weight the relative contribution of each term  maybe computed as suggested in the pap", "in this formulationthen r_me has a high response if the node has saturated responses -us or ``s as one desire such saturated responses a should be neg", "the r_ac penalizes correlation between responses of different nod", "the point is a the second term will introduce  low correlation in saturated vectors then the will be inform", "b why the authors use the softsign instead the tanh  $tahnh in c^ $! meanwhile the derivative id softsign is discontinu"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["c  it is not clear is the softsign is used besides the activation function in page  is said ucr_bre can be applied on ant rectified layer before the nolinearityud", "this seems tt the authors propose to add a second activation function the softsign why not use the one is in teh lay", "d the authors found hard to regularize the gradient $abla_x dx$ even they tray tanh and cosine based activ", "it seems that effectively the  introduce their additional softsign in the process", "e en the definition of r_ac i denoted by kl the pair of nodes k e l"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i think that it should be for pair in the same layer it is not clear in the pap", "f it is supposed that the l_ regularization motes the weights to be informative this work is doing something similar", "how is it compared  the l_ regularization vs the propos", "recommendationi tried to read the paper several times and i accept that it was very hard to m", "the most difficult part is the lack of precision on the maths it is hard to figure out what the authors contribution indeed ar"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think there is some merit in the work", "however it is not very well organized and many points are not defin", "in my opinion the paper is in a preliminary stage and should be refin", "i recommend a ucsoftud reject", "this paper proposes a method to learn a control policy from both interactions with an environment and demonstr"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is inspired by the recent work on max entropy reinforcement learning and links between q-learning and policy gradient method", "especially the work builds upon the recent work by haarnoja et al  and schulman et al  both unpublished arxiv pap", "i'm also not sur to see much differences with the previous work by haarnoja et al and schulman et ", "it uses demonstrations to learn in an off-policy manner as in these pap", "also the fact that the importance sampling ration is always cut at  or not used at all is inherited from these papers too"], "labels": ["GEN", "GEN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors say they compare to dqfd but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit l regularization is also us", "it seems this has not been implemented for comparison and that overfitting may come from this method miss", "i'm also uncomfortable with the way most of the expert data are generated for experi", "using data generated by a pre-trained network is usually not representative of what will happen in real lif", "also corrupting actions with noise in the replay buffer is not simulating correctly what would happen in r"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed a single error in some given state will often generate totally different trajectories and not affect a single transit", "so imperfect demonstration have very typical distribut", "i acknowledge that some real human demonstrations are used but there is not much about them and the experiment is very shortly describ", "the paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning marl which is trainable with policy gradi", "authors experiment with the proposed architecture on a set of synthetic toy tasks and a few starcraft combat levels where they find their approach to perform better than baselin"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall i had a very confusing feeling when reading the pap", "ufirst authors do not formulate what exactly is the problem statement for marl", "is it an mdp or pomdp", "how do different agents perceive their time is it synchronized or not", "do they partially share the incentive or may have completely arbitrary reward"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is exactly the communication protocol", "i find this question especially important for marl because the assumption on synchronous and noise-free communication including gradients is too strong to be useful in many practical task", "second even though the proposed architecture proved to perform empirically better that the considered baselines the extent to which it advances rl research is unclear to m", "currently it looks based on that i canut recommend acceptance of the pap", "to make the paper stronger and justify importance of the proposed architecture i suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication including gradi"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arr", "deep neural networks have found great success in various appl", "this paper presents a theoretical analysis for -layer neural networks nns through a spectral approach", "specifically the authors develop a fourier-based generalization bound", "based on this the authors show that the bandwidth fourier l_ norm and the gradient for local minima of the population risk can be controlled for -layer nns with sine activation funct"], "labels": ["MIN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["numerical experimental results are also presented to verify the theori", "the scope is a bit limit", "the paper only considers -layer nns is there an essential difficulty in extending the result here to nns with more lay", "also the analysis for gradient-based method in section   is only for squared-error loss sine activation and a deterministic target vari", "what would happen if y is random or the activation is relu"], "labels": ["GEN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the generalization bound in corollary  is only for the gradient wrt alpha_j perhaps an object of more interest is the gradient wrt w", "it would be intersting to present some analysis regarding the gradient wrt w", "it is claimed that the bound is tighter than that obtained using only the lipschitz property of the activation funct", "however no comparison is clearly mad", "it would be better if the authors could explain this mor"], "labels": ["MAJ", "GEN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary the application domain of the theoretical results seems a bit restrict", "minor commentseq  dxi should be dxlemma  one hat{g} should be hat{f}", "this paper shows that techniques due to genz & monahan  can be used to achieve low kernel approximation error under the framework of random fourier featur", "pros it is new to apply quadrature rules to improve kernel approxim", "the only other work i found isgaussian quadrature for kernel features nips  the work is pretty recent so the author might not know it when submitting the pap"], "labels": ["MIN", "CNT", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but in either case it will be good to discuss the connect", "the proposed method is shown to outperform a few baselines empir", "cons i donut find the theoretical analysis to be very us", "in particular the theorem shows that the kernel approximation error is o/d which is the same as the original rff pap", "unless the paper can provide a better characterization of the constants like the orf paper it does not provide much insight in the proposed method"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unlike deep neural networks since rff is such a simple model i think providing precise theoretical understanding is cruci", "approximating an integral is a well-studied topic i do not find a good discussion on all the possible method", "why is genz & monahan  better than other alternatives such as monte-carlo qmc etc", "one argument seems to be ucfor kernels with specific specific integrand one can improve on its propertiesud but this trick can be used for monte-carlo as well and i do not see benefit of this trick in the curv", "when choosing the orthogonal matrix i think one obvious choice is to sample a matrix from the stiefel manifold the q matrix of a random gaussian this baseline should be added in additional to h and b"], "labels": ["MIN", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a wall-time experiment is needed to justify the speedup", "minor commentsucfor kennels with qw other than gaussianu obtain very accurate results with little effort by using gaussian approximation of qwud", "what is the citation of this in the kernel approximation context", "the authors propose techniques for multitask and few shot learning where the number of tasks is potentially very large and the different tasks might have different output spac", "prior techniques which can address some of these aspects do not necessarily work with deep learning which is a key focus of the pap"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors suggest computing a similarity matrix amongst the task", "given such a matrix they propose to do multitask learning by clustering the similarity matrix and learning a single model for each clust", "if the tasks in a cluster have different output spaces then a separate output layer is learned for each task in the cluster following a common encoding modul", "to deal with the large number of tasks the authors further propose computing a few randomly sampled entries of the similarity matrix and then using ideas from robust matrix completion to induce the full matrix", "the resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning as well as two datasets from intent classification in dialog system"], "labels": ["GEN", "GEN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think there are some interesting ideas in this paper and the use of matrix completion techniques to deal with a large number of tasks is nic", "but i believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form", "the prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used see eg kang et al which is cited as well as kshirsagar et al in ecml  as two exampl", "in this paper on the other hand the clusters are obtained in a manner which only accounts for pairwise similarities of tasks using a pairwise similarity metric which is quite different from how the cluster is eventually us", "this seems quite suboptim"], "labels": ["MAJ", "MIN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the pairwise similarity measure appears to be one that might have a high false negative r", "that is it might rate many tasks as dissimilar even when they are not", "this is because you train individual model on i and apply it to j", "it is possible that this model does not do well but there is an equally good model for i which also does well on j", "such a model would indeed be found if i and j are put in the same cluster but the method would fail to do so leading to high fragment"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i do not see how you apply the model from task i to task j when the two have different output spac", "since this is a major motivation of the paper i actually do not see how the setup makes sense!", "it seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix", "the inducing of edges in the y matrix by comparing to a mean and standard deviation is completely baseless", "without good reasoning from the authors i see no reason why the entries in the row of a matrix should have a normal-like distribut"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore in the matrix completion scenario you have olog^n entries per row on average which means with high probability few rows should have a constant number of entri", "in this case the means are standard deviations do not even make sense to m", "at the very least i would consider using regret to the model of the task and compute some quantiles on that which is still suspect in the matrix completion set", "in the evaluation why are just  tasks used in the amazon dataset", "why don't you present evaluation results on all tasks in the multitask set"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why is average accuracy the right th", "if the error rates are different for different tasks it is not sensible to measure raw accuraci", "the authors also seem to miss a potentially relevant baseline in cross-stitch networks https//arxivorg/abs/", "besides these major issues there are also a few minor issues i have with the pap", "i do not see why there's need for a proof for the matrix completion result"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this appears to be a direct application of chandrasekaran et al and in fact matrix completion has been used for clustering before https//arxivorg/abs/", "given this the presentation in the paper makes the idea look more novel than it i", "i also think that the authors might benefit from dropping the whole few-shot learning angle here and instead do a more thorough job of evaluating their multitask learning method", "this paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attribut", "pros + the idea of forcing different parts of the latent representation to be responsible for different attributes appears novel"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ a theoretical guarantee of the efficiency of an aspect of the proposed method is given", "cons - the results are not very appealing visu", "the results from the proposed method do not seem much better than the baselin", "what is the objective for the images in fig", "for example i'm looking at the bottom right and that image looks more like a merger of images than a modification of the image in the top-left but adding the attributes of choic"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- quantitative results are miss", "- some unclarity in the description of the method see below", "questions/other- what is meant by implicit model", "by do not anchor a specific meaning into the disentangl", "by circumscribed in two image domain"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- why does the method require two imag", "- in the case of images what is a dominant vs recessive pattern", "- it seems artificial to enforce that the attribute-irrelevant part [should] encode some information of imag", "- why are   and   not useful pair", "- need to be more specific use some channels to encode the id inform"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes an idea to do faster rnn inference via skip rnn state upd", "i like the idea of the paper in particular the design which enables calculating the number of steps to skip in adv", "but the experiments are not convincing enough", "first the tasks it was tested on are very simple --  synthetic tasks plus  small-scaled task", "i'd like to see the idea works on larger scale problems -- as that is where the computation/speed matt"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also besides the number of updates reported in table i think the wall-clock time for inference should also be reported to demonstrate what the paper is trying to claim", "minor -- cite estimating or propagating gradients through stochastic neurons for conditional computation by yoshua bengio nicholas leonard and aaron courville for straight-through estim", "the paper introduces a method for learning graph representations ie vector representations for graph", "an existing node embedding method is used to learn vector representations for the nod", "the node embeddings are then projected into a -dimensional space by pca"], "labels": ["MAJ", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the -dimensional space is binned using an imposed grid structur", "the value for a bin is the normalized number of nodes falling into the corresponding region", "the idea is simple and easily explained in a few minut", "that is an advantag", "also the experimental results look quite promis"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems that the methods outperforms existing methods for learning graph represent", "the problem with the approach is that it is very ad-ho", "c there are several existing ideas of how to combine node representations into a representation for the entire graph", "for instance averaging the node embeddings is something that has shown promising results in previous work", "since the methods is so ad-hoc nodevec - pca - discretized density map - cnn architecure and since a theoretical understanding of why the approach works is miss"], "labels": ["GEN", "MIN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is especially important to compare your method more thoroughly to simpler method", "again pooling operations average max etc on the learned nodevec embeddings are examples of simpler altern", "the experimental results are also not explained thoroughly enough", "for instance since two runs of nodevec will give you highly varying embeddings depending on the initi", "you will have to run nodevec several times to reduce the variance of your resulting discretized density map"], "labels": ["GEN", "GEN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how many times did you run nodevec on each graph", "summarythe manuscript introduces a principled way of network to network compression which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model", "the first policy specialized on architecture selection iteratively removes layers starting with architecture of the teacher model", "after the first policy is finished the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or pad", "this organization of the action space together with a smart reward design achieves impressive compression results given that this approach automates tedious architecture select"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuraci", "as a bonus the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher model", "reviewthe manuscript describes the proposed algorithm in great detail and the description is easy to follow", "the experimental analysis of the approach is very convincing and confirms the authorus claim", "using the teacher network as starting point for the architecture search is a good choice as initialization strategies are a critical component in knowledge distil"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am looking forward to seeing work on the research goals outlined in the future directions sect", "a few questions/comments i understand that l_{} in algorithm  correspond to the number of layers in the network but what do n_{} correspond to", "are these multiple rollouts of the polici", "if so shouldnut the parameter update theta_{{shrinkremove}i} be outside the loop over n and apply the average over rollouts according to equ", "i think i might have missed something her"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor some of the citations are a bit awkward eg on page  ucalgorithm from williams william", "i would use the citet command from natbib for such citations and citep for parenthesized citations eg uc incorporate dark knowledge hinton et al ud or ucthe mnist lecun et al  datasetud", "in section  the transfer learning experiment it would be interesting to compare the performance measures for different numbers of policy update iter", "appendix section  states ucbelow are the resultsud but the figure landed on the next pag", "i would either try to force the figures to be output at that position not in or after section  or write figures x-y show the result"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also in section  figure  should be referenced with the ref command", "just to get a rough idea of training time could you share how long some of the experiments took with the setup you described using  titanx gpu", "did you use data augmentation for both teacher and student models in the cifar/ and caltech experi", "what is the threshold you used to decide if the size of the fc layer input yields a degenerate solut", "overall this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript i would consider giving it the full scor"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper investigates the impact of character-level noise on various flavours of neural machine transl", "it tests  different nmt systems with varying degrees and types of character awareness including a novel meanchar system that uses averaged unigram character embeddings as word representations on the source sid", "the authors test these systems under a variety of noise conditions including synthetic scrambling and keyboard replacements as well as natural human-made errors found in other corpora and transplanted to the training and/or testing bitext via replacement t", "they show that all nmt systems whether bpe or character-based degrade drastically in quality in the presence of both synthetic and natural noise and that it is possible to train a system to be resistant to these types of noise by including them in the training data", "unfortunately they are not able to show any types of synthetic noise helping address natural nois"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of nois", "this is a thorough exploration of a mostly under-studied problem", "the paper is well-written and easy to follow", "the authors do a good job of positioning their study with respect to related work on black-box adversarial techniques but overall by working on the topic of noisy input data at all they are guaranteed novelti", "the inclusion of so many character-based systems is very nice but it is the inclusion of natural sources of noise that really makes the paper work"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["their transplanting of errors from other corpora is a good solution to the problem and one likely to be built upon by oth", "in terms of negatives it feels like this work is just starting to scratch the surface of noise in nmt", "the proposed meanchar architecture doesnut look like a particularly good approach to producing noise-resistant translation systems and the alternative solution of training on data where noise has been introduced through replacement tables isnut extremely satisfi", "furthermore the use of these replacement tables means that even when the noise is natural itus still kind of artifici", "finally this paper doesnut seem to be a perfect fit for iclr as it is mostly experimental with few technical contributions that are likely to be impactful it feels like it might be more at home and have greater impact in a *acl confer"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data but even granting that such a resource does not yet exist what is described here feels unnaturally artifici", "first of all errors learned from the noisy data sources are constrained to exist within a word", "this tilts the comparison in favour of architectures that retain word boundaries such as the charcnn system here while those systems may struggle with other sources of errors such as missing spaces between word", "second if i understand correctly once an error is learned from the noisy data it is applied uniformly and consistently throughout the training and/or test data", "this seems worse than estimating the frequency of the error and applying them stochastically or trying to learn when an error is likely to occur"], "labels": ["MIN", "MIN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i feel like these issues should at least be mentioned in the paper so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noisealso it is somewhat jarring that only the charcnn approach is included in the experiments with noisy training data t", "i realize that this is likely due to computational or time constraints but it is worth providing some explanation in the text for why the experiments were conducted in this mann", "on a related note the line in the abstract stating that uc a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noiseud implies that the other non-charcnn architectures could not learn these representations when in reality they simply werenut given the ch", "section  on the richness of natural noise is extremely interest", "but maybe less so to an iclr audi"], "labels": ["GEN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from my perspective it would be interesting to see that section expanded or used as the basis for future work on improve architectures or training strategi", "i have only one small specific suggestion at the end of section  consider deleting the last paragraph break so there is one paragraph for each system charcnn currently has two paragraphs[edited for typos]", "the manuscript proposed to use prefix codes to compress the input to a neural network for text classif", "it builds upon the work by zhang & lecun  where the same tasks are us", "there are several issues with the paper and i cannot recommend acceptance of the paper in the current st"], "labels": ["MAJ", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it looks like it is not finish", "- the datasets are not described properli", "- it is not clear to me where the baseline results come from", "they do not match up to the zhang paper i have tried to find the matching accuracies ther", "- it is not clear to me what the baselines actually are or how i can found more info on thos"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the results are not remark", "because of this the paper needs to be updated and cleaned up before it can be properly review", "on top of this i do not enjoy the style the paper is written in the language is convolut", "for example ucthe effort to use neural convolution networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques libraries and hardware used especially in the image classification uc", "i do not know which message the paper tries to get across her"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a reviewer my impression which is subjective is that the authors used difficult language to make the manuscript look more impress", "the acknowledgements should not be included here eith", "summary - this paper mainly focuses on a counting problem in visual question answering vqa using attention mechan", "the authors propose a differentiable counting component which explicitly counts the number of object", "given attention weights and corresponding proposals the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for propos"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in experiments the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used vqa v dataset", "strengths - the proposed model begins with reasonable motivation and shows its effectiveness in experiments clearli", "- the architecture of the proposed model looks natural and all components seem to have clear contribution to the model", "- the proposed model can be easily applied to any vqa model using soft attent", "- the paper is well written and the contribution is clear"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["weaknesses - although the proposed model is helpful to model counting information in vqa", "it fails to show improvement with respect to a couple of important baselines prediction from image representation only and from the combination of image representation and attention weight", "- qualitative examples of intermediate values in counting component--adjacency matrix a distance matrix d and count matrix c--need to be presented to show the contribution of each part especially in the real examples that are not compatible with the strong assumptions in modeling counting compon", "comments - it is not clear if the value of count c is same with the final answer in counting quest", "the paper is mostly a survey about clustering methods with neural network"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  presents a taxonomy for the different neural network clustering method", "a rich lists of the possible components of the neural network-based clustering methods are given that include the different neural network architectures feature to use for clustering loss functions used and mor", "in section  a few methods from the literature are classified according to the proposed taxonomi", "furthermore in section  a new method is proposed that is to combine the best parts of the already existing models in the literatur", "unfortunately the experiments is section  reveal that the proposed method yields results that are at most comparable with the existing method"], "labels": ["GEN", "GEN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is written well and provides good insights mostly taxonomy on the existing methods for neural network-based clust", "however the paper lacks novel cont", "the novel content of the paper sums up to the proposed method that is composed of building blocks of existing models and fails to impress in experimental result", "it could be that this paper belongs to another venue that is more appropriate for survey pap", "also it overall rather appears short"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper describes advgan a conditional gan plus adversarial loss", "advgan is able to generate adversarial samples by running a forward pass on gener", "the authors evaluate advgan on semi-white box and black box set", "advgan is a simple and neat solution to for generating adversary sampl", "the author also reports state-of-art result"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comment for mnist samples we can easily find the generated sample is a mixture of two digitals eg for digital  there is a light gray  overlap", "i am wondering this method is trying to mixture several samples into one to generate adversary sampl", "for real color samples it is harder to figure out the mixtur", "based on mixture assumption i suggest the author add one more comparison to other method", "which is relative change from original image to see whether advgan is the most efficient model to generate the adversary sample makes minimal change to original imag"], "labels": ["GEN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors consider learning directly fourier representations of shift/translation invariant kernels for machine learning appl", "they choose the alignment of the kernel to data as the objective function to optim", "they empirically verify that the features they learned lead to good quality svm classifi", "my problem with that paper is that even though at first glance learning adaptive feature maps seems to be an attractive approach authors' contribution is actually very littl", "below i list some of the key problem"], "labels": ["GEN", "GEN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first of all the authors claim in the introduction that their algorithm is very fast and with provable theoretical guarante", "but in fact later they admit that the problem of optimizing the alignment is a non-convex problem and the authors end up with a couple of heuristics to deal with it", "they do not really provide any substantial theoretical justification why these heuristics work in practice even though they observe it empir", "the assumptions that large fourier peaks happen close to origin is probably well-justified from the empirical point of view", "but it is a hack not a well established well-grounded theoretical method the authors claim that in their experiments they found it easy to find informative peaks even in hundreds of dimensions but these experiments are limited to the svm setting i have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive featur"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the langevin dynamics algorithm used by the authors to find the peaks where the gradient is available gives only weak theoretical guarantees as the authors actually admit and this is a well known method certainly not a novelty of that pap", "finally the authors notice that in the rotation-invariant case where ua is a discrete set heuristics are avail", "that is really not very informative the authors refer to the appendix so i carefully read that part of the appendix but it is extremely vague it is not clear at all how the langevin dynamics can be emulated by a discrete markov chain that mixes fast the authors do not provide any justification of that approach what is the mixing tim", "how the good emulation property is exactly measur", "in the conclusions the authors admit that many theoretical questions remain such as accelerating the search for fourier peak"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think that the problem of accelerating this approach is a critical point that this publication is miss", "without this it is actually really hard to talk about general mechanism of learning adaptive fourier features for kernel algorithms which is how the authors present their contribution instead we have a method heavily customized and well-tailored to the not particularly exciting svm scenario with optimization performed by the standard annealing method", "it is not clear at all whether for other downstream kernel applications this approach for optimizing the alignment would provide good quality models that uses lots of task specific hacks and heuristics to efficiently optimize the align", "another problem is that it is not clear at all to me how authors' approach can be extended to non shift-invariant kernels that do not benefit from bochner's theorem", "such kernels are very related to neural networks for instance png kernels with linear rectifier nonlinearities correspond to random layers in nns with relu and in the nn context are much more interesting that radial basis function or in general shift-invariant kernel"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a general kernel method should address this issue the authors just claim in the conclusions that it would be interesting to explore the nn context in more detail", "to sum it up it is a solid submiss", "but in my opinion without a substantial contribution and working only in a  very limited setting when it is heavily relying on many unproven hacks and heurist", "this paper describes drelu a shift version of reludrelu shifts relu from   to -sigma -sigma the author runs a few cifar-/ experiments with drelu", "comments using expectation to explain why drelu works well is not sufficient and convinc"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although dreluus expectation is smaller than expectation of relu but it doesnut explain why drelu is better than very leaky relu elu etc", "cifar-/ is a saturated dataset and it is not convincing drelu will perform will on complex task such as imagenet object detection etc", "in all experiments elu/lrelu are worse than relu which is suspici", "i personally have tried elu/lrelu/rrelu on inception v with batch norm and all are better than relu", "overall i donut think this paper meet iclrus novelty standard although the authors present some good numbers but they are not convinc"], "labels": ["MIN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after reading the rebuttalthis paper does have encouraging result", "but as mentioned earlier it still lacks systematic comparisons with existing and strongest baselines and perhaps a better understanding the differences between approaches and the pros and con", "the writing also needs to be improv", "so i think the paper is not ready for publication and my opinion remain", "===========================================================this paper presents an algorithm for few shot learn"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea is to first learn representation of data using the siamese networks architecture which predicts if a pair of two samples are similar eg from the same class or not using a svm hinge loss and then finetune the classifier using few labeled examples with possibly a different set of label", "i think the idea of representation learning using a somewhat artificial task makes sense in this setting i have several concerns for this submiss", "i am not very familiar with the literature of few shot learn", "i think a very related approach that learns the representation using pretty much the same information is the contrastive loss-- hermann and blunsom", "multilingual distributed representations without word alignment iclr"], "labels": ["GEN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the intuition is similar similar pairs shall have higher similarity in the learned representation than dissimilar pairs by a large margin", "this approach is useful even when there is only weak supervision to provide the similarity/dissimilarity inform", "i wonder how does this approach compare with the proposed method", "the experiments are conducted on a small dataset omniglot and timit", "i do not understand why the compared methods are not consistently used in both experi"], "labels": ["MIN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the experiment of speaker classification on timit where the inputs are audio segments with different durations and sampling frequency is a quite nonstandard task i do not have a sense of how challenging it i", "it is not clear why cnn transfer learning the authors did not give details about how it works performs even worse than the non-deep baseline yet the proposed method achieves very high accuraci", "it would be nice to understand/visualize what information have been extracted in the representation learning phas", "relatively minor the writing of this paper is read", "but could be improv"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it sometimes uses vague/nonstandard terminology parameterless and stat", "the term siamese kernel is not very informative yes you are learning new representations of data using dnns but this feature mapping does not have the properties of rkhs also you are not solving the svm dual problem as one typically does for kernel svm", "in my opinion the introduction of svm can be shortened and more focuses can be put on related deep learning methods and few shot learn", "the authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph", "using this scheme they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problem"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros -authors provide some empirical evidence for the benefits of using their techniqu", "-authors are fairly upfront about how overall it seems their technique isn't doing *too* much--null results are still results and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much", "cons -the grammar in the paper is pretty bad", "it could use a couple more passes with an editor", "-for a more or less entirely empirical paper the choices of experiments aresomewhat befuddl"], "labels": ["MAJ", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["considerably more details on implementation training time/test time and even just *more* experiment domains would do this paper a tremendous amount of good", "-while i mentioned it as a pro it also seems to be that this technique simply doesn't buy you very much as a practition", "if this is true--that learning better graph representations really doesn't help very much that would be good to know and publishable but actually *establishing* that requires considerably more experi", "ultimately i will have to suggest rejection unless the authors considerably beef up their manuscript with more experiments more details and improve the grammar consider", "the authors present rda the recurrent discounted attention unit that improves upon rwa the earlier introduced recurrent weighted average unit by adding a discount factor"], "labels": ["MAJ", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the rwa was an interesting idea", "with bad results far worse than the standard gru or lstm with standard attention except for hand-picked tasks the rda brings it more on-par with the standard method", "on the positive side the paper is clearly written and adding discount to rwa while a small change is origin", "on the negative side in almost all tasks the rda is on par or worse than the standard gru", "- except for multicopy where it trains fast"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but not to better results and it looks like the difference is between few and very-few training steps anyway", "the most interesting result is language modeling on hutter prize wikipedia where rda very significantly improves upon rwa - but again only matches a standard gru or lstm", "so the results are not strongly convincing and the paper lacks any mention of newer work on attent", "this year strong improvements over state-of-the-art have been achieved using attention for translation attention is all you need and image classification eg non-local neural networks but also others in imagenet competit", "to make the evaluation convincing enough for acceptance rda should be combined with those models and evaluated more competitively on multiple widely-studied task"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality and claritythe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minim", "the paper claims that the approach can deal with the partial observable domain better than the standard method", "however the results only show that the algorithm converges in some cases faster than the previous work  reaching asymptotically to a same or worse perform", "whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the pomdp", "the paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain"], "labels": ["GEN", "GEN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no markov assumpt", "the way that i interpret this algorithm is that by adding a+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principl", "this may explains why this algorithm converges faster than the baseline as it produces better exploration strategi", "to me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more effici", "originality and significancethe proposed algorithm seems origin"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in rl though maybe not with the game theoretic justif", "i believe the algorithm introduced  in this paper if it is presented well can be  an interesting addition to the literature of deep rl eg  in terms of improving the rate of converg", "however the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard on", "given the fact that algorithms like dueling dqn and ddpg are   for the best asymptotic results and not  for the best convergence rate this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay schedul", "more experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of converg"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper investigates the iterative estimation view on gated recurrent networks gnn", "authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zero", "this suggest that gnn are bias toward an identity mapping and learn to preserve the activation through tim", "given this observation authors then propose rin a new rnn parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix", "authors evaluate their rin on the adding sequential mnist and the baby tasks and show that their irnn outperforms the irnn and lstm model"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["questions- section  suggests that use of the gate  in gnns encourages to learn an identity map", "does the average iteration error behaves differently in case of a tanh-rnn", "- it seems from figure  a that the average estimation error is higher for rin than irnn and lstm and only decrease toward zero at the very end", "what could explain this phenomenon", "- while the lstm baseline matches the results of le et al later work such as recurrent batch normalization or unitary evolution rnn have demonstrated much better performance with a vanilla lstm on those tasks outperforming both irnn and rin"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what could explain this difference in the perform", "- unless i am mistaken gated orthogonal recurrent units on learning to forget from jing et al also reports better performances for the lstm and gru baselines that outperform rin on the baby tasks with mean performances of  and  for gru and lstm respect", "- quality/claritythe paper is well written and pleasant to read", "n- originalitylooking at rnn from an iterative refinement point of view seems novel", "- significancewhile looking at rnn from an iterative estimation is interest"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental part does not really show what are the advantages of the propose rin", "in particular the lstm baseline seems to weak compared to other work", "this paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same tim", "the entropy regularization on action probability is to encourage the exploration of the policy while the entropy constraint is to stabilize the gradi", "the major weakness of this paper is the unclear present"], "labels": ["MAJ", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the algorithm is never fully described though a handful variants are discuss", "how the off-policy version is implemented is miss", "in experiments why the off-policy version of trpo is not compar", "comparing the on-policy results pcl does not show a significant advantage over trpo", "moreover the curves of trpo is so unstable which is a bit uncommon"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what is the exploration strategy in the experi", "i guess it was softmax prob", "however in many cases softmax does not perform a good exploration even if the entropy regularization is ad", "another issue is the discussion of the entropy regularization in the objective funct", "this regularization while helping exploration do changes the original object"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when a policy is required to pass through a very narrow tunnel of states the regularization that forces a wide action distribution could not have a good perform", "thus it would be more interesting to see experiments on more complex benchmark problems like humanoid", "this paper studies the problem of learning one-hidden layer neural networks and is a theory pap", "a well-known problem is that without good initialization it is not easy to learn the hidden parameters via gradient desc", "this paper establishes an interesting connection between least squares population loss and hermite polynomi"], "labels": ["MAJ", "MIN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["following from this connection authors propose a new loss funct", "interestingly they are able to show that the loss function globally converges to the hidden weight matrix simulations confirm the find", "overall pretty interesting result and solid contribut", "the paper also raises good questions for future work", "for instance is designing alternative loss function useful in practic"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary i recommend accept", "the paper seems rushed to me so authors should polish up the paper and fix typo", "two questions authors do not require a^* to recover b^* is that because b^* is assumed to have unit length row", "if so they should clarify this otherwise it confuses the reader a bit", "what can be said about rate of convergence in terms of network paramet"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["currently a generic bound is employed which is not very insightful in my opinion", "this paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solut", "i think there are some interesting arguments made in the paper eg lemmas    and", "however as i started reading beyond intro i increasingly got the sense that this paper is somewhat incomplete eg while certain claims are made abstract/intro the theoretical justification are rather far from these claim", "of course there is a chance that i might be misunderstanding some things and happy to adjust my score based on the discussions her"], "labels": ["MIN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["detailed comments my main concern is that the abstract and intro claims things that are never proven or even stated in the rest of the pap", "example  from abstract ucwe show that for a wide class of differentiable activation functions this class involved ucalmostud all functions which are not piecewise linear we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singularudthis is certainly not proven and in fact not formally stated anywhere in the pap", "closest result to this is lemma  however because the optimal solution is data dependent this lemma can not be used to conclude thi", "example  from intro when comparing with other results on page the authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data eg do not require gaussian", "however as explained above the final conclusions are also significantly weaker than this prior literature so itus a bit of apples vs oranges comparison"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["page  minor typoswe study training problem --we study the training problemin the regime training objective-- in the regime the training object", "the basic idea argument and derivative calculations in section  is identical to section  of soltanet ", "lemma  is nice well done! that being said it does not seem easy to make it  quantifiable  apply to all w", "it would also be nice to compare with soudry et ", "argument on top of page  is incorrect as the global optima is data dependent and hence lemma  which is for a fixed matrix does not appli"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  on page  again the stated conclusion here that the iterates do not lead to singular w is much weaker than the claims made early on", "i havenut had time yet to verify correctness of lemmas   and lemma  in detail but if this holds is a neat argument to side step invertibility wrt w nicely done!", "what is the difference between lemma  and lemma  of soltanet ", "theorem  given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and w is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent gaussian noise is injected in every iteration over w", "by adjusting the noise variance across time one can ensure a result of the kind in theorem  of course in the absence of a quantifiable version of lemma  which can apply to all w that result will also suffer from the same issu"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this reviewer has found the proposed approach quite compelling but the empirical validation requires significant improv", "you should include in your comparison query-by- bagging & boosting which are two of the best out-of-the-box active learning strategi", "n in your empirical validation you have arbitrarily split the  datasets in  training and testing ones but many questions are still unansw", "-  would any - split work just as well ie cross-validate over the  domain", "- do you what happens if you train on      or  domain"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are the results significantly differ", "other comments- p both images in figure  are labeled figure a- p typo theis -- this abe & mamitsuksa icml-", "query learning strategies using boosting and bag", "this paper presents several theoretical results on the loss functions of cnns and fully-connected neural network", "i summarize the results as follows under certain assumptions if the network contains a wideuc hidden layer such that the layer width is larger than the number of training examples then with random weights this layer almost surely extracts linearly independent features for the training exampl"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the wide layer is at the top of all hidden layers then the neural network can perfectly fit the training data", "under similar assumptions and within a restricted parameter set s_k all critical points are the global minimum", "these solutions achieve zero squared-loss", "i would consider result  as the main result of this paper because  is a direct consequence of", "intuitively  is an easy result"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["under the assumptions of theorem  it is clear that any tiny random perturbation on the weights will make the output linearly independ", "the result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large or at least not exponentially smal", "result  has severe limitations because a there can be infinitely many critical point not in s_k that are spurious local minima", "b even though these spurious local minima have zero lebesgue measure the union of their basins of attraction can have substantial lebesgue measur", "c inside s_k theorem  doesn't exclude the solutions with exponentially small gradients but whose loss function values are bounded away above zero"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if an optimization algorithm falls onto these solutions it will be hard to escap", "overall the paper presents several incremental improvement over existing theori", "however the novelty and the technical contribution are not sufficient for securing an accept", "summary the authors consider the use of attention for sensor or channel select", "the idea is tested on several speech recognition datasets including tidigits and chime where the attention is over audio channels and grid where the attention is over video channel"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["results on tidigits and grid show a clear benefit of attention called stan here over concatenation of featur", "the results on chime show gain over the chime baseline in channel-corrupted data", "reviewthe paper reads wel", "but as a standard application of attention lacks novelti", "the authors mention that related work is generalized but fail to differentiate their work relative to even the cited references kim & lane  hori et "], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore while their approach is sold as a general sensor fusion techniqu", "most of their experimentation is on microphone arrays with attention directly over magnitude-based input features which cannot utilize the most important feature for signal separation using microphone arrays---signal phas", "their results on chime are terrible the baseline chime system is very weak and their system is only slightly better!", "the winning system has a wer of only %vs % for the baseline system while more than half of the submissions to the challenge were able to cut the wer of the baseline system in half or better!", "http//spandhdcsshefacuk/chime_challenge/chime/resultshtml their results wrt channel corruption on chime on the other hand are reasonable because the model matches the problem being addresseduoverall assess"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary the paper lacks novelty wrt technique and as an ucapplication-of-attentionud paper fails to be even close to competitive with the state-of-the-art approaches on the problems being address", "as such i recommend that the paper be reject", "additional comments -tthe experiments in general lack sufficient detail were the attention masks trained supervised or unsupervis", "were the baselines with concatenated features optimized independ", "why is there no multi-channel baseline for the grid result"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tissue with noise bursts plot input + attention does not sum to", "-ta concatenation based model can handle a variable #inputs it just needs to be trained/normalized properly during test ie like dropoutu", "this paper proposes to use deep reinforcement learning to solve a multiagent coordination task", "in particular the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi compani", "the paper does not really introduce new methods and as such this paper should be seen more as an application pap"], "labels": ["MIN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think that such a paper could have merits if it would really push the boundary of the feasible but i do not think that is really the case with this paper the task still seems quite simplistic and the empirical evaluation is not convincing limited analysis weak baselin", "as such i do not really see any real grounds for accept", "finally there are also many other weaknesses the paper is quite poorly written in places has poor formatting citations are incorrect and half a bibtex entry is inlined and is highly inadequate in its treatment of related work", "for instance there are many related papers on-taxi fleet management eg work by pradeep varakantham -coordination in multi-robot systems for spatially distributed tasks eg gerkey and much work since-scaling up multiagent reinforcement learning and multiagent mdps guestrin et al  kok & vlassis  etc", "-dealing with partial observability work on decentralized pomdps by peshkin et al  bernstein amato etc"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-multiagent deep rl has been very active last - years eg see other papers by foerster sukhbataar omidshafiei", "overall i see this as a paper which with improvements could make a nice workshop contribution but not as a paper to be published at a top-tier venu", "* in the flat vs sharp dilemma the experiments display that the dilemma if any is subtl", "table  does not necessarily contradict this view", "it would be a good idea to put the test results directly on fig  as it does not ease reading currently and postpone resnet- in the appendix"], "labels": ["MIN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how was figure  comput", "it is said that *a* random direction was used from each minimiser to plot the loss so how the d directions obtain", "* on the convexity vs non-convexity sec  it is interesting to see how pushing the id through the net changes the look of the loss for deep net", "the difference vgg - resnets is also interesting but it would have been interesting to see how this affects the current state of the art in understanding deep learning something that was done for the flat vs sharp dilemma but is lacking her", "for example does this observation that the local curvature of the loss around minima is different for resnets and vgg allows to interpret the difference in their perform"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* on optimisation paths the choice of pca directions is wise compared to random projections and results are nice as plot", "there is however a phenomenon i would have liked to be discussed the fact that the leading eigenvector captures so much variability which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried and could be useful for optimisation algorithms you trade dimension d for a much smaller effective d' you only have to figure out a generating system for this subspace and carry out optimisation insid", "can this be related to the flat vs sharp dilemma", "i would suppose that flatness tends to increase the variability captured by leading eigenvector", "typoeslegend of figure  red lines are error - red lines are accuracytable  test accuracy - test errorbefore  architecture effects - architecture affect"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the paper proposes a novel formulation of the problem of finding hidden units  that are crucial in making a neural network come up with a certain output", "- the method seems to be work well in terms of isolating a few hidden units that  need to be kept while preserving classification accuraci", "ncons- sections  and  are hard to understand", "there seem to be inconsistencies  in the not", "for example it would help to clarify whether y^b_n is the prediction score or itstransformation into [ ]"], "labels": ["GEN", "GEN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the usage is inconsist", "it is not clear how y^b_n can be expressed as sum_{k=}^k z_{nk}f_kx_nin gener", "this is only true for the penultimate layer and when y^b_n denotesthe input to the output non-linear", "however this analysis seems to beapplied for any hidden layer and y^b_n is the output of the non-linearity unit", "the new prediction scores are transformed into a scalar ranging from  to denoted as y^b_n"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["section  denotes the dnn classifier as f but section  denotes thesame classifier as f why is r_n called the cent", "i could not understand in what sense isthis the center and of what  it seems that the max value has been subtractedfrom all the logits into a softmax which is a fairly standard oper", "- the analysis seems to be about finding neurons that contribute evidence for  a particular class", "this does not address the issue of understanding why thenetwork makes a certain prediction for a particular input", "therefore thisapproach will be of limited us"], "labels": ["GEN", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n- the paper should include more analysis of how this method helps interpret the  actions of the neural net once the core units have been identifi", "currently the focus seems to be on demonstrating that the classifierperformance is maintained as a significant fraction of hidden units are mask", "however there is not enough analysis on showing whether and how the identifiedhidden units help interpret the model", "nqualitythe idea explored in the paper is interesting and the experiments are describedin enough detail", "however the writing still needs to be polish"], "labels": ["MIN", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["claritythe problem formulation and objective function section  was hard to follow", "noriginalitythis approach to finding important hidden units is novel", "significancethe paper addresses an important problem of trying to have more interpretableneural network", "however it only identifies hidden units that are important fora class not what are important for any particular input", "moreover the mainthesis of the paper is to describe a method that helps interpret neural networkclassifi"], "labels": ["MIN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the experiments only focus on identifying important hiddenunits and fall short of actually providing an interpretation using these hiddenunit", "in this paper an alternating optimization approach is explored for training auto encoders a", "the authors treat each layer as a generalized linear model and suggest to use the stochastic normalized gd of [hazan et al ] as the minimization algorithm in each alternating phas", "then they apply the suggested method to several single layer and multi layer aes comparing its performance to standard sgd", "the paper suggests an interesting approach and provides experimental evidence for its usefulness especially for multi-layer a"], "labels": ["MIN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some comments on the theoretical part-the theoretical part is partly mislead", "while it is true that every layer can be treated a generalized linear model the slqc property only applies for the last lay", "regarding the intermediate layers we may indeed treat them as generalized linear models but with non-monotone activations and therefore the slqc property does not appli", "the authors should mention this point", "-showing that generalized relu is slqc with a polynomial dependence on the domain is interest"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-it will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex object", "concretely is there any known theory for such object", "what guarantees can we hope to achiev", "the extension to muti-layer aes makes sense and seems to works quite well in practic", "the experimental part is satisfactory and seems to be done in a decent mann"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it will be useful if the authors could relate to the issue of parameter tuning for their algorithm", "concretely how sensitive/robust is their approach compared to sgd with respect to hyperparameter misspecif", "quality although the research problem is an interesting direct", "the quality of the work is not of a high standard", "my main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how different it will be compared to a perturbation in an input spac", "clarity the use of the term adversarial is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label eg from church to tower or vice-versa", "originality the generation of adversarial examples in black-box classifiers has been looked in gan literature as well and gradient based perturbations are studied too", "what is the main benefit of the proposed mechanism compared to the existing on", "significance the research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["prosa an interesting problem to evaluate the robustness of black-box classifier system", "b generating adversarial examples for image classification as well as text analysi", "c exploiting the recent developments in gan literature to build the framework forge generating adversarial exampl", "consa the proposed search algorithm in the semantic latent space could be computationally intens", "any remedy for this problem"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b searching in the latent space z could be strongly dependent on the matching inverter $i_gamma$", "any comment on thi", "c the application of the search algorithm in case of imbalanced classes could be something that require further investig", "[====================================revision ======================================================]ok so the paper underwent major remodel which significantly improved the clar", "i do agree now on figure  which tips the scale for me to a weak accept"], "labels": ["MIN", "MIN", "MIN", "CNT", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[====================================end of revision ================================================]this paper explores the problems of existing deep variational bottle neck approaches for compact representation learn", "namely the authors adjust deep variational bottle neck to conform to invariance properties by making latent variable space to depend on copula only - they name this model a  copula extension to dvib", "they then go on to explore the sparsity of the latent spac", "my main issues with this paper are experiments the proposed approach is tested only on  datasets one synthetic one real but tiny - k instances and some of the plots like figure  are not convincing to m", "on top of that it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence if it do"], "labels": ["GEN", "GEN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor commentspage  forcing an compact - forcing a compactucand andud =andsection  mention that i is mutual information it is not obvious for everyonefigure  circles/triangles are too small hard to see figure  not really convinc", "b does not appear much more structured than a to me it looks like a simple transformation of a", "this paper presents a sensitivity-penalized loss the loss of the classifier has an additional term in squart of the gradient of the classifier wrt perturbations of the inputs and a minimax or maximin driven algorithm to find attacks and defens", "it has a lemma which claims that the minimax and the maximin solutions provide the best worst-case defense and attack models respectively without proof although that statement is supported experiment", "+ prior work seem adequately cited and compared to but i am not really knowledgeable in the adversarial attacks subdomain"], "labels": ["MIN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the experiments are on small/limited datasets mnist and cifar- because of this confidence intervals over different initializations for instance would be a nice addition to t", "- there is no exact alternating optimization could be considered one evaluation of the impact of the sensitivy loss vs the minimax/maximin algorithm", "- the paper is hard to follow at times and probably that dealing with the point above would help in this regard eg lemma  and experimental analysi", "- it is unclear from figures  and  that alternative optimization and minimax converged fully and/or that the sets of hyperparameters were optim", "+ this paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks and empirical findings support its claim"], "labels": ["MAJ", "GEN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nitpicksthe gradient descent - gradient descent or the gradient descent algorithmseeming - seeminglyarbitrary flexible - arbitrarily flexiblecan name gradient descent that maximizes gradient ascentthe mini- max or the maximin solution is defined - are definedis the follow - is the follow", "this work shows that a simple non-parametric approach of storing state embeddings with the associated monte carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards reacher half-cheetah double pendulum cart pole due to the need to threshold a return the algorithms work less well with dense rewards but with the introduction of a hyper-parameter is capable of solving several tasks ther", "the authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new rl algorithm", "this paper is clearly written", "and it is important to compare simple approaches on benchmark problem"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are a number of interesting and intriguing side-notes and pieces of future work ment", "however the originality and significance of this work is a significant drawback", "the use non-parametric approaches to the action-value function go back to at least [] and probably much further so the algorithms themselves are not particularly novel and are limited to nearly-deterministic domains with either single sparse rewards success or failure rewards or introducing extra hyper-parameters per task", "the significance of this work would still be quite strong if as the author's suggest these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable paramet", "yet the results do not support the claim"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even if we ignore that for most tasks only the sparse reward which favors this algorithm version was examined these author's only demonstrate success on  relatively simple task", "while these simple tasks are useful for diagnostics it is well-known that these tasks are simple and as the author's suggest more challenging tasks   are necessary to properly assess advances made by sophisticated optimization-based policy algorithm", "lillicrap et al  benchmarked against  tasks houtfout et al  compared in the paper also used walkerd and swimmer not used in this paper as did [] openai gym contains many more control environments than the  solved here and significant research is pursing complex manipulation and grasping tasks eg [] this suggests the author's claim has already been widely heeded and this work will be of limited interest", "[] juan c sutton r s & ram a experiments with reinforcement learning in problems with continuous state and action spaces[] henderson p islam r bachman p pineau j precup d & meger d  deep reinforcement learning that matters arxiv preprint arxiv[] nair a mcgrew b andrychowicz m zaremba w & abbeel p  overcoming exploration in reinforcement learning with demonstrations arxiv preprint arxiv", "# summarythis paper presents a new external-memory-based neural network neural map for handling partial observability in reinforcement learn"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memori", "the results on several memory-related tasks in d and d environments show that the proposed method outperforms existing baselines such as lstm and mqn/frmqn", "[pros]- the overall direction toward more flexible/scalable memory is an important research direction in rl", "- the proposed memory architecture is new", "- the paper is well-written"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[cons]- the proposed memory architecture is new but a bit limited to d/d navigation task", "- lack of analysis of the learned memory behavior", "# novelty and significancethe proposed idea is novel in gener", "though [gupta et al] proposed an ego-centric neural memory in the rl context the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory whereas [gupta et al] designed the memory specifically for predicting free spac", "on the other hand the proposed method is also specific to navigation tasks in d or d environment which is hard to apply to more general memory-related tasks in non-spatial environ"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a d environ", "# qualitythe experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the d environ", "it is hard to understand how the agent utilizes the external memory without such an analysi", "# claritythe paper is overall clear and easy-to-follow except for the follow", "in the introduction section the paper claims that the expert must set m to a value that is larger than the time horizon of the currently considered task when mentioning the limitation of the previous work"], "labels": ["MIN", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in some sense however neural map also requires an expert to specify the proper size of the memory based on prior knowledge about the task", "this paper present a method for detecting adversarial examples in a deep learning classification set", "the idea is to characterize the latent feature space a function of inputs as observed vs unobserved and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent spac", "questions/comments- how is the checkpointing module repres", "which parameters are fit using the fine-tuning loss described on pag"], "labels": ["MIN", "GEN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- what is the rationale for setting the gamma concentration parameters to", "is that a general suggestion or a data-set specific recommend", "- are the checkpointing modules designed to only detect adversarial exampl", "or is it designed to still classify adversarial examples in a robust way", "clarity i had trouble understanding some of this pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be nice to have a succinct summary of how all of the pieces presented fit together eg the original victim network fine-tuning loss per-class dictionary learning w/ omp", "technical it is hard to tell how some of the components of this approach are technically justifi", "novel i am not familiar enough with adversarial deep learning to assess novelty or impact", "this paper proposes a multi-view semi-supervised method", "for the unlabelled data a single input eg a picture is partitioned into k new inputs permitting overlap"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data", "to be more precise as seen from the last formula in section  the most important factor is the d function or kl distance used her", "as the author said we could set the noisy parameter in the first part to zero but have to leave this parameter non-zero in the second term", "otherwise the model can't learn anyth", "my understanding is that the key factor is not the so called k views as in the first sight this method resembles conventional ensemble learning very much but the smoothing distribution around some input x consistency related loss"], "labels": ["GEN", "GEN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in another word we set the k for unlabeled data as  but use unlabeled data k times in the scale assuming no duplicate unlabeled data keeping the same training consistency objective method would this new method obtain a similar perform", "if my understanding is correct the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of sect", "one obvious merit is that the unlabeled data is utilized more efficiently k times bett", "the paper aims to address a common issue in many classification applications that the number of training data from different classes is much unbalanc", "the paper proposes a bayesian framework to address it with a gaussian mixture model"], "labels": ["MIN", "GEN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the math looks reason", "i am not sure about the novelty of the paper as it is a relatively standard definition of bayesian math", "essentially instead of computing a softmax prediction which is the discrimination probability of each class given the input one uses a logistic regression type interpretation equ", "this has been used in multi-class classification befor", "for example many early svm papers deal with multi-class classification by training -vs-all classifiers on each class and then choose the one having the highest score possibly with a class-prior adjust"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nnote that this actually changes the underlying assumption a bit softmax basically assumes the classes are mutually exclusive while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple class", "this probably does not match the assumption of many of the datasets being tested upon cifar mnist but i don't consider that a fundamental issu", "i am quite a bit concerned about the experimentation protocol as wel", "the datasets are relatively smaller scale and datasets such as mnist and cifar are known to overfit", "as a result although there are approaches taken to generate unbalanced datasets out of them eg mnist"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["regardless the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but i am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method", "the authors introduce a novel novel for collaborative filt", "the proposed model combines some of the strengths of factorization machines and of polynomial regress", "another way to understand this model is that it's a feed forward neural network with a specific connection structure ie not fully connect", "the paper is well written overall and relatively easy to understand"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the study seems fairly thorough both vanilla and cold-start experiments are report", "overall the paper feels a little bit incomplet", "this is particularly apparent in the empirical studi", "given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental result", "here are some suggestions about how to achieve that  methodically report results for mf fm ctr when meaningful other strong baselines maybe slim and all your methods for all dataset"], "labels": ["MAJ", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["report results on well-known cf dataset", "movielens comes to mind", "shed some light on some of the poor ctr results last paragraph of section  explore the models and shed some lights on where the gains are coming from", "minor - how do you deal with unobserved preferences in the implicit cas", "n- i found the idea of figure  very good but in its current form i didn't find it particularly insightful these clouds are hard to interpret"], "labels": ["GEN", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- it may also be worth adding this reference when discussing neural factorizationhttp//wwwcstorontoedu/~mvolkovs/nips_deepcfpdf", "the basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier kennen-o approach", "it is surprising that some of these hyperparameters can even be predicted with more than chance accuraci", "as a simple example it's possible that there are values of batch size for which the classifiers may become indistinguish", "yet table  shows that batch size can be predicted with much higher accuracy than ch"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be good to provide insights into under what conditions and why hyperparameters can be predicted accur", "that would make the results much more interesting and may even turn out to be useful for other problems such as hyperparameter optim", "the selection of the queries for kennen-o is not explained what is the procedure for selecting the queries how sensitive is the performance of kennen-o to the choice of the queri", "one would expect that there is significant sensitivity in which case it may even make sense to consider learning to select a sequence of queries to maximize accuraci", "in table  it would be useful to show the results for kennen-o as well because split-e seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io"], "labels": ["MIN", "MIN", "MAJ", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the imagenet classifier family prediction how different are the various families from each oth", "without going through all the references it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision read", "overall the results seem interest", "but without more insights it's difficult to judge how generally useful they ar", "the paper is incomplete and nowhere near finished it should have been withdrawn"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theoretical results are presented in a bitmap figure and only referred to in the text not explain", "and  the results on datasets are not explained either and pretty bad a waste of my tim", "generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attent", "this text generation task is traditionally done using recurrent neural network", "this paper proposes to generate text using gan"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["gans are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete natur", "this paper's approach is to use an actor-critic to train the generator of the gan and use the usual maximum likelihood with sgd to train the discrimin", "the whole network is trained on the fill-in-the-blank task using the sequence-to-sequence architecture for both the generator and the discrimin", "at training time the generator's encoder computes a context representation using the masked sequ", "this context is conditioned upon to generate missing word"], "labels": ["MAJ", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or r", "with this approach one can generate text at test time by setting all inputs to blank", "pros and positive remarks --i liked the idea behind this pap", "i find it nice how they benefited from context left context and right context by solving a fill-in-the-blank task at training time and translating this into text generation at test tim", "--the experiments were well carried through and very thorough"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--i second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequ", "i first thought that performance would be better when the generator's encoder uses the unmasked sequ", "passing the masked sequence is the right thing to do to avoid the mismatch between training time and test tim", "cons and negative remarks--there is a lot of pre-training required for the proposed architectur", "there is too much pre-training i find this less eleg"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--there were some unanswered questions             was pre-training done for the baseline as wel", "how was the masking don", "how did you decide on the words to mask was this at random", "it was not made very clear whether the discriminator also conditions on the unmasked sequ", "it needs to but                   that was not explicit in the pap"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--very minor although it is similar to the generator it would have been nice to see the architecture of the discriminator with example input and output as wel", "suggestion for the imdb dataset it would be interesting to see if you generate better sentences by conditioning on the sentiment as wel", "the paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural network", "this is an important area and findings in this paper are interesting!", "however i believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experi"], "labels": ["MIN", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- could we look at the two distributions of inputs that each neuron tries to separ", "- could we perform more extensive empirical study to substantiate the phenomenon here under which conditions do neurons behave like binary classifiers how are network width/depth activation functions affect the result", "also a binarization experiment and finding similar to the one in this paper has been done here[] argawal et al analyzing the performance of multilayer neural networks for object recognit", "+ clarity the paper is easy to read", "a few minor presentation issues- relu -- relu+"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality the paper is incremental work upon previous research tishby et al  argawal et ", "+ significancewhile the results are interest", "the contribution is not significant as the paper misses an important explanation for the phenomenon", "i'm not sure what key insights can be taken away from thi", "this paper interprets deep residual network as a dynamic system and proposes a novel training algorithm to train it in a constructive way"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on three image classification datasets the proposed algorithm speeds up the training process without sacrificing accuraci", "the paper is interesting and easy to follow", "i have several commentstit would be interesting to see a comparison with stochastic depth which is also able to speed up the training process and gives better generalization perform", "moreover is it possible to combine the proposed method with stochastic depth to obtain further improved effici", "tthe mollifying networks [] is related to the proposed method as it also starts with shorter networks and ends with deeper model"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to see a comparison or discussion [] c gulcehre mollifying network", "tcould you show the curves on figure  or another plot for training a short resnet same depth as your starting model and a deep resnet same depth as your final model without using your approach", "in this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem whose regressor is a simple neural network", "leveraged by two recent results in global optimization they showed that with a simple two-layer relu network with two hidden units the problem with a standard mse population loss function does not have spurious local minimum point", "based on the results by lee et al which shows that first order methods converge to local minimum solution instead of saddle points it can be concluded that the global minima of this problem can be found by any manifold descent techniques including standard gradient descent method"], "labels": ["MIN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in general i found this paper clearly written and technically sound", "i also appreciate the effort of developing theoretical results for deep learning even though the current results are restrictive to very simple nn architectur", "contribution as discussed in the literature review section apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit nn this paper extends the convergence results to problems that involves nn with two hidden unit", "the analysis becomes considerably more compl", "and the contribution seems to be novel and signific"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not sure why did the authors mentioned the work on over-parameterization though", "it doesn't seem to be relevant to the results of this paper because the nn architecture proposed in this paper is rather smal", "comments on the assumptions- please explain the motivation behind the standard gaussian assumption of the input vector x", "- please also provide more motivations regarding the assumption of the orthogonality of weights w_^top w_= or the acute angle assumption in sect", "without extra justifications it seems that the theoretical result only holds for an artificial problem set"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the relu activation is very common in nn architecture without more motivations i am not sure what are the impacts of these result", "general comment  the technical section is quite lengthy and unfortunately i am not available to go over every single detail of the proof", "from the analysis in the main paper i believe the theoretical contribution is correct and sound", "while i appreciate the technical contribut", "in order to improve the readability of this paper it would be great to see more motivations of the problem studied in this paper even with simple exampl"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore it is important to discuss the technical assumptions on the  standard gaussianity of the input vector", "and  the orthogonality of the weights and the acute angle assumption in section  on top of the discussions in section  as they are critical to the derivations of the main theorem", "the paper proposes a new neural network model for learning graphs with arbitrary length by extending previous models such as graph lstm liang  and graph convnet", "there are several recent studies dealing with similar topics using recurrent and/or convolutional architectur", "the related work part of this paper makes a good description of both top"], "labels": ["MIN", "MIN", "GEN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would expect the paper elaborate more at least in a more explicit way about the relationship between the two models the proposed graph lstm and the proposed gated graph convnet", "the authors claim that the innovative of the graph residual convnets architecture but experiments and the model section do not clearly explain the merits of gated graph convnets over graph lstm", "the presentation may raise some misunderstand", "a thorough analysis or explanation of the reasons why the convnet-like architecture is better than the rnn-like architecture would be interest", "in the section of experiments they compare  different methods on two graph mining task"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["these two proposed neural network models seem performing well empir", "in my opinion the two different graph neural network models are both suitable for learning graphs with arbitrary length and both models worth future stuies for speicific problem", "summary- the paper proposes a new activation function that looks similar to elu but much cheaper by using the inverse square root funct", "contributions- the paper proposes a cheaper activation and validates it with an mnist experi", "the paper also shows major speedup compared to elu and tanh unit-wise speedup"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the proposed function has similar behavior as elu but x cheap", "- the authors also refer us to faster ways to compute square root functions numerically which can be of general interests to the community for efficient network designs in the futur", "- the paper is clearly written and key contributions are well pres", "cons- clearly the proposed function is not faster than relu", "in the introduction the authors explain the motivation that relu needs centered activation such as bn"], "labels": ["GEN", "GEN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the authors also need to justify that isrlu or elu doesnut need bn", "in fact in a recent study of elu-resnet shah et al  finds that elu without bn leads to gradient explos", "to my knowledge bn at least in training time is much more expensive than the activation function itself so the speedup get from isrlu may be killed by using bn in deeper networks on larger benchmark", "at inference time all of relu elu and isrlu can fuse bn weights into convolution weights so again isrlu will not be faster than relu", "the core question here is whether the smoothness and centered zero property of elu can buy us any win compared to relu"], "labels": ["GEN", "MIN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i couldnut find it based on the results presented her", "- the authors need to validate on larger datasets eg cifar if not imagenet so that their proposed methods can be widely adopt", "- the speedup is only measured on cpu", "for practical usage especially in computer vision gpu speedup is needed to show an impact", "conclusion- based on the comments above i recommend weak reject"], "labels": ["GEN", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["references- shah a shinde s kadam e shah h shingade s deep residual networks with exponential linear unit", "in proceedings of the third international symposium on computer vision and the internet visionnet'", "as there are many kinds of domain adaptation problems the need to mix several learning strategies to improve the existing approaches is obvious however this task is not necessarily easy to succe", "the authors proposed a sound approach to learn a proper representation in an adversarial way and comply the cluster assumpt", "the experiments show that this virtual adversarial domain adaptation network vada achieves great results when compared to existing learning algorithm"], "labels": ["CNT", "GEN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover we also see the learned model is consistently improved using the proposed decision-boundary iterative refinement training with a teacher dirt-t approach", "the proposed methodology relies on multiple choices that could sometimes be better studied and/or explain", "namely i would like to empirically see which role of the locally-lipschitz regularization term equ", "also i wonder why this term is tuned by an hyperparameter lamda_s for the source while a single hyperparamer lambda_t is used for the sum of the two target quant", "on the theoretical side the discussion could be improved namely section  about limitation of domain adversarial training correctly explained that domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capac"], "labels": ["MAJ", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be interesting to explain whether this observation is consistent with theorem  of the paper due to ben-david et al  on which several domain adversarial approaches are bas", "the need to consider supplementary assumptions such as  to achieve good adaptation can also be studied through the lens of more recent ben-david's work eg ben-david and urn", "in the latter the notion of probabilistic lipschitzness which is a relaxation of the cluster assumption seems very related to the actual workreferenceben-david and urn", "domain adaptation-can quantity compensate for quality ann math artif intel", "pros- propose a sound approach to mix two complementary strategies for domain adaptation- great empirical result"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- some choices leading to the optimization problem are not sufficiently explained- the theoretical discussion could be improv", "typos- equation  in the first term target loss theta should have an index t i think- bottom of page   and that as our validation set missing word", "this paper describes a method for computing representations for out-of-vocabulary words eg based on their spelling or dictionary definit", "the main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task rather than trying to produce generically useful embed", "the method leads to better performance than using no external resourc"], "labels": ["MAJ", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but not as high performance as using glove embed", "the paper is clearly written and has useful ablation experi", "however i have a couple of questions/concerns- most of the gains seem to come from using the spelling of the word", "as the authors note this kind of character level modelling has been used in many previous work", "- i would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but i donut know the area well enough to make specific suggest", "- ium a little skeptical about how often this method would really be useful in practic", "it seems to assume that you donut have much unlabelled text or youud use glove but you probably need a large labelled dataset to learn how to read dictionary definitions wel", "all the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task", "- the results on squad seem pretty weak - -% compared to the sota of"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems like the proposed method is quite generic so why not apply it to a stronger baselin", "this paper proposed a class of control variate methods based on stein's ident", "stein's identity has been widely used in classical statistics and recently in statistical machine learning literatur", "nevertheless applying stein's identity to estimating policy gradient is a novel approach in reinforcement learning commun", "to me this approach is the right way of constructing control variates for estimating policy gradi"], "labels": ["MAJ", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors also did a good job in connecting with existing works and gave concrete examples for gaussian polici", "the experimental results also look promis", "it would be nice to include some theoretical analyses like under what conditions the proposed method can achieve smaller sample complexity than existing work", "overall this is a strong paper and i recommend to accept", "the authors introduce the concept of angle bias angle between a weight vector w and input vector x  by which the resultant pre-activation wx is biased if ||x|| is non-zero or ||w|| is non-zero theorm  from the articl"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduct", "authors chose to add an additional optimization constraint lcw |w|= to achieve zero-mean pre-activation while as mentioned in the article other methods like batch normalization bn tend to push for |x|= and unit std to do the sam", "clearly because of lack of scaling factor incase of lcw like that in bn it doesnot perform well when used with relu", "when using with sigmoid the activation being bouded  seems to compensate for the lack of scaling in input", "while bn explicitly makes the activation zero-mean lcw seems to achieve it through constraint on the weight featur"], "labels": ["GEN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["though it is shown to be computationally less expensive lcw seems to work in only specific cases unlike bn", "the authors study the effect of label noise on classification task", "they perform experiments of label noise in a uniform setting structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch s", "although the observations are interesting especially the one on mnist where the network performs well even with correct labels slightly above chance the overall contributions are increment", "most of the observations of label noise such as training with structured noise importance of larger datasets have already been archived in prior work such as in sukhbataar etal  and van horn et "], "labels": ["MIN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["agreed that the authors do a more detailed study on simple mnist classification but these insights are not transferable to more challenging domain", "the main limitation of the paper is proposing a principled way to mitigate noise as done in sukhbataar etal  or an actionable trade-off between data acquisition and training schedul", "the authors contend that the way they deal with noise keeping number of training samples constant is different from previous setting which use label flip", "however the previous settings can be reinterpreted in the authors set", "i found the formulation of the alpha to be non-intuitive and confusing at tim"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the graphs plot number of noisy labels per clean label so a alpha of  would imply  right label and  noisy labels for total  label", "in fact this depends on the task at hand for mnist it is  clean labels for  label", "this can be improved to help readers understand bett", "there are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting and also devise architectures depending on the level of expected noise in the label", "overall i feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable set"], "labels": ["MIN", "GEN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["missing citation training deep neural networks on noisy labels with bootstrapping reed et ", "the paper proposes training ``inference networks'' which are neural network structured predictor", "the setup is analogous to generative adversarial networks where the role of the discriminator is played by a structured prediction energy network spen and the generator is played by an inference network", "the idea is interesting it could be viewed as a type of adversarial training for large-margin structured predictors where counterexamples ie structures with high loss and low energy cannot be found by direct optim", "however it remains unclear why spens are the right choice for an energy funct"], "labels": ["MIN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient desc", "however the experimental results are not clearly pres", "the clarity is poor enough that the paper might not be ready for publ", "comments and questions it is unclear whether this paper is motivated by training spens or by training structured predictor", "the setup focuses on using spens as an inference network but this seems inessenti"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments with simpler energy functions seem to be absent though the experiments are unclear see below", "the confusion over the motivation is confounded by the fact that the experiments are very unclear", "sometimes predictions are described as the output of spens tables    and  sometimes as inference networks table  and sometimes as a crf tables  and", "in  it says that a bilstm is used for the inference network in twitter pos tagging but tables  and  indicate both crfs and bilstm", "it is also unclear when a model eg bilstm or crf is the energy function discriminator or inference network gener"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the third and fourth columns of table  are ident", "the presentation should be made consistent either with dev/test or -retuning/+retuning as the top level head", "n it is also unclear how to compare tables  and", "the second to bottom row of table  seems to correspond with the first row of table  but other methods like slack rescaling have higher perform", "what is the takeaway from these two tables supposed to be part of the motivation for the work is said to be the increasing interest in inference network"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in these and related settings gradient descent has started to be replaced by inference network", "our results below provide more evidence for making this transition however no other work on inference networks is directly cit", "previous work by cai et al  shows how to use neural programmer-interpreter npi framework to prove correctness of a learned neural network program by introducing recurs", "it requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem", "moreover the traces need to be recursive each function only takes a finite bounded number of act"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors show how training set can be generated automatically satisfying the conditions of cai et al's pap", "they iteratively explore allpossible behaviors of the oracle in a breadth-first manner and the bounded nature of the recursiveoracle ensures that the procedure converg", "as a running example they show how this can be be done for bubblesort", "the training set generated in this fprocess may have a lot of duplicates and the authors show how these duplicates can possibly be remov", "it indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the pap"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not an expert in this area so it is difficult for me to judge the technical merit of the work", "my feeling from reading the paper is that it is rather incremental over cai et ", "i am impressed by the results of the three experiments that have been shown here specifically the reduction in the training samples once they have been generated is signific", "but these are also the same set of experiments performed by cai et ", "given the original number of traces generated is huge i do not understand why this method is at all pract"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this also explains why the authors have just tested the performance on extremely small sized data it will not scal", "so i am hesitant accepting the pap", "i would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data", "this paper proposes a ranking-based similarity metric for distributional semantic model", "the main idea is to learn baseline word embeddings retrofitting those and applying localized centering to then calculate similarity using a measure called ranking-based exponential similarity measure resm which is based on the recently proposed apsyn measur"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the work has several important issu", "the work is very light on refer", "there is a lot of previous work on evaluating similarity in word embeddings eg hill et al a lot of the papers in repeval workshops etc specialization for similarity of word embeddings eg kiela et al mrksic et al and many others multi-sense embeddings eg from navigli's group and the hubness problem eg dinu et al for the localized centering approach hara et al's introduced that method", "none of this work is cited which i find inexcusableu", "the evaluation is limited in that the standard evaluations eg simlex would be a good one to add as well as many others please refer to the literature are not used and there is no comparison to previous work"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results are also presented in a confusing way with the current state of the art results separate from the main results of the pap", "it is unclear what exactly helps in which case and whyu", "there are technical issues with what is presented with some seemingly factual error", "for example in this case we could apply the inversion however it is much more convinient [sic] to take the negative of dist", "number  in the equation stands for the normalizing hence the similarity is defined as follows - the  does not stand for normalizing that is the way to invert the cosine distance put differently cosine distance is -cosine similarity which is a metric in euclidean space due to the properties of the dot product"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["another example are obtained using the glove vector not using ppmi - there are close relationships between what glove learns and ppmi which the authors seem unaware of see eg the glove paper and omer levy's worku", "then there is the additional question why should we car", "the paper does not really motivate why it is important to score well on these tests these kinds of tests are often used as ways to measure the quality of word embeddings but in this case the main contribution is the similarity metric used *on top* of the word embed", "in other words what is supposed to be the take-away and why should we car", "as such i do not recommend it for acceptance - it needs significant work before it can be accepted at a confer"], "labels": ["MIN", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor points- typo in eq - typo on page  /cite instead of cit", "overall this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscap", "the analysis answers when empirical gradients are close to true gradi", "n when empirical isolated saddle points are close to true isolated saddle point", "when the empirical risk is close to the true risk"], "labels": ["MIN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers width and the exponential of the norm of the weights with respect to depth then empirical quantities will be close to true quant", "i have not verified the proofs in this paper given short notice to review but the scaling laws in the upper bounds found seem reasonably correct", "another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry but i suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data", "moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not", "an analysis of dynamics on this loss surface was presented in saxe et al iclr  which could be cited to address that reviewer's concern"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice on", "overall i believe this paper is a nice contribution to the deep learning theory literatur", "however  it would even better to help the reader with more intuitive statements about the implications of their results for practice and the gap between their upper bounds and practice especially given the intense interest in the generalization error problem", "because their upper bounds look similar to those based on rademacher complexity or vc dimension although they claim theirs are a little tighter - they should put numbers in to their upper bounds taken from trained neural networks and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of paramet", "i suspect their upper bounds will be loos"], "labels": ["GEN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bound", "this manuscript makes the case for a particular parameterization of conditional gans specifically how to add conditioning information into the network", "it motivates the method by examining the form of the log density ratio in the continuous and discrete cas", "this paper's empirical work is quite strong bringing to bare nearly all of the established tools we currently have for evaluating implicit image models ms-ssim fid inception scor", "what bothers me is mostly that while hyperparameters are stated and thank you for that they seem to be optimized for the candidate method rather than the baselin"], "labels": ["MIN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular beta =  for the adam momentum coefficient seems like a bold choice based on my experi", "it would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control allowing the baseline to put its best foot forward", "the sentence containing assume that the network model can be shared had me puzzled for a few minut", "i think what is meant here is just that we can parameterize the log density ratio directly including some terms that belong to the data distribution to which we do not have explicit access this could be clear", "this paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and docu"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper rightly mentions that existing reading comprehension datasets eg squad where the current methods are already performing at the human level largely due to large lexical overlap between question and docu", "the authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from wikipedia and imdb", "this paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap", "the authors also present an analysis of the data by applying one of the sota techniques on squad to this data", "they also analyze the effect of various span-identification steps and preprocessing steps on the perform"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall this paper contributes a useful new dataset that can be quite useful for reading comprehens", "summarythis paper empirically studies adversarial perturbations dx and what the effects are of adversarial training at with respect to shared dx fools for many x and singular only for a single x perturb", "experiments use a previously published iterative fast-gradient-sign-method and use a resnet on cifar", "the authors conclude that in this experimental setting- at seems to defend models against shared dx'", "- this is visible on universal perturbations which become less effective as more at is appli"], "labels": ["MAJ", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- at decreases the effectiveness of adversarial perturbations eg at decreases the number of adversarial perturbations that fool both an input x and x with eg a contrast chang", "- singular perturbations are easily detected by a detector model as such perturbations don't change much when applying at", "pro- paper addresses an important problem qualitative / quantitative understanding of the behavior of adversarial perturbations is still lack", "- the visualizations of universal perturbations as they change during at are nic", "- the basic observation wrt the behavior of at is clearly commun"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["con- the experiments performed are interesting directions although unfocused and rather limited in scop", "for instance does the same phenomenon happen for different dataset", "different model", "- what happens when we use adversarial attacks different from fgsm", "do we get similar result"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the papers lacks a more in-depth theoretical analysi", "is there a principled reason at+fgsm defends against universal perturb", "overall- as is it seems to me the paper lacks a significant central message due to limited and unfocused experiments or significant new theoretical insight into the effect of at", "a number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigationsdetailed-", "this paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of cnns for object recognition task"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the paper is comprehensive in their derivations very similar to original boosting papers and in many cases one to one translation of derivations it lacks addressing a few fundamental questions- adaboost optimises exponential loss function via functional gradient descent in the space of weak learn", "it's not clear what kind of loss function is really being optimised her", "it feels like it should be the same but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end", "- while the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes it is not clear why this methods should be better than normal boosting if a class is more difficult it's expected that their samples will have higher weights and hence the next base model will focus more on them", "and crudely speaking you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- choice of using large cnns as base models for boosting isn't appealing in practical terms such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learn", "- experimentally paper would benefit with better comparisons and studies  state-of-the-art methods haven't been compared against eg imagenet experiment compares to  years old method", "comparisons to using normal adaboost on more complex methods haven't been studied other than the mnist", "comparison to simply ensembling with random initialis", "other comments- paper would benefit from writing improvements to make it read bett"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- simply use the weighted error function i don't think this is correct adaboost loss function is an exponential loss", "when you train the base learners their loss functions will become weight", "-  to replace the softmax error function used in deep learning i don't think we have softmax error funct", "authors present complex valued analogues of real-valued convolution relu and batch normalization funct", "their related work section brings up uses of complex valued computation such as discrete fourier transforms and holographic reduced represent"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valu", "their contributions are formulate complex valued convolut", "formulate two complex-valued alternatives to relu and compare them", "formulate complex batch normalization as a whitening operation on complex domain", "formulate complex analogue of glorot weight normalization schem"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["since any complex valued computation can be done with a real-valued arithmetic switching to complex arithmetic needs a compelling use-cas", "for instance some existing algorithm may be formulated in terms of complex values and reformulating it in terms of real-valued computation may be awkward", "however cases the authors address which are training batch-norm relu networks on standard datasets are already formulated in terms of real valued arithmet", "switching these networks to complex values doesn't seem to bring any benefit either in simplicity or in classification perform", "the authors show how techniques typically applied to real-valued networks eg with real-valued inputs and parameters can be straighforwardly generalized to complex-valued networks eg with complex-valued inputs and paramet"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors then provide several evaluations of complex-valued networks on some standard ml benchmark task", "they find that the complex-valued networks do not in general perform better than real-valued network", "====================clarity  i found the paper clear and easy to understand", "in a number of places there are clear signs of sloppiness eg undefined cit", "i found the undefined citations in the middle of page  frustrating since i'd have liked to follow up on those citations as comparison points for this work"], "labels": ["GEN", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality the mathematical formulas describing basic complex analysis ideas eg  derivatives of complex functions definitions of complex versions of standard activation functions seem reasonable to m", "the general approach of assigning a parameter budget to ensure fairness in comparison between complex and real-valued networks seems reasonable --", "- although of course since all results should be reported on cross-validated testing subsets anyhow", "parameter equalization is not the only approach to fair evalu", "originality  it seems very unclear to me what is added in this paper in comparison to works like eg trabelsi"], "labels": ["GEN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that and other recent work have provided some systematic evaluations of complex-valued networks and shown their utility in a number of cas", "the current paper's authors talk about previous work not being well-controlled for number of paramet", "however at least in some key cases in the recent literature parameter numbers *were* controlled see eg table  of trabelsi", "so i'm not really sure what is being added her", "significance   the paper does not make a great case for caring about complex-valued network"], "labels": ["GEN", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["of course negative results are of value but it doesn't seem like much is at stake in this work to begin with", "it's not like people expected complex-valued networks to somehow be extremely effective for the tasks discussed here -", "- so the failure to be better than the real-valued alternatives seems unremark", "the paper also doesn't illustrate any novel results on tasks for which it would be reasonable to assume that complex-valued inputs would be particularly import", "the authors reference some signal processing tasks in the introduct"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but don't actually show any results on such task", "summary - this paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count", "contribution- this paper proposes a new object counting module which operates on a graph of object propos", "clarity- the paper is well written and clarity is good", "figure  &  helps the readers understand the core algorithm"], "labels": ["MIN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- de-duplication modules of inter and intra object edges are interest", "- the proposed method improves the baseline by % on counting quest", "cons- the proposed model is pretty hand-craft", "i would recommend the authors to use something more general like graph convolutional neural networks kipf & welling  or graph gated neural networks li et ", "- one major bottleneck of the model is that the proposals are not jointly finetun"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so if the proposals are missing a single object this cannot really be count", "in short if the proposals donut have % recall then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the propos", "the paper didnut study what is the recall of the proposals and how sensitive the threshold i", "- the paper doesnut study a simple baseline that just does nms on the proposal domain", "- the paper doesnut compare experiment numbers with chattopadhyay et "], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the proposed algorithm doesnut handle symmetry breaking when two edges are equally confident in  it basically scales down both edg", "this is similar to a density map approach and the problem is that the model doesnut develop a notion of inst", "- compared to zhou et al  the proposed model does not improve much on the counting quest", "- since the authors have mentioned in the related work it would also be more convincing if they show experimental results on cl", "conclusion- i feel that the motivation is good"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the proposed model is too hand-craft", "also key experiments are missing  nms baselin", "comparison with vqa counting work  chattopadhyay et ", "therefore i recommend reject", "references- kipf tn welling m semi-supervised classification with graph convolutional networks iclr"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- li y tarlow d brockschmidt m zemel r gated graph sequence neural networks iclr", "updatethank you for the rebutt", "the paper is revised and i saw nms baseline is ad", "i understood the reason not to compare with certain related work", "the rebuttal is convincing and i decided to increase my rating because adding the proposed counting module achieve % increase in counting accuraci"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a", "the paper proposed a copula-based modification to an existing deep variational information bottleneck model such that the marginals of the variables of interest x y are decoupled from the dvib latent variable model allowing the latent space to be more compact when compared to the non-modified vers", "the experiments verified the relative compactness of the latent space and also qualitatively shows that the learned latent features are more 'disentangled'", "however i wonder how sensitive are the learned latent features to the hyper-parameters and optim", "quality ok the claims appear to be sufficiently verified in the experi"], "labels": ["MAJ", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however it would have been great to have an experiment that actually makes use of the learned features to make predict", "i struggle a little to see the relevance of the proposed method without a good motivating exampl", "clarity below averag", "section  is a little hard to understand", "is qt|x in fig  a typo"], "labels": ["CNT", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how about t_j in equ", "there is a reference that appeared twice in the bibliography st and nd", "originality and significance average the paper if i understood it correctly appears to be mainly about borrowing the key ideas from rey et al  and applying it to the existing dvib model", "i thank the authors for the thoughtful response and rebutt", "the authors have substantially updated their manuscript and improved the present"], "labels": ["GEN", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["re speed i brought up this point because this was a bulleted item in the introduction in the earlier version of the manuscript", "in the revised manuscript this bullet point is now remov", "i will take this point to be moot", "re high resolut", "the authors point to recent gan literature that provides some first results with high resolution gans but i do not see quantitative evidence in the high resolution setting for this pap"], "labels": ["GEN", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  provides qualitative examples from imagenet but no quantitative assess", "because the authors improved the manuscript i upwardly revised my score to 'ok but not good enough - rejection'", "i am not able to accept this paper because of the latter point", "==========================the authors present an interesting new method for generating adversarial exampl", "namely the author train a generative adversarial network gan to adversarial examples for a target network"], "labels": ["GEN", "GEN", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors demonstrate that the network works well in the semi-white box and black box set", "the authors wrote a clear paper with great references and clear descript", "my primary concern is that this work has limited practical benefit in a realistic set", "addressing each and every concern is quite important spe", "the authors suggest that training a gan provides a speed benefit with respect to other attack techniqu"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the fgsm method goodfellow et al  is basically  inference operation and  backward oper", "the gan is  forward oper", "granted this results in a small difference in timing s versus s however it would seem that avoiding a backward pass is a somewhat small speed gain", "furthermore i would want to question the practical usage of having an 'even faster' method for generating adversarial exampl", "what is the reason that we need to run adversarial attacks 'even faster'"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not aware of any use-cases but if there are some the authors should describe the rationales at length in their pap", "high spatial resolution imag", "previous methods eg fgsm may work on arbitrarily sized imag", "at best gans generate reasonable images that are lower resolutions eg  x", "building gan's that operate above-and-beyond moderate spatial resolution is an open research top"], "labels": ["MIN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the best gan models for generating high resolution images are  difficult to train and it is not clear if they would work in this set", "furthermore images with even higher resolutions eg x which is quite common in imagenet are difficult to synthesizes using current techniqu", "controlling the amount of distort", "a feature of previous optimization based methods is that a user may specify the amount of perturbation epsilon", "this is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["performing such an analysis with this model is challenging ie retraining a gan and it is not clear if a given image generated by a gan will always achieve a given epsilon perturbation/on a more minor note the authors suggest that generating a *diversity* of adversarial images is of practical import", "i do not see the utility of being able to generate a diversity of adversarial imag", "the authors need to provide more justification for this motiv", "the paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketch", "+ new and large dataset"], "labels": ["GEN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ novel algorithm", "+ well written", "- no evaluation of dataset", "- virtually no evaluation of algorithm", "- no baselines or comparison"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is well written and easy to follow", "the presented algorithm sketch-rnn seems novel and significantly different from prior work", "in addition the authors collected the largest sketch dataset i know of", "this is exciting as it could significantly push the state of the art in sketch understanding and gener", "unfortunately the evaluation falls short"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if the authors were to push for their novel algorithm i'd have expected them to compare to prior state of the art on standard metrics ablate their algorithm to show that each component is needed and show where their algorithm shines and where it falls short", "for ablation the bare minimum includes removing the forward and/or reverse encoder and seeing performance drop", "remove the variational component and phrasing it simply as an auto-encod", "table  is good", "but not suffici"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["training loss alone likely does not capture the quality of a sketch", "a comparison the graves  is absolutely required more comparisons are desir", "finally it would be nice to see where the algorithm falls short and where there is room for improv", "if the authors wish to push their dataset it would help to first evaluate the quality of the dataset", "for example how well do humans classify these sketch"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how diverse are the sketch", "are there any obvious mod", "does the discretization into strokes matt", "additionally the authors should present a few standard evaluation metrics they would like to compare algorithms on", "are there any good automated metrics and how well do they correspond to human judg"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in summary i'm both excited about the dataset and new architectur", "but at the same time the authors missed a huge opportunity by not establishing proper baselines evaluating their algorithm and pushing for a standardized evaluation protocol for their dataset", "i recommend the authors to decide if they want to present a new algorithm or a new dataset and focus on a proper evalu", "this paper proposes to use a hybrid of convolutional and recurrent networks to predict the dsl specification of a gui given a screenshot of the gui", "nprosthe paper is clear and the proposed problem is novel and well-defin"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the training data is synthetic allowing for arbitrarily large training sets to be gener", "the authors have made their synthetic dataset publicly avail", "nthe method seems to work well based on the samples and roc curves pres", "consthis is mostly an application of an existing method to a new domain -", "- as stated in the related work section effectively the same convnet+rnn architecture has been in common use for image captioning and other vision appl"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the uis that are represented in the dataset seem quite simple itus not clear that this will transfer to arbitrarily complex and multi-page ui", "the main motivation for the proposed system seems to be for non-technical designers to be able to implement uis just by drawing a mockup screenshot", "however the paper hasnut shown that this is necessarily possible assuming the hand-designed mockups arenut pixel-for-pixel matches with a screenshot that could be generated by the ucdsl code - screenshotud mapping that this system learns to invert", "there exist a number of ucdrag and dropud style ui design products at least for html that would seem to accomplish the same basic goal as the proposed system in a more reliable way", "though the proposed system does have the advantage of only requiring a screenshot created using any software rather than being restricted to a particular piece of softwar"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the paper is well-written but the novelty and applicability seems a bit limit", "the authors present a deep neural network that evaluates plate numb", "the relevance of this problem is that there are auctions for plate numbers in hong kong and predicting their value is a sensible activity in that context", "i find that the description of the applied problem is quite interesting in fact overall the paper is well written and very easy to follow", "there are some typos and grammatical problems indicated below"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but nothing really seri", "so the paper is relevant and well pres", "however i find that the proposed solution is an application of existing techniques so it lacks on novelty and origin", "even though the significance of the work is apparent given the good results of the proposed neural network", "i believe that such material is more appropriate to a focused applied meet"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however even for that sort of setting i think the paper requires some additional work as some final parts of the paper have not been tested yet the interesting part of explan", "hence i don't think the submission is ready for publication at this mo", "concerning the text some questions/suggestions- abstract line  i suppose in the chinese society--- are there many chinese societi", "- the references are not properly formatted they should appear at xxx yyy but appear as xxx yyy in many cases mixed with the main text", "- footnote  line  an exchang"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- page  line  pric", "among- please add commas/periods at the end of equ", "- there are problems with capitalization in the refer", "this paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process", "the draft is well-written and the method is clearly explain"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i have the following concerns for this draft the technical contribution is not enough", "first the use of reinforcement learning is quite straightforward", "second the proposed method seems not significantly different from the architecture search method in [][] u their major difference seems to be the use of ucremoveud instead of ucaddud when manipulating the paramet", "it is unclear whether this difference is substantial and whether the proposed method is better than the architecture search method", "i also have concern with the time efficiency of the proposed method"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["reinforcement learning involves multiple rounds of knowledge distillation and each knowledge distillation is an independent training process that requires many rounds of forward and backward propag", "therefore the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets such as imagenet", "it would be necessary for the authors to make direct discussions on this issue in order to convince others that their proposed method has practical valu", "[] zoph barret and quoc v le neural architecture search with reinforcement learning iclr [] baker bowen et al designing neural network architectures using reinforcement learning iclr", "the paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea is interesting and overall introduction is clear", "however the work lacks justification for this particular way of encoding and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in zhang & lecun", "the results using this particular encoding are not better than any previous work", "the network architecture seems to be arbitrary and unusu", "it was designed with  convolutional layers stacked together for the first layer while a common choice is to just make it one convolutional layer with  times the output channel"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the depth of the network is only  even with many layers listed in t", "it uses -d convolution across the word dimension inferred from the feature size in table  which means the convolutional layers learn intra-word features for the entire text but not any character-level featur", "this does not seem to be reason", "overall the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in iclr", "this paper empirically investigates the performance of character-level nmt systems in the face of character-level noise both synthesized and natur"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results are not surprising* nmt is terrible with nois", "* but it improves on each noise type when it is trained on that noise typ", "what i like about this paper is that the experiments are very carefully designed and thorough", "this problem might actually matt", "out of curiosity i ran the example table  through google translate and the result was gibberish"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but as the paper shows itus easy to make nmt robust to this kind of noise and google and other nmt providers could do this tomorrow", "so this paper could have real-world impact", "most importantly it shows that nmtus handling of natural noise does *not* improve when trained with synthetic noise that is the character of natural noise is very differ", "so solving the problem of natural noise is not so simpleu itus a *real* problem", "speculating again commercial mt providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper but at much larger scal"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so these methods could be applied in the real world", "it would be excellent if an outcome of this paper was that commercial mt providers answered itus call to provide more realistic noise by actually providing exampl", "there are no fancy new methods or state-of-the-art numbers in this pap", "but itus careful curiosity-driven empirical research of the type that matters and it should be in iclr", "the authors propose to use d cnns for graph classification by transforming graphs to an image-like representation from its node embed"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach uses nodevec to obtain a node embedding which is then compacted using pca and turned into a stack of discretized histogram", "essentially the authors propose an approach to use a node embedding to achieve graph classif", "in my opinion there are several weak points the approach to obtain the image-like representation is not well motiv", "ther approaches how to  aggregate the set of node embeddings for graph classification are known see eg representation learning on graphs methods and applications william l hamilton rex ying jure leskovec", "the authors should compare to such methods as a baselin"], "labels": ["GEN", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental evaluation is not convincing- the selection of competing methods is not suffici", "i would like to suggest to add an approach similar to duvenaud et al convolutional networks on graphs for learning molecular fingerprints nip", "n- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison", "the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method cf eg kriege et ", "the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method cf eg kriege et "], "labels": ["MIN", "MIN", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on valid optimal assignment kernels and applications to graph classification nip", "n- it would be interesting to apply the approach to graphs with discrete and continuous label", "the authors argue that their method is preferable to graph kernels in terms of time complex", "this argument is question", "most graph kernels compute explicit feature maps and can therefore be used with efficient linear svms unfortunately most publications use a kernelized svm"], "labels": ["GEN", "MAJ", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover the running of computing the node embedding must be emphasized on page  the authors claim a constant time complexity at the instance level", "which is not true when also considering the running time of nodevec", "moreover i do not think that nodevec is more efficient than eg weisfeiler-lehman refinement used by graph kernel", "in summary since the technical contribution is limited the approach needs to be justified by an authoritative experimental comparison", "this is not yet achieved with the results presented in the submitted pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore it should not be accepted in its current form", "the paper applies tools from online learning to gan", "in the case of a shallow discriminator the authors proved some results on the convergence of their proposed algorithm an adaptation of ftrl in gan games by leveraging the fact that when d update is small the problem setup meets the ideal conditions for no-regret algorithm", "the paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep gan gam", "overall the paper is very well written"], "labels": ["MIN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the theory is significant to the gan literature probably less so to the online learning commun", "in practice with deep d trained by single gradient update steps for g and d instead of the argmin in algo  the assumptions of the theory break", "this is ok as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold tru", "however this is where i have issues with the work in all quantitative results chekhov gan do not significantly beat unrolled gan", "unrolled gan looks at historical d's through unrolled optimization but not the history of g"], "labels": ["GEN", "GEN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so this lack of significant difference in results raise the question of whether any improvement of chekhov gan is coming from the online learning perspective for d and g or simply due to the fact that it considers historical d models which could be motivated by sth other than the online learning theori", "the mixture gan approach suggested in arora et al  is very related to this work as acknowledged in sec  but no in-depth analysis is carried out", "i suggest the authors to either discuss why chekhov gan is obviously superior and hence no experiments are needed or compare them experiment", "in the current state it is hard to place the quantitative results in context with other common methods in the recent literature such as wgan with gradient penalti", "i suggest the authors to either report some results in terms of inception scores on cifar with similar architectures used in other methods for comparison"], "labels": ["MIN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["alternatively please show wgan-gp and/or other method results in at least one or two experiments using the evaluation methods in the pap", "in summary almost all the experiments in the paper are trying to establish improvement over basic gan which would be ok if the gap between theory and practice is smal", "but in this case it is not", "so it is not entirely convincing that the practical algo  works better for the reason suggested by the theory nor it drastically improves practical results that it could become the standard technique in the literatur", "this paper aims at robust image classification against adversarial domain shift"], "labels": ["MIN", "GEN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the used model there are two types of latent features core features and style features and the goal is to achieved by avoiding using the changing style featur", "the proposed method which makes use of grouping information seems reasonable and us", "it is nice that the authors use counterfactual regular", "but i failed to see a clear new contribution of using this causal regularization compared to some of the previous methods to achieve invariance eg relative to translation or rot", "for examples of such methods one may see the paper transform invariant auto-encoder by matsuo et al and references therein"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the data-generating process for the considered model given in figure  seems to be consistent with figure  of the paper domain adaptation with conditional transferable components by gong et al perhaps the authors can draw the connection between their work and gong et al's work and the related work discussed in that pap", "below are some more detailed comments in introduction it would be nice if the authors made it clear that their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand", "why do the features human cognition uses give an optimal predictive accuraci", "on page  the authors claimed that these are arguably one reason why deep learning requires large sample sizes as large sample size is clearly not per se a guarantee that the confounding effect will become weak", "could the authors give more detail on this a reference would be appreci"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents a model to encode and decode trees in distributed represent", "this is not the first attempt of doing these encoders and decod", "however there is not a comparative evalution with these method", "in fact it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters see decoding distributed tree structures and distributed tree kernel", "the paper should present a comparison with such kinds of model"], "labels": ["GEN", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper investigated the problem of attribute-conditioned image generation using generative adversarial network", "more specifically the paper proposed to generate images from attribute and latent code as high-level represent", "to learn the mapping from image to high-level representations an auxiliary encoder was introduc", "the model was trained using a combination of reconstruction auto-encoding and adversarial loss", "to further encourage effective disentangling against trivial solution an annihilating operation was proposed together with the proposed training pipelin"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experimental evaluations were conducted on standard face image databases such as multi-pie and celeba", "== novelty and significance ==multi-attribute image generation is an interesting task but has been explored to some ext", "the integration of generative adversarial networks with auto-encoding loss is not really a novel contribut", "-- autoencoding beyond pixels using a learned similarity metric larsen et al in icml", "== technical quality == first it is not clear how was the proposed annihilating operation used in the experiments there is no explanation in the experimental sect"], "labels": ["GEN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["based on my understanding additional loss was added to encourage effective disentangling prevent trivial solut", "i would appreciate the authors to elaborate this a bit", "second the iterative training section  is not a novel contribution since it was explored in the literature before eg inverse graphics network", "the proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribut", "third it seems that the proposed multi-attribute generation pipeline works for binary attribute onli"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however such assumption limits the generality of the work", "since the title is quite general i would assume to see the results  on datasets with real-valued attributes mixture attributes or even relative attribut", "and  not specific to face imag", "-- learning to generate chairs with convolutional neural networks dosovitskiy et al in cvpr -- deep convolutional inverse graphics network kulkarni et al in nips -- attributeimage conditional image generation from visual attributes yan et al in eccv", "-- infogan interpretable representation learning by information maximizing generative adversarial nets chen et al in nips yan et al in eccv"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["additionally considering the generation quality the celeba samples in the paper are not the state-of-the-art", "i suspect the proposed method only works in a more constrained setting such as multi-pie where the images are all well align", "overall i feel that the submitted version is not ready for publication in the current form", "this work proposes to study the generalization of learning neural networks via the fourier-based method", "it first gives a fourier-based generalization bound showing that rademacher complexity of functions with small bandwidth and fourier l_ norm will be smal"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this leads to generalization for -layer networks with appropriate bounded s", "for -layer networks with sine activation functions assuming that the data distribution has nice spectral property ie bounded bandwidth it shows that the local minimum of the population risk if with isolated component condition will have small size and also shows that the gradient of the empirical risk is close to that of the population risk", "empirical results show that the size of the networks learned on random labels are larger than those learned on true labels and shows that a regularizer implied by their fourier-based generalization bound can effectively reduce the generalization gap on random label", "the idea of applying the fourier-based method to generalization is interest", "however the theoretical results are not very satisfactori"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-- how do the bounds here compared to those obtained by directly applying rademacher complexity to the neural network funct", "-- how to interpret the isolated components condition in theorem", "basically it means that bp_x should be a small const", "what type of distributions of x will be a good exampl", "-- it is not easy to put together the conclusions in section  and"], "labels": ["GEN", "GEN", "MIN", "GEN", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["suppose sgd leads to a local minimum of the empirical loss", "one can claim that this is an approximate local minimum ie small gradient by corollari", "but to apply theorem  one will need a version of theorem  for approximate local minima", "also one needs to argue that the local minimum obtained by sgd will satisfy the isolated component condit", "the argument in section  is not convincing ie there is potentially a large approximation error in  and one cannot claim that lemma  and theorem  are still valid without the isolated component condit"], "labels": ["GEN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythe paper proposes to learn new priors for latent codes z  for gan train", "for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the gener", "to fix this the paper proposes to learn a second gan to learn the prior distributions of real latent code of the first gan", "the first gan then uses the second gan as prior to generate the z cod", "quality/claritythe paper is well written and easy to follow"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originalitypros-the paper while simple sheds some light on important problem with the prior distribution used in gan", "- the second gan solution trained on reverse codes from real data is interest", "- in general the topic is interesting the solution presented is simple but needs more studi", "cons- it related to adversarial learned inference and bigan in term of learning the mapping  z -x x-z and seeking the agr", "- the solution presented is not end to end learning a prior generator on learned models have been done in many previous works on encoder/decod"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["general reviewmore experimentation with the latent codes will be interest", "- have you looked at the decay of the singular values of the latent codes obtained from reversing the gener", "is this data low rank", "how does this change depending on the dimensionality of the latent cod", "maybe adding plots to the paper can help"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequ", "maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior", "- have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc", "maybe also rotating the codes with the singular vector matrix v or sigma^{} v", "- what architecture did you use for the prior generator gan"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- have you thought of an end to end way to learn the prior generator gan", "****** i read the authors reply thank you for your answers and for the svd plots this is  help", "this paper proposes a new semantic way for data augmentation problem specifically targeted for one-shot learning setting ie synthesizing training samples based on semantic similarity with a given sampl", "specifically the authors propose to learn an autoencoder model where the encoder translates image data into the lower dimensional subspace of semantic representation word-to-vec representation of image classes and the decoder translates semantic representation back to the original input spac", "for one-shot learning in addition to a given input image the following data augmentation is proposed a perturbed input image gaussian noise added to input image featur"], "labels": ["MIN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b perturbed decoded imag", "c perturbed decoded neighbour image where neighbourhood is searched in the semantic spac", "the idea is nice and simpl", "however the current framework has several weaknesses the whole pipeline has three neural network components a input image features are extracted from vgg net pre-trained on auxiliary data", "auto-encoder that is trained on data for one-shot learn"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["final classifier for one-shot learning is learned on augmented image space with two if i am not mistaken fully connected lay", "this three networks need to be clearly described ideally combined into one end-to-end training pipelin", "the empirical performance is very poor", "if you look into literature for zero shot learning work by z akata in cvpr  cvpr the performance on awa and on cub-bird goes way above % where in the current paper it is % and % at most for the most recent survey on zero shot learning papers using attribute embeddings please refer to zero-shot learning - the good the bad and the ugly by xian et al cvpr", "it is important to understand why there is such a big drop in performance in one-shot learning comparing to zero-shot learn"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one possible explanation is as follows in the zero-shot learning one has access to large training data to learn the semantic embedding training class", "in contrary in the proposed approach the auto-encoder model with  hidden layers is learned using  training samples in awa and  images of birds or am i missing someth", "i am not sure how can the auto-encoder model not overfit completely to the training data inst", "perhaps one could try to explore the zero-shot learning setting where there is a split between train and test classes training the autoencoder model using large training dataset and adapting the weights using single data points from test classes in one-shot learning set", "overall i like the idea so i am leaning towards accepting the pap"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the empirical evaluations are not convinc", "this paper investigates multiagent reinforcement learning  making used of a master slave architecture msa", "on the positive side the paper is mostly well-written seems technically correct and there are some results that indicate that the msa is working quite well on relatively complex task", "on the negative side there seems to be relatively limited novelty we can think of msa as one particular communication ie star configuration one could use is a multiagent system", "one aspect does does strike me as novel is the gated composition module which allows differentiation of messages to other agents based on the receivers internal st"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so the *interpretation* of the message is learned i like this idea", "however the results are mixed and the explanation given is plausible but far from a clearly demonstrated answ", "there are some important issues that need clarification* sukhbaatar et al  proposed the uccommnetud where broadcasting communication channel among all agents were set up to share a global information which is the summation of all individual agents [] however the summed global signal is hand crafted information and does not facilitate an independently reasoning master ag", "-please explain what is meant here by 'hand crafted information' my understanding is that the f^i in figure  of that paper are learned modul", "-please explain what would be the differences with commnet with  extra agent that takes in the same information as your 'master'"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["*this relates also to this later we empirically verify that even when the overall in-formation revealed does not increase per se an independent master agent tend to absorb the sameinformation within a big picture and effectively helps to make decision in a global mann", "thereforecompared with pure in-between-agent communications ms-marl is more efficient in reasoningand planning once trained []", "specifically we compare the performance among the commnet model ourms-marl model without explicit master state eg the occupancy map of controlled agents in thiscase and our full model with an explicit occupancy map as a state to the master ag", "as shown infigure  ab by only allowed an independently thinking master agent and communication amongagents our model already outperforms the plain commnet model which only supports broadcast-ing communication of the sum of the sign", "-minor i think that the statement which only supports broadcast-ing communication of the sum of the signals is not quite fair surely they have used a -channel communication structure but it would be easy to generalize that"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-major when i look at figure d i see that the proposed approach *also* only provides the master with the sum or really mean with of the individual messages so it is not quite clear to me what explains the difference*in  it is not quite clear exactly how the figure of master and slave actions is cr", "this seems to suggest that the only thing that the master can communicate is action inform", "it this the cas", "* in table  it is not clear how significant these differences ar", "what are the standard error"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the section  explains standard things policy gradient but the details are a bit unclear", "in particular i do not see how the gaussian/softmax layers are integrated they do not seem to appear in figur", "* i cannot understand figure  without more explan", "the background is all black - did something go wrong with the pdf", "details* references are wrongly formatted throughout"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* in this regard we are among the first to combine both the centralized perspective and the decentralized perspectivethis is a weak statement eg i suppose that in the greater scheme of things all of us will be amongst the first people that have walked this earth", "* therefore they tend to work more like a commentator analyzing and criticizing the play rather thana coach coaching the gam", "-this sounds somewhat vagu", "can it be made crisp", "* note here that although we explicitly input an occupancy map to the master agent the actual infor-mation of the whole system remains the sam"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a somewhat peculiar stat", "clearly the distribution of information over the agents is cruci", "for more insights on this one could refer to the literature on decentralized pomdp", "thanks for all the explanations on my review and the other com", "while i can now clearly see the contributions of the paper the minimal revisions in the paper do not make the contributions clear yet in my opinion that should already be clear after having read the introduct"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the new section intuitive analysis is very nic", "*******************************my problem with this paper that all the theoretical contributions / the new approach refer to  arxiv papers what's then left is an application of that approach to learning form imperfect demonstr", "quality======the approach seems sound", "but the paper does not provide many details on the underlying approach", "the application to learning from partially adversarial demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy sampl"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are ok", "but i would have liked a more thorough analysi", "clarity=====the paper reads wel", "but it is not really clear what the claimed contribution i", "originality=========the application seems origin"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["significance==========having an rl approach that can benefit from truly off-policy samples is highly relev", "pros and cons============+ good result", "+ interesting idea of using the algorithm for rlfd", "- weak experiments for an application pap", "- not clear what's new"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors define a simulated multi-agent uctaxi pickupud task in a gridworld environ", "in the task there are multiple taxi agents that a model must learn to control", "uccustomersud randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a custom", "since there are multiple customer and taxi agents there is a multi-agent coordination problem", "further the taxi agents have ucbatteriesud which starts at a positive number ticks down by one on each time step and a large negative reward is given if this number reaches zero"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the battery can be ucrechargedud by moving to a ucchargeud til", "cooperative multi-agent problem solving is an important problem in machine learning artificial intelligence and cognitive sci", "this paper defines and examines an interesting cooperative problem assignment and control of agents to move to certain squares under ucphysicalud constraint", "the authors propose a centralized solution to the problem by adapting the deep q-learning network model", "i do not know whether using a centralized network where each agent has a window of observations is a novel algorithm"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the manuscript itself makes it difficult to assess more on this lat", "if it were novel it would be an incremental develop", "they assess their solution quantitatively demonstrating their model performs better than first a simple heuristic model i believe de-centralized dijkstraus for each agent but there is not enough description in the manuscript to know for sure and then two other baselines that i could not figure out from the manuscript i believe it was dijkstraus with two added rules for when to recharg", "although the manuscript has many positive aspects to it", "i do not believe it should be accepted for the following reason"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first the manuscript is poorly written to the point where it has inhibited my ability to assess it", "second given its contribution the manuscript is better suited for a conference specific to multi-agent decision-mak", "there are a few reasons for this  i was not convinced that deep q-learning was necessary to solve this problem", "the manuscript would be much stronger if the authors compared their method to a more sophisticated baseline for example having each agent be a simple q-learner with no centralization or ucdeepnessud", "this would solve another issue which is the weakness of their baseline measur"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are many multi-agent techniques that can be applied to the problem that would have served as a better baselin", "although the problem itself is interest", "it is a bit too applied and specific to the particular task they studied than is appropriate for a conference with as broad interests as iclr", "it also is a bit simplistic i had expected the agents to at least need to learn to move the customer to some square rather than get reward and move to the next job from just getting to the customerus squar", "can you apply this method to other multi-agent problem"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["how would it compare to other methods on those problem", "i encourage the authors to develop the problem and method further as well as the analysis and evalu", "the paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction mak", "however these discovered core units are specific to a particular class which are retained to maintain the deep neural networkus ability to separate that particular class from the other on", "thus these non-core units for a particular class could be core units for separating another class from the remaining on"], "labels": ["MIN", "MIN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consequently the aggregation of all class-specific core units could include all hidden units of a lay", "therefore it is hard for me to understand whatus the motivation to identify the core units in a one-vs-remaining mann", "at this moment these identified class-specific core units are useful for neither reducing the size of the network nor accelerating comput", "this paper analyzes the expressiveness and loss surface of deep cnn", "i think the paper is clearly written and has some interesting insight"], "labels": ["GEN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# summary of paperthe authors propose a parallel algorithm for training deep neural networks unlike other parallel variants of sgd this parallelizes across the layers and not across sampl", "# summary of reviewthe idea of model-parallelism as opposed to data parallelism is appealing and an important open problem", "however this contribution is far from correctly addressing the problem", "the algorithm is poorly described and crucial parts of the algorithm are very confus", "mathematical rigor in the proof and discussion is lacking proof has mathematical error"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["# detailed comments* key definitions are scattered across the paper making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a vari", "to make things worse some variables are simply not defin", "for example i can't find the definition of d", "from the context it seems to be the number of layers in the network i shouldn't need to guess", "* from algorithm  the bracket notation is used for both indexing and specifying the size of the vari"], "labels": ["MIN", "MAJ", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is nonstandard and confus", "* again from algorithm  it is not clear which parts can be performed asynchron", "it is even not clear to me if the algorithm can be run asynchronously as some of the other reviewers seem to imply or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their continuous-propagation factor", "* eq  and  i doubt this is true without some assumptions on the distribution of the data generating process", "* the proof despite being a trivial application of existing work has obvious flaw"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after equation  it is stated that the left-hand side is independent of x_{k m l} which is not true since theta_{k+} is computed **precisely** using x_{k m l} and so is not independent this is actually done correctly in lian  where the expectation is correctly carried on that term", "* the proof relies on an inequality  in which key quantities are not defined what is l is l = l_d and which is impossible to verify in practice t is not known", "this crucial detail is only mentioned in the appendix giving the impression in the main text that the algorithm is always converg", "it should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quant", "* as mentioned in the other reviews key references are lacking eg for ode interpretation eq  and"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in appendix * assumption   why is upper superindex d", "in any case be consistent most of the time these are used but then its stated for all theta whithout superindex * proposition what is l is l = l_d other  * assumption  decay - delay", "the approach solves an important problem as getting labelled data is hard", "the focus is on the key aspect which is generalisation across heteregeneous data", "the novel idea is the dataset embedding so that their rl policy can be trained to work across diverse datasetspro"], "labels": ["MIN", "MIN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach performs well against all the baselines and also achieves good cross-task generalisation in the tasks they evaluated on", "in particular they alsoevaluated on test datasets with fairly different statistics from the training dataset", "which isnt very common in most meta-learning papers today so itus encouraging that the method works in that regim", "cons  the embedding strategy especially the representative and discriminative histograms is compl", "it is unclear if the strategy is general enough to work on harder problems / larger datasets or with higher dimensional data like imag"], "labels": ["MAJ", "GEN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more evidence in the paper for why it would work on harder problems would be great", "the policy network would have to output a probability for each datapoint in the dataset u", "which could be fairly large thus the method is computationally much more expensive than random sampl", "a section devoted to showing what practical problems could be potentially solved by this method would be us", "it is unclear to me if the results in table  and  are achieved by retraining from scratch with an rbf svm"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or by freezing the policy network trained on a linear svm and directly evaluating it with a rbf svm base learn", "significance/conclusion the idea of meta-learning or learning to learn is fairly common now", "while they do show good perform", "itus unclear if the specific embedding strategy suggested in this paper will generalise to harder task", "comments thereus lots of typos please proof read to improve the pap"], "labels": ["MIN", "GEN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["revision i thank the authors for the updates and addressing some of my concern", "i agree the computational budget makes sense for cross data transf", "however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder task", "i update my review to", "the paper studies the theoretical properties of the two-layer neural network"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to summarize the result let's use the theta to denote the layer closer to the label and w to denote the layer closer to the data", "the paper shows that a if w is fixed then with respect to the randomness of the data with prob  the jacobian matrix of the model is full rank", "b suppose that we run an algorithm with fresh samples then with respect to the randomness of the k-th sample we have that with prob  w_k is full rank and the jacobian of the model is full rank", "it's know essentially from the proof of carmon and soudry that if the jacobian of the model is full rank for any matrix w wrt the randomness of the data then all stationary points are glob", "but the paper cannot establish such a result"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is not very clear and after figuring out what it's doing i don't feel it really provides many new things beyond c-s and xie et ", "the paper argues that it works for activation beyond relu but result a is much much weaker than the one with for all quantifier for w", "result b is very sensitive to the exactness of the events such as w is exactly full rank --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degener", "as the author admitted the results don't provide any formal guarantees for the convergence to a global minimum", "it's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this quest"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["--------------------additional review after seeing the author's response the author's response pointed out some of the limitation of soudry and carmon and xie et al's which i agre", "however none of this limitation is addressed by this paper or addressed in a misleading way to some ext", "the key technical limitation is the dependency of the local minima on the weight paramet", "soudry and carmon addresses this in a partial way by using the random dropout which is a super cool idea", "xie et al couldn't address this globally but show that the jacobian is well conditioned for a class of weight"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper here doesn't have either and only shows that for a single fixed weight matrix the jacobian is well-condit", "i don't also see the value of extension to other activation funct", "to some extent this is not consistent with the empirical observation that relu is very important for deep learn", "regarding the effect of randomness since the paper only shows the convergence to a first-order optimal solution i don't see why randomness is necessari", "gradient descent can converge to a first order optimal solut"], "labels": ["MAJ", "GEN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed i have a typo in my previous review regarding wrt k-th sample which should be wrt k-th upd", "moreover to justify the effect of the randomness the paper should have empirical experi", "i think the writing of the paper is also misleading in several plac", "[ =========================== revision ===============================================================]i am satisfied with the answers to my quest", "the paper still needs some work on clarity and authors defer the changes to the next version but as i understood they did no changes for this paper as of now which is a bit frustr"], "labels": ["GEN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i am fine accepting it", "[ ============================== end of revision =====================================================]this paper concerns with addressing the issue of sgd not converging to the optimal parameters on one hidden layer network for a particular type of data and label gaussian features label generated using a particular function that should be learnable with neural net", "authors demonstrate empirically that this particular learning problem is hard for sgd with l loss due to apparently bad local optima and suggest two ways of addressing it on top of the known way of dealing with this problem which is overparameter", "first is to use a new activation function the second is by designing a new objective function that has only global optima and which can be efficiently learnt with sgd", "overall the paper is well written"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors first introduce their suggested loss function and then go into details about what inspired its cr", "i do find interesting the formulation of population risk in terms of tensor decomposition this is insight", "my issues with the paper are as follows- the loss function designed seems overly compl", "on top of that authors notice that to learn with this loss efficiently much larger batches had to be us", "i wonder how applicable this in practice - i frankly didn't see insights here that i can apply to other problems that don't fit into this particular narrowly defined framework"], "labels": ["GEN", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i do find it somewhat strange that no insight to the actual problem is provided eg it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima but authors are concerned with developing new loss function that has provable properties about global optima", "since it is all empirical the first fix activation function seems sufficient to me and new loss is very far-fetch", "- it seems that changing activation function from relu to their proposed one fixes the problem without their new loss so i wonder whether it is a problem with relu itself and may be other activations funcs like sigmoids will not suffer from the same problem", "- no comparison with overparameterization in experiments results is given which makes me wonder why their method is bett", "minor fix margins in formula"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main strength of this paper i think is the theoretical result in theorem", "this result is quite nic", "i wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result furth", "the authors ended the discussion on thm  on page  just above sec  by saying what is sufficiently close to w*", "if one goes back to  it is easy to see that what converges to w* when one of three things happen assuming beta is fixed once loss l is select"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["k goes to infinity alpha goes to  gw* goes to", "the authors discussed how alpha is close to  by virtue of submodular optimization lower bounds there for what is close to w*", "in fact this proof shows the situation is much better than that", "if we are really concerned about making what converge to w* and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k we can schedule k to increase over time which guarantees that both alpha goes to  and gw* goes to zero", "there is also a remark that ga tends to be modular when lambda is small which is us"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from the algorithm it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearli", "it would be really nice to complete the analysis of thm with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion", "such an analysis would help practitioners make better choices for the hyper parameters gamma and delta", "clarity the paper is well-written and clear", "originalitythe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropi"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper leverages a novel method in determining the coefficient of relative entropi", "significance- trust-pcl achieves overall competitive with state-of-the-art external implement", "- trust-pcl off-policy significantly outperform trpo in terms of data efficiency and final perform", "- even though the paper claims trust-pcl on-policy is close to trpo the initial performance of trpo looks better in halfcheetah hopper walkerd and ", "- some ablation studies eg on entropy regularization and relative entropy and sensitivity analysis on parameters eg alpha and update frequency on phi would be help"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- the paper is well-written and clear", "- competitive with state-of-the-art external implement", "- significant empirical advantage over trpo", "-  open source cod", "cons- no ablation studi"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["score before author revision score after author revision i think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account running several ablations as well as reporting nice results on an entirely new dataset multinli where they show how their multi level fusion mechanism improves a baseline significantli", "i think this is nice since it shows how their mechanism helps on two different tasks question answering and natural language infer", "therefore i would now support accepting this pap", "------------original review below -----------------------the authors present an enhancement to the attention mechanism called multi-level fusion that they then incorporate into a reading comprehension system", "it basically takes into account a richer context of the word at different levels in the neural net to compute various attention scor"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ie the authors form a vector how called history of the word that is defined as a concatenation of several vectorshow_i = [g_i c_i h_i^l h_i^h]where g_i = glove embeddings c_i = cove embeddings mccann et al  and h_i^l and h_i^h are different lstm states for that word", "the attention score is then a function of these concatenated vectors ie alpha_{ij} = expshow_i^c how_j^q", "results on squad show a small gain in accuracy - exact match", "the gains on the adversarial set are larger but that is because some of the higher performing more recent baselines don't seem to have adversarial numb", "the authors also compare various attention functions table  showing a particularone symmetric + relu works the best"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["comments-i feel overall the contribution is not very novel", "the general neural architecture that the authors propose in section  is generally quite similar to the large number of neural architectures developed for this dataset eg some combination of attention between question/context and lstms over question/context", "the only novelty is these how inputs to the extra attention mechanism that takes a richer word representation into account", "-i feel the model is seems overly complicated for the small gain ie - exact match especially on a relatively exhausted dataset squad that is known to have lots of pecularities see anonymous comment below", "it is possible the gains just come from having more paramet"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-the authors on page  claim that that by running attention multiple times with different parameters but different inputs ie alpha_{ij}^l alpha_{ij}^h alpha_{ij}^u it will learn to attend to different regions for different level", "however there is nothing enforcing this and the gains just probably come from having more parameters/complex", "the paper compares some recently proposed method for validation of propertiesof piece-wise linear neural networks and claims to propose a novel method forthe sam", "unfortunately the proposed branch and bound method does not explainhow to implement the bound part compute lower bound -- and has been used several times in the same appl", "inclruediger ehlers planet https//githubcom/progirep/planetchih-hong cheng georg nuhrenberg and harald ruess  maximum resilience of artificial neural networks automated technology for verification and analysisalessio lomuscio and lalit maganti  an approach to reachability analysis for feed-forward relu neural networks arxiv"], "labels": ["MIN", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["specifically the authors say in our experiments we use the result of minimising the variable corresponding to the output of the network subject to the constraints of the linear approximation introduced by ehlers awhich sounds a bit like using linear programming relaxations which is whatthe approaches using branch and bound cited above us", "if that is the casethe paper does not have any original contribut", "if that is not the casethe authors may have some contribution to make but have not made it in thispaper as it does not explain the lower bound computation other than the onebased on lp", "generally i find a jarring mis-fit between the motivation deep learningfor driving presumably involving millions or billions of parameters andthe actual reach of the methods proposed hundreds of paramet", "this reach is not inherent in integer programming per s"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["modern solversroutinely solve instances with tens of millions of non-zeros in the constraintmatrix but require a strong relax", "the authors may hence considerimproving the lp relaxation noting that the big-m constraint are notoriousfor producing weak relax", "here are my main critics of the papers equation    are those expectations wrt the data distribution otherwise i can't think of any other stochast", "if so your phrase is zero given a sequence of inputs x t is mislead", "lack of motivation for ie or ui"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["where is your background materi", "i do not understand why we would like to assum", "why the same intuition of uie can be applied to rnn", "the paper proposed the new architecture rin but it is not much different than a simple rnn with identity initi", "not much novelti"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
