"This paper suggests a simple yet effective approach for learning with weak supervision.[[INT-NEU], [null], [SMY], [GEN]] This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision.[[DAT-NEU], [null], [SMY], [GEN]]  The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \"fidelity\" of the weak label when training the student at the final step.[[MET-NEU], [null], [SMY], [GEN]] The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The suggested method seems to work well on several document classification tasks.[[MET-POS], [null], [APC], [MAJ]] \n\nOverall, I liked the paper.[[OAL-POS], [null], [APC], [MAJ]]  I would like the authors to consider the following questions;[[OAL-NEU], [null], [QSN], [GEN]] - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few).[[RWK-NEU], [null], [DIS], [GEN]]  First, I'd suggest acknowledging these works and discussing the differences to your work. [[RWK-NEU], [CMP-NEU], [SUG], [MAJ]]Second - Is your approach applicable to these frameworks?[[MET-NEU], [CMP-NEU], [QSN], [MAJ]]  It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve.[[RWK-NEU,RES-NEU], [CMP-NEU], [DIS], [MAJ]] \n\n- Can this approach be applied to semi-supervised learning?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. [[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] Is it clear that this step in needed?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] using different architecture than the student?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.[[EXT-POS], [null], [APC], [MAJ]]"