"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data. [[INT-NEU], [null], [SMY], [GEN]] This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.[[MET-POS], [EMP-POS], [APC], [MAJ]]  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The authors evaluate their proposed methods on one toy problem and two real-world problems.[[EXP-NEU], [null], [SMY], [GEN]] The paper is well written, easy to follow, and have good experimental study.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]  My main problem with the paper is the lack of enough motivation and justification for the proposed method;[[PDI-NEG,MET-NEU], [EMP-NEG], [CRT], [MAJ]] the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]] Here are some questions that comes to my mind:[[OAL-NEU], [null], [DIS], [GEN]]  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?[[DAT-NEU,MET-NEU], [SUB-NEU], [QSN], [MIN]] (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?[[DAT-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]] (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?[[DAT-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]] Why not learning the representation using an unsupervised learning method (unsupervised pre training)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] This should be at least one of the baselines.[[RWK-NEU], [null], [SUG], [MIN]]\n(4) the idea of using surrogate labels to learn representation is also not new.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] One example work is \"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\". The authors didn't compare their method with this one.[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]]"