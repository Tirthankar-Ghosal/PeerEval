"The paper proposes combining classification-specific neural networks with auto-encoders.[[INT-NEU], [null], [SMY], [GEN]] This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.[[MET-NEU], [null], [SMY], [GEN]] The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works. [[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]]The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).[[EXP-NEU], [null], [SMY], [GEN]] For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different.[[EXP-NEU,RES-NEU,TNF-NEU], [CMP-NEU], [SMY], [GEN]] The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.[[EXP-NEU,RES-NEU], [CMP-NEU], [SMY], [GEN]]\n\nWhile the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders.[[EXP-NEU,RES-POS], [IMP-NEG], [CRT], [MAJ]] Training the classification-features along with reconstruction-features does not seem to give any significantly new insights[[EXP-NEU], [IMP-NEG], [CRT], [MAJ]]. "