"This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL. The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output. The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types. The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach.\n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications. Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously. However, there are several issues both in the approach and the \ncurrent preliminary evaluation, which unfortunately leads me to a reject score,\nbut the general idea of combining different specifications is quite promising.\n\nFirst, the paper does not compare against a very similar approach of Parisotto et al.\nNeuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network\nfor generating the program tree incrementally by decoding one node at a time.\nCan the authors comment on the similarity/differences between the approaches?\nWould it be possible to empirically evaluate how the R3NN performs on this dataset?\n\nSecond, it seems that the current model does not use the input-output examples at \nall for training the model. The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search. It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy.\n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples.\n\nFinally, there is also an issue with the synthetic evaluation dataset. Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics. The paper mentions that it is prohibitively \nexpensive to obtain human-annotated set, but can it be possible to at least obtain a \nhandful of real tasks to evaluate the learnt model? There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future.\n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100? What happens when it is set to 10^4 or 10^6?\n\nThe Search algorithm only shows an accuracy of 0.6% with MAX_VISITED=100. What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)? \n\nCan you clarify the contributions of the paper in comparison to the R3NN?\n\nMinor typos:\n\npage 2: allows to add constrains --> allows to add constraints\npage 5: over MAX_VISITED programs has been --> over MAX_VISITED programs have been\n\n"