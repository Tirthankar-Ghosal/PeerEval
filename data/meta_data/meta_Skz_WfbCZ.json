{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This is a strong paper presenting a very clean proof of a result that is similar, though now incomparable to one due to Bartlett et al. These bounds (and Bartlett's) are among the most promising norm-based bounds for NNs.\n\nI would simply add that the citation of Dziugaite and Roy (2017) could be improved. There work also connects sharpness (or flatness) with generalization via the PAC-Bayes framework, and moreover, there bounds are nonvacuous.  Are the bounds in this paper nonvacuous, say, on MNIST for 60,000 training data, for the network learned by SGD?  If not, how close do they get to 1.0?", "decision": "Accept (Poster)"}