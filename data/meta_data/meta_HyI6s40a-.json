{"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:\n\n- It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples\n\n- The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker\n\n- It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)", "decision": "Reject"}