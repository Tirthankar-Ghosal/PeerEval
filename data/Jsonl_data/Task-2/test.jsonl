{"abstract_id": 0, "sentences": ["it's not compared against any previous published result", "eg the addition tasks and smnist tasks are not as good as those reported in []", "also it only has been tested on very simple dataset", "[] path-normalized optimization of recurrent neural networks with relu activ", "behnam neyshabur yuhuai wu ruslan salakhutdinov nathan srebro"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["make svm great again with siamese kernel for few-shot learn", "** paper summary **the author proposes to combine siamase networks with an svm for pair classif", "the proposed approach is evaluated on few shot learning tasks on omniglot and timit", "** review summary **the paper is read", "but it could be more flu"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it lacks a few references and important technical aspects are not discuss", "it contains a few error", "empirical contribution seems inflated on omniglot as the authors omit other papers reporting better result", "overall the contribution is modest at bestv** detailed review **on mistakes it is wrong to say that an svm is a parameterless classifi", "it is wrong to cite boser et al  for the soft-margin svm"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think slack variables come from cortes et ", "consistent has a specific definition in machine learning https//enwikipediaorg/wiki/consistent_estimator  you must use a different word in", "you mention that a non linear svm need a similarity measure it actually need a positive definite kernel which has a specific definition https//enwikipediaorg/wiki/positive-definite_kernel", "on incompleteness it is not obvious how the classifier is used at test tim", "could you explain how classes are predicted given a test problem"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the setup of the experiments on timit is extremely unclear", "what are the class you are interested in", "how many classes and examples does the testing problems hav", "on clarity i do not understand why you talk again about non-linear svm in the last paragraph of  since you mention at the end of page  that you will only rely on linear svms for computational reason", "you need to mention explicitely somewhere that wtheta are optimized jointli"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the sentence this paper investigates only the one versus rest approach is confusing as you have only two classes from the svm perspective ie pairs xx where both examples come from the same class and pairs xx where they come from different class", "so you use a binary svm not one versus rest", "you need to find a better justification for using l-svm than l-svm loss variant is considered to be the best by the author of the paper did you try classical svm and found them performing wors", "also could you motivate your choice for l norm as opposed to l in eq", "on empirical evaluation i already mentioned that it impossible to understand what the classification problem on timit i"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i suspect it might be speaker identif", "so i will focus on the omniglot experi", "few-shot learning through an information retrieval lens eleni triantafillou richard zemel raquel urtasun nips  [arxiv july']and the reference therein give a few more recent baselines than your t", "some of the results are bett", "than your approach"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not sure why you do not evaluate on mini-imagenet as well as most work on few shot learning generally do", "this dataset offers a clearer experimental setup than your timit setting and has abundant published baseline result", "also most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set", "this paper extends the recurrent weight average rwa ostmeyer and cowell  in order to overcome the limitation of the original method while maintaining its advantag", "the motivation of the paper and the approach taken by the authors are sensible such as adding discounting was applied to introduce forget mechanism to the rwa and manipulating the attention and squash funct"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method is using elman nets as the base rnn", "i think the same method can be applied to grus or lstm", "some parameters might be redund", "however assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently it would be nice to see the outcomes of this combin", "is there any explanation why lstms perform so badly compared to grus the rwa and the rda"], "labels": ["GEN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall the proposed method seems to be very useful for the rwa", "the paper presents a novel adversarial training setup based on distance based loss of the feature embed", "+ novel loss", "+ good experimental evalu", "+ better perform"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- way too long", "- structure could be improv", "- pivot loss seems hacki", "the distance based loss is novel and significantly different from prior work", "it seems to perform well in practice as shown in the experimental sect"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental section is extensive and offers new insights into both the presented algorithm and baselin", "judging the content of the paper alone it should be accept", "however the exposition needs significant improvements to warrant accept", "first the paper is way too long and unfocus", "the recommended length is  pages +  page for cit"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper is + pages long plus a  page suppl", "i'd highly recommend the authors to cut a third of their text it would help focus the paper on the actual message pushing their new algorithm", "try to remove any sentence or word that doesn't serve a purpose help sell the algorithm", "the structure of the paper could also be improv", "for example the cascade adversarial training is buried deep inside the experimental sect"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["considering that it is part of the title i would have expected a proper exposition of the idea in the technical section before any results are pres", "while condensing the paper consider presenting all technical material before evalu", "finally the pivot loss seems a bit hacki", "first the pivot objective and bidirectional loss are exactly the same th", "while the bidirectional loss is a proper loss and optimized as such by optimizing both e^adv and e the pivot objective is no loss function as it does not correspond to any function any optimization algorithm could minim"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i'd recommend the just remove the pivot objective or at least not call it a loss", "in summary the results and presented method are good and eventually deserve publ", "however the exposition needs to significantly improve for the paper to be ready for iclr", "this paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores the network is trained in end-to-end thus training the parallel branches jointli", "existing literature with branching architecture either employ a  stage training approach training branches independently and then training the fusion network or the branching is restricted to local regions set of contiguous lay"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in effect this paper extends the existing literature suggesting end-to-end branch", "while the technical novelty as described in the paper is relatively limited the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research commun", "+ paper is well written and easy to follow", "+ proposed branching architecture clearly outperforms the baseline network same number of parameters with a single branch and thus offer yet another interesting choice while creating the network architecture for a problem", "+ detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branch"], "labels": ["MAJ", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["+ [ease of implementation] suggested architecture can be easily implemented using existing deep learning framework", "- although joint end-to-end training of branches certainly brings value compared to independent training but the increased resource requirements may limits the applicability to large benchmarks such as imagenet", "while authors suggests a way to circumvent such limitations by training branches on separate gpus but this would still impose limits on the number of branches as well as its ease of implement", "- adding an overview figure of the architecture in the main paper instead of supplementary would be help", "- branched architecture serve as a regularization by distributing the gradients across different branches however this also suggests that early layers on the network across branches would be independ"], "labels": ["GEN", "GEN", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would helpful if authors would consider an alternate archiecture where early layers may be shared across branches suggesting a delayed branching with fusion at the final lay", "- one of the benefits of architectures such as densenet is their usefulness as a feature extractor output of lower layers which generalizes even to domain other that the dataset the branched architecture could potentially diminish this benefit", "minor edits page  'significantly match and improve' = 'either match or improve'additional not", "- it would interesting to compare this approach with a conditional training pipeline that sequentially adds branches keeping the previous branches fix", "this may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branch"], "labels": ["GEN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the centre loss the centre is learn", "now it's calculated as the average of the last layer's featur", "to enable training with sgd the authors calculate the centre within a mini batch", "in this work the authors suggest the use of control variate schemes for estimating gradient values within a reinforcement learning  framework", "the authors also introduce a specific control variate technique based on the so-called steinus ident"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper is interesting and well-written", "i have some question and some consideration that can be useful for improving the appealing of the pap", "- i believe that different monte carlo or quasi-monte carlo strategies can be applied in order to estimate the integral expected value in eq  as also suggested in this work", "are there other alternatives in the literatur", "please please discuss and cite some papers if requir"], "labels": ["MAJ", "GEN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- i suggest to divide section  in two subsections the first one introducing steinus identity and the related comments that you need and a second one starting after theorem  with title ucstein control variateud", "-  please also discuss the relationships connections and possible applications of your technique to other algorithms used in bayesian optimization active learning and/or sequential learning for instance asm", "u gutmann and j corander ucbayesian optimization for likelihood-free inference of simulator-based statistical mod- elsud journal of machine learning research vol  pp u   g da silva ferreira and d gamerman ucoptimal design in geostatistics under preferential samplingud bayesian analysis vol  no  pp u  l martino j vicent g camps-valls automatic emulator and optimized look-up table generation for radiative transfer models ieee international geoscience and remote sensing symposium igarss", "-  please also discuss the dependence of your algorithm with respect to the starting baseline function phi_", "this paper introduces a new nonlinear activation function for  neural networks ie inverse square root linear units isrlu"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments show that isrlu is promising compared to competitors like relu and elu", "pros the paper is clearly written", "the proposed isrlu function has similar curves with elu and has a learnable parameter alpha although only fixed value is used in the experiments to control the negative saturation zon", "cons authors claim that isrlu is faster than elu while still achieves eluus perform", "however they only show the reduction of computation complexity for convolution and speed comparison between relu isrlu and elu on high-end cpu"], "labels": ["MAJ", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i know even though modern cnns have reduced convolutionus computation complex", "the computation cost of activation function is still only a very small part less than % in the overall running time of training/infer", "authors only experimented with two very simple cnn architectures and with three different nonlinear activation functions ie isrlu/elu/relu and showed their accuracies on mnist", "they did not provide the comparison of running time which i believe is important here as the efficiency is emphasized a lot throughout the pap", "for isrlu of cnn experiments on larger scale dataset such as cifar or imagenet would be more convinc"], "labels": ["GEN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover authors also propose isru which is similar to tanh for rnn but do not provide any experimental result", "overall i think the current version of the paper is not ready for iclr confer", "as i suggested above authors need more experiments to show the effectiveness of their approach", "this paper illustrates a method to compute produce word embeddings on the fly for rare words using a pragmatic combination of existing idea", "* backing off to a separate decoder for rare words a la luong and manning https//arxivorg/pdf/pdf should be cited though the idea might be old"], "labels": ["GEN", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* using character-level models a la ling et ", "* using dictionary embeddings a la hill et ", "none of these ideas are new before but i havenut seen them combined in this way befor", "this is a very practical idea well-explained with a thorough set of experiments across three different task", "the paper is not surpris"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but this seems like an effective technique for people who want to build effective systems with whatever data theyuve got", "this paper presents two complementary models for unsupervised domain adaptation classification task  the virtual adversarial domain adaptation vada and  the decision-boundary iterative refinement training with a teacher dirt-t", "the authors make use of the so-called cluster assumption ie decision boundaries should not cross high-density data region", "vada extends the standard domain-adversarial training by introducing an additional objective l_t that measures the target-side cluster assumption violation namely the conditional entropy wrt the target distribut", "since the empirical estimate of the conditional entropy breaks down for non-locally-lipschitz classifiers the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behav"], "labels": ["MAJ", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of l_t using natural gradient descent dirt-t which ensures that the decision boundary changes incrementally and slowli", "pros+ the paper is written clearly and easy to read", "+ the idea to keep the decision boundary in the low-density region of the target domain makes sens", "+ the both proposed methods seem to be quite easy to implement and incorporate into existing datnn-based framework", "+ the combination of vada and dirt-t performs better than existing da algorithms on a range of visual da benchmark"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- table  can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base nn architectures and different optim", "- the paper deals exclusively with visual domains applying the proposed methods to other modalities would make this submission strong", "overall i think it is a good paper and deserves to be accepted to the confer", "ium especially appealed by the fact that the ideas presented in this work despite being simple demonstrate excellent perform", "post-rebuttal revisionafter reading the authors' response to my review i decided to leave the score as i"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["quality the work focuses on a novel problem of generating text sample using gan and a novel in-filling mechanism of word", "using gan to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issu", "as a remedy to these problems an in-filling-task conditioning on the surrounding text has been propos", "but the use of the rewards at every time step rl mechanism to employ the actor-critic training procedure could be challenging computationally challeng", "clarity the mechanism of generating the text samples using the proposed methodology has been described clearli"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the description of the reinforcement learning step could have been made a bit more clear", "originality the work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of gan training in text set", "there has been some work using gan to generate adversarial examples in textual context too to check the robustness of classifi", "how this current work compares with the existing such literatur", "significance the research problem is indeed significant since the use of gan in generating adversarial examples in image analysis has been more prevalent compared to text set"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the proposed actor-critic training procedure via rl methodology is indeed significant from its application in natural language process", "prosa human evaluations applications to several datasets show the usefulness of maskgen over the maximum likelihood trained model in generating more realistic text sampl", "b using a novel in-filling procedure to overcome the complexities in gan train", "c generation of high quality samples even with higher perplexity on ground truth set", "consa use of rewards at every time step to the actor-critic training procure could be computationally expens"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["b how to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding word", "c depending on the mask quality gan can produce low quality sampl", "any practical way of choosing the mask", "i enjoyed reading the pap", "this is a very well written paper the authors propose a method for speeding up the training time of residual networks based on the dynamical system view interpretation of resnet"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in general i have a positive opinion about the paper however iud like to ask for some clarif", "ium not fully convinced by the interpretation of eq  ucu d is inversely proportional to the norm of the residual modules gyjud", "since fyj is not a constant i think that d is inversely proportional to ||gyj||/||fyj|| however in the interpretation the dependence on ||fyj|| is ignor", "could the authors comment on that", "section   uc each cycle itself can be regarded as a training process thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycl"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud is there any empirical evidence for thi", "what would happen if the learning rate is not reset at the beginning of each cycl", "questions with respect to dynamical systems point of view eq  assumes small value of h", "however for resnet there is no guarantee that the h would be small e g in appendix c the values between  and  are us", "would the authors be willing to comment on the importance of the value of h"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in figure  pooling strided convolutions are not depicted between network stag", "i have one question wrt feature maps dimensionality changes inside a cnn how does pooling or strided convolution fit into dynamical systems view", "table  and  i assume that the training time unit is a minute i couldnut find this information in the pap", "is the batch size the same for all models  for cifar and  for stl-", "i understand that the models with different #blocks have different capacity for clarity would it be possible to add # of parameters to each model"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for multilevel method would it be possible to show intermediate results in table  and  e g at the end of cycle  and", "i see these results in figure  however the plots are condensed and it is difficult to see the exact number at the end of each cycl", "the citation e  seems to be wrong could the authors check it", "--------------------review updatesrating  - confidence  - the rebuttal and update addressed a number of my concerns cleared up confusing sections and moved the paper materially closer to being publication-worthy thus iuve increased my score-----", "---------------i want to love this pap"], "labels": ["MIN", "MIN", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results seem like they may be very import", "however a few parts were poorly explained which led to this reviewer being unable to follow some of the jumps from experimental results to their conclus", "i would like to be able to give this paper the higher score it may deserve but some parts first need to be further explain", "unfortunately the largest single confusion i had is on the first most basic set of gradient results of sect", "without understanding this first result itus difficult to decide to what extent the rest of the paperus results are to be believ"], "labels": ["GEN", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["fig  shows ucthe histograms of the average sign of partial derivatives of the loss with respect to activations as collected over training for a random neuron in five different lay", "ud letus consider the top-left subplot of fig  showing a heavily bimodal distribution modes near - and + is this plot made using data from a single neuron or from  multiple neuron", "for now letus assume it is for a single neuron as the caption and text in  seem to suggest if it is for a single neuron then that neuron will have for a single input example a single scalar activation value and a single scalar gradient valu", "the sign of the gradient will either be + or - if we compute the sign for each input example and then aggregate over all training examples seen by this neuron over the course of training or a subset for computational reasons this will give us a list of sign", "letus collect these signs into a long list [+ + + - + + u]"], "labels": ["GEN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["now what do we do with this list", "as far as i can tell we can either average it giving say  if the list has far more + values than - values or we can show a histogram of the list which would just be two bars at - and +", "but we canut do both indicating that some assumption above was incorrect which assumption in reading the text was incorrect", "further in this direction section  claims uczero partial derivatives are ignored to make the signal more clear", "ud are these zero partial derivatives of the post-relu or pre-relu"], "labels": ["GEN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the text sec  points to activations as being post-relu but in this case zero-gradients should be a very small set only occuring if all neurons on the next layer had either zero pre-relu gradients which is common for individual neurons but i would think not for all at onc", "or does this mean the pre-relu gradient is zero eg the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slop", "in this case we would be excluding a large set about half! of the gradient values and it didnut seem from the context in the paper that this would be desir", "it would be great if the above could be address", "below are some less important commentssec  great results!"], "labels": ["GEN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["fig  this figure studies ucthe first and last layers of each networkud is the last layer really the last linear layer the one followed by a softmax", "in this case there is no relu and the  pre-activation is not meaningful softmax is shift invari", "or is the layer shown eg ucstagelayerud the penultimate lay", "minor in this figure it would be great if the plots could be labeled with which networks/datasets they are from", "sec  states ucneuron partitions the inputs in two distinct but overlapping categories of quasi equal s"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ud this experiment only shows that this is true in aggregate not for specific neuron", "ie the partition percent for each neuron could be sampled from u  or from u  and this experiment would not tell us which correct", "perhaps this statement could be qualifi", "table  ucth percentile vs actual  percentile shownud  table  the more fuzzy the higher the percentile rank of the threshold", "this is true for the cifar net but the opposite is true for resnet right"], "labels": ["MIN", "MAJ", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summary the paper considers the problem of a single hidden layer neural network with  relu units this is what i got from the paper - as i describe below it was not clear at all the setting of the problem", "- if i'm mistaken i will also wait for the rest of the reviews to have a more complete picture of the problem", "given this architecture the authors focus on characterizing the objective landscape of such a problem", "the techniques used depend on previous work", "according to the authors this paper extends previous results on a nn with a single layer with a single unit"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality the paper heavily depends on the approach followed by brutzkus and globerson", "to this end slighly novel", "importance understanding the landscape local vs global minima vs saddle points is an important direction in order to further understand when and why deep neural networks work", "i would say that the topic is an important on", "presentation/clarity to the best of my understanding the paper has some misconcept"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the title is not clear whether the paper considers a two layer relu network or a single layer with with two relu unit", "in the abstract the authors state that it has to do with a two-layer relu network with two hidden units per layer in tot", "later on in section  the expression at the bottom of page  seems to consider a single-layer relu network with two unit", "these are crucial for understanding the contribution of the paper while reading the paper i assumed that the authors consider the case of a single hidden unit with k =  relu activations however that complicated my understanding on how it compares with state of the art", "another issue is the fact that on my humble opinion the main text looks like a long proof"], "labels": ["MIN", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be great to have more intuit", "comments the paper mainly focuses on a specific problem instance where the weight vectors are unit-normed and orthogonal to each oth", "while the authors already identify that this might be a restriction it still does not lessen the fact that the configuration considered is a really specific on", "the paper reads like a collection of lemmas with no verbose connect", "it was hard to read and understand their value just because mostly the text was structured as one lemma after the oth"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear from the text whether the setting is already considered in brutzkus and globerson", "please clarify how your work is different/new from previous work", "-----update------having read the responses from the authors and the other reviews i am happy with my rating and maintain that this paper should be accept", "----------------------in this paper the authors trains a large number of mnist classifier networks with differing attributes batch-size activation function no layers etc and then utilises the inputs and outputs of these networks to predict said attributes success", "they then show that they are able to use the methods developed to predict the family of imagenet-trained networks and use this information to improve adversarial attack"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i enjoyed reading this pap", "it is a very interesting set up and a novel idea", "a few commentsthe paper is easy to read and largely written wel", "the article is missing from the nouns quite often though so this is something that should be amended there are a few spelling slip ups to a certain extend -- to a certain extent as will see -- as we will see]", "it appears that the output for kennen-o is a discrete probability vector for each attribute where each entry corresponds to a possibility for example for batch-size it is a length  vector where the first entry corresponds to  the second  and the third"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what happens if you instead treat it as a regression task would it then be able to hint at intermediates a batch size of  or extremes say", "na flaw of this paper is that kennen-i and io appear to require gradients from the network being probed you do mention this in passing which realistically you would never have access to please do correct me if i have misunderstood thi", "it would be helpful if section  had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict", "also the caption for table  could contain more information regarding the network output", "you have jumped from predicting  attributes on mnist to  attribute on imagenet"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it could be beneficial to do an intermediate experiment a handful of attributes on a middling task", "i think this paper should be accepted as it is interesting and novel", "pros------- interesting idea", "- reads wel", "- fairly good experimental result"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons------- kennen-i seems like it couldn't be realistically deployed- lack of an intermediate difficulty task", "this paper proposes to improve time complexity of factorization machin", "unfortunately the paper's claim that fm's time complexity is quadratic to feature size is wrong", "specifically the dot product can be computed as which is linear to feature sizesum x_i beta_i^t sum x_i beta_i - sum_i x_i^ beta_i^t beta_i", "the projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals on"], "labels": ["GEN", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when the number of feature groups do not equal one they correspond to field aware factorization machineffm", "summary the paper presents an unsupervised method for detecting adversarial examples of neural network", "the method includes two independent components an uinput defenderu which tried to inspect the input and a ulatent defenderu trying to inspect a hidden represent", "both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples and modeling this sub-space hence enables their detect", "the input defender is based on sparse coding and the latent defender on modeling the latent activity as a mixture of gaussian"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments are presented on minst cifar and imagenet", "-tintroduction the motivation for detecting adversarial examples is not stated clearly enough", "how can such examples be used by a malicious agent to cause damage to a system", "sketching some such scenarios would help the reader understand why the issue is practically import", "i was not convinced it i"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["page  -tstep  of the algorithm is not clearothow exactly does hdda model the data formally and how does it estimate the paramet", "in the current version the paper does not explain the hdda formalism and learning algorithm which is a main building block in the proposed system as it provides the density score used for adversarial examples detect", "hence the paper cannot be read as a standalone docu", "i went on to read the relevant hdda paper but it is also not clear which of the model variants presented there is used in this pap", "otwhat is the relation between the model learned at stage  the centers c^i and the model learnt by hdda"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are they completely different models or are the c^i used when learning the hdda model and how if these are separate models how are they used in conjunction to give a final density scor", "if i understand correctly only the hdda model is used to get the final score and the c^i are only used to make the phyx representation more class-seperable is that right", "-tfigure  b and c it is not clear what the xyz measurements plotted in these d drawings are what are the axi", "page -tsection  the risk analysis is done in a standard bayesian way and leads to a ratio of pdfs in equ", "however this form is not appropriate for the case presented at this paper since the method presented only models one of these pdfs specifically px | w  - there is not generative model of px|w"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tthe authors claim in the last sentence of the section that px|w is equivalent to -px|w but this is not true these are two continuous densities they do not sum to  and a model of px|w is not available as far as i understand the method", "page -thow is equation  optim", "-twhich patchs are extracted from images for training and at inference tim", "are these patchs a dense coverage of the image sparsely sampled densely sampled with overlap", "-tits not clear enough what exactly is the upsnru value which is used for the adversarial example detection and what exactly is uprofile the psnr of legitimate samples within each classu"], "labels": ["MAJ", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a formal definition of psnr anduprofilingu is missing does profiling simply mean finding a threshold for filt", "page -tfigure  is not very inform", "given the roc curves in figure   and table  it is redund", "page -tthe results in general indicate that the method is much better than ch", "but it is not clear if it is practical because the false alarm rates for high detection are quite high"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example on imagenet % of the innocent images are mistakenly rejected as malicious to get % detection r", "i do not think this working point is useful for a real appl", "-tgiven the high flares alarm rate it is surprising that experiments with multiple checkpoints are not presented specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the pap", "experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method", "-tthe experiments show that essentially the latent defenders are stronger than the input defender in most cas"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however an ablation study of the latent defender is missing specifcially it is not clear a how much does stage  model refinement with clusters  contribute to the accuracy how does the model do without it", "and  how important is the hdda and the specific variant used which is not clear important is it important to model the gaussians using a sub-spac", "of which dimens", "overallpros-t a nice idea with some novelty  based on a non-trivial observ", "-tthe experimental results how the idea holds some promis"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons-tthe method is not presented clearly enough the main component modeling the network activity is not explained the hdda module us", "-tthe results presented show that the method is probably not suitable for a practical application yet high false alarm rate for good detection r", "-texperimental results are partial results are not presented for multiple defenders no ablation experi", "after revisionsome of my comments were addressed and some were not", "specifically results were presented for multiple defenders and some ablation experiments were highlihgt"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["things not addressed - the risk analysis is still not relev", "the authors removed a clearly flawed sentence but the analysis still assumes that two densities of 'good' and 'bad' examples are modeled while in the work presented only one of them i", "hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear", "it would have been better to omit it and use the space to describe hdda and the specific variant used in this work as this is the main tool doing the distinct", "i believe the paper should be accept"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss funct", "the discussion is largely based on a sequence of experiments some of which are interesting and insight", "the discussion here can be useful for other research", "my main concern is that the result here is purely empirical with no concrete theoretical justif", "what the experiments reveal is an empirical correlation between the eigval index and training accuracy which can be caused by lots of reasons and cofounders and does not necessarily establish a causal rel"], "labels": ["GEN", "MAJ", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore i found many of the discussion to be question", "i would love to see more solid theoretical discussion to justify the hypothesis proposed in this pap", "do you have a sense how accurate is the estimation of the tail probabilities of the eigenvalu", "because the whole paper is based on the approximation of the eigval indexes it is critical to exam the estimation is accurate enough to draw the conclusions in the pap", "all the conclusions are based on one or two dataset"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["could you consider testing the result on more different datasets to verify if the results are generaliz", "the main goal of this paper is to learn a convnet classifier which performs better for classes in the tail of the class occurrence distribution ie for classes with relatively few annotated exampl", "in order to do so they constrain the final softmax layer using weights and biases based on the class means in a nearest-class-mean style lay", "in practice the class means are learned yet regularised towards the batch class mean", "my main concern with the paper is in the theoretical underpinning of the work"], "labels": ["MIN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["from the title a bayesian approach is suggested while in practice a rather standard softmax classifier is learned albeit with a different regulariser last layer is regularised towards batch class mean", "also the gaussian mixture model is not a true mixture model in the sense that normally gmms are used for describing a distribution of unlabelled data in this case each class is described with a gaussian and thus the class probabilities are the reseponsibilities proportional to the class gaussian", "to take this one further it is assumed that there is equal class probabilities and each class has a the same identity matrix as covariance matrix", "taking away a large part of the gaussian distribut", "the relation eq  with softmax is insightful yet already discussed in eg mensink et al  already cited for the nearest class mean classifi"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a second concern is the experimental explor", "first of all it is unclear if the method works much better for the tail than the standard softmax", "that is not apparent from the result", "for example fig  shows -except for cifar- not a clear relation between class index and proposed relative improvement it is also unclear if there is just a difficult class eg at index  or that the experiment has been repeated several tim", "moreover when the performance becomes more stable for the classes in the tail i'd have expected that the standard deviation of the mean class accuracy would decrease from the results there is no difference between softmax and the proposed method  +/- for softmax miniimagenet to  +/- for the proposed ncm approach"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the final experiment is the regularised version  compared to an unregularised one which shows that the first performs bett", "however i'm a little unsure about these conclusions what is the unregularized version exactly doing how is it different from a standard softmax", "remaining minor remarks- it is unclear how icarl has been used - it has been proposed as an iterative classification method", "- eq  how would this perform on a learned softmax representation preferably including the covariance and class prior", "- figure  gain - relative performance- the batch size must have a great influence on the functioning of the regularisation especially when there are many classes in that case just a single example counts for the class mean this is not explored in the pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythe paper deal with the problem of rl", "it proposes a non-parametric approach that maps trajectories to the optimal polici", "it avoids learning parameterized polici", "the fundamental idea is to store passed trajectories  when a policy is to be executed it does nearest neighbor search to find then closest trajectory and executes it", "commentswhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely differ"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["not certain about the claim that standard rl policy learning algorithms make it difficult to assess the difficulty of a problem", "how do you execute a trajectori", "actions in rl are by definition stochastic and this would make it unlikely that a same trajectory can be reproduced exactli", "his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free dna", "they key innovation is a convolutional architecture that represents the invariance around the target bas"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the method is validated on simulations as well as in cfdna and is shown to provide increased precision over competing method", "while the method is of interest there are more recent mutation callers that should be compar", "for example snooper which uses a randomforest  https//bmcgenomicsbiomedcentralcom/articles//s--- and hence would be of interest as another machine learning framework", "they also should compare to strelka which interestingly they included only to make final calls of mutations but not in the comparison", "further i  would also have liked to see the use of standard benchmark datasets for mutation calling  https//wwwnaturecom/articles/ncomm"], "labels": ["GEN", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it appears that the proposed method kittyhawk has a steep decrease in ppv and enrichment for low tumor fraction which are presumably the parameter of greatest interest the authors should explore this behavior in greater detail", "the game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and i appreciate the authors to initiate this kind of studi", "lemma  summarizes properties of the solutions that are expected to have after reaching equilibria", "important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave wrt to the target vari", "in case of deep learning the convexity is not guaranteed and the resulting solutions do not have necessarily follow lemma"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["unonetheless this type of analysis can be useful under appropriate solutions if non-trivial claims are derived however lemma  simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claim", "as long as the analysis is experimental the state of the art should be consid", "as long as the reviewer knows the cw attack gives the most powerful attack and this should be considered for comparison the results with mnist and cifar- are differ", "in some cases mnist is too easy to consider the complex structure of deep architectur", "i prefer to have discussions on experimental results with both dataset"], "labels": ["MAJ", "GEN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main takeaway from the entire paper is not clear very much", "it contains a game-theoretic framework of adversarial examples/training novel attack method and many experimental result", "minordefinition of g in the beginning of sec  seems to be a typo", "what is u this is revealed in the latter sections but should be specified her", "in section  this is in stark contrast with the near-perfect misclassification of the undefended classifier in t"], "labels": ["MAJ", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results shown in the table seems to indicate the ucperfectud misclassif", "sentence after eq  seems to contain a grammatical error", "the paragraph after eq  is duplicated with a paragraph introduced befor", "summary a method for creation of semantical adversary examples in suggest", "the usemanticu property is measured by building a latent space with mapping from this space to the observable generator and back invert"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the generator is trained with a wgan optim", "semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that spac", "the method is tested for generation of images on mnist and part of lsum data and for creation of text examples which are adversarial in some sense to inference and translation sent", "it is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspect", "page  it seems that the search algorithm has a additional parameter r_ the size of the area in which search is initi"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be explicitly said and the parameter value should be statedpage  -tthe implementation details of the generator critic and invertor networks are not given in enough details and instead the reader is referred to other pap", "this makes this paper non-clear as a stand alone document and is a problem for a paper which is mostly based on experiments and their results the main networks used are not describ", "-tthe visual examples are interesting but it seems that they are able to find good natural adversary examples only for a weak classifi", "in the mnist case the examples for thr random forest are nautral and surprising but those for the le-net are often not they often look as if they indeed belong to the other class the one pointed by the classifi", "in the churce-vs tower case a  relatively weak mlp classifier was us"], "labels": ["MIN", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be more instructive to see the results for a better convolutional classifi", "page -tthe description of the various networks used for text generation is insufficient for understandingotthe area is described in two sent", "it is not clear how this module is built was loss was it used to optimize in the first place and what elements of it are reused for the current taskot uinverteru here is used in a sense which is different than in previous sections of the paper earlier it denoted the mapping from output images to the underlying latent space here it denote  a mapping between two latent spac", "ot it is not clear what the ufour-layers strided cnnu is its structure its role in the system how is it optim", "otin general a block diagram showing the relation between all the systemus components may be useful plus the details about the structure and optimization of the various modul"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems that the system here contains  modules instead of the three used before critic generator and inverter but this is not clear enough", "also which modules are pre-trained which are optimized togethera nd which are optimized separately is not clearotsnli data should be described content size the task it is used for", "pro-ta novel idea of producing natural adversary examples with a gan", "-tthe generated examples are in some cases useful for interpretation and network understandin", "g -tthe method enables creation of adversarial examples for block box classifi"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons-tthe idea implementation is bas", "specifically search algorithm presented is quite simplistic and no variations other than plain local search were developed and test", "-tthe generated adversarial examples created for successful complex classifiers are often not impressive and useful they are either not semantical or semantical but correctly classified by the classifi", "hence it is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifi", "this paper analyses adversarial training and its effect on universal adversarial examples as well as standard basic iteration adversarial exampl"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it also analyses how adversarial training affects detect", "the robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem", "the paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect", "it's also unclear from which epoch the adversarial examples were generated from in figur", "further the transformations in figure  are limited to artificially controlled situations it would be much more interesting to see how the destruction rate changes under real-world test scenario"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surpris", "there is also no analysis of what happens for adversarial examples for the detector", "also it's not clear from section  what inputs are used to generate the adversarial exampl", "are they a random sample across the whole datasetfinally the paper spends significant time on describing maxmin and minmax and the graphical visualizations but the paper fails to show these graphical profiles for real model", "in conventional boosting methods one puts a weight on each sampl"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the wrongly classified samples get large weights such that in the next round those samples will be more likely to get right", "thus the learned weak learner at this round will make different mistak", "this idea however is difficult to be applied to deep learning with a large amount of data", "this paper instead designed a new boosting method which puts large weights on the category with large error in this round", "in other words samples in the same category will have the same weight error bound is deriv"], "labels": ["GEN", "GEN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments show its us", "though experiments are limit", "the paper suggests taking glove word vectors adjust them and then use a non-euclidean similarity function between them", "the idea is tested on very small data sets  and  examples respect", "the proposed techniques are a combination of previously published steps and the new algorithm fails to reach state-of-the-art on the tiny data set"], "labels": ["MAJ", "MIN", "GEN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it isn't clear what the authors are trying to prove nor whether they have successfully proven what they are trying to prov", "is the point that glove is a bad algorithm", "that these steps are gener", "if the latter then the experimental results are far weaker than what i would find convinc", "why not try on multiple different word embed"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what happens if you start with random vector", "what happens when you try a bigger data set or a more complex problem", "summarythe paper proposes a new dataset for reading comprehension called duorc", "the questions and answers in the duorc dataset are created from different versions of a movie plot narrating the same underlying stori", "the duorc dataset offers the following challenges compared to the existing reading comprehension rc datasets u  low lexical overlap between questions and their corresponding passages  requires use of common-sense knowledge to answer the question  requires reasoning across multiples sentences to answer the question  consists of those questions as well that cannot be answered from the given passag"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper experiments with two types of models u  a model which only predicts the span in a document and  a model which generates the answer after predicting the span", "both these models are built off of an existing model on squad u the bidirectional attention flow bidaf model", "the experimental results show that the span based model performs better than the model which generates the answ", "but the accuracy of both the models is significantly lower than that of their base model bidaf on squad demonstrating the difficulty of the duorc dataset", "tstrengthstthe data collection process is interest"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the challenges in the proposed dataset as outlined in the paper seem worth pushing for", "tthe paper is well written making it easy to follow", "tthe experiments and analysis presented in the paper are insight", "weaknessestit would be good if the paper can throw some more light on the comparison between the existing movieqa dataset and the proposed duorc dataset other than the s", "tthe dataset is motivated as consisting of four challenges described in the summary above that do not exist in the existing rc dataset"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challeng", "such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset", "tit is not clear from the paper how should the questions which are unanswerable be evalu", "as in what should be the ground-truth answer against which the answers should such questions be evalu", "clearly string matching would not work because a model could say ucdonut knowud whereas some other model could say ucunanswerableud"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so does the training data have a particular string as the ground truth answer for such questions so that a model can just be trained to spit out that particular string when it thinks it canut answer the quest", "tone of the observations made in the paper is that uctraining on one dataset and evaluating on the other results in a drop in the perform", "ud however in table  evaluating on paraphrase rc is better when trained on self rc as opposed to when trained on paraphrase rc", "this seems to be in conflict with the observation drawn in the pap", "could authors please clarify thi"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also could authors please throw some light on why this might be happen", "tin the third phase of data collection paraphrase rc was waiting for - weeks the only step taken in order to ensure that the workers for this stage are different from those in stage  or was something more sophisticated implemented which did not allow a worker who has worked in stage  to be able to participate in stag", "ttypo dataset section phrases -- phas", "overall the challenges proposed in the duorc dataset are interest", "the paper is well written and the experiments are interest"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are some questions as mentioned in the weaknesses section which need to be clarified before i can recommend acceptance for the pap", "paper is well written and clearly explain", "the paper is a experimental paper as it has more content on the experiment", "and less content on problem definition and formul", "the experimental section is strong and it has evaluated across different datasets and various scenario"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venu", "it only considers a slight modification into the loss function by adding a trace norm regular", "this paper proposes a variant of neural architecture search", "it uses established work on network morphisms as a basis for defining a search spac", "experiments search for effective cnn architectures for the cifar image classification task"], "labels": ["MAJ", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["positives the approach is straightforward to implement and trains networks in a reasonable amount of tim", "an advantage over prior work this approach integrates architectural evolution with the training procedur", "networks are incrementally grown child networks are initialized with learned parameters from their par", "this eliminates the need to restart training when making an architectural change and drastically speeds the search", "negatives the state-of-the-art cnn architectures are not mysterious or difficult to find despite the paper's characterization of them being so"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["indeed resnet and densenet designs are both guided by extremely simple principles stack a series of convolutional layers pool occasionally and use some form of skip-connection throughout", "the need for architectural search is unclear", "the proposed search space is bor", "as described in section  the possibly evolutionary changes are limited to deepening the network widening the network and adding a skip connect", "but these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already avail"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a consequence of  and  the result is essentially rig", "since only depth width and skip connections are considered the end network must end up looking like a resnet or densenet but with some connections prun", "there is no way to discover a network outside of the principled design space articulated in point  abov", "indeed the discovered network diagrams figures  and  fall in this spac", "performance is worse than the best hand-designed baselin"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one would hope that even if the search space is limited the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same spac", "however the results in tables  and  show this not to be the cas", "the best human designs outperform the evolved network", "moreover the evolved networks are woefully inefficient in terms of parameter count", "together these negatives imply the proposed approach is not yet at the point of being useful in practic"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think further work is required perhaps expanding the search space to resolve the current limitations of automated architecture search", "misctables  and  would be easier to parse if resources were simply reported in terms of total gpu hour", "= quality = overall the authors do a good job of placing their work in the context of related research and employ a variety of non-trivial technical details to get their methods to work wel", "= clarity = overall the exposition regarding the method is good", "i found the setup for the sequence tagging experiments confusing tough"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["see more comments below= originality / significance = the paper presents a clever idea that could help make spens more pract", "the paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compress", "= major comment =i'm concerned by the quality of your results and the overall setup of your experi", "in particular the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the pap", "most of your empirical success is obtained by taking a pretrained crf energy function and using this as a teacher model to train a feed-forward inference network you have have very few experiments using a spen energy function parametrization that doesn't correspond to a crf even though you could have used an arbitrary convnet rnn etc"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the one exception is when you use the tag language model this is a good idea but it is pretrained not trained using the saddle-point objective you introduc", "in fact you don't have any results demonstrating that the saddle-point approach is better than simpler altern", "it seems that you could have written a very different paper about model compression with crfs that would have been very interesting and you could've have used many of the same experiments it's unclear why spens are so import", "the idea of amortizing inference is perhaps more gener", "my recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with spens that demonstrate empirical gains that leverage their flexible deep-network-based energy funct"], "labels": ["GEN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["= minor comments = * you should mention 'energy based gan", "* i don't understand", "this approach performs backpropagation through each step of gradient descent permitting more stable training but also evidently more overfit", "why would it overfit more simply because training was more stable couldn't you prevent overfitting by regularizing more*", "you spend too much space talking about specific hyperparameter ranges etc"], "labels": ["MIN", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should be moved to the appendix you should also add a short summary of the tlm architecture to the main paper bodi", "n* regarding your footnote discussing using a positive vs negative sign on the entropy regularization term i recommend checking out regularizing neural networks by penalizing confident output distribut", "* you should add citations for the statement in these and related settings gradient descent has started to be replaced by inference network", "* i didn't find table  particularly illumin", "all of the approaches seem to perform about the sam"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what conclusions should i make from it* why not use kl divergence as your delta function* why are the results in table  on the dev data* i was confused by t", "first of all it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained crf energy and amortizing inference by training an inference network", "this idea of training with a standard loss conditional log lik and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the pap", "second i was very surprised that the inference network outperformed viterbi  vs  for the same crf energi", "why is this* i'm confused by the difference between table  and table  why not just include the tlm results in t"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this contribution the authors propose an improvement of a tensor decomposition method for decoding spike train", "relying on a non-negative matrix factorization the authors tackle the influence of the baseline activity on the decomposit", "the main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coeffici", "an experimental validation shows that for high frequency baseline   hz the baseline corrected algorithm yields better classification results than non-corrected version and other common factorization techniqu", "the objective function is defined with a frobenius norm which has an important influence on the obtained solutions as it could be seen on figur"], "labels": ["GEN", "GEN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed method seems to provide a more discriminant factorization than the nmf one at the expense of the sparsity of spatial and temporal components impeding the biological interpret", "a possible solution is to add a regularization term to the objective function to ensure the sparsity of the factor", "this contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach the embedding vectors generated from cell images show spurious correl", "the authors define a wasserstein distance network to find  a suitable affine transformation that reduces the nuisance factor", "the evaluation on a real dataset yields correct results this approach is quite general and could be applied to different problem"], "labels": ["MAJ", "MIN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the contribution of this approach could be better highlight", "the early stopping criteria tend to favor suboptimal solution indeed relying on the cramer distance is possible improv", "as a side note the k-nn moa is central to for the evaluation of the proposed approach", "a possible improvement is to try other means for the embedding instead of the euclidean on", "this paper studies the impact of angle bias on learning deep neural networks where angle bias is defined to be the expected value of the inner product of a random vectors eg an activation vector and a given vector eg a weight vector"], "labels": ["MIN", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero", "this suggests that the some of the units in a deep neural network have large values either positive or negative regardless of the input which in turn suggests vanishing gradi", "the proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero", "although this does not rule out angle bias in general it does so for the very special case where the expected value of the random vector is a vector consisting of a common valu", "nevertheless numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the cifar- task"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["test accuracy is not improved howev", "overall this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks but the results are rather preliminary both on theory and experi", "on the theoretical side the linearly constrained weights are only shown to work for a very special cas", "there can be many other approaches to mitigate the impact of angle bia", "for example how about scaling each variable in a way that the mean becomes zero instead of scaling it into [-+] as is done in the experi"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when the mean of input is zero there is no angle bias in the first lay", "also what about if we include the bias term so that b + w a is the preactivation valu", "on the experimental side it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error but the test error is unfortunately increased for the particular task with the particular dataset in the experi", "it would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselin", "it is intuitively expected that the proposed approach has some merit in some domains but it is unclear exactly when and where it i"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor commentsin section  is layer  the input layer or the next", "in this work the objective is to analyze the robustness of a neural network to any sort of attack", "this is measured by naturally linking the robustness of the network to the local lipschitz properties of the network funct", "this approach is quite standard in learning theory i am not aware of how original this point of view is within the deep learning commun", "this is estimated by obtaining values of the norm of the gradient also naturally linked to the lipschitz properties of the function by backpropagation this is again a natural idea"], "labels": ["MIN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes to combine reinforcement learning with supervised learning to speed up learn", "unlike their claim in the paper the idea of combining supervised and rl is not new", "a good example of this is a supervised actor-critic by barto", "i think even alphago uses some form of supervis", "however if i understand correctly it seems that combining supervision of rl at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel"], "labels": ["GEN", "MAJ", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["having the luxury of some supervised episodes is of course us", "the first step of building a supervised initial model looks straight forward", "the next step of the algorithm is less easy to follow and presentation of the ideas could be much bett", "this part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization doesnut this prevent exploration what are the network parameters eg size of layers etc", "i am not sure i could redo the work from the provided inform"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["overall it is unclear to me what the advantage of the algorithm is over pure supervised learning and i donut think a compelling case has been mad", "since the influence of the supervision is increased by increasing alpha it can be expected that results should be better for increasing alpha", "the results seem to indicate that an intermediate level of alpha is best though i would even question the statistical significance by looking at the curves in figur", "also what is the epoch number and why is this  for alpha=", "if the combination of supervised learning with rl is better than this should be clearly st"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some argument is made that pure supervision is overfitting but would one then not simply add some other regular", "the presentation could also be improved with some language edit", "several articles are wrongly placed and even some meaning is unclear", "for example the phrase uccontinuous input sequenceud does not make sense maybe you mean ucinput sequence of real valued quantitiesud", "in summary while the paper contains some good ideas i certainly think it needs more work to make a clear case for this method"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors define a novel method for creating a pair of models a student and a teacher model that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to peopl", "they do this by adapting a technique from computational cognitive science called rational pedagogi", "rather than jointly optimize the student and teacher as done previously they have form a coupled relation between the student and teacher where each is providing a best response to the oth", "the authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teach", "understanding how to make complex models interpretable is an extremely important problem in ml for a number of reasons eg ai ethics explainable ai"], "labels": ["MAJ", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach proposed by the authors is an excellent first step in this direction and they provide a convincing argument for why a previous approach joint optimization did not work", "it is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable exampl", "they second part of their article where they test the examples created by their models using behavioral experiments was less convinc", "this is because they used the wrong statistical tests for analyzing the studies and it is unclear whether their results would stand with proper tests i hope they will! u it seems clear that random samples will be harder to learn from eventually but i also hoped there was a stronger baselin", "for analysis the authors use t-tests directly on kl-divergence and accuracy scores however this is inappropriate see jaeger  categorical data analysis away from anovas transformation or not and towards logit mixed model"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["journal of memory and language  -", "this is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced abov", "with respect to kl-divergence a g-test can be used see https//enwikipediaorg/wiki/g-test#relation_to_kullbackeleibler_diverg", "i suspect the results will still be meaningful but the appropriate analysis is essential to be able to interpret the human result", "also a related article one article testing rational pedagogy in more ml contexts and using it to train ml models that isho m k littman m macglashan j cushman f & austerweil j l nip"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["showing versus doing teaching by demonstr", "for future work it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks which motivated the current push for explainable ai in the first plac", "the paper presents a clever trick for updating the actor in an actor-critic setting computing a guide actor that diverges from the actor to improve critic value then updating the actor parameters towards the guide actor", "this can be done since when the parametrized actor is gaussian and the critic value can be well-approximated as quadratic in the action the guide actor can be optimized in closed form", "the paper is mostly clear and well-pres"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["except for two issues  there is virtually nothing novel presented in the first half of the paper before sect", "and  the actual learning step is only presented on page  making it hard to understand the motivation behind the guide actor until very late through the paperthe presented method itself seems to be an important contribution even if the results are not overwhelmingly posit", "it'd be interesting to see a more elaborate analysis of why it works well in some domains but not in oth", "more trials are also needed to alleviate any suspicion of lucky tri", "there are some other issues with the presentation of the method but these don't affect the merit of the method"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["returns are defined from an initial distribution that is stationary for the polici", "while this makes sense in well-mixing domains the experiment domains are not well-mixing for most policies during training for example a fallen humanoid will not get up on its own and must be reset", "the definition of betaa|s as a mixture of past actors is inconsistent with the sampling method which seems to be a mixture of past trajectori", "in the first paragraph of section  [] the quality of a guide actor mostly depends on the accuracy of taylor's approxim", "what else does it depend on"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then [] the action a_ should be in a local vicinity of a", "and [] the action a_ should be similar to actions sampled from pi_thetaa|s what do you mean should", "in order for the taylor approximation to be good", "the line before  is confusing since  is exact and not an approxim", "for the approximation  it isn't clear if this is a good approxim"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why/when is the nd term in  smal", "the parametrization nu of hat{q} is never specified in sect", "this is important in order to evaluate the complexities involved in computing its hessian", "the authors show how the regularization procedure called batch normalizationcurrently being used by most deep learning systems can be understood asperforming approximate bayesian infer", "the authors compare this approach tomonte carlo dropout another regularization technique which can also beconsidered to perform approximate bayesian infer"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimentsperformed show that the bayesian view of batch normalization performs similarlyas mc dropout in terms of the estimates of uncertainty that it produc", "qualityi found the quality to be low in some aspect", "first the description of whatis the prior used by batch normalization in section  is unsatisfactori", "theauthors basically refer to appendix  for the case in which the weight decaypenalty is not zero", "the details in that appendix are almost none they justsay it is thus possible to derive the prior"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results in table  are a bit confus", "the authors should highlight inbold face the results of the best performing method", "the authors indicate that they do not need to compare to variational methodsbecause gal and ghahramani  compare already to those method", "however galand ghahramani's code used bayesian optimization methods to tunehyper-parameters and this code contains a bug that optimizes hyper-parametersby maximizing performance on the test data", "in particular for hyperparameterselection they average performance across subsets of  of the training setsfrom the x train/test split and then using the tau which got the bestaverage performance for all of x train/test splits to evaluate performancehttps//githubcom/yaringal/dropoutuncertaintyexps/blob/master/bostonhousing/net/experiment_bopy#l"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore the claim that since we have established that mcbn performs on par with mcdo by proxy wemight conclude that mcbn outperforms those vi methods as wellis not valid", "at the beginning of section  the authors indicate that they follow in theirexperiments the setup of gal and ghahramani", "however gal and ghahramani actually follow hernuendez-lobato and adams  so the correctreference should be the latter on", "claritythe paper is clearly written and easy to follow and understand", "i found confusing how to use the proposed method to obtain estimates ofuncertainty for a particular test data point x_star"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paragraph just abovesection  says that the authors sample a batch of training data for this butassume that the test point x_star has to be included in this batch", "how is this actually done in practic", "originalitythe proposed contribution is origin", "this is the first time that a bayesianinterpretation has been given to the batch normalization regularizationpropos", "significancethe paper's contributions are signific"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["batch normalization is a verypopular regularization technique and showing that it can be used to obtainestimates of uncertainty is relevant and signific", "many existing deeplearning systems can use this to produce estimates of uncertainty in theirpredict", "the paper discusses dropping out the pre-softmax logits in an adaptive mann", "this isn't a huge conceptual leap given previous work for instance that of ba and frey  or the sequence of papers by gal and his coauthors on variational interprations of dropout", "in the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational infer"], "labels": ["MAJ", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the variational approximation is a bit odd in that it doesn't have any variational parameters and indeed a further regulariser in equation  is needed to give the desired behaviour", "a fairly small but consistent improvement on the base model and other similar ideas is reported in t", "i would have liked to have seen results on imagenet", "i don't find the too small figure  to be compelling evidence that our dropmax effectively preventsoverfiting by converging to much lower test loss", "the test loss in question looks like a noisy version of the base test loss with a slightly lower mean"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stag", "figure  illustrates the idea nic", "which of the mnist models from table  was us", "in the medical context this paper describes the classic problem of knowledge base completion from structured data only no text", "the authors argue for the advantages of a generative vae approach but without being convinc"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they do not cite the extensive literature on kb complet", "they present experimental results on their own data set evaluating only against simpler baselines of their own vae approach not the pre-existing kb method", "the authors seem unaware of a large literature on knowledge base completion  eg [bordes weston collobert bengio aaai ]  [socher et al  nips] [wang wang guo  ijcai] [gardner mitchell  emnlp] [lin liu sun liu zhu aaai ] [neelakantan roth mccallum ]", "the paper claims that operating on pre-structured data only without using text is an advantag", "i don't find the argument convinc"], "labels": ["MIN", "MAJ", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are many methods that can operate on pre-structured data only but also have the ability to incorporate text data when available eg universal schema [riedel et al ]", "the paper claims that discriminative approaches need to iterate over all possible entity pairs to make predict", "in their generative approach they say they find outputs by nearest neighbor search", "but the same efficient search is possible in many of the classic discriminatively-trained kb completion models also", "it is admirable that the authors use an interesting and to my knowledge novel data set"], "labels": ["GEN", "GEN", "GEN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the method should also be evaluated on multiple now-standard data sets such as fbk- or nell-", "the method is evaluated only against their own vae-based altern", "it should be evaluated against multiple other standard kb completion methods from the literature such as jason weston's trans-e richard socher's tensor neural nets and neelakantan's rnn", "this paper presents methods to reduce the variance of policy gradient using an action dependent baselin", "such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the st"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper shows that using separate baselines for actions each of which can depend on the state and other actions is bias-fre", "derive the optimal action-dependent baseline showing that it does not degenerate into state-only dependent baseline ie there is potentially room for improvement over state-only baselin", "suggests using marginalized action-value q function as a practical baseline generalizing the use of value function in state-only baseline cas", "suggests using mc marginalization and also using the average action to improve computational feas", "combines the method with gae techniques to further improve convergence by trading off bias and vari"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the suggested methods are empirically evaluated on a number of set", "overall action-dependent baseline outperform state-only vers", "using a single average action marginalization is on par with mc sampling which the authors attribute to the low quality of the q estim", "combining gae shows that a hint of bias can be traded off with further variance reduction to further improve the perform", "i find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action spac"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in light of such results one might change the policy space to enforce such structur", "notes- elaborate further on the assumption made in eqn", "does it mean that the actions factors cannot share too many parameters in the policy construction or that shared parameters can only be applied to the st", "- eqn  should use simeq", "- how can the notion of average be extended to handle multi-modal distributions or categorical or structural act"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["consider expanding on that in sect", "- the discussion on the dag graphical model is lacking experimental analysis where separate baselines models are need", "how would you train such baselin", "- figure  is impossible to read in print", "the fonts are too small for the numbers and the legend"], "labels": ["MIN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples for few-shot classification or action-reward pairs for reinforcement learning in order to take the appropriate act", "the resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning task", "strengths- the proposed model is a generic meta-learning useful for both classification and reinforcement learn", "- a wide range of experiments are conducted to demonstrate performance of the proposed method", "weaknesses- design choices made for the reinforcement learning setup eg temporal convolutions are not necessarily applicable to few-shot classif"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- discussion of results relative to baselines is somewhat lack", "the proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining effici", "the depth of the tc block is determined by the sequence length", "in few-shot classification the sequence length can be known a prior", "how is the sequence length determined for reinforcement learning task"], "labels": ["MIN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in addition what is done at test-time if the sequence length differs from the sequence length at training tim", "the causality assumption does not seem to apply to the few-shot classification cas", "have the authors considered lifting this restriction for classification and if so does performance improv", "the prototypical networks results in tables  and  do not appear to match the performance reported in snell et ", "the paper is well-written overal"], "labels": ["GEN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some additional discussion of the results would be appreciated for example explaining why the proposed method achieves similar performance to the lstm/opsrl baselin", "i am not following the assertion in  that maml adaption curves can be seen as an upper bound on the performance of gradient-based method", "i am wondering if the authors can clarify this point", "overall the proposed approach is novel and achieves good results on a range of task", "edit i have read the author's comments and am satisfied with their response i believe the paper is suitable for publication in iclr"], "labels": ["MIN", "GEN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper examines a distributed deep rl system in which experiences rather than gradients are shared between the parallel workers and the centralized learn", "the experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the work", "using this system the authors are able to harness much more compute to learn very high quality policies in little tim", "the results very convincingly show that ape-x far outperforms competing algorithms such as recently published rainbow", "itus hard to take issue with a paper that has such overwhelmingly convincing experimental result"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however there are a couple additional experiments that would be quite niceutin order to understand the best way for training a distributed rl agent it would be nice to see a side-by-side comparison of systems for distributed gradient sharing eg gorila versus experience sharing eg ape-x", "utit would be interesting to get a sense of how ape-x performs as a function of the number of frames it has seen rather than just wall-clock tim", "for example in table  is ape-x at m frames doing better than rainbow at m fram", "prosutwell written and clear", "utvery impressive result"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["utitus remarkable that ape-x preforms as well as it does given the simplicity of the algorithm", "consuthard to replicate experiments without the deep computational pockets of deepmind", "summary the paper focuses on the characterization of the landscape of deep neural networks ie when and why local minima are global what are the conditions for saddle critical points etc", "the paper covers a somewhat wide range of deep nets from shallow with linear activation to deeper with non-linear activation it focuces only on feed forward neural network", "as the authors state this paper provides a unifying perspective to the subject it justifies the results of others through this unifying theory but also provides new results eg there are results that do not depend on assumptions on the target data matrix i"], "labels": ["MAJ", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["originality the paper provides similar results to previous work while removing some of the assumptions made in previous work", "in that sense the originality of the results is weak but definitely there is some novelty in the methodology used to get to these result", "thus i would say origin", "importance the paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical point", "while there are no direct connections with generalization properties characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning it will attract some attention at the confer"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["clarity the paper is well-written - some parts need improvement but overall i'm satisfied with the current vers", "comments if problem  is not considered at all in this paper in its full generality that considers matrix completion and matrix sensing as special cases then the authors could just start with the model in", "remark  has a nice example - could this example be shown with y not being the all-zeros vector", "in section  the authors make a connection with the work of ge et ", "they state that the problems in - constitute generalizations of the symmetric matrix completion case considered in ge et "], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however in that work the main difficulty of proving global optimality comes from the randomness of the sampling mask operator which introduces the notion of incoherence and requires results in expect", "it is not clear and maybe it is an overstatement that the results in section  generalize that work", "if that is the case could the authors describe this a bit furth", "this paper proposes to use graph neural networks for the purpose of few-shot learning as well as semi-supervised learning and active learn", "the paper first relies on convolutional neural networks to extract image featur"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then these image features are organized in a fully connected graph", "then this graph is processed with an graph neural network framework that relies on modelling the differences between features maps propto phiabsx_i-x_j", "for few-shot classification then the cross-entropy classification loss is used on the nod", "the paper has some interesting contributions and ideas mainly from the point of view of applications since the basic components convnets graph neural networks are roughly similar to what is already propos", "however the novelty is hurt by the lack of clarity with respect to the model design"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["first as explained in  a fully connected graph is used although in fig  the graph nodes do not have connections to all other nod", "if all nodes are connected to all nodes what is the different of this model from a fully connected multi-stream networks composed of s^ branch", "to rephrase what is the benefit of having a graph structure when all nodes are connected with all nod", "besides what is the effect when having more and more support imag", "is the generalization hurt"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["second it is not clear whether the label used as input in eq  is a model choice or a model requir", "the reason is that the label already appears in the loss of the nodes  in  isn't using the label also as input redund", "third the paper is rather vague or imprecise at point", "in eq  many of the notations remain rather unclear until later in the text and even then they are not entirely clear", "for instance what is s r t"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experimental section is also ok although not perfect", "the proposed method appears to have a modest improvement for few-shot learn", "however in the case of  active learning and semi-supervised learning the method is not compared to any baselines other than the random one which makes conclusions hard to reach", "in general i tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications requir", "in this very good paper the objective is to perform robust learning to minimize not only the risk under some distribution p_ but also against the worst case distribution in a ball around p_"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nsince the min-max problem is intractable in gener", "what is actually studied here is a relaxation of the problem it is possible to give a non-convex dual formulation of the problem", "if the duality parameter is large enough the functions become convex given that the initial losses are smooth", "what follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distribut", "experiments show that this performs as expected and gives a good intuition for the reasons why this occurs separation lines are 'pushed away' from samples and a margin seems to be increased with this procedur"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main contribution of the paper is a method that provides visual explanations of classification decis", "the proposed method uses  - a generator trained in a gan setup - an autoencoder to obtain a latent space representation - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image or rather the reconstruction of it", "the method is evaluated on a medical images dataset and some additional demonstration on mnist is provid", "- the paper proposes a i believe novel method to obtain visual explan", "the results are visually compel"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although most results are shown on a medical dataset - which i feel is very hard for most readers to follow", "the mnist explanations help a lot", "it would be great if the authors could come up with an additional way to demonstrate their method to the non-medical read", "- the paper shows that the results are plausible using a neat trick", "the authors train their system with the testdata included which leads to very different visu"], "labels": ["MIN", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it would be great if this analysis could be performed for mnist as wel", "from the related work it would be nice to mention that generative models px|c also often allow for explaining their decisions eg the work by lake and tenenbaum on probabilistic program inductionalso there is the work by hendricks et al on generating visual explan", "this should probably also be referenc", "minor comments - some figures with just two parts are labeled from left to right - it would be better to just write left  right", "n- figure  do these images correspond to each oth"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if yes it would be good to show them pairwise- figure  please explain why the saliency map is relev", "this looks very noisy and non-interest", "the paper proposes a method for generating images from attribut", "the core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired sampl", "and additionally learn individual inference networks from images or attributes to the latent space using unpaired sampl"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["during training the auto-encoder is trained on paired data image attribute whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa", "the authors propose handling missing data using a product of experts where the product is taken over available attributes and it sharpens the prior distribut", "the authors evaluate their method using correctness ie if the generated images have the desired attributes coverage i", "if the generated images sample unspecified attributes well and compositionality ie if  images can be generated from unseen attribut", "although the proposed method performs slightly poor compared to jmvae in terms of concreteness when all attributes are provid"], "labels": ["GEN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it outperforms when some of the attributes are missing figure a", "it also outperforms existing methods in terms of coverage and composition", "major commentsthe paper is well written and summarizes its contribution succinctli", "i did not fully understand the 'retrofitting' idea", "if i understood correctly the authors first train theta and phi and then fix theta to train phi_x and phi_i"], "labels": ["MIN", "GEN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if that is true then is calltheta phi phi_x phi_y are right cost function since one does not maximize all three elbo terms when optimizing theta please clarifi", "minor comments- 'in order of increasing abstraction' does the order of gender- smiling or not - hair color matter or is male * blackhair a valid option-", "what are the image sizes for the celeba dataset- page  double th", "- which multi-label classifier is used to classify images in attribut", "this paper proposes a tensor train decomposition with a ring structure for function approximation and data compress"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["most of the techniques used are well-known in the tensor community outside of machine learn", "the main contribution of the paper is the introduce such techniques to the ml community and presents experimental results for support", "the paper is rather preliminary in its examin", "for example it is claimed that the proposed decomposition provides enhanced representation ability but this is not justified rigorously either via more comprehensive experimentation or via a theoretical justif", "furthermore the paper lacks in novelty aspect as it is uses mostly well-known techniqu"], "labels": ["GEN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning evolutionary computation and reinforcement learning in a novel way", "it combines the following ingredientsa a population-based setup for rlb a pair-selection and crossover operatorc a policy-gradient based ucmutationud operatord filtering data by high-reward trajectoriese two-stage policy distil", "in its current shape it has a couple of major flaws but those can be fixed during the revision/rebuttal period", "related work it is presented in a somewhat ahistoric fashion", "in fact ideas for evolutionary methods applied to rl tasks have been widely studied and there is an entire research field called ucneuroevolutionud that specifically looks into which mutation and crossover operators work well for neural network"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["ium listing a small selection of relevant papers below but iud encourage the authors to read a bit more broadly and relate their work to the myriad of related older method", "ideally a more reasonable form of parameter-crossover see references could be compared to -- the naive one is too much of a straw man in my opinion", "to clarify i think the proposed method is genuinely novel but a bit of context would help the reader understand which aspects are and which aspects arenut", "ablations the proposed method has multiple ingredients and some of these could be beneficial in isolation for example a population of size  with an interleaved distillation phase where only the high-reward trajectories are preserved could be a good algorithm on its own", "or conversely gpo without high-reward filtering during crossov"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["or a simpler genetic algorithm that just preserves the kills off the worst members of the population and replaces them by mutated clones of better ones etc", "reproducibility there are a lot of details missing the setup is quite complex but only partially describ", "examples of missing details are how are the high-reward trajectories filt", "what is the total computation time of the different variants and baselin", "the x-axis on plots does it include the data required for crossover/dagg"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["what are do the shaded regions on plots ind", "the loss on pi_s should be made explicit", "an open-source release would be id", "minor points- naively the selection algorithm might not scale well with the population size exhaustively comparing all pairs maybe discuss that", "- the filtering of high-reward trajectories is what estimation of distribution algorithms [] do as well and they have a known failure mode of premature convergence because diversity/variance shrinks too fast"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["did you investigate thi", "n- for figure a it would be clearer to normalize such that  is the best and  is the random policy instead of  being scor", "- the language at the end of section  is very vague and noncommittal -- maybe just state what you did and separately give future work suggest", "- there are multiple distinct metrics that could be used on the x-axis of plots namely wallclock time sample complexity number of upd", "i suspect that the results will look different when plotted in different ways and would enjoy some extra plots in the appendix"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the ordering in figure  would be inverted if plotting as a function of sample complex", "- the ac results are much worse presumably because batchsizes are differ", "so ium not sure how to interpret them should they have been run for long", "maybe they could be relegated to the appendix", "references[] gomez f j & miikkulainen r  solving non-markovian control tasks with neuroevolut"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["[] larranaga p  a review on estimation of distribution algorithm", "[] stanley k o & miikkulainen r  evolving neural networks through augmenting topolog", "[] igel c  neuroevolution for reinforcement learning using evolution strategi", "[] hausknecht m lehman j miikkulainen r & stone p  a neuroevolution approach to general atari game play", "[] gomez f schmidhuber j & miikkulainen r  efficient nonlinear control through neuroevolut"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pros- results- novelty of idea- crossover visualization analysis- scal", "cons- missing background- missing ablations- missing detail", "[after rebuttal revised the score from  to ]", "this paper proposes to fine-tune the last layer while keeping the others fixed after initial end-to-end training viewing the last layer learning under the light of kernel theory well actually it's just a linear model", "summary of evaluationthere is not much novelty in this idea of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step which dates back at least a decad"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["so the only real contribution would be in the experi", "however the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method", "more detailsprevious work on the same idea at least a decade old eg huang and lecun  see a review of such work in 'deep learning using linear support vector machines' more rec", "experimentsyou should also have a weight norm penalty in the end-to-end 'regular training' case and make sure it is appropriately and separately tuned not necessarily the same value as for the post-train", "otherwise the 'improvements' may simply be due to better regularization in one case vs the other and the experimental curves suggest that interpretation is correct"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["summarythis paper proposes generative models for point cloud", "first they train an auto-encoder for d point clouds  somewhat similar to pointnet by qi et ", "then they train generative models over the auto-encoder's latent space both using a latent-space gan l-gan that outputs latent codes and a gaussian mixture model", "to generate point clouds they sample a latent code and pass it to the decod", "they also introduce a raw point cloud gan r-gan that instead of generating a latent code directly produces a point cloud"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nthey evaluate the methods on several metr", "first they show that the autoencoder's latent space is a good representation for classification problems using the modelnet dataset", "second they evaluate the generative model on several metrics such as jensen-shannon divergence and study the benefits and drawbacks of these metrics and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over chamfer dist", "methods such as the r-gan score well on the latter by over-representing parts of an object that are likely to be fil", "pros- it is interesting that the latent space models are most successful including the relatively simple gmm-based model"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["is there a reason that these models have not been as successful in other domain", "n- the comparison of the evaluation metrics could be useful for future work on evaluating point cloud gan", "due to the simplicity of the method this paper could be a useful baseline for future work", "- the part-editing and shape analogies results are interesting and it would be nice to see these expanded in the main pap", "cons- how does a model that simply memorizes and randomly samples the training set compare to the auto-encoder-based models on the proposed metrics how does the diversity of these two models diff"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- the paper simultaneously proposes methods for generating point clouds and for evaluating them", "the paper could therefore be improved by expanding the section comparing to prior voxel-based d methods particularly in terms of the diversity of the output", "although the performance on automated metrics is encourag", "it is hard to conclude much about under what circumstances one representation or model is better than anoth", "- the technical approach is not particularly novel"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the auto-encoder performs fairly wel", "but it is just a series of mlp layers that output a nx matrix representing the point cloud trained to optimize emd or chamfer dist", "the most successful generative models are based on sampling values in the auto-encoder's latent space using simple models a two-layer mlp or a gmm", "- while it is interesting that the latent space models seem to outperform the r-gan this may be due to the relatively poor performance of r-gan than to good performance of the latent space models and directly training a gan on point clouds remains an important problem", "n- the paper could possibly be clearer by integrating more of the background section into later sect"], "labels": ["MAJ", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some of the gan figures could also benefit from having capt", "overall i think that this paper could serve as a useful baseline for generating point cloud", "but i am not sure that the contribution is significant enough for accept", "this is an emergency review as the replacement of an overdue review", "------------------------------------------------------------------------this paper proposes three variants of the exponential linear unit elu by adding a learnable shift variable for each hidden unit"], "labels": ["MIN", "MAJ", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this modification to elu is motivated by the claimed observation that a learned piecewise linear activation function appears to have the elu shape despite a bias factor", "however the motivation above is not justified wel", "no theoretic results are present to support this design", "figure  shows the only experimental results to ucsupportud the motiv", "however it is a bit weird that  % tuned results are not shown and  the learned activation goes up as the input goes negative which is not the shape of elu"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as a result the motivation does not seem clear", "the shift variables seem only useful when they are not shared for different pixel", "otherwise the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization if us", "the question is if it is worth adding so many pixel-wise paramet", "moreover the proposed formulation does not seem useful for the fully connected layer at any tim"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the experiments are limit", "only the basic lenet and another network are considered on cifar-", "the results are not as good as the state-of-the-art", "more importantly the proposed activation functions reduce the errors only a bit %", "stronger results on more datasets are necessary to justify the usefulness of the proposed method"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a simple modification to the dqn algorithm they call episodic backward upd", "the algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new reward", "this issue of fast propagation of updates is a common theme in rl cf eligibility traces prioritised sweeping and more recently dqn with prioritised replay etc", "here the proposed update applies the max bellman operator recursively on a trajectory unsure whether this is novel with some decay to prevent accumulating errors with the nested max", "the paper is written in a clear way"], "labels": ["GEN", "GEN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed approach seems reasonable but i would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that td-errors would at first be higher towards the end of an episode and progress backwards from ther", "i think this should have been one of the baselines to compare to for that reason", "the experimental results seem promising in the illustrative mnist domain", "atari results seem decent especially given that experiments are limited to m frames though the advantage compared to the related approach of optimality tightening is not obvi", "this paper explores the idea of using policy gradients to learn a stochastic policy on complex control problem"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the central idea is to frame learning in terms of a new kind of q-value that attempts to smooth out q-values by framing them in terms of expectations over gaussian polici", "to be honest i didn't really get this paper*", "as far i understand all of the original work policy gradients involved stochastic polici", "many are/were gaussian*", "all q-value estimators are designed to marginalize out the randomness in these stochastic policies*"], "labels": ["GEN", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as far as i can tell this is equivalent to a slightly different formulation where the agent emits a deterministic action musigma and the environment samples an action from that distribut", "in other words it seems that if we just draw the box a bit differently the environment soaks up the nondeterminism instead of needing to define a new type of q-valu", "ultimately i couldn't discern /why/ this was a significant advance for rl or even a meaningful new perspective on classic idea", "i thought the little -mode mog was a nice example of the premise of the model", "while i may or may not have understood the core technical contribution i think the experiments can be critiqued they didn't really seem to work out"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figures & are unconvincing - the differences do not appear to be statistically signific", "also i was disappointed to see that the authors only compared to ddpg they could have at least compared to trpo which they ment", "they dismiss it by saying that it takes  times as long but gets a better answer - to which i respond very well run your algorithm x longer and see where you end up!", "i think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be publish", "the idea of penalizing a policy based on kl-divergence from a reference policy was explored at length by bert kappen's work on kl-mdps  perhaps you should cite that"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper presents two methods for imposing a margin on discriminative loss functions one which uses the margin between the reference transcription and alternatively hypothesized transcriptions lmlm and another which compares all alternative candidates and uses a margin between those with a better system objective wer or bleu and those with a worse system objective rank-lmlm", "some interesting results on the development set show the importance of things like warm starting on large language model training data", "the methods presented here could be of interest to those training language models for use in specific systems and the paper reads reasonably clearli", "the principal shortcoming of the paper is that there was essentially no effort to establish that the baseline systems that are being improved through reranking via these methods are decent baselines for such a use or to really specify these systems in a way that would allow for replication of the results being presented in the pap", "sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselin"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["yet such information is sorely lacking in this pap", "further the speech data sets fisher and wall st journal have what would seem to be very high word error rates versus what should be possible with standard open-source speech recognizers such as kaldi", "for example by referencing a page that attempts to establish the state of the art on standard data sets https//githubcom/syhw/wer_are_we we can find links to papers by povey et al http//wwwdanielpoveycom/files/_interspeech_mmipdf and the deep  paper in your citations which themselves include baselines from other papers that cut the error rate in half versus even your best scoring systems let alone your baselin", "similarly your bleu score on vietnamese to english translation is way below what were reported even by the organizer baseline for the iwslp conference where the data became available https//githubcom/magizbox/underthesea/wiki/sota-machine-translation-iwslt-granted the competing systems also were outperformed by the organizer baseline for that task at iwslt  but not by the degree to which your system i", "again your best performing system using your new methods has performance far below the worst reported competing system"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the cavalier presentation of specific details regarding your baseline systems which is critical for any sort of replicability and the uniformly weak performance of these systems relative to widely reported results leads me to discount the probability that your methods would actually result in improvements on truly solid baselin", "i would have preferred one domain experiment carried out with appropriately rock solid documentation of the ball-park competitive baseline system to these result", "overall the method is interesting and the dev set experiments were inform", "but ultimately the experiments were not", "revision  having read the author response for this paper i am encouraged by the updated baseline for wsj and the additional explication about the competing fisher system"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the additional information about the systems included in section  is pretty nominal though and i worry about the ability of others to replicate these result", "i would have felt better about the results if there were reported results from other papers included here instead of the authors' attempt to create a baseline from the given data which may or may not as we have seen represent a strong enough baseline from which to draw conclus", "that is to say that i still have some reservations though less than i had befor", "i am including this additional information in my review for use by the area chair but otherwise leaving my assessment as-i", "this paper proposes a new method called vcl for continual learn"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this method is a combination of the online variational inference for streaming environment with monte carlo method", "the authors further propose to maintain a coreset which consists of representative data points from the past task", "such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learn", "extensive experiments shows that vcl performs very well compared with some state-of-the-art method", "the authors present two ideas for continual learning in this paper  combination of online variational inference and sampling method  use of coreset to deal with the catastrophic forgetting problem"], "labels": ["GEN", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["both ideas have been investigated in bayesian literature while  has been recently investigated in continual learn", "therefore the authors seems to be the first to investigate the effectiveness of  for continual learn", "from extensive experiments the authors find that the first idea results in vcl which can outperform other state-of-the-art approaches while the second idea plays little rol", "the finding of the effectiveness of idea  seems to be signific", "the authors did a good job when providing a clear presentation a detailed analysis about related work an employment to deep discriminative models and deep generative models and a thorough investigation of empirical perform"], "labels": ["MAJ", "MAJ", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are some concerns the authors should consider- since the coreset plays little role in the superior performance of vcl it might be better if the authors rephrase the title of the pap", "when the coreset is empty vcl turns out to be online variational inference [broderich et al  ghahramani & attias ]", "their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as wel", "- it is unclear about the sensitivity of vcl with respect to the size of the coreset the authors should investigate this aspect", "- what is the trade-off when the size of the coreset increas"], "labels": ["MIN", "GEN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this paper proposes a new machine comprehension model which integrates several contributions like different embeddings for gate function and passage representation function self-attention layers and highway network based fusion lay", "the proposed method was evaluated on the squad dataset only and marginal improvement was observed compared to the baselin", "one concern i have for this paper is about the evaluation the paper only evaluates the proposed method on the squad data with systems submitted in july  and the improvement is not very larg", "as a result the results are not suggesting significance or generalizability of the proposed method", "the paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding which help a lot for understanding how the proposed methods contribute to the improv"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the results show that the deeper self-attention layers are indeed useful but still not improving a lot about -%", "the other proposed components contribute less signific", "as a result i suggest the authors add more ablation tests regarding  replacing the outer-fusion with simple concatenation it should work for two attention layers  removing the inner-fusion layer and only use the final layer's output and using residual connections like many nlp papers did instead of the more complicated gru stuff", "regarding the ablation in table  my first concern is that the improvement seems small ~%", "as a result i am wondering whether this separated question embedding really brings new information or the similar improvement can be achieved by increasing the size of lstm lay"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example if we use the single shared question embeddings but increase the size from  to some larger number like  can we observe similar improv", "i suggest the authors try this experiment as well and i hope the answer is no as separated input embeddings for gate functions was verified to be useful in some old works with syntactic features as gate values like semantic frame identification with distributed word representations and learning composition models for phrase embeddings etc", "please specify which version of the squad leaderboard is used in t", "is it a snapshot of the jul  on", "because this paper is not comparing to the state-of-the-art no specification of the leaderboard version may confuse the other reviewers and read"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["by the way it will be better to compare to the snapshot of oct  as well indicating the position of this work during the submission deadlin", "minor issues there are typos in figure  regarding the notations of question features and passage featur", "in figure  i suggest adding an n times symbol to the left of the q-p attention layer and remove the current list of such layers in order to be consistent to the other parts of the figur", "what is the relation between the phasecond qpatt+b in table  and the phasecond in t", "i was assuming that those are the same system but did not see the numbers match each oth"], "labels": ["MAJ", "MIN", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors propose a data-dependent channel pruning approach to simplify cnns with batch-norm", "the authors view cnns as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter gamma which is seen as a ucgateud to the information flow", "specifically the approach uses iterative soft-thresholding algorithm step to induce sparsity in gamma during the overall training phase of the cnn with additional rescaling to improve effici", "in the experiments section the authors apply their pruning approach on a few representative problems and network", "the concept of applying sparsity on gamma to prune channels is an interesting one compared to the usual approaches of sparsity on weight"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however the ista which is equivalent to l penalty on gamma is in spirit same as ucsmaller-norm-less-informativeud assumpt", "hence the title seems a bit mislead", "the quality and clarity of the paper can be improved in some sect", "some specific comments by section rethinking assumptions-twhile both issues outlined here are true in general the specific examples are either artificial or can be resolved fairly easili", "for example l- norm penalties only applied on alternate layers is artificial and applying the penalties on all ws would fix the issue in this cas"], "labels": ["GEN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also the scaling issue of w can be resolved by setting the norm of w to  as shown in he et ", "can the authors provide better examples her", "-tcan the authors add specific citations of the existing works which claim to use lasso group lasso thresholding to enforce parameter spars", "channel pruning-tthe notation can be improved by defining or replacing ucsum_reducedud-tista u is only an algorithm the basic assumption is still l - sparsity or smaller-norm-less-inform", "can the authors address the earlier comment about uca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithmsud"], "labels": ["MIN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["-tcan the authors address the earlier comment on uchow to set thresholds for weights across different layersud by providing motivation for choice of penalty for each lay", "-tcan the authors address the earlier comment on how their approach provides ucguarantees for preserving neural net functionality approximatelyud", "experiments-tcifar- since there is loss of accuracy with channel pruning it would be useful to compare accuracy of a pruned model with other simpler models with similar params", "like pruned-resnet- vs resnet- in islvrc subsection-tislvrc the comparisons between similar param-size models is exteremely useful in highlighting the contribution of thi", "however resnet-// top- error rates from table / in he etal  seem to be lower than reported in table  here can the authors clarifi"], "labels": ["MIN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["n-tfore/background can the authors add citations for datasets metrics for this problem", "overall the channel pruning with sparse gammas is an interesting concept and the numerical results seem promis", "the authors have started with right motivation and the initial section asks the right questions howev", "some of those questions are left unanswered in the subsequent work as detailed abov", "some of those questions are left unanswered in the subsequent work as detailed abov"], "labels": ["GEN", "MAJ", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper presents a multi-task multi-domain model based on deep neural network", "the proposed model is able to take inputs from various domains image text speech and solves multiple tasks such as image captioning machine translation or speech recognit", "the proposed model is composed of several features learning blocks one for each input type and of an encoder and an auto-regressive decoder which are domain-agnost", "the model is evaluated on  different tasks and is compared with a model trained separately on each task showing improvements on each task", "the paper is well written and easy to follow"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the contributions of the paper are novel and signific", "the approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspir", "the experiments clearly show the viability of the approach and give interesting insight", "this is surely an important step towards more general deep learning model", "comments* in the introduction where the  databases are presented the tasks should also be explained clearly as several domains are involved and the reader might not be familiar with the task linked to each databas"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover some databases could be used for different tasks such as wsj or imagenet", "* the training procedure of the model is not explained in the pap", "what is the cost function and what is the strategy to train on multiple task", "the paper should at least outline the strategi", "* the experiments are sufficient to demonstrate the viability of the approach"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the experimental setup is not clear", "specifically there is an issue about the speech recognition part of the experi", "it is not clear what the task exactly is continuous speech recognition isolated word recognit", "the metrics used in table  are also not clear they should be explained in the text", "also if the task is continuous speech recognition the wer word error rate metric should be us"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["information about the detailed setup is also lacking specifically which test and development sets are used the wsj corpus has several set", "* using raw waveforms as audio modality is very interest", "but this approach is not standard for speech recognit", "some references should be provided such asp golik z tuske r schluter h ney convolutional neural networks for acoustic modeling of raw time signal in lvcsr in proceedings of the annual conference of the international speech communication association interspeech  pp u", "d palaz m magimai doss and r collobert  april convolutional neural networks-based continuous speech recognition using raw speech sign"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in acoustics speech and signal processing icassp  ieee international conference on pp - iee", "t n sainath r j weiss a senior k w wilson and o viny", "learning the speech front-end with raw waveform cldnn", "proceedings of the annual conference of the international speech communication association interspeech", "revised reviewthe main idea of the paper is very interesting and the work presented is impress"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i tend to agree with reviewer as a more comprehensive analysis should be presented to show that the network is not simply multiplexing task", "the experiments are interest", "except for the wsj speech task which is almost meaningless", "indeed it is not clear what the network has learned given the metrics presented as the wer on wsj should be around % for speech recognit", "i thus suggest to either drop the speech experiment or the modify the network to do continuous speech recognit"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a simpler speech task such as keyword spotting could also be investig", "this paper investigates an effect of time dependencies in a specific type of rnn", "the idea is important and this paper seems sound", "however i am not sure that the main result theorem  explains an effect of depth suffici", "--main commentabout the deep network case in theorem  how $l$ affects the bound on rank"], "labels": ["MIN", "GEN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the current statement the result seems independent to $l$ when $l geq $", "i think that this paper should quantify the effect of an increase of $l$", "--sub commentnumerical experiments for calculating the separation rank is necessary to provide evidence of the main result", "only a simple example will make this paper more convinc", "this paper describes an approach to decode non-autoregressively for neural machine translation and other tasks that can be solved via seqseq model"], "labels": ["GEN", "MAJ", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the advantage is the possibility of more parallel decoding which can result in a significant speed-up up to a factor of  in the experiments describ", "the disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach yet the same bleu scores as standard beam search", "overall this is an interesting pap", "it would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved blue score on one of the standard benchmarks like wmt en-fr or en-de to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same bleu scor", "table  gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as nat -- losing - bleu points on wmt is signific"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while the ro-en results are goodg this particular language pair has not been used much by others it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why wmt en-de and de-en are not improving mor", "finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for nat + npd is actually higher depending on the choice of ", "summarythe major contribution of the paper is a generalization of lambda-returns called confidence-based autodidactic returns car wherein the rl agent learns the weighting of the n-step returns in an end-to-end mann", "these cars are used in the ac algorithm", "the weights are based on the confidence of the value function of the n-step return"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["even though this idea in not new", "the authors propose a simple and robust approach for doing it by using the value function estimation network of ac", "during experiments the autodidactic returns perform better only half of the time as compared to lambda return", "commentsthe j-step returns td error is not written correctli", "in figure  it is not obvious how the confidence of the values is estim"], "labels": ["MAJ", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figure  is unread", "a lighter version of algorithm  in appendix f should be moved in the text since this is the novelty of the pap", "the paper is well written and clear", "the main idea is to exploit a schema of semisupervised learning based on acol and gar for an unsupervised learning task", "the idea is to introduce the notion of pseudo label"], "labels": ["MIN", "MIN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pseudo labelling can be obtained by transformations of original input data", "pseudo labelling can be obtained by transformations of original input data", "the key point is the definition of the transform", "only whether the design of transformation captures the latent representation of the input data the pseudo-labelling might improve the performance of the unsupervised learning task", "since it is not known in advance what might be a good set of transform"], "labels": ["MIN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clust", "the idea of using mcmcp with gans is well-motivated and well-pres", "in the paper and the approach is new as far as i know", "figures  and  areconvincing evidence that mcmcp compares favorably to direct sampling ofthe gan feature space using the classification images approach", "however as discussed in the introduction the reason an efficientsampling method might be interesting would be to provide insighton the components of percept"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["on these insights the paper feltincomplet", "for example it was not investigated whether the method identifiesclassification features that gener", "the faces experiment issimilar to previous work done by martin  and kontsevich but unlike that previous work does not investgiate whetherclassification features have been identified that can be added to anarbitrary image to change the attribute happy vs sad or male vs femal", "similarly the second experiment in table  compares classificationaccuracy between different sampling methods but it does not provideany comparison as done in vondrick  to a classifier trainedin a conventional way such as an svm so it is difficult to discernwhether the learned distributions are inform", "finally the effect of choosing gan features vs a more naive featurespace is not explored in detail"], "labels": ["MAJ", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the gan is trainedon an image data set with many birds and cars but not manyfire hydr", "is the method giving us a picture of this data set", "the paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consist", "the approach is then experimented on various image and text task", "the description of the model is laborious and hard to follow figure  helps but is only referred to at the end of the description at the end of section  which instead explains each step without the big picture and loses the reader with confusing not"], "labels": ["MIN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance it only became clear at the end of the section that e was learn", "one of the motivations behing the model is to force label representations to be in a semantic space where two labels with similar meanings would be nearbi", "one of the motivations behing the model is to force label representations to be in a semantic space where two labels with similar meanings would be nearbi", "the assumption given in the introduction is that softmax would not yield such a representation but nowhere in the paper this assumption is verifi", "i believe that using cross-entropy with softmax should also push semantically similar labels to be nearby in the weight space entering the softmax"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this should at least be verified and compared appropri", "another motivation of the paper is that targets are given as s or s while soft targets should work bett", "i believe this is true but there is a lot of prior work on these such as adding a temperature to the softmax or using distillation etc none of these are discussed appropriately in the pap", "section  describes a way to compress the label embedding representation but it is not clear if this is actually used in the experiments h is never discussed after sect", "experiments on known datasets are interesting but none of the results are competitive with current state-of-the-art results sota despite what is said in appending d"], "labels": ["MIN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for instance one can find sota results for cifar around % and for cifar around % similarly one can find sota results for iwslt around  bleu", "it can be fine to not be sota as long as it is acknowledged and discussed appropri", "summary the authors show that using visual modality as a pivot they can train a model to translate from l to l", "please find my detailed comments/questions/suggestions below imo the paper could have been written much bett", "at the core this is simply a model which uses images as a pivot for learning to translate between l and l by learning a common representation space for {l image} or {l image}"], "labels": ["GEN", "MAJ", "GEN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["there are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these work", "imo this leads to unnecessary confusion and does more harm than good", "for example the abstract gives an impression that the authors have designed a game to collect data and it took me a while to set this confusion asid", "n continuing on the above point this is essentially about learning a common multimodal representation and then decode from this common represent", "however the authors do not cite enough work on such multimodal representation learning for example look at spandana et "], "labels": ["GEN", "MIN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["image pivoting for learning multilingual multimodal representations emnlp  for a good set of refer", "this omission of related work also weakens the experimental sect", "at least for the word translation task many of these common representation learning frameworks could have been easily evalu", "for example find the nearest german neighbour of the word dog in the common representation space the authors instead compare with very simple baselin", "even when comparing with simple baselines the proposed model does not convincingly outperform them"], "labels": ["GEN", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular  the p@ and p@ numbers are only slightly bett", "some of the choices made in the experimental setup seem questionable to m", "- why  use a nmt model without attention that is not standard and does not make sense to use when a better baseline model with attention is avail", "- it is mentioned that while their model unit-normalizes the output of every encoder we found this to consistently hurt performance so do not use normalization for fair comparison with our model", "i don't think this is a fair comparison"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of n&n if that gives better perform", "please mention the numbers with unit normalization to give a better pictur", "it does not make sense to weaken an existing baseline and then compare with it", "it would be good to mention the results of the nmt model in table  itself instead of mentioning them separately in a paragraph", "this again leads to poor readability and it is hard to read and compare the corresponding numbers from t"], "labels": ["GEN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not sure why this cannot be accommodated in the table itself", "in figure  what exactly do you mean by results are averaged over  translation scenarios can you please elabor", "the authors propose the n-gram machine to answer questions over long docu", "the model first encodes the document via tuple extract", "an autoencoder objective is used to produce meaningful tupl"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["then the model generates a program based on the extracted tuple collection and the question to find an answ", "i am very disappointed in the authors' choice of evaluation namely babi - a toy synthetic task long abandoned by the nlp community because of its lack of pract", "if the authors would like to demonstrate question answering on long documents they have the luxury of choosing amongst several large scale realistic question answering datasets such as the stanford question answering dataset or triviaqa", "beyond the problem of evaluation the model the authors propose does not provide new ideas and rather merges existing ones this in itself is not a problem", "however the authors decline to cite many many important prior work for example the tuple extraction described by the authors has significant prior work in the information retrieval community eg knowledge base population relation extract"], "labels": ["GEN", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the idea of generating programs to query over populated knowledge bases again has significant related work in semantic parsing and program synthesi", "question answering over much more complex probabilistic knowledge graphs have been proposed before as well in fact i believe matt gardner wrote his entire thesis on this top", "finally textual question answering on realistic datasets has seen significant breakthroughs in the last few year", "non of these areas with the exception of semantic parsing are addressed by the author", "with sufficient knowledge of related works from these areas i find that the authors' proposed method lacks proper evaluation and sufficient novelti"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in this paper the authors explore how using random projections can be used to make ocsvm robust to adversarially perturbed training data", "while the intuition is nice and interest", "the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robust", "detailshave been successfully in anomaly detection -- have been successfully used in anomaly detectionp", "the adversary would select a random subset of anomalies push them towards the normal data cloud and inject these perturbed points into the training set -- this seems backward"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["as in the example that follows if the adversary wants to make anomalies seem normal at test time it should move normal points outward from the normal point cloud eg making a  look like a weird", "as s_attack increases the anomaly data points are moved farther away from the normal data cloud altering the position of the separating hyperplane -- this seems backwards from fig", "from a to b the red points move closer to the center while in c they move further away whi", "the blue points seem to consistently become more dense from a to c", "the attack model is too rough"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it seems that without bounding d we can make the model arbitrarily bad no", "assumption  alludes to this but doesn't specify what is smal", "also the attack model is described without considering if the adversary knows the learner's algorithm", "even if there is randomness can the adversary take actions that account for that random", "does selecting a projection based on compactness remove the random"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["experiments -- why/how would you have distorted test data", "making an anomaly seem normal by distorting it is easi", "i don't see experiments comparing having random projections and not", "this seems to be the fundamental question -- do random projects help in the train_d | test_c cas", "experiments don't vary the attack much to understand how robust the method i"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper proposes to make the inner layers in a neural network be block diagonal mainly as an alternative to prun", "the implementation of this seems straightforward and can be done either via initialization or via pruning on the off-diagon", "there are a few ideas the paper discusses compared to pruning weight matrices and making them sparse block diagonal matrices are more efficient since they utilize level  blas rather than sparse operations which have significant overhead and are not worth it until the matrix is extremely sparse i think this case is well supported via their experiments and i largely agre", "that therefore block diagonal layers lead to more efficient network", "this point is murkier because the paper doesn't discuss possible increases in *training time* due to increased number of iterations in much detail"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["at if we only care about running the net then reducing the time from s to s doesn't seem to be that useful maybe it is for real-time predict", "please cite some work in that cas", "to summarize points  and  block diagonal architectures are a nice alternative to pruned architectures with similar accuracy and more benefit to speed mainly speed at run-time or speed of a single iteration not necessarily speed to train", "[as i am not primarly a neural net researcher i had always thought pruning was done to decrease over-fitting not to increase computation speed so this was a surprise to me also note that the sparse matrix format can increase runtime if implemented as a sparse object as demonstrated in this paper but one could always pretend it is sparse so you never ought to be slower with a sparse matrix]", "there is some vague connection to random matrices with some limited experiments that are consistent with this observation but far from establish it and without any theoretical analysis martingale or markov chain theori"], "labels": ["MIN", "MIN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this is an experimental/methods paper that proposes a new algorithm explained only in general details and backs up it up with two reasonable experiments that do a good job of convincing me of point  abov", "the authors seem to restrict themselves to convolutional networks in the first paragraph and experiments but don't discuss the implications or reasons of this assumpt", "the authors seem to understand the literature well and not being an expert myself i have the impression they are doing a fair job", "the paper could have gone farther experimentally or theoretically in my opinion", "for example with sparse and block diagonal matrices reducing the size of the matrix to fit into the cache on the gpu must obviously make a difference but this did not seem to be investig"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i was also wondering about when  or more layers are block sparse do these blocks overlap", "ie are they randomly permuted between layers so that the blocks mix", "and even with a single block does it matter what permutation you us", "or perhaps does it not matter due to the convolutional structur", "the section on the variance of the weights is rather unclear mathematically starting with the abstract and even continuing into the pap"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["we are talking about sample vari", "what does deltavar mean in eq", "the marchenko-pastur theorem seemed to even be imprecise since if y then a   implying that there is a nonzero chance that the positive semi-definite matrix xx' has a negative eigenvalu", "i agree this relationship with random matrices could be interesting but it seems too vague right now", "is there some central limit theorem explan"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["are you sure that you've run enough iterations to fully convergefig  was still trending up for b=", "was it due to the convolutional net structure you could test thi", "or perhaps train a network on two datasets one which is not learnable iid random labels and one which is very easily learnable eg linearly separ", "would this affect the distribut", "furthermore i think i misunderstood parts because the scaling in mnist and cifar was different and i didn't see why for mnist it was proportional to block size and for cifar it was independent of block size almost"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["minor comment last paragraph of  comparing with sindhwani et al was confusing to m", "why was this mentioned and it doesn't seem to be compar", "i have no idea what toeplitz  i", "this paper presents a new idea to use pact to quantize networks and showed improved compression and comparable accuracy to the original network", "the idea is interesting and novel that pact has not been applied to compressing networks in the past"], "labels": ["MIN", "MIN", "MIN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the results from this paper is also promising that it showed convincing compression result", "the experiments in this paper is also solid and has done extensive experiments on state of the art datasets and network", "results look promising too", "overall the paper is a descent one but with limited novelti", "i am a weak reject"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["after reading rebuttals from the authors the authors have addressed all of my concern", "the additional experiments are a good addit", "************************the authors provide an algorithm-agnostic active learning algorithm for multi-class classif", "the core technique is to construct a coreset of points whose labels inform the labels of other point", "the coreset construction requires one to construct a set of  points which can cover the entire dataset"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["while this is np-hard problem in general the greedy algorithm is -approxim", "the authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time  this cover tells us which points are to be queri", "the reason why choosing the cover is a good idea is because under suitable lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data spac", "the authors use the coreset construction with a cnn to demonstrate an active learning algorithm for multi-class classif", "the experimental results are convincing enough to show that it outperforms other active learning algorithm"], "labels": ["GEN", "GEN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i have a few major and minor commentsmajor comments the proof of lemma  is incomplet", "we need the lipschitz constant of the loss function the loss function is a function of the cnn function and the true label", "the proof of lemma  only establishes the lipschitz constant of the cnn funct", "some more extra work is needed to derive the lipschitz constant of the loss function from the cnn funct", "the statement of prop  seems a bit confusing to me the hypothsis says that the loss on the coreset =  but the equation in proposition  also includes the loss on the coreset"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why is this term included is this term not equal to", "n some important works are missing  especially works related to pool based active learning and landmark results on labell complexity of agnostic active learn", "upal unbiased pool based active learning by ganti & gray http//proceedingsmlrpress/v/ganti/gantipdfefficient active learning of half-spaces by gonen et al http//wwwjmlrorg/papers/volume/gonena/gonenapdfa bound on the label complexity of agnostic active learning http//wwwmachinelearningorg/proceedings/icml/papers/pdf", "the authors use l_ loss as their objective function this is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer making it a natural fit to work with something like a cross-entropy loss funct", "i guess the theoretical results do not extend to cross-entropy loss but the authors do not mention these points anywhere in the pap"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["for example the ladder network which is one of the networks used by the authors is a network that uses cross-entropy for train", "minor-comment  the feasibility program in  is an milp however the way it is written it does not look like an milp", "it would have been great had the authors mentioned that u_j in {}", "the authors write on page  moreover zero training error can be enforced by converting average loss into maximal loss it is not clear to me what the authors mean her", "for example can i replace the average error in proposition  by maximal loss why can i do that"], "labels": ["GEN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["why would that result in zero training error", "on the whole this is interesting work and the results are very nic", "but the proof for lemma  seems incomplete to me and some choices such as choice of loss function are unjustifi", "also important references in active learning literature are miss", "the paper studies the global convergence for policy gradient methods for linear control problem"], "labels": ["MIN", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the topic of this paper seems to have minimal connection with icrl", "it might be more appropriate for this paper to be reviewed at a control/optimization conference so that all the technical analysis can be evaluated car", "i am not convinced if the main results are novel", "the convergence of policy gradient does not rely on the convexity of the loss function which is known in the community of control and dynamic program", "the convergence of policy gradient is related to the convergence of actor-critic which is essentially a form of policy iter"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i am not sure if it is a good idea to examine the convergence purely from an optimization perspect", "the main results of this paper seem technical sound", "however the results seem a bit limited because it does not apply to neural-network function approxim", "it does not apply to the more general control problem rather than quadratic cost function which is quite restrict", "i might have missed something her"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i strongly suggest that these results be submitted to a more suitable venu", "this paper suggests a reparametrization of the transition matrix", "the proposed reparametrization which is based on singular value decomposition can be used for both recurrent and feedforward network", "the paper is well-written and authors explain related work adequ", "the paper is a follow up on unitary rnns which suggest a reparametrization that forces the transition matrix to be unitari"], "labels": ["MAJ", "GEN", "GEN", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact", "i have two comments on the experiment section- choice of experi", "authors have chosen ucr datasets and mnist for the experiments while other experiments are more common", "for example the adding problem the copying problem and the permuted mnist problem and language modeling are the common experiments in the context of rnn", "for feedforward settings classification on cifar and cifar is often report"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- stopping condit", "the plots suggest that the optimization has stopped earlier for some model", "is this because of some stopping condition or because of gradient explos", "is there a way to avoid thi", "- quality of figur"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["figures are very hard to read because of small font", "also the captions need to describe more details about the figur", "this paper proposes an adaptation of the searn algorithm to rnns for generating text", "in order to do so they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explor", "pros- good literature review"], "labels": ["MIN", "MIN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but the future work on bandits is already happen", "paper accepted at acl  bandit structured prediction for neural sequence-to-sequence learning julia kreutzer artem sokolov stefan riezl", "cons- the key argument of the paper is that searnn is a better il-inspired algorithm than the previously proposed on", "however there is no direct comparison either theoretical or empirical against them", "in the examples on spelling using the dataset of bahdanau et al  no comparison is made against their actor-critic method"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["furthermore given its simplicity i would expect a comparison against scheduled sampl", "- a lot of important experimental details are in the appendices and they differ among experi", "for example while mixed rollins are used in most experiments reference rollins are used in mt which is odd since it is a bad option theoret", "also  no details are given on how the mixing in the rollouts was tun", "finally in the nmt comparison while it is stated that similar architecture is used in order to compare fairly against previous work this is not the case eventually as it is acknowledged at least in the case of mix"], "labels": ["MIN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i would have expected the same encoder-decoder architecture to have been used for all the methods consid", "- the two losses introduced are not really new", "the log-loss is just mle only assuming that instead of a fixed expert that always returns the same target we have a dynamic on", "note that the notion of dynamic expert is present in the searn paper too", "goldberg and nivre just adapted it to transition-based dependency pars"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["similarly since the kl loss is the same as xent why give it a new nam", "- the top-k sampling method is essentially the same as the targeted exploration of goodman et al  which the authors cit", "thus it is not a novel contribut", "- not sure i see the difference between the stochastic nature of searnn and the online one of lols mentioned in sect", "they both could be mini-batched similarli"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["also not sure i see why searnn can be used on any task in comparison to other method", "they all seem to be equally cap", "minor comments- figure  what is the difference between cost-sensitive loss and just loss", "- local vs sequence-level losses the point in ranzato et al and wiseman & rush is that the loss they optimizise bleu/rouge do not decompose over the the predictions of the rnn", "- can't see why searnn can help with the vanishing gradient problem"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["seem to be rather orthogon", "this paper is well written and easy to follow", "the authors propose pixel deconvolutional layers for convolutional neural network", "the motivation of the proposed method pixeldcl is to remove the checkerboard effect of deconvolutoinal lay", "the method consists of adding direct dependencies among the intermediate feature maps generated by the deconv lay"], "labels": ["MIN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["pixeldcl is applied sequentially therefore it is slower than the original deconvolutional lay", "the authors evaluate the model in two different problems semantic segmentation on pascal voc and mscoco datasets and in image generation vae with the celeba dataset", "the authors justify the proposed method as a way to alleviate the checkerboard effect while introducing more complexity to the model and making it slow", "in the experimental section however they do not compare with other approaches to do so for example the upsampling+conv approach which has been shown to remove the checkerboard effect while being more efficient than the proposed method as it does not require any sequential comput", "moreover the pixeldcl does not seem to bring substantial improvements on deeplab a state-of-the-art semantic segmentation algorithm"], "labels": ["MAJ", "GEN", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["more comments and further exploration on this results should be don", "why no performance boost", "is it because of the residual connect", "or other component of deeplab", "is the proposed layer really useful once a powerful model is us"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i also think the experiments on vae are not conclus", "the authors simply show set of generated imag", "first it is difficult to see the different of the image generated using deconv and pixeldcl", "second a set of  qualitative images does not and cannot validate any research idea", "summarythe paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated upd"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the architecture consists of a uslowu network which provides weight updates for the ufastu network which outputs the predictions of the system", "the experiments show that the architecture outperforms a couple of related models on an associative retrieval problem", "qualitythe authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in danihelka et ", "the difference with the original task seems to be that the network is also trained to predict a ublanku symbol which indicates that no prediction has been mad", "while this task is artificial it does make sense in the context of what the authors want to show"], "labels": ["GEN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models adds to the quality of the experi", "it is somewhat unfortunate that the paper doesnut give more detail about the precise hyper-parameters involved and that there is no comparison with the associative lstm from danihelka et ", "did these hyper-parameters also include the sizes of the model", "otherwise itus not very clear to me why the numbers of parameters are so much higher for the baseline model", "while i think that this experiment is well done it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of task"], "labels": ["MAJ", "MIN", "GEN", "MIN", "CNT"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it is commendable that the authors also discuss the memory requirements and increased wall clock time of the model", "clarityi found the paper hard to read at times and it is often not very clear what the most important differences are between the proposed methods and earlier ones in the literatur", "ium not saying those differences arenut there but the paper simply didnut emphasize them very well and i had to reread the paper from ba et al  to get the full pictur", "originality/significancewhile the architecture is new it is based on a combination of previous ideas about fast weights hypernetworks and activation gating and iud say that the novelty of the approach is averag", "the architecture does seem to work well on the associative retrieval task but it is not clear yet if this will also be true for other types of task"], "labels": ["MAJ", "MIN", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["until that has been shown the impact of this paper seems somewhat limited to m", "prosexperiments seem well don", "good baselin", "good result", "conshard to extract the most important changes from the text"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["only a single synthetic task is report", "this paper considers the problem of self-normalizing models this kindof approaches such as nce noise contrastive estimation is verypromising and important to provide efficient and large vocabularylanguage model", "by interpreting the nce in terms of matrix factorization allows theauthors to better explain this learning criterion and morespecifically the self-normalizing mechan", "however the first theoritical contribution is to make the linkbetween matrix decomposition and sampling based object", "this wasalready shown for negative sampling in the paper of melamud et al inemnlp  therefore nothing new here the difference is slight"], "labels": ["GEN", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["moreover this paper is only cited in the experimental part while thecontribution should be far more emphasized by the author", "the second part makes the link with the self-normalization this isnot really surprising this was already explained in the same way inpapers from pihlajagutmann and hyvarinen published in /", "the paper is relatively clear to follow and impl", "the main concern is that this looks like a class project rather than a scientific pap", "for a class project this could get an a in a ml class!"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in particular the authors take an already existing dataset design a trivial convolutional neural network and report results on it", "there is absolutely nothing of interest to iclr except for the fact that now we know that a trivial network is capable of obtaining % accuracy on this dataset", "the paper presents the word embedding technique which consists of a construction of a positive ie with truncated negative values pointwise mutual information order- tensor for triples of words in a sent", "and b symmetric tensor cp factorization of this tensor", "the authors propose the cp-s stands for symmetric cp decomposition approach which tackles such factorization in a batch manner by considering small random subsets of the original tensor"], "labels": ["GEN", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["they also consider the jcp-s approach where the als alternating least squares objective is represented as the joint objective of the matrix and order- tensor als object", "the approach is evaluated experimentally on several tasks such as outlier detection supervised analogy recovery and sentiment analysis task", "clarity the paper is very well written and is easy to follow", "however some implementation details are missing which makes it difficult to assess the quality of the experimental result", "quality i understand that the main emphasis of this work is on developing faster computational algorithms which would handle large scale problems for factorizing this tensor"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i have several concerns about the algorithms proposed in this pap", "- first of all i do not see why using small random subsets of the original tensor would give a desirable factor", "indeed a cp decomposition of a tensor can not be reconstructed from cp decompositions of its subtensor", "note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations which then leads to an approximation of desirable quantities eg the gradient in expectation and the current approach where subtensors are considered as batch", "i would expect some further elaboration of this question in the pap"], "labels": ["MIN", "MIN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although similar methods appeared in the tensor literature before i don't see any theoretical ground for their correctness  - second there is a significant difference between the symmetric cp tensor decomposition and the non-negative symmetric cp tensor decomposit", "in particular the latter problem is well posed and has good properties see eg lim comon nonengative approximations of nonnegative tensor", "however this is not the case for the former see eg comon et al  as cited in this pap", "therefore a computing the symmetric and not non-negative symmetric decomposition does not give any good theoretical guarantees while achieving such guarantees seems to be one of the motivations of this pap", "and b although the tensor is non-negative its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to m"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- third the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem", "this is an important question that has not been addressed in the literature and is clearly a pro of the pap", "however it seems to me that this goal is not fully impl", "indeed a i mentioned in the previous paragraph the issues with the symmetric cp decomposition and b although the paper is motivated by the recent algorithm proposed by sharan&valiant  the algorithms proposed in this paper are not based on this or other known algorithms with theoretical guarante", "this is therefore confusing and i would be interested in the author's point of view to this issu"], "labels": ["GEN", "MAJ", "MIN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- further the proposed joint approach where the second and third order information are combined requires further analysi", "indeed in the current formulation the objective is completely dominated by the order- tensor factor because it contributes od^ terms to the objective vs od^ terms contributed by the matrix part", "it would be interesting to see further elaboration of the pros and cons of such problem formul", "- minor comment in the shifted pmi section the authors mention the parameter alpha and set specific values of this parameter based on experi", "however i don't think that enough information is provided because given the author's approach the value of this parameter most probably depends on other parameters such as the bach s"], "labels": ["MIN", "MIN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- finally although the empirical evaluation is quite extensive and outperforms the state-of the art i think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned abov", "originality the idea of using a pointwise mutual information tensor for word embeddings is not new but the authors fairly cite all the relevant literatur", "my understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evalu", "however such batch approaches for tensor factorization are not new and i am quite skeptical about their correctness see abov", "the experimental evaluation presents indeed interesting result"], "labels": ["MAJ", "MAJ", "MAJ", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["however i think it would also be important to compare to other tensor factorization approach", "i would also be quite interested to see the performance of the proposed algorithm for different values of parameters such as the butch s", "significance i think the paper addresses very interesting problem and significant amount of work is done towards the evaluation but there are some further important questions that should be answered before the paper can be publish", "to summarize the following are the pros of the paper  - clarity and good present", "- good overview of the related literatur"], "labels": ["MIN", "MIN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- extensive experimental comparison and good experimental result", "while the following are the cons  - the mentioned issues with the proposed algorithm which in particular does not have any theoretical guarante", "- lack of details on how experimental results were obtained in particular lack of the details on the values of the free parameters in the proposed algorithm", "- lack of comparison to other tensor approaches to the word embedding problem ie other algorithms for the tensor decomposition subproblem", "- the novelty of the approach is somewhat limit"], "labels": ["MAJ", "MAJ", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["although the idea of the extensive experimental comparison is good", "this paper introduces a machine learning adaptation of the active inference framework proposed by friston  and applies it to the task of image classification on mnist through a foveated inspection of imag", "it describes a cognitive architecture for the same and provide analyses in terms of processing compression and confirmation biases in the model", "u active perception and more specifically recognition through saccades or viewpoint selection is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve effici", "the problem and its potential applications are well motiv"], "labels": ["MAJ", "GEN", "GEN", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["u the perception-driven control formulation is well-detailed and simple to follow", "u the achieved compression rates are significant and impressive though additional demonstration of performance on more challenging datasets would have been more compel", "questions and commentsu while an % compression rate is significant % accuracy on mnist seems poor", "a plot demonstrating the tradeoff of accuracy for compression by varying href or other parameters would provide a more complete picture of perform", "knowing baseline performance without active inference would help put numbers in perspective by providing a performance bound due to modeling choic"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["uuawhat does the distribution of number of saccades required per recognition for a given threshold look like over the entire dataset ie how many are dead-easy vs difficult", "u steady state assumption how can this be relaxed to further generalize to non-static scen", "u figure  is low resolution and difficult to read", "post-rebuttal commentsi have revised my score after considering comments from other reviewers and the revised pap", "while the revised version contains more experimental detail"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context", "the paper also contains grammatical errors and is somewhat difficult to understand", "finally while it proposes an interesting formulation of a well-studied problem", "more comparisons and analysis are required to validate the approach", "the paper proposes an additional transform in the recurrent neural network unit"], "labels": ["MIN", "MIN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the transform allows for explicit rotations and swaps of the hidden cell dimens", "the idea is illustrated for lstm units where the transform is applied after the cell values are computed via the typical lstm upd", "my first concern is the motiv", "i think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via g", "in the proposed example it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swap"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["alternatively experimentally showing that the network makes use of swaps in an interpretable way eg at certain sentence positions could strengthen the motiv", "secondly the experimental analysis is not very extens", "the method is only evaluated on the babi qa dataset which is a synthetic dataset", "i think a language modeling benchmark and/or a larger scale question answering dataset should be consid", "regarding the experimental setup how are the hyper-parameters for the baseline tun"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["have you considered training jointly across the tasks as wel", "also is the setting the same as in weston et ", "while for many tasks the numbers reported by weston et al  and the ones reported here for the lstm baseline are aligned in the order of magnitude suggesting that some tasks are easier or more difficult for lstms there are large differences in other cases for task # here  weston  for task # here  weston  and so on", "finally do you have an intuition wrt to swaps and rotations regarding the accuracy improvements on tasks # and #", "some minor issues- the references are somewhat inconsistent in style some have urls others do not some have missing authors ending with et "], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- section  second paragraph senstence- section  first paragraph thorugh- section  architetur", "the paper proposes to combine the recently proposed densenet architecture with lstms to tackle the problem of predicting different pathologic patterns from chest x-ray", "in particular the use of lstms helps take into account interdependencies between pattern label", "strengths- the paper is very well written", "contextualization with respect to previous work is adequ"], "labels": ["MIN", "GEN", "GEN", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["explanations are clear", "novelties are clearly identified by the author", "- quantitative improvement with respect to the state the art", "weaknesses- the paper does not introduce strong technical novelties -- mostly it seems to apply previous techniques to the medical domain", "it could have been interesting to know if there are more insights / lessons learned in this process"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this could be of interest for a broader audi", "for instance what are the implications of using higher-resolution images as input to densenet / decreasing the number of lay", "how do the features learned at different layers compare to the ones of the original network trained for image classif", "how do features of networks pre-trained on imagenet and then fine-tuned for the medical domain compare to features learned from medical images from scratch", "- the impact of the proposed approach on medical diagnostics is unclear"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors could better discuss how the approach could be adopted in practic", "also it could be interesting also to discuss how the results in table  and  compare to human classification capabilities and if that performance would be already enough for building a computer-aided diagnosis system", "finally -- is it expected that the ordering of the factorization in eq  does not count much results in t", "as a non-expert in the field i'd expect that ordering between pathologic patterns matters mor", "the work was prompted by  an interesting observation a phase transition can be observed in deep learning with stochastic gradient descent and tikhonov regular"], "labels": ["MIN", "MIN", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["when the regularization parameter exceeds a data-dependent threshold the parameters of the model are driven to zero thereby preventing any learn", "the authors then propose to moderate this problem by letting the regularization parameter to be zero for  to  epochs and then applying the strong penalty paramet", "in their experimental results the phase transition is not observed anymore with their protocol", "this leads to better performances by using penalty parameters that would have prevent learning with the usual protocol", "the problem targeted is important in the sense that it reveals that some of the difficulties related to non-convexity and the use of sgd that are often overlook"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the proposed protocol is reported to work well but since it is really ad hoc it fails to convince the reader that it provides the right solution to the problem", "i would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy or to explore standard optimization tools such as constrained optimization ie ivanov regularization  that could be for example implemented by stochastic projected gradient or barrier funct", "i think that the problem would be better handled that way than with the proposed strategi", "which seems to rely only on a rather limited amount of experiments and which may prove to be inefficient when dealing with big databas", "to summarize i believe that the paper addresses an important point"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["but that the tools advocated are really rudimentary compared with what has been already proposed elsewher", "details - there is a typo in the definition of the proximal operator in eq", "- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the pap", "- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion", "[reviewed on january th]this article applies the notion of ucconceptorsud -- a form of regulariser introduced by the same author a few years ago exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learningmore precisely in the training of neural networks on sequential task"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["it proposes itself as an improvement over the main recent development of the field namely elastic weight consolid", "after a brief and clear introduction to conceptors and their application to ridge regression the authors explain how to inject conceptors into stochastic gradient descent and finally the real innovation of the paper into backpropag", "follows a section of experiments on variants of mnist commonly used for continual learn", "continual learning in neural networks is a hot topic and this article contributes a very interesting idea", "the notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of boolean log"], "labels": ["GEN", "GEN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the numeric examples although quite toy provide a clear illustr", "a few things are still missing to back the strong claims of this paper* some considerations of the computational costs the reliance on the full nxn correlation matrix r makes me fear it might be costly as it is applied to every layer of the neural networks and hence is the largest number of units in a lay", "this is of course much lighter than if it were the covariance matrix of all the weights which would be daunting but still deserves to be addressed if only with wall time measur", "* it could also be welcome to use a more grounded vocabulary eg on p ucfigure  shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical -neuron recurrent network that was drive with input signals from three difference sourcesud could be much more simply said as ucfigure  shows the ellipses corresponding to three sets of r^ pointsud", "being less grandiose would make the value of this article nicely on its own*"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["some examples beyond the contrived mnist toy examples would be welcom", "for example the main method this article is compared to ewc had a very strong section on reinforcement learning examples in the atari framework not only as an illustration but also as a motiv", "i realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learn", "nevertheless without going to that extreme it might be worth adding an extra demo on something bigger than mnist", "the authors transparently explain in their answer that they do not yet! belong to the deep learning community and hope finding some collaborations to pursue this furth"], "labels": ["MIN", "MAJ", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["if i may make a suggestion i think their work would get much stronger impact by  doing it the reverse way first finding the collaboration then adding this extra empirical results which then leads to a bigger impact publ", "the later point would normally make me attribute a score of  marginally above acceptance threshold by current dl community standard", "but because there is such a pressing need for methods to tackle this problem and because this article can generate thinking along new lines about this i give it a   good paper accept", "this paper presents a new method for obtaining a bilingual dictionary without requiring any parallel data between the source and target languag", "the method consists of an adversarial approach for aligning two monolingual word embedding spaces followed by a refinement step using frequent aligned words according to the adversarial map"], "labels": ["MAJ", "MIN", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the approach is evaluated on single word translation cross-lingual word similarity and sentence translation retrieval task", "nthe paper presents an interesting approach which achieves good perform", "the work is presented clearly the approach is well-motivated and related to previous studies and a thorough evaluation is perform", "nmy one concern is that the supervised approach that the paper compares to is limit", "it is trained on a small fixed number of anchor points while the unsupervised method uses significantly more word"], "labels": ["GEN", "MAJ", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["i think the paper's comparisons are valid", "but the abstract and introduction make very strong claims about outperforming state-of-the-art supervised approach", "i think either a stronger supervised baseline should be included trained on comparable data as the unsupervised approach or the language/claims in the paper should be soften", "the same holds for statements like  our method is a first step  which is very hard to justifi", "i also do not think it is necessary to over-sell given the solid work in the pap"], "labels": ["MAJ", "GEN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["further comments questions and suggestions- it might be useful to add more details of your actual approach in the abstract not just what it achiev", "- given you use trained word embeddings it is not a given that the monolingual word embedding spaces would be alignable in a linear way", "the actual word embedding method therefore has a big influence on performance as you show", "could you comment on how crucial it would be to train monolingual embedding spaces on similar domain", "/data with similar co-occurrence statistics in order for your method to be appropri"], "labels": ["MIN", "MIN", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- would it be possible to add weights to the terms in eq  or is this done implicitli", "- how were the k source words for procrustes supervised baseline select", "- have you considered non-linear mappings or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spac", "- do you think your approach would benefit from having a few parallel training point", "some minor grammatical mistakes/typos nitpicking- gives a good performance -"], "labels": ["GEN", "GEN", "GEN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["gives good performance- recent works several works most works etc-", "recent studies several studies etc- ie the improvements - ie the improv", "nthe paper is well-written relevant and interest", "i therefore recommend that the paper be accept", "summarythe paper presents three different methods of training a low precision student network from a teacher network using knowledge distil"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["scheme a consists of training a high precision teacher jointly with a low precision stud", "scheme b is the traditional knowledge distillation method and scheme c uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mod", "reviewthe paper is well written", "the experiments are clear and the three different schemes provide good analytical insight", "using scheme b  and c student model with low precision could achieve accuracy close to teacher while compressing the model"], "labels": ["GEN", "GEN", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["commentstensorflow citation is miss", "conclusion is short and a few directions for future research would have been us", "the paper proposes a method to synthesize adversarial examples that remain robust to different d and d perturb", "the paper shows this is effective by transferring the examples to d objects that are color d-printed and show some nice result", "the experimental results and video showing that the perturbation is effective for different camera angles lighting conditions and background is quite impress"], "labels": ["MIN", "MIN", "GEN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interest", "however the authors claim that standard techniques require complete control and careful setups eg in the camera case is quite misleading especially with regards to the work by kurakin et ", "this paper also seems to have some problems of its own for example the turtle is at relatively the same distance from the camera in all the examples i expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the hd texture of the turtl", "one interesting point this work raises is whether the algorithm is essentially learning universal perturbations moosavi-dezfooli et ", "if that's the case then complicated transformation sampling and d mapping setup would be unnecessari"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["this may already be the case since the training set already consists of multiple lighting rotation and camera type transformations so i would expect universal perturbations to already produce similar results in the real-world", "minor commentssection  a affine - an affinetypo in section  of a of a", "it's interesting in figure  that the crossword puzzle appears in the image of the lighthous", "moosavi-dezfooli s m fawzi a fawzi o & frossard p universal adversarial perturbations cvpr", "this work proposes an lstm based model for time-evolving probability dens"], "labels": ["MIN", "MIN", "MAJ", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the model does not assume an explicit prior over the underlying dynamical systems instead only uncertainty over observation noise is explicitly consid", "experiments results are good for given synthetic scenario", "but less convincing for real data", "clarity the paper is well-written", "some notations in the lstm section could be better explained for readers who are unfamiliar with lstm"], "labels": ["GEN", "MAJ", "MAJ", "MAJ", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["otherwise the paper is well-structured and easy to follow", "noriginality i'm not familiar with lstms it is hard for me to judge the originality her", "significance averag", "the work would be stronger if the authors can extend this to higher dimensional time seri", "there are also many papers on this topic using gaussian process state-space gp-ssm models where an explicit prior is assumed over the underlying dynamical system"], "labels": ["MAJ", "GEN", "MAJ", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors might want to comment on the relative merits between gp-ssms and de-rnn", "the smc algorithm used is a sequential-importance-sampling sis method", "i think it's correct but may not scale well with dimens", "this article tackles the extraction of sentiments at a fine-grained level", "thus the authors insist on context modeling to obtain a relevant analysis of a word's mean"], "labels": ["MIN", "MIN", "MIN", "GEN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the authors propose a first modeling called asc", "identifying some weakness in the formulation the authors propose  solut", "the authors apply their different models on a small dataset semeval  they compare basic memory network implementations with their approach", "==the authors do not put into perspective their approach", "given the literature in topics / sentiment modeling it is a real weakness of this articl"], "labels": ["GEN", "MIN", "GEN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["to improve readability the authors should propose a diagram of the network summarizing all not", "dimension of c_i / o", "ddefinition of u eq  = v", "at the beginning of section  the discussion about the independence of the terms in the decomposition  is not completely relevant alpha terms embedded the relation between the target and the context i", "among the different solutions it and ci are very redund"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["a kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in m-  why not using those terms instead of ai didt", "we see that the authors build models that become more and more complex but their motivation in combining attention and it/ci is not clear they learn the relation between context and target twice without any factor", "in section  the authors mention briefly some works from the rnn domain & the classical use case of memory network", "they claim thatthe above studies cannot be directly applied to the asc task as they are not capable of mining finer-grained aspect dependent senti", "i do not agree with them latent representations from rnn are fine-grained & context dependent latent representations from socher et al are also fine-grained & target dependent the position in the latent space -modeling context- has an impact on the estimated senti"], "labels": ["MIN", "MAJ", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["in the experimental section there is no discussion about the ration between the dataset size and the number of parameters to estim", "however there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to lucky tri", "it is true that the authors use strong regularization techniques drop out external knowledge of words embed", "however the validation procedure is not clear in the articl", "the main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis none of the state-of-the-art approaches are implemented here rnn basic models on wv aggreg"], "labels": ["MIN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["that makes  the contribution very difficult to evalu", "the analysis of the results are interesting both from the quantitative & qualitative point of view", "the paper describes a deep q-learning approach to the problem of lane changing whereby the action space is abstracted to high-level maneuvers that are then associated with low-level control", "the paper proposes a q-masking strategy that reduces the action space according to constraints or prior knowledg", "the method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human driv"], "labels": ["MAJ", "MAJ", "GEN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["planning lane-change maneuvers is an interesting important problem for self-driving vehicl", "what makes this problem particularly challenging is the need to predict/respond to the actions of other driv", "however these issues are ignored here  and it is is unclear why existing optimization/planning approaches are poorly suited to this problem which is a fundamental assumption being made her", "indeed there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles as well as work that plans using predictive models of vehicle behavior eg see the work by jon how's group", "however the related work discussion is significantly lack"], "labels": ["MAJ", "MIN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the paper does an insufficient job describing why deep rl is the right way to formulate this problem", "there are vague references to the policy being difficult to define but that motivates the importance of learning in general not deep rl", "why is it reasonable given i the challenge in defining appropriate rewards ie it's not clear to me what would constitute the right reward for this problem", "ii the large amount of data required to learn the polici", "and iii the significant risks associated with training with a physical vehicl"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["one can see the merits in employing a hierarchical action space whereby decision making operates over high-level actions each associated with low-level controllers but that the adopted formulation is not fundamental to this abstract", "indeed this largely regulates the hard problems ie controlling the low-level actions of the vehicle while avoiding collisions to a separate control", "further q-masking largely amounts to simply removing actions that are infeasible eg changing lanes to the left when in the left-most lane but is seems to be no more than a heuristic the advantages of which are not evalu", "the method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human perform", "in the runs that reach the goal the proposed method is about % faster than the simple baseline though it does not reach the goal every tim"], "labels": ["MIN", "MIN", "MIN", "MIN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["given the claim that not reaching the goal is considered a failure it isn't clear which performance is pref", "meanwhile the evaluation could be improved with the use of a better baseline eg using an existing planning framework such as a predictive rrt that plans to the go", "additional comments/questions* the description of the q-learning implementation is unclear", "how is the terminal time known a priori", "why are two buffers necessari"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the paper claims that the method permits training without any collisions even for real training runs strong claim however it isn't clear how this is guaranteed beyond the assumption that you have a low-level controller that can ensure collisions are avoid", "this is secondary to the proposed framework", "* the paper overstates the contributions of q-masking emphasizing improvements to data efficiency among oth", "the authors should validate these claims with an ablation study that compares performance with and without mask", "this would help to address the contribution of q-masking vs simply abstracting the action spac"], "labels": ["MAJ", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["* the network takes as input a m this is large occupancy grid representation of the local environ", "how sensitive is the network to errors in this model", "does the occupancy grid account for sensing limitations eg occlus", "the paper is clearly written with a good coverage of previous relevant literatur", "the contribution itself is slightly incremental as several different parameterization of orthogonal or almost-orthogonal weight matrices for rnn have been introduc"], "labels": ["MIN", "MIN", "MIN", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["therefore the paper must show that this new method performs better in some way compared with previous method", "they show that the proposed method is competitive on several datasets and a clear winner on one task mse on timit", "pros new relatively simple method for learning orthogonal weight matrices for rnn", "clearly written", "n quite good results on several relevant task"], "labels": ["MAJ", "MAJ", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons technical novelty is somewhat limit", "experiments do not evaluate run time memory use computational complexity or st", "therefore it is more difficult to make comparisons perhaps restricted-capacity urnn is  times faster than the proposed method", "this paper derived an upper bound on adversarial perturbation for neural networks with one hidden lay", "the upper bound is derived via  theorem of middle value  replace the middle value by the maximum eq   replace the maximum of the gradient value locally by the global maximal value eq   this leads to a non-convex quadratic program and then the authors did a convex relaxation similar to maxcut to upper bound the function by a sdp which then can be solved in polynomial tim"], "labels": ["MAJ", "MAJ", "MIN", "GEN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the main idea of  using upper bound as opposed to lower bound is reason", "however i find there are some limitations/weakness of the proposed method the method is likely not extendable to more complicated and more practical networks beyond the ones discussed in the paper ie with one hidden lay", "sdp while tractable would still require very expensive computation to solve exactli", "the relaxation seems a bit loose - in particular in above step  and  the authors replace the gradient value by a global upper bound on that which to me seems can be pretty loos", "in this paper the authors show how a deep learning model for sea surface temperature prediction can be designed to incorporate the classical advection diffusion model"], "labels": ["MAJ", "MAJ", "MIN", "MIN", "GEN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["the architecture includes a differentiable warping scheme which allows back propagation of the error and is inspired by the fundamental solution of the pde model", "they evaluate the suggested model on synthetic data and outperform the current state of the art in terms of accuraci", "pros- the paper is written in a clear and concise mann", "- it suggests an interesting connection between a traditional model and deep learning techniqu", "n- in the experiments they trained the network on  x  patches and achieved convincing result"], "labels": ["GEN", "GEN", "MAJ", "GEN", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["cons- please provide the value of the diffusion coefficient for the sake of reproduc", "- medium resolution of the resulting predict", "i enjoyed reading this paper and would like it to be accept", "minor comments- on page five in the last paragraph there is a left parenthesis missing in the inline formula nabla dot w_tx^", "- on page nine in the last paragraph there is the word 'flow' missing ' estimating the optical [!] between  [!] images'"], "labels": ["MIN", "MIN", "MAJ", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["- in the introduction page two the authors refer to sst prediction as a 'relatively complex physical modeling problem' whereas in the conclusion page ten it is referred to as 'a problem of intermediate complexity' this seems to be inconsist", "the authors proposed to compress word embeddings by approximate matrix factorization and to solve the problem with the gumbel-soft trick", "the proposed method achieved compression rate % in a sentiment analysis task and compression rate over % in machine translation tasks without a performance loss", "this paper is well-written and easy to follow", "the motivation is clear and the idea is simple and effect"], "labels": ["MIN", "GEN", "MAJ", "MAJ", "MAJ"], "confs": [1, 1, 1, 1, 1]}
{"abstract_id": 0, "sentences": ["nit would be better to provide deeper analysis in subsect", "the current analysis is too simpl", "it may be interesting to explain the meanings of individual compon", "does each component is related to a certain top", "is it meaningful to perform add or substract on the leaned cod"], "labels": ["MIN", "MIN", "MIN", "MIN", "MIN"], "confs": [1, 1, 1, 1, 1]}
