"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information.[[INT-NEU], [null], [SMY], [GEN]] The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem.[[PDI-POS], [IMP-POS], [APC], [MAJ]] The results in the paper are mostly qualitative and only on MNIST.[[RES-NEU], [null], [SMY], [GEN]] They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations[[RES-NEG], [SUB-NEG], [CRT], [MAJ]]. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior.[[TNF-NEU], [CLA-NEG], [CRT], [MAJ]] The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.[[TNF-NEU], [CLA-NEG], [CRT], [MAJ]]\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] It may also be interesting to consider class-specific representations that are more general than just the class label.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)[[RWK-NEU,BIB-NEU], [CMP-NEU], [DIS], [MAJ]]"