"The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \"standard\" form and then into their correct morphological form,\n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,\n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.[[INT-NEU], [null], [SMY], [GEN]]\n\n \n\n As an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. [[TNF-NEG], [PNF-NEG], [SUG], [MIN]]\n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.[[MET-NEU], [EMP], [DFT], [MAJ]]"