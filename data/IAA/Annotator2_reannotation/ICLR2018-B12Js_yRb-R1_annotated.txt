"\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count.[[INT-NEU], [null], [SMY], [GEN]]\n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals.[[PDI-NEU], [null], [SMY], [GEN]]\n\nClarity:\n- The paper is well written and clarity is good.[[OAL-POS], [CLA-POS], [APC], [MAJ]] Figure 2 & 3 helps the readers understand the core algorithm.[[MET-POS,TNF-POS], [PNF-POS], [APC], [MIN]]\n\nPros:\n- De-duplication modules of inter and intra object edges are interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- The proposed method improves the baseline by 5% on counting questions.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n- The proposed model is pretty hand-crafted.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016).[[RWK-NEG,MET-NEG], [SUB-NEG], [SUG], [MAJ]]\n- One major bottleneck of the model is that the proposals are not jointly finetuned.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] So if the proposals are missing a single object, this cannot really be counted.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is.[[MET-NEG], [SUB-NEG], [CRT], [MAJ]]\n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain.[[RWK-NEG], [EMP-NEG], [CRT], [MAJ]]\n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017).[[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MAJ]]\n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).[[MET-NEG], [SUB-NEG], [CRT], [MAJ]] This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance.[[MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL[[RWK-NEU,EXP-NEU,RES-NEU], [CMP-NEU], [SUG], [MAJ]]\\n\nConclusion:\n- I feel that the motivation is good,[[PDI-POS], [EMP-POS], [APC], [MAJ]] but the proposed model is too hand-crafted.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Also, key experiments are missing: 1) NMS baseline[[EXP-NEG], [SUB-NEG], [CRT], [MAJ]] 2) Comparison with VQA counting work  (Chattopadhyay et al., 2017).[[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MAJ]] Therefore I recommend reject.[[OAL-NEG], [REC-NEG], [FWK], [MAJ]]\n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.[[RWK-NEU,BIB-NEU], [SUB-NEU], [DIS], [MIN]]\n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016.[[RWK-NEU,BIB-NEU], [SUB-NEU], [DIS], [MIN]]\n\nUpdate:\nThank you for the rebuttal.[[EXT-NEU], [null], [DIS], [MIN]] The paper is revised and I saw NMS baseline is added.[[OAL-POS,RWK-POS], [SUB-POS], [APC], [MAJ]] I understood the reason not to compare with certain related work.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy.[[RES-POS], [REC-POS], [FBK], [MAJ]] However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]"