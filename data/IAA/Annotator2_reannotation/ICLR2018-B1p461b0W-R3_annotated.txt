"The problem the authors are tackling is extremely relevant to the research community.[[PDI-NEU], [null], [DIS], [GEN]] The paper is well written with considerable number of experiments.[[EXP-POS], [CLA-POS,SUB-POS], [APC], [MAJ]] While a few conclusions are made in the paper a few things are missing to make even broader conclusions.[[ANA-NEU], [EMP-NEU], [DIS], [MAJ]] I think adding those experiments will make the paper stronger![[EXP-POS], [IMP-NEU], [APC], [MAJ]] \n\n1. Annotation noise is one of the biggest bottleneck while collecting fully supervised datasets.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [MAJ]] This noise is mainly driven by lack of clear definitions for each concept (fine-grained, large label dictionary etc.).[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] It would be good to add such type of noise to the datasets and see how the networks perform.[[MET-NEU,ANA-NEU], [EMP-NEU], [SUG], [MAJ]]\n2. While it is interesting to see large capacity networks more resilient to noise I think the paper spends more effort and time on small datasets and smaller models even in the convolutional space.[[DAT-NEU,EXP-NEU,ANA-NEU], [CMP-NEU], [DIS], [MAJ]] It would be great to add more experiments on the state of the art residual networks and large datasets like ImageNet.[[DAT-NEU,EXP-NEU], [SUB-NEU], [SUG], [MAJ]]\n3. Because the analysis is very empirical and authors have a hypothesis that the batch size is effectively smaller when there are large batches with noisy examples it would be good to add some analysis on the gradients to throw more light and make it less empirical.[[DAT-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SUG], [MAJ]] Batch size and learning rate analysis was very informative but should be done on ResNets and larger datasets to make the paper strong and provide value to the research community.[[DAT-NEU,EXP-NEU], [SUB-NEU], [SUG], [MAJ]]\n\nOverall, with these key things missing the paper falls a bit short making it more suitable for a re submission with further experiments.[[OAL-NEG], [REC-NEG], [CRT], [MAJ]]"