"This paper proposes \"spectral normalization\" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function. [[INT-NEU], [null], [SMY], [GEN]]The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.[[MET-NEU,ANA-NEU], [null], [SMY], [GEN]] Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.[[RWK-NEU,DAT-NEU,EXP-NEU], [CMP-NEU], [SMY], [GEN]]\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.[[PDI-POS], [CLA-POS], [APC], [MAJ]] The experimental results seem solid and seem to support the authors' claims.[[RES-POS], [EMP-POS], [APC], [MAJ]] I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] Like the anonymous commenter, I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper.[[RWK-NEU,MET], [CMP-NEU], [DIS], [MAJ]]\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\nSmall Nits: \n\nSection 4: \"In order to evaluate the efficacy of our experiment\": I think you mean \"approach\".[[ANA-NEU], [CLA-NEU], [DIS], [MIN]]\n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \"As we prophesied ...\", and in the paragraph below \n * \"... is a tad slower ...\".[[OAL-POS], [CLA-NEU], [DIS], [MIN]]"